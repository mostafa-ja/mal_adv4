{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXV9tAQnclByVyUEbmq3nO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/mal_adv4/blob/main/RBF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "download_links = ['https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_0.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_1.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_2.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y0.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y1.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y2.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_DNN_drebin_best.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM_weightedLoss.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/insertion_array.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/removal_array.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/adverserial_attacks_functions.py'\n",
        "]"
      ],
      "metadata": {
        "id": "1IW4pHac9VLq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "output_filepath = '/content/'\n",
        "for link in download_links:\n",
        "  gdown.download(link, output_filepath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kzSbjaXGVeG",
        "outputId": "7578df50-487b-4d24-9bea-670c171731c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_0.npz\n",
            "To: /content/sparse_matrix_0.npz\n",
            "100%|██████████| 461k/461k [00:00<00:00, 12.5MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_1.npz\n",
            "To: /content/sparse_matrix_1.npz\n",
            "100%|██████████| 148k/148k [00:00<00:00, 5.89MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_2.npz\n",
            "To: /content/sparse_matrix_2.npz\n",
            "100%|██████████| 150k/150k [00:00<00:00, 5.32MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y0.npz\n",
            "To: /content/sparse_matrix_y0.npz\n",
            "100%|██████████| 5.79k/5.79k [00:00<00:00, 11.9MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y1.npz\n",
            "To: /content/sparse_matrix_y1.npz\n",
            "100%|██████████| 2.64k/2.64k [00:00<00:00, 7.50MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y2.npz\n",
            "To: /content/sparse_matrix_y2.npz\n",
            "100%|██████████| 2.71k/2.71k [00:00<00:00, 6.16MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_DNN_drebin_best.pth\n",
            "To: /content/model_DNN_drebin_best.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 58.2MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM_weightedLoss.pth\n",
            "To: /content/model_AT_rFGSM_weightedLoss.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 64.8MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM.pth\n",
            "To: /content/model_AT_rFGSM.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 60.4MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/insertion_array.pkl\n",
            "To: /content/insertion_array.pkl\n",
            "100%|██████████| 80.2k/80.2k [00:00<00:00, 2.44MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/removal_array.pkl\n",
            "To: /content/removal_array.pkl\n",
            "100%|██████████| 80.2k/80.2k [00:00<00:00, 2.17MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/adverserial_attacks_functions.py\n",
            "To: /content/adverserial_attacks_functions.py\n",
            "67.1kB [00:00, 18.8MB/s]                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,balanced_accuracy_score\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "from adverserial_attacks_functions import *\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "JKDdI3K9LrlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd0d5cf-195e-44a6-f4e8-6db6f98eb819"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78b0a366d810>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the .pkl file\n",
        "with open('/content/insertion_array.pkl', 'rb') as f:\n",
        "    # Load the object\n",
        "    insertion_array = pickle.load(f)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n",
        "\n",
        "insertion_array = torch.tensor(insertion_array).to(device)\n",
        "print(len(insertion_array))\n",
        "\n",
        "# Open the .pkl file\n",
        "with open('/content/removal_array.pkl', 'rb') as f:\n",
        "    # Load the object\n",
        "    removal_array = pickle.load(f)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n",
        "\n",
        "removal_array = torch.tensor(removal_array).to(device)\n",
        "print(len(removal_array))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXV0WIjsJG_F",
        "outputId": "e8f8a8ec-d547-4200-eae4-59cbd741229e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset\n",
        "X_train = sparse.load_npz(\"/content/sparse_matrix_0.npz\").toarray()\n",
        "X_val = sparse.load_npz(\"/content/sparse_matrix_1.npz\").toarray()\n",
        "X_test = sparse.load_npz(\"/content/sparse_matrix_2.npz\").toarray()\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.int8)\n",
        "X_val = torch.tensor(X_val, dtype=torch.int8)\n",
        "X_test = torch.tensor(X_test, dtype=torch.int8)\n",
        "\n",
        "\n",
        "y_train = sparse.load_npz(\"/content/sparse_matrix_y0.npz\").toarray().reshape((-1, 1))\n",
        "y_val = sparse.load_npz(\"/content/sparse_matrix_y1.npz\").toarray().reshape((-1, 1))\n",
        "y_test = sparse.load_npz(\"/content/sparse_matrix_y2.npz\").toarray().reshape((-1, 1))\n",
        "\n",
        "y_train = torch.tensor(y_train, dtype=torch.int8)\n",
        "y_val = torch.tensor(y_val, dtype=torch.int8)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int8)\n",
        "\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"x_train:\", X_train.shape)\n",
        "print(\"x_val:\", X_val.shape)\n",
        "print(\"x_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_val:\", y_val.shape)\n",
        "print(\"y_test:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5blmEg4h-GKy",
        "outputId": "851bcb3b-5d1b-457c-ffdf-49d8371a1c8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "x_train: torch.Size([28683, 10000])\n",
            "x_val: torch.Size([9562, 10000])\n",
            "x_test: torch.Size([9562, 10000])\n",
            "y_train: torch.Size([28683, 1])\n",
            "y_val: torch.Size([9562, 1])\n",
            "y_test: torch.Size([9562, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of benigns and malicious sample in training dataset\n",
        "n_ben = (y_train.squeeze()== 0).sum().item()\n",
        "n_mal = (y_train.squeeze()== 1).sum().item()\n",
        "print('the proportion of malwares : ', n_mal/(n_mal+n_ben))\n",
        "\n",
        "# Combine features and labels into datasets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Define the DataLoader for training, validation, and test sets\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del train_dataset, val_dataset, test_dataset, y_train, y_val, y_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81AZSXOV-HoW",
        "outputId": "1e1ce4da-3a53-43c5-cea6-189fd9e5cabe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the proportion of malwares :  0.11386535578565701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_DNN = MalwareDetectionModel().to(device)\n",
        "# Load model parameters\n",
        "model_DNN.load_state_dict(torch.load('model_DNN_drebin_best.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "id": "0MavlKAt6mb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbfce58f-dcf2-487e-8d24-a0b5cda1092c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your model\n",
        "model_AT_rFGSM = MalwareDetectionModel().to(device)\n",
        "\n",
        "# Load model parameters\n",
        "model_AT_rFGSM.load_state_dict(torch.load('model_AT_rFGSM.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE8WMAUgSCms",
        "outputId": "a958cc08-6dbc-47df-b191-17c6e22af7e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your model\n",
        "model_AT_rFGSM_weightedLoss = MalwareDetectionModel().to(device)\n",
        "\n",
        "# Load model parameters\n",
        "model_AT_rFGSM_weightedLoss.load_state_dict(torch.load('model_AT_rFGSM_weightedLoss.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGs5E9_2SDbJ",
        "outputId": "0433198d-fa8b-4eef-98a9-0d5df80d7b09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Function to initialize centers and sigmas separately for benign and malware samples using KMeans clustering\n",
        "def initialize_centers_sigmas_separate(data_loader, num_centers_per_class, removal_array = removal_array, non_removal_features = False):\n",
        "    centers_benign = []\n",
        "    centers_malware = []\n",
        "    non_removal_mask = torch.logical_not(removal_array).to('cpu')\n",
        "\n",
        "    # Collect data into a single tensor\n",
        "    all_data = torch.cat([batch for batch, _ in data_loader], dim=0)\n",
        "    if non_removal_features:\n",
        "        all_data = all_data[:,non_removal_mask]\n",
        "\n",
        "    all_data = all_data.numpy()\n",
        "    # Collect labels into a single tensor\n",
        "    all_labels = torch.cat([labels for _, labels in data_loader], dim=0)\n",
        "    all_labels = all_labels.numpy()\n",
        "\n",
        "    # Separate benign and malware samples\n",
        "    benign_indices = np.where(all_labels == 0)[0]\n",
        "    malware_indices = np.where(all_labels == 1)[0]\n",
        "\n",
        "    # Clustering for benign samples\n",
        "    benigns = all_data[benign_indices]\n",
        "    if non_removal_features:\n",
        "        subset_benigns = benigns\n",
        "    else:\n",
        "        subset_benigns = benigns[:20000]\n",
        "\n",
        "    kmeans_benign = KMeans(n_clusters=num_centers_per_class, init='k-means++', n_init='auto')\n",
        "    kmeans_benign.fit(subset_benigns)\n",
        "    centers_selected_benign = kmeans_benign.cluster_centers_\n",
        "\n",
        "    # Clustering for malware samples\n",
        "    kmeans_malware = KMeans(n_clusters=num_centers_per_class, init='k-means++', n_init='auto')\n",
        "    kmeans_malware.fit(all_data[malware_indices])\n",
        "    centers_selected_malware = kmeans_malware.cluster_centers_\n",
        "\n",
        "    # Combine selected centers\n",
        "    all_centers = np.concatenate([centers_selected_benign, centers_selected_malware], axis=0)\n",
        "\n",
        "    return all_centers"
      ],
      "metadata": {
        "id": "vh2VQ93Od2vL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_centers = initialize_centers_sigmas_separate(train_loader, 500, removal_array = removal_array, non_removal_features = False)"
      ],
      "metadata": {
        "id": "AQEr5VdJ4Upt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_centers.shape"
      ],
      "metadata": {
        "id": "1Uz_cJ926sv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the array to a file\n",
        "np.save('all_centers.npy', all_centers)"
      ],
      "metadata": {
        "id": "oSnXMB926oVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace negative values with zero\n",
        "all_centers = np.maximum(all_centers, 0)\n",
        "all_centers[0][:30]"
      ],
      "metadata": {
        "id": "w9WiyEJMk8bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_centers_non_removal_features, sigma_non_removal_features = initialize_centers_sigmas_separate(train_loader, 500, removal_array = removal_array, non_removal_features = True)"
      ],
      "metadata": {
        "id": "V6p9knzUXngT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_centers_non_removal_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eshb64sLZ7nr",
        "outputId": "8dd03ae3-36ee-4d41-a08e-6bb6802d5bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 1144)\n",
            "3.967844595954441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_centers_non_removal_features[0][:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_zhMZhOaAyZ",
        "outputId": "4023e63d-70f9-4ac5-b67e-e4c1d89cede1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4.76190476e-02, -5.55111512e-17,  4.76190476e-02, -4.16333634e-17,\n",
              "        4.76190476e-02,  0.00000000e+00, -1.38777878e-17, -6.93889390e-18,\n",
              "        9.52380952e-02, -5.55111512e-17,  4.76190476e-02, -2.77555756e-17,\n",
              "       -1.38777878e-17,  8.09523810e-01, -6.93889390e-18,  0.00000000e+00,\n",
              "        0.00000000e+00,  2.08166817e-17,  1.42857143e-01,  1.04083409e-17])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the array to a file\n",
        "np.save('all_centers_non_removal_features.npy', all_centers_non_removal_features)"
      ],
      "metadata": {
        "id": "4p_gIe72abKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the array from the file\n",
        "all_centers = np.load('all_centers.npy')"
      ],
      "metadata": {
        "id": "Ss6Qdd-H6otO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move all_centers back to GPU\n",
        "all_centers = torch.tensor(all_centers, device=device)"
      ],
      "metadata": {
        "id": "5llZpwJP3ZsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_centers_non_removal_features = torch.tensor(all_centers_non_removal_features, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ozBK-hajKm",
        "outputId": "d517c2e3-0085-47b3-f835-2308ba26fcee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-b585ae409818>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  all_centers_non_removal_features = torch.tensor(all_centers_non_removal_features, device=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define the RBF model\n",
        "class RBFModel(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, init_centers, init_sigmas, kernel):\n",
        "        super(RBFModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.centers = nn.Parameter(torch.Tensor(init_centers))\n",
        "        self.sigmas = nn.Parameter(torch.Tensor(init_sigmas))\n",
        "        self.kernel = kernel\n",
        "        # Linear layer for output\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def gaussian(self, x, c, sigma):\n",
        "        return torch.exp(-torch.sum((x[:, None, :] - c) ** 2, dim=-1) / (2 * sigma ** 2))\n",
        "\n",
        "    def laplacian(self, x, c, sigma):\n",
        "        return torch.exp(-torch.sum(torch.abs(x[:, None, :] - c) , dim=-1) / sigma)\n",
        "\n",
        "    def forward(self, x):\n",
        "      if self.kernel == 'gaussian':\n",
        "        radial_out = self.gaussian(x, self.centers, self.sigmas)\n",
        "      elif self.kernel == 'laplacian':\n",
        "        radial_out = self.laplacian(x, self.centers, self.sigmas)\n",
        "      else:\n",
        "        raise ValueError(\"Invalid kernel type. Choose 'gaussian' or 'laplacian'.\")\n",
        "\n",
        "      output = self.linear(radial_out.to(torch.float32))\n",
        "      return output\n",
        "\n",
        "# Function to evaluate the model on a dataset\n",
        "def evaluate_model(model, data_loader, criterion, removal_array = removal_array, non_removal_features = False):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    non_removal_mask = torch.logical_not(removal_array).to('cpu')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in data_loader:\n",
        "            if non_removal_features:\n",
        "                batch_x, batch_y = batch_x[:,non_removal_mask].to(torch.float32).to(device), batch_y.to(device)  # Move data to GPU\n",
        "            else:\n",
        "                batch_x, batch_y = batch_x.to(torch.float32).to(device), batch_y.to(device)  # Move data to GPU\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y.squeeze().to(torch.long))\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct_predictions += (predicted == batch_y.squeeze()).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = correct_predictions / len(data_loader.dataset)\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "WHAI-VGJSGa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to release GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "e1cutWXY3eza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_removal_features = False\n",
        "non_removal_mask = torch.logical_not(removal_array).to('cpu')\n",
        "sigma = 20.\n",
        "\n",
        "model = RBFModel(1000, 2, all_centers, [sigma])\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)  # Decrease lr by a factor of 0.95 every 1 epoch\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "best_val_accuracy = 0.0\n",
        "best_model_state_dict = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # Print the current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print('************************************')\n",
        "    print(f'Current Learning Rate: {current_lr}')\n",
        "\n",
        "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
        "        # Remove features based on removal_array\n",
        "        if non_removal_features:\n",
        "            batch_x = batch_x[:, non_removal_mask].to(torch.float32).to(device)  # Select features\n",
        "        else:\n",
        "            batch_x = batch_x.to(torch.float32).to(device)  # Move all features to GPU\n",
        "        batch_y = batch_y.to(device)  # Move labels to GPU\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_x)\n",
        "        loss = criterion(output, batch_y.squeeze().to(torch.long))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        correct_predictions += (predicted == batch_y.squeeze()).sum().item()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct_predictions / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss}, Train Accuracy: {accuracy}')\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, removal_array = removal_array, non_removal_features=non_removal_features)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "    # Check if the current model has the best validation accuracy\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        print(f\"New best validation accuracy: {best_val_accuracy}\")\n",
        "        # Save the current best model state dict\n",
        "        best_model_state_dict = model.state_dict()\n",
        "\n",
        "# Save the best model\n",
        "if best_model_state_dict is not None:\n",
        "    torch.save(best_model_state_dict, 'best_model3.pth')\n",
        "    print(\"Best model saved with validation accuracy:\", best_val_accuracy)\n",
        "else:\n",
        "    print(\"No improvement in validation accuracy.\")\n"
      ],
      "metadata": {
        "id": "bZCQWg8qlXPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_removal_features = True\n",
        "non_removal_mask = torch.logical_not(removal_array).to('cpu')\n",
        "\n",
        "model = RBFModel(1000, 2, all_centers_non_removal_features, [sigma_non_removal_features])\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)  # Decrease lr by a factor of 0.95 every 1 epoch\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "best_val_accuracy = 0.0\n",
        "best_model_state_dict = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # Print the current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print('************************************')\n",
        "    print(f'Current Learning Rate: {current_lr}')\n",
        "\n",
        "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
        "        # Remove features based on removal_array\n",
        "        if non_removal_features:\n",
        "            batch_x = batch_x[:, non_removal_mask].to(torch.float32).to(device)  # Select features\n",
        "        else:\n",
        "            batch_x = batch_x.to(torch.float32).to(device)  # Move all features to GPU\n",
        "        batch_y = batch_y.to(device)  # Move labels to GPU\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_x)\n",
        "        loss = criterion(output, batch_y.squeeze().to(torch.long))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        correct_predictions += (predicted == batch_y.squeeze()).sum().item()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct_predictions / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss}, Train Accuracy: {accuracy}')\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, removal_array = removal_array, non_removal_features=non_removal_features)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "    # Check if the current model has the best validation accuracy\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        print(f\"New best validation accuracy: {best_val_accuracy}\")\n",
        "        # Save the current best model state dict\n",
        "        best_model_state_dict = model.state_dict()\n",
        "\n",
        "# Save the best model\n",
        "if best_model_state_dict is not None:\n",
        "    torch.save(best_model_state_dict, 'best_model3.pth')\n",
        "    print(\"Best model saved with validation accuracy:\", best_val_accuracy)\n",
        "else:\n",
        "    print(\"No improvement in validation accuracy.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbyx6gEWdmyq",
        "outputId": "02f1ccc7-abd6-4afd-ef62-210cf5bf594d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************************\n",
            "Current Learning Rate: 0.0005\n",
            "Epoch [1/30], Step [10/113], Loss: 0.07187538594007492\n",
            "Epoch [1/30], Step [20/113], Loss: 0.09854941815137863\n",
            "Epoch [1/30], Step [30/113], Loss: 0.04180261120200157\n",
            "Epoch [1/30], Step [40/113], Loss: 0.058634303510189056\n",
            "Epoch [1/30], Step [50/113], Loss: 0.09666981548070908\n",
            "Epoch [1/30], Step [60/113], Loss: 0.07460201531648636\n",
            "Epoch [1/30], Step [70/113], Loss: 0.13678571581840515\n",
            "Epoch [1/30], Step [80/113], Loss: 0.10045940428972244\n",
            "Epoch [1/30], Step [90/113], Loss: 0.1356290876865387\n",
            "Epoch [1/30], Step [100/113], Loss: 0.13436344265937805\n",
            "Epoch [1/30], Step [110/113], Loss: 0.08568780869245529\n",
            "Epoch [1/30], Train Loss: 0.07825522694275179, Train Accuracy: 0.9724226893979012\n",
            "Epoch [1/30], Validation Loss: 0.07745550651299327, Validation Accuracy: 0.9741685839782472\n",
            "New best validation accuracy: 0.9741685839782472\n",
            "************************************\n",
            "Current Learning Rate: 0.000475\n",
            "Epoch [2/30], Step [10/113], Loss: 0.11234556138515472\n",
            "Epoch [2/30], Step [20/113], Loss: 0.05829741060733795\n",
            "Epoch [2/30], Step [30/113], Loss: 0.056680768728256226\n",
            "Epoch [2/30], Step [40/113], Loss: 0.05257630720734596\n",
            "Epoch [2/30], Step [50/113], Loss: 0.12208449095487595\n",
            "Epoch [2/30], Step [60/113], Loss: 0.0996771976351738\n",
            "Epoch [2/30], Step [70/113], Loss: 0.05868631601333618\n",
            "Epoch [2/30], Step [80/113], Loss: 0.04106824845075607\n",
            "Epoch [2/30], Step [90/113], Loss: 0.0849347785115242\n",
            "Epoch [2/30], Step [100/113], Loss: 0.06164516508579254\n",
            "Epoch [2/30], Step [110/113], Loss: 0.07002674043178558\n",
            "Epoch [2/30], Train Loss: 0.07779044067833275, Train Accuracy: 0.9723180978279817\n",
            "Epoch [2/30], Validation Loss: 0.07809401146675411, Validation Accuracy: 0.9740640033465802\n",
            "************************************\n",
            "Current Learning Rate: 0.00045125\n",
            "Epoch [3/30], Step [10/113], Loss: 0.05022071674466133\n",
            "Epoch [3/30], Step [20/113], Loss: 0.08287765085697174\n",
            "Epoch [3/30], Step [30/113], Loss: 0.0889655277132988\n",
            "Epoch [3/30], Step [40/113], Loss: 0.07617849856615067\n",
            "Epoch [3/30], Step [50/113], Loss: 0.06501622498035431\n",
            "Epoch [3/30], Step [60/113], Loss: 0.07968007028102875\n",
            "Epoch [3/30], Step [70/113], Loss: 0.08938738703727722\n",
            "Epoch [3/30], Step [80/113], Loss: 0.05089860409498215\n",
            "Epoch [3/30], Step [90/113], Loss: 0.04809034615755081\n",
            "Epoch [3/30], Step [100/113], Loss: 0.05621154233813286\n",
            "Epoch [3/30], Step [110/113], Loss: 0.11312980204820633\n",
            "Epoch [3/30], Train Loss: 0.07730071132887253, Train Accuracy: 0.9727364641076596\n",
            "Epoch [3/30], Validation Loss: 0.07768472715428001, Validation Accuracy: 0.9732273582932441\n",
            "************************************\n",
            "Current Learning Rate: 0.0004286875\n",
            "Epoch [4/30], Step [10/113], Loss: 0.10972265154123306\n",
            "Epoch [4/30], Step [20/113], Loss: 0.0771891251206398\n",
            "Epoch [4/30], Step [30/113], Loss: 0.07219039648771286\n",
            "Epoch [4/30], Step [40/113], Loss: 0.06282317638397217\n",
            "Epoch [4/30], Step [50/113], Loss: 0.09941751509904861\n",
            "Epoch [4/30], Step [60/113], Loss: 0.10020700842142105\n",
            "Epoch [4/30], Step [70/113], Loss: 0.07048963010311127\n",
            "Epoch [4/30], Step [80/113], Loss: 0.08089987933635712\n",
            "Epoch [4/30], Step [90/113], Loss: 0.06925437599420547\n",
            "Epoch [4/30], Step [100/113], Loss: 0.09941525757312775\n",
            "Epoch [4/30], Step [110/113], Loss: 0.07902515679597855\n",
            "Epoch [4/30], Train Loss: 0.07697172168708216, Train Accuracy: 0.9730851026740578\n",
            "Epoch [4/30], Validation Loss: 0.07707307850451846, Validation Accuracy: 0.9743777452415813\n",
            "New best validation accuracy: 0.9743777452415813\n",
            "************************************\n",
            "Current Learning Rate: 0.00040725312499999993\n",
            "Epoch [5/30], Step [10/113], Loss: 0.0378507524728775\n",
            "Epoch [5/30], Step [20/113], Loss: 0.10885113477706909\n",
            "Epoch [5/30], Step [30/113], Loss: 0.07601484656333923\n",
            "Epoch [5/30], Step [40/113], Loss: 0.07548623532056808\n",
            "Epoch [5/30], Step [50/113], Loss: 0.07710321247577667\n",
            "Epoch [5/30], Step [60/113], Loss: 0.08027280122041702\n",
            "Epoch [5/30], Step [70/113], Loss: 0.10630694776773453\n",
            "Epoch [5/30], Step [80/113], Loss: 0.07164627313613892\n",
            "Epoch [5/30], Step [90/113], Loss: 0.046285826712846756\n",
            "Epoch [5/30], Step [100/113], Loss: 0.1287090927362442\n",
            "Epoch [5/30], Step [110/113], Loss: 0.047367069870233536\n",
            "Epoch [5/30], Train Loss: 0.07774059622820499, Train Accuracy: 0.9726318725377401\n",
            "Epoch [5/30], Validation Loss: 0.07842454186787731, Validation Accuracy: 0.9738548420832461\n",
            "************************************\n",
            "Current Learning Rate: 0.0003868904687499999\n",
            "Epoch [6/30], Step [10/113], Loss: 0.06108412519097328\n",
            "Epoch [6/30], Step [20/113], Loss: 0.15349289774894714\n",
            "Epoch [6/30], Step [30/113], Loss: 0.08851087093353271\n",
            "Epoch [6/30], Step [40/113], Loss: 0.07586811482906342\n",
            "Epoch [6/30], Step [50/113], Loss: 0.10086224228143692\n",
            "Epoch [6/30], Step [60/113], Loss: 0.08580663055181503\n",
            "Epoch [6/30], Step [70/113], Loss: 0.042638830840587616\n",
            "Epoch [6/30], Step [80/113], Loss: 0.09476399421691895\n",
            "Epoch [6/30], Step [90/113], Loss: 0.12628160417079926\n",
            "Epoch [6/30], Step [100/113], Loss: 0.07581839710474014\n",
            "Epoch [6/30], Step [110/113], Loss: 0.07328587770462036\n",
            "Epoch [6/30], Train Loss: 0.08092771106259486, Train Accuracy: 0.9730153749607782\n",
            "Epoch [6/30], Validation Loss: 0.0769496522353668, Validation Accuracy: 0.9743777452415813\n",
            "************************************\n",
            "Current Learning Rate: 0.0003675459453124999\n",
            "Epoch [7/30], Step [10/113], Loss: 0.04257126525044441\n",
            "Epoch [7/30], Step [20/113], Loss: 0.12868718802928925\n",
            "Epoch [7/30], Step [30/113], Loss: 0.07947181165218353\n",
            "Epoch [7/30], Step [40/113], Loss: 0.06433039903640747\n",
            "Epoch [7/30], Step [50/113], Loss: 0.11596110463142395\n",
            "Epoch [7/30], Step [60/113], Loss: 0.07350758463144302\n",
            "Epoch [7/30], Step [70/113], Loss: 0.07920385897159576\n",
            "Epoch [7/30], Step [80/113], Loss: 0.0665273517370224\n",
            "Epoch [7/30], Step [90/113], Loss: 0.07537170499563217\n",
            "Epoch [7/30], Step [100/113], Loss: 0.07328938692808151\n",
            "Epoch [7/30], Step [110/113], Loss: 0.05415751039981842\n",
            "Epoch [7/30], Train Loss: 0.08120770742539811, Train Accuracy: 0.9727713279642994\n",
            "Epoch [7/30], Validation Loss: 0.07710836215042755, Validation Accuracy: 0.9735411001882451\n",
            "************************************\n",
            "Current Learning Rate: 0.00034916864804687486\n",
            "Epoch [8/30], Step [10/113], Loss: 0.044496484100818634\n",
            "Epoch [8/30], Step [20/113], Loss: 0.058095429092645645\n",
            "Epoch [8/30], Step [30/113], Loss: 0.09018661081790924\n",
            "Epoch [8/30], Step [40/113], Loss: 0.04507625848054886\n",
            "Epoch [8/30], Step [50/113], Loss: 0.05992037057876587\n",
            "Epoch [8/30], Step [60/113], Loss: 0.08022136986255646\n",
            "Epoch [8/30], Step [70/113], Loss: 0.11141017824411392\n",
            "Epoch [8/30], Step [80/113], Loss: 0.06026001647114754\n",
            "Epoch [8/30], Step [90/113], Loss: 0.10076066106557846\n",
            "Epoch [8/30], Step [100/113], Loss: 0.10341861844062805\n",
            "Epoch [8/30], Step [110/113], Loss: 0.056852445006370544\n",
            "Epoch [8/30], Train Loss: 0.07746323926654537, Train Accuracy: 0.9736777882369347\n",
            "Epoch [8/30], Validation Loss: 0.0770704535963504, Validation Accuracy: 0.9739594227149132\n",
            "************************************\n",
            "Current Learning Rate: 0.0003317102156445311\n",
            "Epoch [9/30], Step [10/113], Loss: 0.1569991111755371\n",
            "Epoch [9/30], Step [20/113], Loss: 0.06825050711631775\n",
            "Epoch [9/30], Step [30/113], Loss: 0.052882611751556396\n",
            "Epoch [9/30], Step [40/113], Loss: 0.05529714748263359\n",
            "Epoch [9/30], Step [50/113], Loss: 0.07369682192802429\n",
            "Epoch [9/30], Step [60/113], Loss: 0.04528920724987984\n",
            "Epoch [9/30], Step [70/113], Loss: 0.048541855067014694\n",
            "Epoch [9/30], Step [80/113], Loss: 0.04814869165420532\n",
            "Epoch [9/30], Step [90/113], Loss: 0.06914970278739929\n",
            "Epoch [9/30], Step [100/113], Loss: 0.06564437597990036\n",
            "Epoch [9/30], Step [110/113], Loss: 0.09703268110752106\n",
            "Epoch [9/30], Train Loss: 0.07690991292320258, Train Accuracy: 0.9730851026740578\n",
            "Epoch [9/30], Validation Loss: 0.07683840644006666, Validation Accuracy: 0.9742731646099142\n",
            "************************************\n",
            "Current Learning Rate: 0.0003151247048623045\n",
            "Epoch [10/30], Step [10/113], Loss: 0.0287388414144516\n",
            "Epoch [10/30], Step [20/113], Loss: 0.0609205886721611\n",
            "Epoch [10/30], Step [30/113], Loss: 0.050048038363456726\n",
            "Epoch [10/30], Step [40/113], Loss: 0.0700136125087738\n",
            "Epoch [10/30], Step [50/113], Loss: 0.07477733492851257\n",
            "Epoch [10/30], Step [60/113], Loss: 0.070177361369133\n",
            "Epoch [10/30], Step [70/113], Loss: 0.10614962875843048\n",
            "Epoch [10/30], Step [80/113], Loss: 0.1130187064409256\n",
            "Epoch [10/30], Step [90/113], Loss: 0.03673391044139862\n",
            "Epoch [10/30], Step [100/113], Loss: 0.0738421157002449\n",
            "Epoch [10/30], Step [110/113], Loss: 0.08837756514549255\n",
            "Epoch [10/30], Train Loss: 0.07684877399454075, Train Accuracy: 0.9729107833908587\n",
            "Epoch [10/30], Validation Loss: 0.07665193575973574, Validation Accuracy: 0.9750052290315834\n",
            "New best validation accuracy: 0.9750052290315834\n",
            "************************************\n",
            "Current Learning Rate: 0.00029936846961918924\n",
            "Epoch [11/30], Step [10/113], Loss: 0.05732974782586098\n",
            "Epoch [11/30], Step [20/113], Loss: 0.06544657796621323\n",
            "Epoch [11/30], Step [30/113], Loss: 0.0992717444896698\n",
            "Epoch [11/30], Step [40/113], Loss: 0.05755922198295593\n",
            "Epoch [11/30], Step [50/113], Loss: 0.041846323758363724\n",
            "Epoch [11/30], Step [60/113], Loss: 0.11369239538908005\n",
            "Epoch [11/30], Step [70/113], Loss: 0.11837591230869293\n",
            "Epoch [11/30], Step [80/113], Loss: 0.08732005208730698\n",
            "Epoch [11/30], Step [90/113], Loss: 0.11803213506937027\n",
            "Epoch [11/30], Step [100/113], Loss: 0.04161931574344635\n",
            "Epoch [11/30], Step [110/113], Loss: 0.08202110230922699\n",
            "Epoch [11/30], Train Loss: 0.08070185604150844, Train Accuracy: 0.9735383328103755\n",
            "Epoch [11/30], Validation Loss: 0.07659885032396567, Validation Accuracy: 0.9743777452415813\n",
            "************************************\n",
            "Current Learning Rate: 0.00028440004613822977\n",
            "Epoch [12/30], Step [10/113], Loss: 0.10946014523506165\n",
            "Epoch [12/30], Step [20/113], Loss: 0.12875685095787048\n",
            "Epoch [12/30], Step [30/113], Loss: 0.07095449417829514\n",
            "Epoch [12/30], Step [40/113], Loss: 0.08544616401195526\n",
            "Epoch [12/30], Step [50/113], Loss: 0.10185297578573227\n",
            "Epoch [12/30], Step [60/113], Loss: 0.04217613488435745\n",
            "Epoch [12/30], Step [70/113], Loss: 0.06902526319026947\n",
            "Epoch [12/30], Step [80/113], Loss: 0.07432528585195541\n",
            "Epoch [12/30], Step [90/113], Loss: 0.09568622708320618\n",
            "Epoch [12/30], Step [100/113], Loss: 0.07399776577949524\n",
            "Epoch [12/30], Step [110/113], Loss: 0.06810597330331802\n",
            "Epoch [12/30], Train Loss: 0.07613082125420327, Train Accuracy: 0.9731548303873374\n",
            "Epoch [12/30], Validation Loss: 0.0781332948490193, Validation Accuracy: 0.9740640033465802\n",
            "************************************\n",
            "Current Learning Rate: 0.00027018004383131826\n",
            "Epoch [13/30], Step [10/113], Loss: 0.05937325954437256\n",
            "Epoch [13/30], Step [20/113], Loss: 0.050889793783426285\n",
            "Epoch [13/30], Step [30/113], Loss: 0.08060960471630096\n",
            "Epoch [13/30], Step [40/113], Loss: 0.0970921441912651\n",
            "Epoch [13/30], Step [50/113], Loss: 0.0746452808380127\n",
            "Epoch [13/30], Step [60/113], Loss: 0.053794968873262405\n",
            "Epoch [13/30], Step [70/113], Loss: 0.06571891903877258\n",
            "Epoch [13/30], Step [80/113], Loss: 0.07331938296556473\n",
            "Epoch [13/30], Step [90/113], Loss: 0.07073341310024261\n",
            "Epoch [13/30], Step [100/113], Loss: 0.07049910724163055\n",
            "Epoch [13/30], Step [110/113], Loss: 0.12894189357757568\n",
            "Epoch [13/30], Train Loss: 0.07642673908741074, Train Accuracy: 0.9732594219572569\n",
            "Epoch [13/30], Validation Loss: 0.07651717078528907, Validation Accuracy: 0.9751098096632503\n",
            "New best validation accuracy: 0.9751098096632503\n",
            "************************************\n",
            "Current Learning Rate: 0.00025667104163975234\n",
            "Epoch [14/30], Step [10/113], Loss: 0.044221725314855576\n",
            "Epoch [14/30], Step [20/113], Loss: 0.10790756344795227\n",
            "Epoch [14/30], Step [30/113], Loss: 0.10389183461666107\n",
            "Epoch [14/30], Step [40/113], Loss: 0.07536884397268295\n",
            "Epoch [14/30], Step [50/113], Loss: 0.0644155889749527\n",
            "Epoch [14/30], Step [60/113], Loss: 0.11958843469619751\n",
            "Epoch [14/30], Step [70/113], Loss: 0.1093892827630043\n",
            "Epoch [14/30], Step [80/113], Loss: 0.08471222966909409\n",
            "Epoch [14/30], Step [90/113], Loss: 0.10779233276844025\n",
            "Epoch [14/30], Step [100/113], Loss: 0.07979453355073929\n",
            "Epoch [14/30], Step [110/113], Loss: 0.0726785883307457\n",
            "Epoch [14/30], Train Loss: 0.07660673651020083, Train Accuracy: 0.9735731966670153\n",
            "Epoch [14/30], Validation Loss: 0.07645547468411296, Validation Accuracy: 0.9752143902949174\n",
            "New best validation accuracy: 0.9752143902949174\n",
            "************************************\n",
            "Current Learning Rate: 0.00024383748955776472\n",
            "Epoch [15/30], Step [10/113], Loss: 0.08408922702074051\n",
            "Epoch [15/30], Step [20/113], Loss: 0.07436475902795792\n",
            "Epoch [15/30], Step [30/113], Loss: 0.05950674042105675\n",
            "Epoch [15/30], Step [40/113], Loss: 0.0553317628800869\n",
            "Epoch [15/30], Step [50/113], Loss: 0.057456862181425095\n",
            "Epoch [15/30], Step [60/113], Loss: 0.07546988129615784\n",
            "Epoch [15/30], Step [70/113], Loss: 0.11007674783468246\n",
            "Epoch [15/30], Step [80/113], Loss: 0.06846079230308533\n",
            "Epoch [15/30], Step [90/113], Loss: 0.036879949271678925\n",
            "Epoch [15/30], Step [100/113], Loss: 0.04079762101173401\n",
            "Epoch [15/30], Step [110/113], Loss: 0.07694476842880249\n",
            "Epoch [15/30], Train Loss: 0.07629273317556466, Train Accuracy: 0.9733640135271764\n",
            "Epoch [15/30], Validation Loss: 0.07637203327919308, Validation Accuracy: 0.9745869065049153\n",
            "************************************\n",
            "Current Learning Rate: 0.00023164561507987649\n",
            "Epoch [16/30], Step [10/113], Loss: 0.07494638860225677\n",
            "Epoch [16/30], Step [20/113], Loss: 0.05821038782596588\n",
            "Epoch [16/30], Step [30/113], Loss: 0.040334779769182205\n",
            "Epoch [16/30], Step [40/113], Loss: 0.08052600920200348\n",
            "Epoch [16/30], Step [50/113], Loss: 0.05499398708343506\n",
            "Epoch [16/30], Step [60/113], Loss: 0.10641384869813919\n",
            "Epoch [16/30], Step [70/113], Loss: 0.07414926588535309\n",
            "Epoch [16/30], Step [80/113], Loss: 0.06302332133054733\n",
            "Epoch [16/30], Step [90/113], Loss: 0.04616771638393402\n",
            "Epoch [16/30], Step [100/113], Loss: 0.0699312686920166\n",
            "Epoch [16/30], Step [110/113], Loss: 0.13845136761665344\n",
            "Epoch [16/30], Train Loss: 0.07640812697663771, Train Accuracy: 0.973642924380295\n",
            "Epoch [16/30], Validation Loss: 0.07799581085380755, Validation Accuracy: 0.9740640033465802\n",
            "************************************\n",
            "Current Learning Rate: 0.00022006333432588265\n",
            "Epoch [17/30], Step [10/113], Loss: 0.1045929417014122\n",
            "Epoch [17/30], Step [20/113], Loss: 0.07517941296100616\n",
            "Epoch [17/30], Step [30/113], Loss: 0.08373808115720749\n",
            "Epoch [17/30], Step [40/113], Loss: 0.09118339419364929\n",
            "Epoch [17/30], Step [50/113], Loss: 0.0671510323882103\n",
            "Epoch [17/30], Step [60/113], Loss: 0.10295727103948593\n",
            "Epoch [17/30], Step [70/113], Loss: 0.05226076766848564\n",
            "Epoch [17/30], Step [80/113], Loss: 0.07679564505815506\n",
            "Epoch [17/30], Step [90/113], Loss: 0.07218635082244873\n",
            "Epoch [17/30], Step [100/113], Loss: 0.05248354747891426\n",
            "Epoch [17/30], Step [110/113], Loss: 0.09788861125707626\n",
            "Epoch [17/30], Train Loss: 0.07731657897212864, Train Accuracy: 0.973433741240456\n",
            "Epoch [17/30], Validation Loss: 0.07633778678351327, Validation Accuracy: 0.9745869065049153\n",
            "************************************\n",
            "Current Learning Rate: 0.00020906016760958852\n",
            "Epoch [18/30], Step [10/113], Loss: 0.055773425847291946\n",
            "Epoch [18/30], Step [20/113], Loss: 0.06290899962186813\n",
            "Epoch [18/30], Step [30/113], Loss: 0.04794731363654137\n",
            "Epoch [18/30], Step [40/113], Loss: 0.0978291928768158\n",
            "Epoch [18/30], Step [50/113], Loss: 0.05945959687232971\n",
            "Epoch [18/30], Step [60/113], Loss: 0.07295479625463486\n",
            "Epoch [18/30], Step [70/113], Loss: 0.08398468792438507\n",
            "Epoch [18/30], Step [80/113], Loss: 0.07345157116651535\n",
            "Epoch [18/30], Step [90/113], Loss: 0.12183670699596405\n",
            "Epoch [18/30], Step [100/113], Loss: 0.05103709548711777\n",
            "Epoch [18/30], Step [110/113], Loss: 0.049812424927949905\n",
            "Epoch [18/30], Train Loss: 0.0756387373152296, Train Accuracy: 0.9735034689537356\n",
            "Epoch [18/30], Validation Loss: 0.07668706828630284, Validation Accuracy: 0.9741685839782472\n",
            "************************************\n",
            "Current Learning Rate: 0.00019860715922910907\n",
            "Epoch [19/30], Step [10/113], Loss: 0.11303865909576416\n",
            "Epoch [19/30], Step [20/113], Loss: 0.09928254038095474\n",
            "Epoch [19/30], Step [30/113], Loss: 0.04388883337378502\n",
            "Epoch [19/30], Step [40/113], Loss: 0.0923215001821518\n",
            "Epoch [19/30], Step [50/113], Loss: 0.07721459120512009\n",
            "Epoch [19/30], Step [60/113], Loss: 0.09171364456415176\n",
            "Epoch [19/30], Step [70/113], Loss: 0.11582095175981522\n",
            "Epoch [19/30], Step [80/113], Loss: 0.057143960148096085\n",
            "Epoch [19/30], Step [90/113], Loss: 0.06676094233989716\n",
            "Epoch [19/30], Step [100/113], Loss: 0.04805104807019234\n",
            "Epoch [19/30], Step [110/113], Loss: 0.054430559277534485\n",
            "Epoch [19/30], Train Loss: 0.07543833646923304, Train Accuracy: 0.9736777882369347\n",
            "Epoch [19/30], Validation Loss: 0.07623290172532986, Validation Accuracy: 0.9749006483999163\n",
            "************************************\n",
            "Current Learning Rate: 0.0001886768012676536\n",
            "Epoch [20/30], Step [10/113], Loss: 0.045467622578144073\n",
            "Epoch [20/30], Step [20/113], Loss: 0.0662037655711174\n",
            "Epoch [20/30], Step [30/113], Loss: 0.06981288641691208\n",
            "Epoch [20/30], Step [40/113], Loss: 0.09712255001068115\n",
            "Epoch [20/30], Step [50/113], Loss: 0.09159903228282928\n",
            "Epoch [20/30], Step [60/113], Loss: 0.05781067535281181\n",
            "Epoch [20/30], Step [70/113], Loss: 0.051912806928157806\n",
            "Epoch [20/30], Step [80/113], Loss: 0.10108552873134613\n",
            "Epoch [20/30], Step [90/113], Loss: 0.07253522425889969\n",
            "Epoch [20/30], Step [100/113], Loss: 0.06981488317251205\n",
            "Epoch [20/30], Step [110/113], Loss: 0.0656987726688385\n",
            "Epoch [20/30], Train Loss: 0.07543075586199365, Train Accuracy: 0.9737475159502144\n",
            "Epoch [20/30], Validation Loss: 0.07635397655203154, Validation Accuracy: 0.9746914871365823\n",
            "************************************\n",
            "Current Learning Rate: 0.0001792429612042709\n",
            "Epoch [21/30], Step [10/113], Loss: 0.1390373259782791\n",
            "Epoch [21/30], Step [20/113], Loss: 0.06441978365182877\n",
            "Epoch [21/30], Step [30/113], Loss: 0.0765090361237526\n",
            "Epoch [21/30], Step [40/113], Loss: 0.050855688750743866\n",
            "Epoch [21/30], Step [50/113], Loss: 0.037691857665777206\n",
            "Epoch [21/30], Step [60/113], Loss: 0.1015603318810463\n",
            "Epoch [21/30], Step [70/113], Loss: 0.05283472687005997\n",
            "Epoch [21/30], Step [80/113], Loss: 0.04480685293674469\n",
            "Epoch [21/30], Step [90/113], Loss: 0.06485994905233383\n",
            "Epoch [21/30], Step [100/113], Loss: 0.05599583685398102\n",
            "Epoch [21/30], Step [110/113], Loss: 0.059043142944574356\n",
            "Epoch [21/30], Train Loss: 0.07545682336244963, Train Accuracy: 0.9737475159502144\n",
            "Epoch [21/30], Validation Loss: 0.07618594336274423, Validation Accuracy: 0.9752143902949174\n",
            "************************************\n",
            "Current Learning Rate: 0.00017028081314405735\n",
            "Epoch [22/30], Step [10/113], Loss: 0.10586435347795486\n",
            "Epoch [22/30], Step [20/113], Loss: 0.03593595325946808\n",
            "Epoch [22/30], Step [30/113], Loss: 0.11807281523942947\n",
            "Epoch [22/30], Step [40/113], Loss: 0.12482543289661407\n",
            "Epoch [22/30], Step [50/113], Loss: 0.08295649290084839\n",
            "Epoch [22/30], Step [60/113], Loss: 0.04951699823141098\n",
            "Epoch [22/30], Step [70/113], Loss: 0.0631539449095726\n",
            "Epoch [22/30], Step [80/113], Loss: 0.06988073140382767\n",
            "Epoch [22/30], Step [90/113], Loss: 0.0485135093331337\n",
            "Epoch [22/30], Step [100/113], Loss: 0.08165096491575241\n",
            "Epoch [22/30], Step [110/113], Loss: 0.08121390640735626\n",
            "Epoch [22/30], Train Loss: 0.07543445937335491, Train Accuracy: 0.9738172436634941\n",
            "Epoch [22/30], Validation Loss: 0.07612180562787935, Validation Accuracy: 0.9746914871365823\n",
            "************************************\n",
            "Current Learning Rate: 0.00016176677248685447\n",
            "Epoch [23/30], Step [10/113], Loss: 0.05049755424261093\n",
            "Epoch [23/30], Step [20/113], Loss: 0.10154594480991364\n",
            "Epoch [23/30], Step [30/113], Loss: 0.0814284011721611\n",
            "Epoch [23/30], Step [40/113], Loss: 0.07652611285448074\n",
            "Epoch [23/30], Step [50/113], Loss: 0.05316790193319321\n",
            "Epoch [23/30], Step [60/113], Loss: 0.035580914467573166\n",
            "Epoch [23/30], Step [70/113], Loss: 0.09004608541727066\n",
            "Epoch [23/30], Step [80/113], Loss: 0.08730195462703705\n",
            "Epoch [23/30], Step [90/113], Loss: 0.06980759650468826\n",
            "Epoch [23/30], Step [100/113], Loss: 0.06985524296760559\n",
            "Epoch [23/30], Step [110/113], Loss: 0.06612809002399445\n",
            "Epoch [23/30], Train Loss: 0.07678739561707573, Train Accuracy: 0.9735731966670153\n",
            "Epoch [23/30], Validation Loss: 0.07888006860096204, Validation Accuracy: 0.9740640033465802\n",
            "************************************\n",
            "Current Learning Rate: 0.00015367843386251173\n",
            "Epoch [24/30], Step [10/113], Loss: 0.07311612367630005\n",
            "Epoch [24/30], Step [20/113], Loss: 0.08908618986606598\n",
            "Epoch [24/30], Step [30/113], Loss: 0.05210265517234802\n",
            "Epoch [24/30], Step [40/113], Loss: 0.11275468021631241\n",
            "Epoch [24/30], Step [50/113], Loss: 0.07000089436769485\n",
            "Epoch [24/30], Step [60/113], Loss: 0.06688845902681351\n",
            "Epoch [24/30], Step [70/113], Loss: 0.07811321318149567\n",
            "Epoch [24/30], Step [80/113], Loss: 0.08881322294473648\n",
            "Epoch [24/30], Step [90/113], Loss: 0.12017085403203964\n",
            "Epoch [24/30], Step [100/113], Loss: 0.03857845813035965\n",
            "Epoch [24/30], Step [110/113], Loss: 0.10069254785776138\n",
            "Epoch [24/30], Train Loss: 0.07576713129976946, Train Accuracy: 0.9739218352334135\n",
            "Epoch [24/30], Validation Loss: 0.07612167107627581, Validation Accuracy: 0.9747960677682493\n",
            "************************************\n",
            "Current Learning Rate: 0.00014599451216938612\n",
            "Epoch [25/30], Step [10/113], Loss: 0.08273782581090927\n",
            "Epoch [25/30], Step [20/113], Loss: 0.06511726975440979\n",
            "Epoch [25/30], Step [30/113], Loss: 0.052983105182647705\n",
            "Epoch [25/30], Step [40/113], Loss: 0.05498505383729935\n",
            "Epoch [25/30], Step [50/113], Loss: 0.07293172180652618\n",
            "Epoch [25/30], Step [60/113], Loss: 0.0511639341711998\n",
            "Epoch [25/30], Step [70/113], Loss: 0.08666004240512848\n",
            "Epoch [25/30], Step [80/113], Loss: 0.11475364863872528\n",
            "Epoch [25/30], Step [90/113], Loss: 0.07522253692150116\n",
            "Epoch [25/30], Step [100/113], Loss: 0.04792894050478935\n",
            "Epoch [25/30], Step [110/113], Loss: 0.04123307392001152\n",
            "Epoch [25/30], Train Loss: 0.07523328299939105, Train Accuracy: 0.974026426803333\n",
            "Epoch [25/30], Validation Loss: 0.0761405915806168, Validation Accuracy: 0.9752143902949174\n",
            "************************************\n",
            "Current Learning Rate: 0.00013869478656091682\n",
            "Epoch [26/30], Step [10/113], Loss: 0.09577388316392899\n",
            "Epoch [26/30], Step [20/113], Loss: 0.07796042412519455\n",
            "Epoch [26/30], Step [30/113], Loss: 0.06380514055490494\n",
            "Epoch [26/30], Step [40/113], Loss: 0.09617630392313004\n",
            "Epoch [26/30], Step [50/113], Loss: 0.059507712721824646\n",
            "Epoch [26/30], Step [60/113], Loss: 0.11425802856683731\n",
            "Epoch [26/30], Step [70/113], Loss: 0.0819694846868515\n",
            "Epoch [26/30], Step [80/113], Loss: 0.05818946287035942\n",
            "Epoch [26/30], Step [90/113], Loss: 0.031238291412591934\n",
            "Epoch [26/30], Step [100/113], Loss: 0.08778205513954163\n",
            "Epoch [26/30], Step [110/113], Loss: 0.0838148221373558\n",
            "Epoch [26/30], Train Loss: 0.0751303672724593, Train Accuracy: 0.9737126520935746\n",
            "Epoch [26/30], Validation Loss: 0.07647723126176156, Validation Accuracy: 0.9742731646099142\n",
            "************************************\n",
            "Current Learning Rate: 0.00013176004723287096\n",
            "Epoch [27/30], Step [10/113], Loss: 0.050928156822919846\n",
            "Epoch [27/30], Step [20/113], Loss: 0.04791327193379402\n",
            "Epoch [27/30], Step [30/113], Loss: 0.09415003657341003\n",
            "Epoch [27/30], Step [40/113], Loss: 0.0748954489827156\n",
            "Epoch [27/30], Step [50/113], Loss: 0.0681387186050415\n",
            "Epoch [27/30], Step [60/113], Loss: 0.08039619028568268\n",
            "Epoch [27/30], Step [70/113], Loss: 0.12848637998104095\n",
            "Epoch [27/30], Step [80/113], Loss: 0.06224910169839859\n",
            "Epoch [27/30], Step [90/113], Loss: 0.12373507767915726\n",
            "Epoch [27/30], Step [100/113], Loss: 0.08127732574939728\n",
            "Epoch [27/30], Step [110/113], Loss: 0.10176113247871399\n",
            "Epoch [27/30], Train Loss: 0.07512643715712876, Train Accuracy: 0.9738521075201338\n",
            "Epoch [27/30], Validation Loss: 0.07603627011964195, Validation Accuracy: 0.9750052290315834\n",
            "************************************\n",
            "Current Learning Rate: 0.0001251720448712274\n",
            "Epoch [28/30], Step [10/113], Loss: 0.058256711810827255\n",
            "Epoch [28/30], Step [20/113], Loss: 0.08761607855558395\n",
            "Epoch [28/30], Step [30/113], Loss: 0.11328063905239105\n",
            "Epoch [28/30], Step [40/113], Loss: 0.09178874641656876\n",
            "Epoch [28/30], Step [50/113], Loss: 0.1345890760421753\n",
            "Epoch [28/30], Step [60/113], Loss: 0.04847031831741333\n",
            "Epoch [28/30], Step [70/113], Loss: 0.05268009006977081\n",
            "Epoch [28/30], Step [80/113], Loss: 0.08708199113607407\n",
            "Epoch [28/30], Step [90/113], Loss: 0.05355777591466904\n",
            "Epoch [28/30], Step [100/113], Loss: 0.10320472717285156\n",
            "Epoch [28/30], Step [110/113], Loss: 0.05215109512209892\n",
            "Epoch [28/30], Train Loss: 0.07485748752457115, Train Accuracy: 0.9738521075201338\n",
            "Epoch [28/30], Validation Loss: 0.07629091268111217, Validation Accuracy: 0.9742731646099142\n",
            "************************************\n",
            "Current Learning Rate: 0.00011891344262766602\n",
            "Epoch [29/30], Step [10/113], Loss: 0.06949431449174881\n",
            "Epoch [29/30], Step [20/113], Loss: 0.04206494614481926\n",
            "Epoch [29/30], Step [30/113], Loss: 0.10264195501804352\n",
            "Epoch [29/30], Step [40/113], Loss: 0.0731099471449852\n",
            "Epoch [29/30], Step [50/113], Loss: 0.06188012287020683\n",
            "Epoch [29/30], Step [60/113], Loss: 0.11491060256958008\n",
            "Epoch [29/30], Step [70/113], Loss: 0.03010750748217106\n",
            "Epoch [29/30], Step [80/113], Loss: 0.06907627731561661\n",
            "Epoch [29/30], Step [90/113], Loss: 0.08463310450315475\n",
            "Epoch [29/30], Step [100/113], Loss: 0.05548020452260971\n",
            "Epoch [29/30], Step [110/113], Loss: 0.06218723952770233\n",
            "Epoch [29/30], Train Loss: 0.07502732566038592, Train Accuracy: 0.9741658822298923\n",
            "Epoch [29/30], Validation Loss: 0.07610418397541109, Validation Accuracy: 0.9746914871365823\n",
            "************************************\n",
            "Current Learning Rate: 0.00011296777049628272\n",
            "Epoch [30/30], Step [10/113], Loss: 0.05296799913048744\n",
            "Epoch [30/30], Step [20/113], Loss: 0.039436839520931244\n",
            "Epoch [30/30], Step [30/113], Loss: 0.025303510949015617\n",
            "Epoch [30/30], Step [40/113], Loss: 0.0946955755352974\n",
            "Epoch [30/30], Step [50/113], Loss: 0.0754215270280838\n",
            "Epoch [30/30], Step [60/113], Loss: 0.11110884696245193\n",
            "Epoch [30/30], Step [70/113], Loss: 0.0815919041633606\n",
            "Epoch [30/30], Step [80/113], Loss: 0.1187276691198349\n",
            "Epoch [30/30], Step [90/113], Loss: 0.08352774381637573\n",
            "Epoch [30/30], Step [100/113], Loss: 0.03936643898487091\n",
            "Epoch [30/30], Step [110/113], Loss: 0.06661563366651535\n",
            "Epoch [30/30], Train Loss: 0.07509299581952855, Train Accuracy: 0.9739566990900533\n",
            "Epoch [30/30], Validation Loss: 0.07641524841126643, Validation Accuracy: 0.9742731646099142\n",
            "Best model saved with validation accuracy: 0.9752143902949174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, you can extract the trained parameters\n",
        "trained_centers = model.centers.detach().cpu().numpy()\n",
        "trained_sigmas = model.sigmas.detach().cpu().numpy()\n",
        "\n",
        "# Print or use the trained parameters\n",
        "print(\"Trained Centers:\", trained_centers)\n",
        "print(\"Trained Sigmas:\", trained_sigmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvj498VR87gn",
        "outputId": "9b9855eb-ffc3-416b-b02e-93aba6ee3e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Centers: [[-6.72877218e-02  5.28677201e-02  1.48266270e-01 ...  2.69777879e-06\n",
            "   3.73975518e-06 -9.17385143e-04]\n",
            " [-2.14057201e-01  1.86571142e-02 -9.47955544e-01 ... -2.88002616e-06\n",
            "  -1.12124075e-06 -9.37939458e-04]\n",
            " [-2.22309262e-01 -1.64610102e-01 -7.93011726e-01 ... -3.61319814e-06\n",
            "  -2.56563590e-06 -8.19786334e-04]\n",
            " ...\n",
            " [ 7.84233617e-02 -1.96447672e-01  1.18477684e+00 ...  2.95412757e-05\n",
            "   2.15170042e-05  1.09293086e-03]\n",
            " [ 3.41224334e-01  2.15215011e-01  1.06755409e+00 ...  3.61441717e-05\n",
            "   8.51299991e-05  1.59761195e-03]\n",
            " [ 7.56436409e-01 -3.89683388e-02 -1.01914606e+00 ... -3.12372701e-06\n",
            "  -2.06403810e-06 -9.01615991e-04]]\n",
            "Trained Sigmas: [3.272349]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_centers.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJGskhNEM-UM",
        "outputId": "a70b6dfa-fc9c-463b-f5e0-8078279ee623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1144)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_centers[0][:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHc5HGHRMSQi",
        "outputId": "d0a9919a-2c0f-4d6b-ff75-21b4f5d9831f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.06728772,  0.05286772,  0.14826627,  0.10858831,  0.73251667,\n",
              "       -0.31598319,  0.84853289, -0.3052243 ,  0.4397249 ,  0.2005334 ,\n",
              "        0.91057174, -0.10050466, -0.03121453,  1.00701555,  0.27813542,\n",
              "        0.08260053, -0.27219494,  0.14633815,  0.50917671, -0.23590404,\n",
              "        0.17625967, -0.07525321,  0.33876415, -0.23252008,  0.39667067,\n",
              "       -0.52118714,  0.28684472,  0.12878577, -0.45861407, -0.20414072,\n",
              "       -0.19059005, -0.14260681, -0.19978095, -0.38665129, -0.0157628 ,\n",
              "       -0.18960417, -0.18796049, -0.50119469, -0.18496574, -0.15641157,\n",
              "       -0.14795656, -0.12750056, -0.02451814, -0.41335375, -0.29829602,\n",
              "       -0.38242726,  0.11990683,  0.28861171,  0.02903654,  0.19225395,\n",
              "       -0.16946435, -0.16863965, -0.13654089,  0.25108567, -0.23583435,\n",
              "        0.55016197,  0.3419944 ,  0.46616686, -0.28680909,  0.30642635,\n",
              "       -0.04406383, -0.30059522, -0.12552965, -0.1763888 , -0.18170115,\n",
              "       -0.25665917,  0.07063957,  0.50430855, -0.18616898, -0.1211585 ,\n",
              "       -0.17578326, -0.19032728, -0.16146761, -0.16015938, -0.15899602,\n",
              "        0.02939042, -0.18850165,  0.056633  ,  0.3291561 , -0.11685275,\n",
              "       -0.11896258,  0.27503104, -0.05593386, -0.12672606, -0.45113452,\n",
              "       -0.05628004,  0.13429897, -0.32679508, -0.20219942,  0.48181981,\n",
              "       -0.09508639,  0.39622198, -0.06226352, -0.01527713, -0.17034929,\n",
              "       -0.18334001,  0.44484875, -0.02646934, -0.12001885,  0.14902716])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-instantiate the model\n",
        "all_centers = torch.rand((1000, 10000))\n",
        "sigma = 5.\n",
        "model = RBFModel(1000, 2, all_centers, [sigma])\n",
        "\n",
        "# Load the saved model state dictionary\n",
        "model_state_dict = torch.load('/content/drive/MyDrive/best_model.pth')\n",
        "\n",
        "# Load the model state dictionary into the model\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-rclGF3LlSh",
        "outputId": "9cc386bb-577a-4ed0-c775-df2e206319e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RBFModel(\n",
              "  (linear): Linear(in_features=1000, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-instantiate the model\n",
        "model_non_removal_features = RBFModel(1000, 2, all_centers_non_removal_features, [sigma_non_removal_features])\n",
        "\n",
        "# Load the saved model state dictionary\n",
        "model_state_dict = torch.load('best_model_non_removal_features3.pth')\n",
        "\n",
        "# Load the model state dictionary into the model\n",
        "model_non_removal_features.load_state_dict(model_state_dict)\n",
        "\n",
        "model_non_removal_features = model_non_removal_features.to(device)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model_non_removal_features.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7cb5ab-e515-47ef-8478-3fc79fe95c6f",
        "id": "Bos266HhufCL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RBFModel(\n",
              "  (linear): Linear(in_features=1000, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, labels_batch in test_loader:\n",
        "            X_batch, labels_batch = X_batch[:, non_removal_mask].to(torch.float32).to(device), labels_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.topk(outputs, k=1)\n",
        "            predictions.extend(predicted.tolist())\n",
        "            true_labels.extend(labels_batch.tolist())\n",
        "\n",
        "    # Convert predictions and true labels to numpy arrays\n",
        "    predictions = np.array(predictions)\n",
        "    true_labels = np.array(true_labels)\n",
        "\n",
        "    # Calculate and print test accuracy\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    balanced_acc = balanced_accuracy_score(true_labels, predictions)\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Test balanced Accuracy: {balanced_acc:.4f}')\n",
        "\n",
        "    # Calculate and print precision, recall, and F1-score\n",
        "    precision = precision_score(true_labels, predictions)\n",
        "    recall = recall_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions)\n",
        "\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-score: {f1:.4f}')\n",
        "\n",
        "    # Calculate and print true positives, true negatives, false positives, and false negatives\n",
        "    TP = ((predictions == 1) & (true_labels == 1)).sum()\n",
        "    TN = ((predictions == 0) & (true_labels == 0)).sum()\n",
        "    FP = ((predictions == 1) & (true_labels == 0)).sum()\n",
        "    FN = ((predictions == 0) & (true_labels == 1)).sum()\n",
        "\n",
        "    print(f'True Positives (TP): {TP}')\n",
        "    print(f'True Negatives (TN): {TN}')\n",
        "    print(f'False Positives (FP): {FP}')\n",
        "    print(f'False Negatives (FN): {FN}')\n",
        "\n",
        "    # Calculate and print False Negative Rate (FNR) and False Positive Rate (FPR)\n",
        "    FNR = (FN / (FN + TP)) * 100\n",
        "    FPR = (FP / (FP + TN)) * 100\n",
        "\n",
        "    print(f'False Negative Rate (FNR): {FNR:.4f}')\n",
        "    print(f'False Positive Rate (FPR): {FPR:.4f}')"
      ],
      "metadata": {
        "id": "-kIboO8luDlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_evaluation(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyq9pqq2u6ty",
        "outputId": "4fab492a-c7bd-43bb-e109-e5ba05574e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9855\n",
            "Test balanced Accuracy: 0.9623\n",
            "Precision: 0.9444\n",
            "Recall: 0.9319\n",
            "F1-score: 0.9381\n",
            "True Positives (TP): 1053\n",
            "True Negatives (TN): 8370\n",
            "False Positives (FP): 62\n",
            "False Negatives (FN): 77\n",
            "False Negative Rate (FNR): 6.8142\n",
            "False Positive Rate (FPR): 0.7353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_evaluation(model_non_removal_features, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTAkFQ3TuJ0i",
        "outputId": "fa441523-573c-49cb-cf5b-9a235e476636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9723\n",
            "Test balanced Accuracy: 0.9099\n",
            "Precision: 0.9295\n",
            "Recall: 0.8283\n",
            "F1-score: 0.8760\n",
            "True Positives (TP): 936\n",
            "True Negatives (TN): 8361\n",
            "False Positives (FP): 71\n",
            "False Negatives (FN): 194\n",
            "False Negative Rate (FNR): 17.1681\n",
            "False Positive Rate (FPR): 0.8420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9JL7fSGAMiO",
        "outputId": "fc6f9a07-7cf7-4134-f21b-56b81dbff225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}