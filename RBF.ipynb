{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQSkQs2DgrYyDXLR3sUrHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/mal_adv4/blob/main/RBF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "download_links = ['https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_0.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_1.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_2.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y0.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y1.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y2.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_DNN_drebin_best.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM_weightedLoss.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/insertion_array.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/removal_array.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/adverserial_attacks_functions.py'\n",
        "]"
      ],
      "metadata": {
        "id": "1IW4pHac9VLq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "output_filepath = '/content/'\n",
        "for link in download_links:\n",
        "  gdown.download(link, output_filepath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kzSbjaXGVeG",
        "outputId": "167c633a-013b-491a-ea98-46266861a628"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_0.npz\n",
            "To: /content/sparse_matrix_0.npz\n",
            "100%|██████████| 461k/461k [00:00<00:00, 4.35MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_1.npz\n",
            "To: /content/sparse_matrix_1.npz\n",
            "100%|██████████| 148k/148k [00:00<00:00, 2.61MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_2.npz\n",
            "To: /content/sparse_matrix_2.npz\n",
            "100%|██████████| 150k/150k [00:00<00:00, 2.27MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y0.npz\n",
            "To: /content/sparse_matrix_y0.npz\n",
            "100%|██████████| 5.79k/5.79k [00:00<00:00, 13.5MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y1.npz\n",
            "To: /content/sparse_matrix_y1.npz\n",
            "100%|██████████| 2.64k/2.64k [00:00<00:00, 8.47MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y2.npz\n",
            "To: /content/sparse_matrix_y2.npz\n",
            "100%|██████████| 2.71k/2.71k [00:00<00:00, 8.63MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_DNN_drebin_best.pth\n",
            "To: /content/model_DNN_drebin_best.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 17.5MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM_weightedLoss.pth\n",
            "To: /content/model_AT_rFGSM_weightedLoss.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 57.6MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM.pth\n",
            "To: /content/model_AT_rFGSM.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 79.0MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/insertion_array.pkl\n",
            "To: /content/insertion_array.pkl\n",
            "100%|██████████| 80.2k/80.2k [00:00<00:00, 4.12MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/removal_array.pkl\n",
            "To: /content/removal_array.pkl\n",
            "100%|██████████| 80.2k/80.2k [00:00<00:00, 3.76MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/adverserial_attacks_functions.py\n",
            "To: /content/adverserial_attacks_functions.py\n",
            "67.1kB [00:00, 47.3MB/s]                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,balanced_accuracy_score\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "from adverserial_attacks_functions import *\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "JKDdI3K9LrlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f84686d-d4a9-48f3-e8c3-59228dc8e5b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7db9157504d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the .pkl file\n",
        "with open('/content/insertion_array.pkl', 'rb') as f:\n",
        "    # Load the object\n",
        "    insertion_array = pickle.load(f)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n",
        "\n",
        "insertion_array = torch.tensor(insertion_array).to(device)\n",
        "print(len(insertion_array))\n",
        "\n",
        "# Open the .pkl file\n",
        "with open('/content/removal_array.pkl', 'rb') as f:\n",
        "    # Load the object\n",
        "    removal_array = pickle.load(f)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n",
        "\n",
        "removal_array = torch.tensor(removal_array).to(device)\n",
        "print(len(removal_array))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXV0WIjsJG_F",
        "outputId": "1f965468-9628-4531-87f4-09e6cdb4d672"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset\n",
        "X_train = sparse.load_npz(\"/content/sparse_matrix_0.npz\").toarray()\n",
        "X_val = sparse.load_npz(\"/content/sparse_matrix_1.npz\").toarray()\n",
        "X_test = sparse.load_npz(\"/content/sparse_matrix_2.npz\").toarray()\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.int8)\n",
        "X_val = torch.tensor(X_val, dtype=torch.int8)\n",
        "X_test = torch.tensor(X_test, dtype=torch.int8)\n",
        "\n",
        "\n",
        "y_train = sparse.load_npz(\"/content/sparse_matrix_y0.npz\").toarray().reshape((-1, 1))\n",
        "y_val = sparse.load_npz(\"/content/sparse_matrix_y1.npz\").toarray().reshape((-1, 1))\n",
        "y_test = sparse.load_npz(\"/content/sparse_matrix_y2.npz\").toarray().reshape((-1, 1))\n",
        "\n",
        "y_train = torch.tensor(y_train, dtype=torch.int8)\n",
        "y_val = torch.tensor(y_val, dtype=torch.int8)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int8)\n",
        "\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"x_train:\", X_train.shape)\n",
        "print(\"x_val:\", X_val.shape)\n",
        "print(\"x_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_val:\", y_val.shape)\n",
        "print(\"y_test:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5blmEg4h-GKy",
        "outputId": "a6892267-ce91-40da-c4c7-70f85ff1cc6e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "x_train: torch.Size([28683, 10000])\n",
            "x_val: torch.Size([9562, 10000])\n",
            "x_test: torch.Size([9562, 10000])\n",
            "y_train: torch.Size([28683, 1])\n",
            "y_val: torch.Size([9562, 1])\n",
            "y_test: torch.Size([9562, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of benigns and malicious sample in training dataset\n",
        "n_ben = (y_train.squeeze()== 0).sum().item()\n",
        "n_mal = (y_train.squeeze()== 1).sum().item()\n",
        "print('the proportion of malwares : ', n_mal/(n_mal+n_ben))\n",
        "\n",
        "# Combine features and labels into datasets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Define the DataLoader for training, validation, and test sets\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del train_dataset, val_dataset, test_dataset, y_train, y_val, y_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81AZSXOV-HoW",
        "outputId": "c9e931b1-30a6-4389-ca18-2c57ee754b86"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the proportion of malwares :  0.11386535578565701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_DNN = MalwareDetectionModel().to(device)\n",
        "# Load model parameters\n",
        "model_DNN.load_state_dict(torch.load('model_DNN_drebin_best.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "id": "0MavlKAt6mb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6225056-a5ee-4cac-b3bb-2e71413f6b52"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your model\n",
        "model_AT_rFGSM = MalwareDetectionModel().to(device)\n",
        "\n",
        "# Load model parameters\n",
        "model_AT_rFGSM.load_state_dict(torch.load('model_AT_rFGSM.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE8WMAUgSCms",
        "outputId": "a5f1f7a0-6c99-4d4d-b6eb-3f3379b40f60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your model\n",
        "model_AT_rFGSM_weightedLoss = MalwareDetectionModel().to(device)\n",
        "\n",
        "# Load model parameters\n",
        "model_AT_rFGSM_weightedLoss.load_state_dict(torch.load('model_AT_rFGSM_weightedLoss.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGs5E9_2SDbJ",
        "outputId": "bdaa06d8-9737-4cf3-a8f0-722da5253524"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Function to initialize centers and sigmas separately for benign and malware samples using KMeans clustering\n",
        "def initialize_centers_sigmas_separate(data_loader, num_centers_per_class):\n",
        "    centers_benign = []\n",
        "    centers_malware = []\n",
        "\n",
        "    # Collect data into a single tensor\n",
        "    all_data = torch.cat([batch for batch, _ in data_loader], dim=0)\n",
        "    all_data = all_data.numpy()\n",
        "\n",
        "    # Collect labels into a single tensor\n",
        "    all_labels = torch.cat([labels for _, labels in data_loader], dim=0)\n",
        "    all_labels = all_labels.numpy()\n",
        "\n",
        "    # Separate benign and malware samples\n",
        "    benign_indices = np.where(all_labels == 0)[0]\n",
        "    malware_indices = np.where(all_labels == 1)[0]\n",
        "\n",
        "    # Clustering for benign samples\n",
        "    benigns = all_data[benign_indices]\n",
        "    subset_benigns = benigns[:20000]\n",
        "    kmeans_benign = KMeans(n_clusters=num_centers_per_class, init='k-means++', n_init='auto')\n",
        "    kmeans_benign.fit(subset_benigns)\n",
        "    centers_selected_benign = kmeans_benign.cluster_centers_\n",
        "\n",
        "    # Clustering for malware samples\n",
        "    kmeans_malware = KMeans(n_clusters=num_centers_per_class, init='k-means++', n_init='auto')\n",
        "    kmeans_malware.fit(all_data[malware_indices])\n",
        "    centers_selected_malware = kmeans_malware.cluster_centers_\n",
        "\n",
        "    # Combine selected centers\n",
        "    all_centers = np.concatenate([centers_selected_benign, centers_selected_malware], axis=0)\n",
        "\n",
        "    # Calculate sigma based on the average distance between centers\n",
        "    total_distance = 0.0\n",
        "    num_pairs = 0\n",
        "\n",
        "    # Calculate pairwise distances between centers for benign samples\n",
        "    for i in range(len(centers_selected_benign)):\n",
        "        for j in range(i + 1, len(centers_selected_benign)):\n",
        "            distance_ij = np.sqrt(((centers_selected_benign[i] - centers_selected_benign[j]) ** 2).sum())\n",
        "            total_distance += distance_ij\n",
        "            num_pairs += 1\n",
        "\n",
        "    # Calculate pairwise distances between centers for malware samples\n",
        "    for i in range(len(centers_selected_malware)):\n",
        "        for j in range(i + 1, len(centers_selected_malware)):\n",
        "            distance_ij = np.sqrt(((centers_selected_malware[i] - centers_selected_malware[j]) ** 2).sum())\n",
        "            total_distance += distance_ij\n",
        "            num_pairs += 1\n",
        "\n",
        "    # Calculate mean sigma\n",
        "\n",
        "    sigma = total_distance / num_pairs\n",
        "\n",
        "    return all_centers, sigma\n"
      ],
      "metadata": {
        "id": "vh2VQ93Od2vL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_centers, sigma = initialize_centers_sigmas_separate(train_loader, 500)"
      ],
      "metadata": {
        "id": "AQEr5VdJ4Upt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAHioaOp6rIS",
        "outputId": "fad54326-e63f-4e88-ee07-cd9284e98d77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.325346210357184"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_centers.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uz_cJ926sv2",
        "outputId": "9bad2139-fe62-432a-dd6f-fcfd54d9a1cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the array to a file\n",
        "np.save('all_centers.npy', all_centers)"
      ],
      "metadata": {
        "id": "oSnXMB926oVh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the array from the file\n",
        "all_centers = np.load('all_centers.npy')\n",
        "\n",
        "sigma = 6.281732838681898"
      ],
      "metadata": {
        "id": "Ss6Qdd-H6otO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move all_centers back to GPU\n",
        "all_centers = torch.tensor(all_centers, device=device)"
      ],
      "metadata": {
        "id": "5llZpwJP3ZsP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define the RBF model\n",
        "class RBFModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, init_centers, init_sigmas):\n",
        "        super(RBFModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.centers = nn.Parameter(torch.Tensor(init_centers))\n",
        "        self.sigmas = nn.Parameter(torch.Tensor(init_sigmas))\n",
        "\n",
        "        # Linear layer for output\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def gaussian(self, x, c, sigma):\n",
        "        return torch.exp(-torch.sum((x[:, None, :] - c) ** 2, dim=-1) / (2 * sigma ** 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        radial_out = self.gaussian(x, self.centers, self.sigmas)\n",
        "        output = self.linear(radial_out.to(torch.float32))\n",
        "        return output\n",
        "\n",
        "# Function to evaluate the model on a dataset\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in data_loader:\n",
        "            batch_x, batch_y = batch_x.to(torch.float32).to(device), batch_y.to(device)  # Move data to GPU\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y.squeeze().to(torch.long))\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct_predictions += (predicted == batch_y.squeeze()).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = correct_predictions / len(data_loader.dataset)\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "WHAI-VGJSGa2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to release GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "e1cutWXY3eza"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the RBF model with initialized centers and sigmas\n",
        "model = RBFModel(10000, 1000, 2, all_centers, [sigma])\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
        "        batch_x, batch_y = batch_x.to(torch.float32).to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_x)\n",
        "        loss = criterion(output, batch_y.squeeze().to(torch.long))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        correct_predictions += (predicted == batch_y.squeeze()).sum().item()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct_predictions / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss}, Train Accuracy: {accuracy}')\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9ZesZJEGxD09",
        "outputId": "b71111c9-bc90-4e97-ce87-73991d94246b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/897], Loss: 0.0172998309135437\n",
            "Epoch [1/10], Step [20/897], Loss: 0.032817691564559937\n",
            "Epoch [1/10], Step [30/897], Loss: 0.05968206748366356\n",
            "Epoch [1/10], Step [40/897], Loss: 0.1018967255949974\n",
            "Epoch [1/10], Step [50/897], Loss: 0.11222320050001144\n",
            "Epoch [1/10], Step [60/897], Loss: 0.04457662254571915\n",
            "Epoch [1/10], Step [70/897], Loss: 0.033884137868881226\n",
            "Epoch [1/10], Step [80/897], Loss: 0.17707860469818115\n",
            "Epoch [1/10], Step [90/897], Loss: 0.018819378688931465\n",
            "Epoch [1/10], Step [100/897], Loss: 0.05857442319393158\n",
            "Epoch [1/10], Step [110/897], Loss: 0.008322658017277718\n",
            "Epoch [1/10], Step [120/897], Loss: 0.0827416181564331\n",
            "Epoch [1/10], Step [130/897], Loss: 0.21955044567584991\n",
            "Epoch [1/10], Step [140/897], Loss: 0.08162592351436615\n",
            "Epoch [1/10], Step [150/897], Loss: 0.04201076924800873\n",
            "Epoch [1/10], Step [160/897], Loss: 0.012118374928832054\n",
            "Epoch [1/10], Step [170/897], Loss: 0.05060906335711479\n",
            "Epoch [1/10], Step [180/897], Loss: 0.05158982798457146\n",
            "Epoch [1/10], Step [190/897], Loss: 0.0396631620824337\n",
            "Epoch [1/10], Step [200/897], Loss: 0.03682459145784378\n",
            "Epoch [1/10], Step [210/897], Loss: 0.0202835351228714\n",
            "Epoch [1/10], Step [220/897], Loss: 0.11783022433519363\n",
            "Epoch [1/10], Step [230/897], Loss: 0.047487422823905945\n",
            "Epoch [1/10], Step [240/897], Loss: 0.0077865272760391235\n",
            "Epoch [1/10], Step [250/897], Loss: 0.02382413111627102\n",
            "Epoch [1/10], Step [260/897], Loss: 0.004144906997680664\n",
            "Epoch [1/10], Step [270/897], Loss: 0.09004415571689606\n",
            "Epoch [1/10], Step [280/897], Loss: 0.022889001294970512\n",
            "Epoch [1/10], Step [290/897], Loss: 0.08283652365207672\n",
            "Epoch [1/10], Step [300/897], Loss: 0.04067430645227432\n",
            "Epoch [1/10], Step [310/897], Loss: 0.10013846307992935\n",
            "Epoch [1/10], Step [320/897], Loss: 0.02121582441031933\n",
            "Epoch [1/10], Step [330/897], Loss: 0.026210449635982513\n",
            "Epoch [1/10], Step [340/897], Loss: 0.17528997361660004\n",
            "Epoch [1/10], Step [350/897], Loss: 0.2374746948480606\n",
            "Epoch [1/10], Step [360/897], Loss: 0.06421441584825516\n",
            "Epoch [1/10], Step [370/897], Loss: 0.02705688774585724\n",
            "Epoch [1/10], Step [380/897], Loss: 0.013756124302744865\n",
            "Epoch [1/10], Step [390/897], Loss: 0.0499810166656971\n",
            "Epoch [1/10], Step [400/897], Loss: 0.02633685991168022\n",
            "Epoch [1/10], Step [410/897], Loss: 0.005928258411586285\n",
            "Epoch [1/10], Step [420/897], Loss: 0.14504821598529816\n",
            "Epoch [1/10], Step [430/897], Loss: 0.019646475091576576\n",
            "Epoch [1/10], Step [440/897], Loss: 0.103722482919693\n",
            "Epoch [1/10], Step [450/897], Loss: 0.040736209601163864\n",
            "Epoch [1/10], Step [460/897], Loss: 0.0359342135488987\n",
            "Epoch [1/10], Step [470/897], Loss: 0.17353175580501556\n",
            "Epoch [1/10], Step [480/897], Loss: 0.017251281067728996\n",
            "Epoch [1/10], Step [490/897], Loss: 0.0468558669090271\n",
            "Epoch [1/10], Step [500/897], Loss: 0.06174331530928612\n",
            "Epoch [1/10], Step [510/897], Loss: 0.008531146682798862\n",
            "Epoch [1/10], Step [520/897], Loss: 0.058013465255498886\n",
            "Epoch [1/10], Step [530/897], Loss: 0.011901096440851688\n",
            "Epoch [1/10], Step [540/897], Loss: 0.023583518341183662\n",
            "Epoch [1/10], Step [550/897], Loss: 0.08865416795015335\n",
            "Epoch [1/10], Step [560/897], Loss: 0.0342487171292305\n",
            "Epoch [1/10], Step [570/897], Loss: 0.09579937160015106\n",
            "Epoch [1/10], Step [580/897], Loss: 0.12033991515636444\n",
            "Epoch [1/10], Step [590/897], Loss: 0.009527410380542278\n",
            "Epoch [1/10], Step [600/897], Loss: 0.00777784176170826\n",
            "Epoch [1/10], Step [610/897], Loss: 0.01833079755306244\n",
            "Epoch [1/10], Step [620/897], Loss: 0.03545314073562622\n",
            "Epoch [1/10], Step [630/897], Loss: 0.08049192279577255\n",
            "Epoch [1/10], Step [640/897], Loss: 0.010248158127069473\n",
            "Epoch [1/10], Step [650/897], Loss: 0.04432675614953041\n",
            "Epoch [1/10], Step [660/897], Loss: 0.021949946880340576\n",
            "Epoch [1/10], Step [670/897], Loss: 0.015775227919220924\n",
            "Epoch [1/10], Step [680/897], Loss: 0.17962317168712616\n",
            "Epoch [1/10], Step [690/897], Loss: 0.024799762293696404\n",
            "Epoch [1/10], Step [700/897], Loss: 0.005249750334769487\n",
            "Epoch [1/10], Step [710/897], Loss: 0.1261080652475357\n",
            "Epoch [1/10], Step [720/897], Loss: 0.046214550733566284\n",
            "Epoch [1/10], Step [730/897], Loss: 0.05663180723786354\n",
            "Epoch [1/10], Step [740/897], Loss: 0.06494242697954178\n",
            "Epoch [1/10], Step [750/897], Loss: 0.21661965548992157\n",
            "Epoch [1/10], Step [760/897], Loss: 0.04014447331428528\n",
            "Epoch [1/10], Step [770/897], Loss: 0.09956666082143784\n",
            "Epoch [1/10], Step [780/897], Loss: 0.14317671954631805\n",
            "Epoch [1/10], Step [790/897], Loss: 0.01949899271130562\n",
            "Epoch [1/10], Step [800/897], Loss: 0.07123851776123047\n",
            "Epoch [1/10], Step [810/897], Loss: 0.022457033395767212\n",
            "Epoch [1/10], Step [820/897], Loss: 0.016367517411708832\n",
            "Epoch [1/10], Step [830/897], Loss: 0.0067733959294855595\n",
            "Epoch [1/10], Step [840/897], Loss: 0.09975296258926392\n",
            "Epoch [1/10], Step [850/897], Loss: 0.2036389857530594\n",
            "Epoch [1/10], Step [860/897], Loss: 0.011422192677855492\n",
            "Epoch [1/10], Step [870/897], Loss: 0.12583932280540466\n",
            "Epoch [1/10], Step [880/897], Loss: 0.0699787586927414\n",
            "Epoch [1/10], Step [890/897], Loss: 0.04015975072979927\n",
            "Epoch [1/10], Train Loss: 0.0657955295031988, Train Accuracy: 0.9769898546177178\n",
            "Epoch [1/10], Validation Loss: 0.0638318183609163, Validation Accuracy: 0.9786655511399289\n",
            "Epoch [2/10], Step [10/897], Loss: 0.02065727859735489\n",
            "Epoch [2/10], Step [20/897], Loss: 0.005432089790701866\n",
            "Epoch [2/10], Step [30/897], Loss: 0.06773380935192108\n",
            "Epoch [2/10], Step [40/897], Loss: 0.024961210787296295\n",
            "Epoch [2/10], Step [50/897], Loss: 0.012811752036213875\n",
            "Epoch [2/10], Step [60/897], Loss: 0.018390124663710594\n",
            "Epoch [2/10], Step [70/897], Loss: 0.259723961353302\n",
            "Epoch [2/10], Step [80/897], Loss: 0.213558167219162\n",
            "Epoch [2/10], Step [90/897], Loss: 0.029518887400627136\n",
            "Epoch [2/10], Step [100/897], Loss: 0.0355253703892231\n",
            "Epoch [2/10], Step [110/897], Loss: 0.012657479383051395\n",
            "Epoch [2/10], Step [120/897], Loss: 0.01126452349126339\n",
            "Epoch [2/10], Step [130/897], Loss: 0.025202585384249687\n",
            "Epoch [2/10], Step [140/897], Loss: 0.07335410267114639\n",
            "Epoch [2/10], Step [150/897], Loss: 0.033669427037239075\n",
            "Epoch [2/10], Step [160/897], Loss: 0.07313495129346848\n",
            "Epoch [2/10], Step [170/897], Loss: 0.12058825045824051\n",
            "Epoch [2/10], Step [180/897], Loss: 0.029384875670075417\n",
            "Epoch [2/10], Step [190/897], Loss: 0.03146432340145111\n",
            "Epoch [2/10], Step [200/897], Loss: 0.03831305354833603\n",
            "Epoch [2/10], Step [210/897], Loss: 0.07319249212741852\n",
            "Epoch [2/10], Step [220/897], Loss: 0.049038346856832504\n",
            "Epoch [2/10], Step [230/897], Loss: 0.19153620302677155\n",
            "Epoch [2/10], Step [240/897], Loss: 0.1082967221736908\n",
            "Epoch [2/10], Step [250/897], Loss: 0.016942890360951424\n",
            "Epoch [2/10], Step [260/897], Loss: 0.019259614869952202\n",
            "Epoch [2/10], Step [270/897], Loss: 0.008082972839474678\n",
            "Epoch [2/10], Step [280/897], Loss: 0.010226882994174957\n",
            "Epoch [2/10], Step [290/897], Loss: 0.03994883969426155\n",
            "Epoch [2/10], Step [300/897], Loss: 0.16656729578971863\n",
            "Epoch [2/10], Step [310/897], Loss: 0.059689752757549286\n",
            "Epoch [2/10], Step [320/897], Loss: 0.2994016706943512\n",
            "Epoch [2/10], Step [330/897], Loss: 0.026395617052912712\n",
            "Epoch [2/10], Step [340/897], Loss: 0.11142242699861526\n",
            "Epoch [2/10], Step [350/897], Loss: 0.08325937390327454\n",
            "Epoch [2/10], Step [360/897], Loss: 0.04451725631952286\n",
            "Epoch [2/10], Step [370/897], Loss: 0.008589161559939384\n",
            "Epoch [2/10], Step [380/897], Loss: 0.015178495086729527\n",
            "Epoch [2/10], Step [390/897], Loss: 0.0042954361997544765\n",
            "Epoch [2/10], Step [400/897], Loss: 0.12009280174970627\n",
            "Epoch [2/10], Step [410/897], Loss: 0.027792351320385933\n",
            "Epoch [2/10], Step [420/897], Loss: 0.13446754217147827\n",
            "Epoch [2/10], Step [430/897], Loss: 0.01838175766170025\n",
            "Epoch [2/10], Step [440/897], Loss: 0.0155930882319808\n",
            "Epoch [2/10], Step [450/897], Loss: 0.08888021856546402\n",
            "Epoch [2/10], Step [460/897], Loss: 0.04357949271798134\n",
            "Epoch [2/10], Step [470/897], Loss: 0.004294405225664377\n",
            "Epoch [2/10], Step [480/897], Loss: 0.006491368170827627\n",
            "Epoch [2/10], Step [490/897], Loss: 0.0877600610256195\n",
            "Epoch [2/10], Step [500/897], Loss: 0.006050074938684702\n",
            "Epoch [2/10], Step [510/897], Loss: 0.011966322548687458\n",
            "Epoch [2/10], Step [520/897], Loss: 0.060591235756874084\n",
            "Epoch [2/10], Step [530/897], Loss: 0.03996145352721214\n",
            "Epoch [2/10], Step [540/897], Loss: 0.061857882887125015\n",
            "Epoch [2/10], Step [550/897], Loss: 0.31720760464668274\n",
            "Epoch [2/10], Step [560/897], Loss: 0.03465442359447479\n",
            "Epoch [2/10], Step [570/897], Loss: 0.04759655520319939\n",
            "Epoch [2/10], Step [580/897], Loss: 0.07917309552431107\n",
            "Epoch [2/10], Step [590/897], Loss: 0.09962116926908493\n",
            "Epoch [2/10], Step [600/897], Loss: 0.08031691610813141\n",
            "Epoch [2/10], Step [610/897], Loss: 0.21602503955364227\n",
            "Epoch [2/10], Step [620/897], Loss: 0.03197082132101059\n",
            "Epoch [2/10], Step [630/897], Loss: 0.04836540296673775\n",
            "Epoch [2/10], Step [640/897], Loss: 0.02989037148654461\n",
            "Epoch [2/10], Step [650/897], Loss: 0.03444162383675575\n",
            "Epoch [2/10], Step [660/897], Loss: 0.009585414081811905\n",
            "Epoch [2/10], Step [670/897], Loss: 0.04405108839273453\n",
            "Epoch [2/10], Step [680/897], Loss: 0.003737039864063263\n",
            "Epoch [2/10], Step [690/897], Loss: 0.1193387433886528\n",
            "Epoch [2/10], Step [700/897], Loss: 0.0186010729521513\n",
            "Epoch [2/10], Step [710/897], Loss: 0.0517575666308403\n",
            "Epoch [2/10], Step [720/897], Loss: 0.15109123289585114\n",
            "Epoch [2/10], Step [730/897], Loss: 0.05840105935931206\n",
            "Epoch [2/10], Step [740/897], Loss: 0.00922258011996746\n",
            "Epoch [2/10], Step [750/897], Loss: 0.06179061159491539\n",
            "Epoch [2/10], Step [760/897], Loss: 0.015017954632639885\n",
            "Epoch [2/10], Step [770/897], Loss: 0.07168533653020859\n",
            "Epoch [2/10], Step [780/897], Loss: 0.08368102461099625\n",
            "Epoch [2/10], Step [790/897], Loss: 0.014487683773040771\n",
            "Epoch [2/10], Step [800/897], Loss: 0.002902659587562084\n",
            "Epoch [2/10], Step [810/897], Loss: 0.028554411605000496\n",
            "Epoch [2/10], Step [820/897], Loss: 0.047977153211832047\n",
            "Epoch [2/10], Step [830/897], Loss: 0.057743337005376816\n",
            "Epoch [2/10], Step [840/897], Loss: 0.022445516660809517\n",
            "Epoch [2/10], Step [850/897], Loss: 0.008760550059378147\n",
            "Epoch [2/10], Step [860/897], Loss: 0.008180246688425541\n",
            "Epoch [2/10], Step [870/897], Loss: 0.13497936725616455\n",
            "Epoch [2/10], Step [880/897], Loss: 0.007066573016345501\n",
            "Epoch [2/10], Step [890/897], Loss: 0.1035943552851677\n",
            "Epoch [2/10], Train Loss: 0.05931803797085644, Train Accuracy: 0.9794651884391451\n",
            "Epoch [2/10], Validation Loss: 0.05747868648004472, Validation Accuracy: 0.9810709056682703\n",
            "Epoch [3/10], Step [10/897], Loss: 0.014809014275670052\n",
            "Epoch [3/10], Step [20/897], Loss: 0.005450609605759382\n",
            "Epoch [3/10], Step [30/897], Loss: 0.05139346420764923\n",
            "Epoch [3/10], Step [40/897], Loss: 0.09596284478902817\n",
            "Epoch [3/10], Step [50/897], Loss: 0.02777020074427128\n",
            "Epoch [3/10], Step [60/897], Loss: 0.39436501264572144\n",
            "Epoch [3/10], Step [70/897], Loss: 0.010759761556982994\n",
            "Epoch [3/10], Step [80/897], Loss: 0.03327760472893715\n",
            "Epoch [3/10], Step [90/897], Loss: 0.009942512959241867\n",
            "Epoch [3/10], Step [100/897], Loss: 0.005588733125478029\n",
            "Epoch [3/10], Step [110/897], Loss: 0.14591732621192932\n",
            "Epoch [3/10], Step [120/897], Loss: 0.04619435966014862\n",
            "Epoch [3/10], Step [130/897], Loss: 0.01119632925838232\n",
            "Epoch [3/10], Step [140/897], Loss: 0.036309462040662766\n",
            "Epoch [3/10], Step [150/897], Loss: 0.09207246452569962\n",
            "Epoch [3/10], Step [160/897], Loss: 0.015726547688245773\n",
            "Epoch [3/10], Step [170/897], Loss: 0.03935644030570984\n",
            "Epoch [3/10], Step [180/897], Loss: 0.0839344784617424\n",
            "Epoch [3/10], Step [190/897], Loss: 0.027647394686937332\n",
            "Epoch [3/10], Step [200/897], Loss: 0.016338983550667763\n",
            "Epoch [3/10], Step [210/897], Loss: 0.028006309643387794\n",
            "Epoch [3/10], Step [220/897], Loss: 0.20126095414161682\n",
            "Epoch [3/10], Step [230/897], Loss: 0.0038423906080424786\n",
            "Epoch [3/10], Step [240/897], Loss: 0.23096278309822083\n",
            "Epoch [3/10], Step [250/897], Loss: 0.04674649238586426\n",
            "Epoch [3/10], Step [260/897], Loss: 0.006271206773817539\n",
            "Epoch [3/10], Step [270/897], Loss: 0.06674405187368393\n",
            "Epoch [3/10], Step [280/897], Loss: 0.06260858476161957\n",
            "Epoch [3/10], Step [290/897], Loss: 0.009879276156425476\n",
            "Epoch [3/10], Step [300/897], Loss: 0.018785329535603523\n",
            "Epoch [3/10], Step [310/897], Loss: 0.03910086303949356\n",
            "Epoch [3/10], Step [320/897], Loss: 0.154388427734375\n",
            "Epoch [3/10], Step [330/897], Loss: 0.022812262177467346\n",
            "Epoch [3/10], Step [340/897], Loss: 0.17003899812698364\n",
            "Epoch [3/10], Step [350/897], Loss: 0.051460348069667816\n",
            "Epoch [3/10], Step [360/897], Loss: 0.30120110511779785\n",
            "Epoch [3/10], Step [370/897], Loss: 0.05179288238286972\n",
            "Epoch [3/10], Step [380/897], Loss: 0.017886118963360786\n",
            "Epoch [3/10], Step [390/897], Loss: 0.01194507721811533\n",
            "Epoch [3/10], Step [400/897], Loss: 0.00793523620814085\n",
            "Epoch [3/10], Step [410/897], Loss: 0.009251641109585762\n",
            "Epoch [3/10], Step [420/897], Loss: 0.06777403503656387\n",
            "Epoch [3/10], Step [430/897], Loss: 0.10703203082084656\n",
            "Epoch [3/10], Step [440/897], Loss: 0.05710114166140556\n",
            "Epoch [3/10], Step [450/897], Loss: 0.07430464029312134\n",
            "Epoch [3/10], Step [460/897], Loss: 0.016200637444853783\n",
            "Epoch [3/10], Step [470/897], Loss: 0.041017089039087296\n",
            "Epoch [3/10], Step [480/897], Loss: 0.09111779183149338\n",
            "Epoch [3/10], Step [490/897], Loss: 0.10682890564203262\n",
            "Epoch [3/10], Step [500/897], Loss: 0.1850760132074356\n",
            "Epoch [3/10], Step [510/897], Loss: 0.07033468782901764\n",
            "Epoch [3/10], Step [520/897], Loss: 0.018270449712872505\n",
            "Epoch [3/10], Step [530/897], Loss: 0.03263852000236511\n",
            "Epoch [3/10], Step [540/897], Loss: 0.0562145821750164\n",
            "Epoch [3/10], Step [550/897], Loss: 0.1434146761894226\n",
            "Epoch [3/10], Step [560/897], Loss: 0.05691729113459587\n",
            "Epoch [3/10], Step [570/897], Loss: 0.07539956271648407\n",
            "Epoch [3/10], Step [580/897], Loss: 0.04715512692928314\n",
            "Epoch [3/10], Step [590/897], Loss: 0.012008322402834892\n",
            "Epoch [3/10], Step [600/897], Loss: 0.05906157195568085\n",
            "Epoch [3/10], Step [610/897], Loss: 0.028607632964849472\n",
            "Epoch [3/10], Step [620/897], Loss: 0.12230042368173599\n",
            "Epoch [3/10], Step [630/897], Loss: 0.10165863484144211\n",
            "Epoch [3/10], Step [640/897], Loss: 0.011035899631679058\n",
            "Epoch [3/10], Step [650/897], Loss: 0.05971821770071983\n",
            "Epoch [3/10], Step [660/897], Loss: 0.12211691588163376\n",
            "Epoch [3/10], Step [670/897], Loss: 0.03148304298520088\n",
            "Epoch [3/10], Step [680/897], Loss: 0.0538843609392643\n",
            "Epoch [3/10], Step [690/897], Loss: 0.011818090453743935\n",
            "Epoch [3/10], Step [700/897], Loss: 0.005710249301046133\n",
            "Epoch [3/10], Step [710/897], Loss: 0.045025475323200226\n",
            "Epoch [3/10], Step [720/897], Loss: 0.053506266325712204\n",
            "Epoch [3/10], Step [730/897], Loss: 0.01899797096848488\n",
            "Epoch [3/10], Step [740/897], Loss: 0.01837632991373539\n",
            "Epoch [3/10], Step [750/897], Loss: 0.014454380609095097\n",
            "Epoch [3/10], Step [760/897], Loss: 0.022095467895269394\n",
            "Epoch [3/10], Step [770/897], Loss: 0.06654134392738342\n",
            "Epoch [3/10], Step [780/897], Loss: 0.09107466787099838\n",
            "Epoch [3/10], Step [790/897], Loss: 0.10639850050210953\n",
            "Epoch [3/10], Step [800/897], Loss: 0.027171265333890915\n",
            "Epoch [3/10], Step [810/897], Loss: 0.034027937799692154\n",
            "Epoch [3/10], Step [820/897], Loss: 0.007020914927124977\n",
            "Epoch [3/10], Step [830/897], Loss: 0.2145824432373047\n",
            "Epoch [3/10], Step [840/897], Loss: 0.17526881396770477\n",
            "Epoch [3/10], Step [850/897], Loss: 0.027303729206323624\n",
            "Epoch [3/10], Step [860/897], Loss: 0.009367628023028374\n",
            "Epoch [3/10], Step [870/897], Loss: 0.010965553112328053\n",
            "Epoch [3/10], Step [880/897], Loss: 0.06162393093109131\n",
            "Epoch [3/10], Step [890/897], Loss: 0.012938333675265312\n",
            "Epoch [3/10], Train Loss: 0.05303832426523933, Train Accuracy: 0.9823588885402503\n",
            "Epoch [3/10], Validation Loss: 0.05289150745355614, Validation Accuracy: 0.9829533570382765\n",
            "Epoch [4/10], Step [10/897], Loss: 0.05169522017240524\n",
            "Epoch [4/10], Step [20/897], Loss: 0.013141797855496407\n",
            "Epoch [4/10], Step [30/897], Loss: 0.014509154483675957\n",
            "Epoch [4/10], Step [40/897], Loss: 0.04682707414031029\n",
            "Epoch [4/10], Step [50/897], Loss: 0.05065365508198738\n",
            "Epoch [4/10], Step [60/897], Loss: 0.038634561002254486\n",
            "Epoch [4/10], Step [70/897], Loss: 0.026840586215257645\n",
            "Epoch [4/10], Step [80/897], Loss: 0.013162856921553612\n",
            "Epoch [4/10], Step [90/897], Loss: 0.016125081107020378\n",
            "Epoch [4/10], Step [100/897], Loss: 0.009749856777489185\n",
            "Epoch [4/10], Step [110/897], Loss: 0.04857990890741348\n",
            "Epoch [4/10], Step [120/897], Loss: 0.04444722458720207\n",
            "Epoch [4/10], Step [130/897], Loss: 0.007053392007946968\n",
            "Epoch [4/10], Step [140/897], Loss: 0.014297820627689362\n",
            "Epoch [4/10], Step [150/897], Loss: 0.06800137460231781\n",
            "Epoch [4/10], Step [160/897], Loss: 0.015181345865130424\n",
            "Epoch [4/10], Step [170/897], Loss: 0.013759087771177292\n",
            "Epoch [4/10], Step [180/897], Loss: 0.052932675927877426\n",
            "Epoch [4/10], Step [190/897], Loss: 0.006063926499336958\n",
            "Epoch [4/10], Step [200/897], Loss: 0.0422346256673336\n",
            "Epoch [4/10], Step [210/897], Loss: 0.027765819802880287\n",
            "Epoch [4/10], Step [220/897], Loss: 0.02093193680047989\n",
            "Epoch [4/10], Step [230/897], Loss: 0.0034445251803845167\n",
            "Epoch [4/10], Step [240/897], Loss: 0.010884376242756844\n",
            "Epoch [4/10], Step [250/897], Loss: 0.06307493150234222\n",
            "Epoch [4/10], Step [260/897], Loss: 0.014727453701198101\n",
            "Epoch [4/10], Step [270/897], Loss: 0.0286980289965868\n",
            "Epoch [4/10], Step [280/897], Loss: 0.05639339238405228\n",
            "Epoch [4/10], Step [290/897], Loss: 0.006216089706867933\n",
            "Epoch [4/10], Step [300/897], Loss: 0.02613392099738121\n",
            "Epoch [4/10], Step [310/897], Loss: 0.0778178796172142\n",
            "Epoch [4/10], Step [320/897], Loss: 0.05308839678764343\n",
            "Epoch [4/10], Step [330/897], Loss: 0.035630110651254654\n",
            "Epoch [4/10], Step [340/897], Loss: 0.02381262369453907\n",
            "Epoch [4/10], Step [350/897], Loss: 0.024676041677594185\n",
            "Epoch [4/10], Step [360/897], Loss: 0.033852994441986084\n",
            "Epoch [4/10], Step [370/897], Loss: 0.06563463807106018\n",
            "Epoch [4/10], Step [380/897], Loss: 0.007883358746767044\n",
            "Epoch [4/10], Step [390/897], Loss: 0.07126093655824661\n",
            "Epoch [4/10], Step [400/897], Loss: 0.007603136356920004\n",
            "Epoch [4/10], Step [410/897], Loss: 0.0027303986717015505\n",
            "Epoch [4/10], Step [420/897], Loss: 0.013853210024535656\n",
            "Epoch [4/10], Step [430/897], Loss: 0.007214264012873173\n",
            "Epoch [4/10], Step [440/897], Loss: 0.009717081673443317\n",
            "Epoch [4/10], Step [450/897], Loss: 0.05809706449508667\n",
            "Epoch [4/10], Step [460/897], Loss: 0.025372620671987534\n",
            "Epoch [4/10], Step [470/897], Loss: 0.09775157272815704\n",
            "Epoch [4/10], Step [480/897], Loss: 0.01872239261865616\n",
            "Epoch [4/10], Step [490/897], Loss: 0.0558842308819294\n",
            "Epoch [4/10], Step [500/897], Loss: 0.019563432782888412\n",
            "Epoch [4/10], Step [510/897], Loss: 0.19703181087970734\n",
            "Epoch [4/10], Step [520/897], Loss: 0.07328542321920395\n",
            "Epoch [4/10], Step [530/897], Loss: 0.06074473634362221\n",
            "Epoch [4/10], Step [540/897], Loss: 0.0295538529753685\n",
            "Epoch [4/10], Step [550/897], Loss: 0.010976503603160381\n",
            "Epoch [4/10], Step [560/897], Loss: 0.11637595295906067\n",
            "Epoch [4/10], Step [570/897], Loss: 0.04289589077234268\n",
            "Epoch [4/10], Step [580/897], Loss: 0.002612615702673793\n",
            "Epoch [4/10], Step [590/897], Loss: 0.01890866458415985\n",
            "Epoch [4/10], Step [600/897], Loss: 0.01677769422531128\n",
            "Epoch [4/10], Step [610/897], Loss: 0.036412376910448074\n",
            "Epoch [4/10], Step [620/897], Loss: 0.02531169354915619\n",
            "Epoch [4/10], Step [630/897], Loss: 0.01328478567302227\n",
            "Epoch [4/10], Step [640/897], Loss: 0.022161131724715233\n",
            "Epoch [4/10], Step [650/897], Loss: 0.006966589484363794\n",
            "Epoch [4/10], Step [660/897], Loss: 0.12187151610851288\n",
            "Epoch [4/10], Step [670/897], Loss: 0.10930103063583374\n",
            "Epoch [4/10], Step [680/897], Loss: 0.17111562192440033\n",
            "Epoch [4/10], Step [690/897], Loss: 0.08164796978235245\n",
            "Epoch [4/10], Step [700/897], Loss: 0.03707870841026306\n",
            "Epoch [4/10], Step [710/897], Loss: 0.06637713313102722\n",
            "Epoch [4/10], Step [720/897], Loss: 0.002096292097121477\n",
            "Epoch [4/10], Step [730/897], Loss: 0.06088411062955856\n",
            "Epoch [4/10], Step [740/897], Loss: 0.026330647990107536\n",
            "Epoch [4/10], Step [750/897], Loss: 0.02880302257835865\n",
            "Epoch [4/10], Step [760/897], Loss: 0.010804766789078712\n",
            "Epoch [4/10], Step [770/897], Loss: 0.001745371613651514\n",
            "Epoch [4/10], Step [780/897], Loss: 0.030026588588953018\n",
            "Epoch [4/10], Step [790/897], Loss: 0.021468928083777428\n",
            "Epoch [4/10], Step [800/897], Loss: 0.07919161021709442\n",
            "Epoch [4/10], Step [810/897], Loss: 0.005672087427228689\n",
            "Epoch [4/10], Step [820/897], Loss: 0.19247271120548248\n",
            "Epoch [4/10], Step [830/897], Loss: 0.0371660552918911\n",
            "Epoch [4/10], Step [840/897], Loss: 0.09172995388507843\n",
            "Epoch [4/10], Step [850/897], Loss: 0.015574396587908268\n",
            "Epoch [4/10], Step [860/897], Loss: 0.013254783116281033\n",
            "Epoch [4/10], Step [870/897], Loss: 0.008164131082594395\n",
            "Epoch [4/10], Step [880/897], Loss: 0.004890544805675745\n",
            "Epoch [4/10], Step [890/897], Loss: 0.12948305904865265\n",
            "Epoch [4/10], Train Loss: 0.050487425684053636, Train Accuracy: 0.9829167102464874\n",
            "Epoch [4/10], Validation Loss: 0.053004962820435146, Validation Accuracy: 0.9814892281949383\n",
            "Epoch [5/10], Step [10/897], Loss: 0.007130760233849287\n",
            "Epoch [5/10], Step [20/897], Loss: 0.005436392035335302\n",
            "Epoch [5/10], Step [30/897], Loss: 0.01119382306933403\n",
            "Epoch [5/10], Step [40/897], Loss: 0.03269203007221222\n",
            "Epoch [5/10], Step [50/897], Loss: 0.021849174052476883\n",
            "Epoch [5/10], Step [60/897], Loss: 0.01338212564587593\n",
            "Epoch [5/10], Step [70/897], Loss: 0.004617422819137573\n",
            "Epoch [5/10], Step [80/897], Loss: 0.018474265933036804\n",
            "Epoch [5/10], Step [90/897], Loss: 0.007409151643514633\n",
            "Epoch [5/10], Step [100/897], Loss: 0.0029635531827807426\n",
            "Epoch [5/10], Step [110/897], Loss: 0.06633808463811874\n",
            "Epoch [5/10], Step [120/897], Loss: 0.10475140064954758\n",
            "Epoch [5/10], Step [130/897], Loss: 0.03443491831421852\n",
            "Epoch [5/10], Step [140/897], Loss: 0.019152967259287834\n",
            "Epoch [5/10], Step [150/897], Loss: 0.05842386931180954\n",
            "Epoch [5/10], Step [160/897], Loss: 0.016469718888401985\n",
            "Epoch [5/10], Step [170/897], Loss: 0.06917641311883926\n",
            "Epoch [5/10], Step [180/897], Loss: 0.042612385004758835\n",
            "Epoch [5/10], Step [190/897], Loss: 0.018398214131593704\n",
            "Epoch [5/10], Step [200/897], Loss: 0.009205707348883152\n",
            "Epoch [5/10], Step [210/897], Loss: 0.004793531261384487\n",
            "Epoch [5/10], Step [220/897], Loss: 0.007307914551347494\n",
            "Epoch [5/10], Step [230/897], Loss: 0.015490975230932236\n",
            "Epoch [5/10], Step [240/897], Loss: 0.008449127897620201\n",
            "Epoch [5/10], Step [250/897], Loss: 0.02161213755607605\n",
            "Epoch [5/10], Step [260/897], Loss: 0.03767973557114601\n",
            "Epoch [5/10], Step [270/897], Loss: 0.05025862529873848\n",
            "Epoch [5/10], Step [280/897], Loss: 0.01036357507109642\n",
            "Epoch [5/10], Step [290/897], Loss: 0.015402700752019882\n",
            "Epoch [5/10], Step [300/897], Loss: 0.025314996019005775\n",
            "Epoch [5/10], Step [310/897], Loss: 0.03357117250561714\n",
            "Epoch [5/10], Step [320/897], Loss: 0.11216786503791809\n",
            "Epoch [5/10], Step [330/897], Loss: 0.09440114349126816\n",
            "Epoch [5/10], Step [340/897], Loss: 0.009468873031437397\n",
            "Epoch [5/10], Step [350/897], Loss: 0.024312598630785942\n",
            "Epoch [5/10], Step [360/897], Loss: 0.06525868922472\n",
            "Epoch [5/10], Step [370/897], Loss: 0.015379813499748707\n",
            "Epoch [5/10], Step [380/897], Loss: 0.006991040892899036\n",
            "Epoch [5/10], Step [390/897], Loss: 0.009193162433803082\n",
            "Epoch [5/10], Step [400/897], Loss: 0.1409202218055725\n",
            "Epoch [5/10], Step [410/897], Loss: 0.04966306686401367\n",
            "Epoch [5/10], Step [420/897], Loss: 0.04955599829554558\n",
            "Epoch [5/10], Step [430/897], Loss: 0.008688663132488728\n",
            "Epoch [5/10], Step [440/897], Loss: 0.0350002683699131\n",
            "Epoch [5/10], Step [450/897], Loss: 0.0030711444560438395\n",
            "Epoch [5/10], Step [460/897], Loss: 0.020807338878512383\n",
            "Epoch [5/10], Step [470/897], Loss: 0.03592672199010849\n",
            "Epoch [5/10], Step [480/897], Loss: 0.02868889831006527\n",
            "Epoch [5/10], Step [490/897], Loss: 0.08842048048973083\n",
            "Epoch [5/10], Step [500/897], Loss: 0.085393525660038\n",
            "Epoch [5/10], Step [510/897], Loss: 0.022946571931242943\n",
            "Epoch [5/10], Step [520/897], Loss: 0.15934987366199493\n",
            "Epoch [5/10], Step [530/897], Loss: 0.22903752326965332\n",
            "Epoch [5/10], Step [540/897], Loss: 0.02605857327580452\n",
            "Epoch [5/10], Step [550/897], Loss: 0.10270866006612778\n",
            "Epoch [5/10], Step [560/897], Loss: 0.03356960415840149\n",
            "Epoch [5/10], Step [570/897], Loss: 0.08095312118530273\n",
            "Epoch [5/10], Step [580/897], Loss: 0.008337920531630516\n",
            "Epoch [5/10], Step [590/897], Loss: 0.047214195132255554\n",
            "Epoch [5/10], Step [600/897], Loss: 0.016819005832076073\n",
            "Epoch [5/10], Step [610/897], Loss: 0.008092368021607399\n",
            "Epoch [5/10], Step [620/897], Loss: 0.07378007471561432\n",
            "Epoch [5/10], Step [630/897], Loss: 0.01614150032401085\n",
            "Epoch [5/10], Step [640/897], Loss: 0.006226608529686928\n",
            "Epoch [5/10], Step [650/897], Loss: 0.07140612602233887\n",
            "Epoch [5/10], Step [660/897], Loss: 0.01146041601896286\n",
            "Epoch [5/10], Step [670/897], Loss: 0.006326897535473108\n",
            "Epoch [5/10], Step [680/897], Loss: 0.019472690299153328\n",
            "Epoch [5/10], Step [690/897], Loss: 0.009168514981865883\n",
            "Epoch [5/10], Step [700/897], Loss: 0.20002850890159607\n",
            "Epoch [5/10], Step [710/897], Loss: 0.26522988080978394\n",
            "Epoch [5/10], Step [720/897], Loss: 0.05579523369669914\n",
            "Epoch [5/10], Step [730/897], Loss: 0.021507054567337036\n",
            "Epoch [5/10], Step [740/897], Loss: 0.15663938224315643\n",
            "Epoch [5/10], Step [750/897], Loss: 0.08840714395046234\n",
            "Epoch [5/10], Step [760/897], Loss: 0.09652016311883926\n",
            "Epoch [5/10], Step [770/897], Loss: 0.009763938374817371\n",
            "Epoch [5/10], Step [780/897], Loss: 0.04794884845614433\n",
            "Epoch [5/10], Step [790/897], Loss: 0.0055796485394239426\n",
            "Epoch [5/10], Step [800/897], Loss: 0.11076141893863678\n",
            "Epoch [5/10], Step [810/897], Loss: 0.14165958762168884\n",
            "Epoch [5/10], Step [820/897], Loss: 0.09448347240686417\n",
            "Epoch [5/10], Step [830/897], Loss: 0.04507714509963989\n",
            "Epoch [5/10], Step [840/897], Loss: 0.03294970467686653\n",
            "Epoch [5/10], Step [850/897], Loss: 0.012008854188024998\n",
            "Epoch [5/10], Step [860/897], Loss: 0.050404004752635956\n",
            "Epoch [5/10], Step [870/897], Loss: 0.010043280199170113\n",
            "Epoch [5/10], Step [880/897], Loss: 0.0034848195500671864\n",
            "Epoch [5/10], Step [890/897], Loss: 0.08449579775333405\n",
            "Epoch [5/10], Train Loss: 0.046723224278272066, Train Accuracy: 0.9846947669351184\n",
            "Epoch [5/10], Validation Loss: 0.06058382811698359, Validation Accuracy: 0.9805480025099351\n",
            "Epoch [6/10], Step [10/897], Loss: 0.022657621651887894\n",
            "Epoch [6/10], Step [20/897], Loss: 0.0053977444767951965\n",
            "Epoch [6/10], Step [30/897], Loss: 0.05335429310798645\n",
            "Epoch [6/10], Step [40/897], Loss: 0.022156625986099243\n",
            "Epoch [6/10], Step [50/897], Loss: 0.07151039689779282\n",
            "Epoch [6/10], Step [60/897], Loss: 0.0682206004858017\n",
            "Epoch [6/10], Step [70/897], Loss: 0.00846246536821127\n",
            "Epoch [6/10], Step [80/897], Loss: 0.004011599812656641\n",
            "Epoch [6/10], Step [90/897], Loss: 0.11194968223571777\n",
            "Epoch [6/10], Step [100/897], Loss: 0.013607637025415897\n",
            "Epoch [6/10], Step [110/897], Loss: 0.1671663373708725\n",
            "Epoch [6/10], Step [120/897], Loss: 0.005149691831320524\n",
            "Epoch [6/10], Step [130/897], Loss: 0.00856601633131504\n",
            "Epoch [6/10], Step [140/897], Loss: 0.047788143157958984\n",
            "Epoch [6/10], Step [150/897], Loss: 0.03232072666287422\n",
            "Epoch [6/10], Step [160/897], Loss: 0.09144279360771179\n",
            "Epoch [6/10], Step [170/897], Loss: 0.16913677752017975\n",
            "Epoch [6/10], Step [180/897], Loss: 0.009674269706010818\n",
            "Epoch [6/10], Step [190/897], Loss: 0.029164645820856094\n",
            "Epoch [6/10], Step [200/897], Loss: 0.0053155445493757725\n",
            "Epoch [6/10], Step [210/897], Loss: 0.08590269833803177\n",
            "Epoch [6/10], Step [220/897], Loss: 0.011794481426477432\n",
            "Epoch [6/10], Step [230/897], Loss: 0.008626457303762436\n",
            "Epoch [6/10], Step [240/897], Loss: 0.0326656810939312\n",
            "Epoch [6/10], Step [250/897], Loss: 0.0043189688585698605\n",
            "Epoch [6/10], Step [260/897], Loss: 0.056874558329582214\n",
            "Epoch [6/10], Step [270/897], Loss: 0.025471802800893784\n",
            "Epoch [6/10], Step [280/897], Loss: 0.02334214188158512\n",
            "Epoch [6/10], Step [290/897], Loss: 0.06340336799621582\n",
            "Epoch [6/10], Step [300/897], Loss: 0.0034093528520315886\n",
            "Epoch [6/10], Step [310/897], Loss: 0.009688305668532848\n",
            "Epoch [6/10], Step [320/897], Loss: 0.1985929012298584\n",
            "Epoch [6/10], Step [330/897], Loss: 0.02820179983973503\n",
            "Epoch [6/10], Step [340/897], Loss: 0.06884366273880005\n",
            "Epoch [6/10], Step [350/897], Loss: 0.021565448492765427\n",
            "Epoch [6/10], Step [360/897], Loss: 0.010325687006115913\n",
            "Epoch [6/10], Step [370/897], Loss: 0.08972714096307755\n",
            "Epoch [6/10], Step [380/897], Loss: 0.008931895717978477\n",
            "Epoch [6/10], Step [390/897], Loss: 0.04841619357466698\n",
            "Epoch [6/10], Step [400/897], Loss: 0.021350860595703125\n",
            "Epoch [6/10], Step [410/897], Loss: 0.011706233024597168\n",
            "Epoch [6/10], Step [420/897], Loss: 0.01268357690423727\n",
            "Epoch [6/10], Step [430/897], Loss: 0.0012858130503445864\n",
            "Epoch [6/10], Step [440/897], Loss: 0.02218250185251236\n",
            "Epoch [6/10], Step [450/897], Loss: 0.01650291308760643\n",
            "Epoch [6/10], Step [460/897], Loss: 0.013248690403997898\n",
            "Epoch [6/10], Step [470/897], Loss: 0.045497067272663116\n",
            "Epoch [6/10], Step [480/897], Loss: 0.014600704424083233\n",
            "Epoch [6/10], Step [490/897], Loss: 0.024391377344727516\n",
            "Epoch [6/10], Step [500/897], Loss: 0.0135259460657835\n",
            "Epoch [6/10], Step [510/897], Loss: 0.02342939004302025\n",
            "Epoch [6/10], Step [520/897], Loss: 0.0117114819586277\n",
            "Epoch [6/10], Step [530/897], Loss: 0.23181381821632385\n",
            "Epoch [6/10], Step [540/897], Loss: 0.0062558031640946865\n",
            "Epoch [6/10], Step [550/897], Loss: 0.01323146652430296\n",
            "Epoch [6/10], Step [560/897], Loss: 0.1230141669511795\n",
            "Epoch [6/10], Step [570/897], Loss: 0.017402397468686104\n",
            "Epoch [6/10], Step [580/897], Loss: 0.11542324721813202\n",
            "Epoch [6/10], Step [590/897], Loss: 0.004299677908420563\n",
            "Epoch [6/10], Step [600/897], Loss: 0.13118411600589752\n",
            "Epoch [6/10], Step [610/897], Loss: 0.10804872959852219\n",
            "Epoch [6/10], Step [620/897], Loss: 0.06256752461194992\n",
            "Epoch [6/10], Step [630/897], Loss: 0.01121154148131609\n",
            "Epoch [6/10], Step [640/897], Loss: 0.02628585696220398\n",
            "Epoch [6/10], Step [650/897], Loss: 0.01523782778531313\n",
            "Epoch [6/10], Step [660/897], Loss: 0.015613628551363945\n",
            "Epoch [6/10], Step [670/897], Loss: 0.01687942072749138\n",
            "Epoch [6/10], Step [680/897], Loss: 0.036397382616996765\n",
            "Epoch [6/10], Step [690/897], Loss: 0.012228317558765411\n",
            "Epoch [6/10], Step [700/897], Loss: 0.017343807965517044\n",
            "Epoch [6/10], Step [710/897], Loss: 0.036484479904174805\n",
            "Epoch [6/10], Step [720/897], Loss: 0.016675150021910667\n",
            "Epoch [6/10], Step [730/897], Loss: 0.008051370270550251\n",
            "Epoch [6/10], Step [740/897], Loss: 0.008394455537199974\n",
            "Epoch [6/10], Step [750/897], Loss: 0.05105259642004967\n",
            "Epoch [6/10], Step [760/897], Loss: 0.02136974036693573\n",
            "Epoch [6/10], Step [770/897], Loss: 0.15173499286174774\n",
            "Epoch [6/10], Step [780/897], Loss: 0.029761236160993576\n",
            "Epoch [6/10], Step [790/897], Loss: 0.009116400964558125\n",
            "Epoch [6/10], Step [800/897], Loss: 0.028716158121824265\n",
            "Epoch [6/10], Step [810/897], Loss: 0.008178307674825191\n",
            "Epoch [6/10], Step [820/897], Loss: 0.013000820763409138\n",
            "Epoch [6/10], Step [830/897], Loss: 0.013115563429892063\n",
            "Epoch [6/10], Step [840/897], Loss: 0.07283874601125717\n",
            "Epoch [6/10], Step [850/897], Loss: 0.04073184356093407\n",
            "Epoch [6/10], Step [860/897], Loss: 0.023354902863502502\n",
            "Epoch [6/10], Step [870/897], Loss: 0.02152022160589695\n",
            "Epoch [6/10], Step [880/897], Loss: 0.024922695010900497\n",
            "Epoch [6/10], Step [890/897], Loss: 0.0725942999124527\n",
            "Epoch [6/10], Train Loss: 0.04475394291390531, Train Accuracy: 0.9859150019175121\n",
            "Epoch [6/10], Validation Loss: 0.046071601042642345, Validation Accuracy: 0.9857770340932859\n",
            "Epoch [7/10], Step [10/897], Loss: 0.02374383620917797\n",
            "Epoch [7/10], Step [20/897], Loss: 0.13031625747680664\n",
            "Epoch [7/10], Step [30/897], Loss: 0.009316411800682545\n",
            "Epoch [7/10], Step [40/897], Loss: 0.003113195300102234\n",
            "Epoch [7/10], Step [50/897], Loss: 0.0338822677731514\n",
            "Epoch [7/10], Step [60/897], Loss: 0.0017674198606982827\n",
            "Epoch [7/10], Step [70/897], Loss: 0.036149874329566956\n",
            "Epoch [7/10], Step [80/897], Loss: 0.006115343887358904\n",
            "Epoch [7/10], Step [90/897], Loss: 0.05145707726478577\n",
            "Epoch [7/10], Step [100/897], Loss: 0.00854396540671587\n",
            "Epoch [7/10], Step [110/897], Loss: 0.007800137158483267\n",
            "Epoch [7/10], Step [120/897], Loss: 0.023281455039978027\n",
            "Epoch [7/10], Step [130/897], Loss: 0.05001186579465866\n",
            "Epoch [7/10], Step [140/897], Loss: 0.08837372809648514\n",
            "Epoch [7/10], Step [150/897], Loss: 0.0057106767781078815\n",
            "Epoch [7/10], Step [160/897], Loss: 0.011489590629935265\n",
            "Epoch [7/10], Step [170/897], Loss: 0.013143789023160934\n",
            "Epoch [7/10], Step [180/897], Loss: 0.027957819402217865\n",
            "Epoch [7/10], Step [190/897], Loss: 0.13139811158180237\n",
            "Epoch [7/10], Step [200/897], Loss: 0.07913000881671906\n",
            "Epoch [7/10], Step [210/897], Loss: 0.014221151359379292\n",
            "Epoch [7/10], Step [220/897], Loss: 0.0521550327539444\n",
            "Epoch [7/10], Step [230/897], Loss: 0.015468012541532516\n",
            "Epoch [7/10], Step [240/897], Loss: 0.01888694055378437\n",
            "Epoch [7/10], Step [250/897], Loss: 0.15617859363555908\n",
            "Epoch [7/10], Step [260/897], Loss: 0.03380510210990906\n",
            "Epoch [7/10], Step [270/897], Loss: 0.004666799213737249\n",
            "Epoch [7/10], Step [280/897], Loss: 0.034607257694005966\n",
            "Epoch [7/10], Step [290/897], Loss: 0.009381876327097416\n",
            "Epoch [7/10], Step [300/897], Loss: 0.016739951446652412\n",
            "Epoch [7/10], Step [310/897], Loss: 0.03598814830183983\n",
            "Epoch [7/10], Step [320/897], Loss: 0.05334104970097542\n",
            "Epoch [7/10], Step [330/897], Loss: 0.006588434334844351\n",
            "Epoch [7/10], Step [340/897], Loss: 0.03158298879861832\n",
            "Epoch [7/10], Step [350/897], Loss: 0.0105131845921278\n",
            "Epoch [7/10], Step [360/897], Loss: 0.04843701049685478\n",
            "Epoch [7/10], Step [370/897], Loss: 0.07589365541934967\n",
            "Epoch [7/10], Step [380/897], Loss: 0.0368160717189312\n",
            "Epoch [7/10], Step [390/897], Loss: 0.012398541904985905\n",
            "Epoch [7/10], Step [400/897], Loss: 0.02074483595788479\n",
            "Epoch [7/10], Step [410/897], Loss: 0.03713260218501091\n",
            "Epoch [7/10], Step [420/897], Loss: 0.06343974173069\n",
            "Epoch [7/10], Step [430/897], Loss: 0.02607920952141285\n",
            "Epoch [7/10], Step [440/897], Loss: 0.012013502418994904\n",
            "Epoch [7/10], Step [450/897], Loss: 0.006899538915604353\n",
            "Epoch [7/10], Step [460/897], Loss: 0.08303998410701752\n",
            "Epoch [7/10], Step [470/897], Loss: 0.01912035420536995\n",
            "Epoch [7/10], Step [480/897], Loss: 0.003690341953188181\n",
            "Epoch [7/10], Step [490/897], Loss: 0.004521538037806749\n",
            "Epoch [7/10], Step [500/897], Loss: 0.05667717382311821\n",
            "Epoch [7/10], Step [510/897], Loss: 0.09824667125940323\n",
            "Epoch [7/10], Step [520/897], Loss: 0.2307158261537552\n",
            "Epoch [7/10], Step [530/897], Loss: 0.023671070113778114\n",
            "Epoch [7/10], Step [540/897], Loss: 0.06753521412611008\n",
            "Epoch [7/10], Step [550/897], Loss: 0.03150322288274765\n",
            "Epoch [7/10], Step [560/897], Loss: 0.012424360029399395\n",
            "Epoch [7/10], Step [570/897], Loss: 0.0406307578086853\n",
            "Epoch [7/10], Step [580/897], Loss: 0.20514458417892456\n",
            "Epoch [7/10], Step [590/897], Loss: 0.1145293191075325\n",
            "Epoch [7/10], Step [600/897], Loss: 0.0029665918555110693\n",
            "Epoch [7/10], Step [610/897], Loss: 0.00723720109090209\n",
            "Epoch [7/10], Step [620/897], Loss: 0.08637047559022903\n",
            "Epoch [7/10], Step [630/897], Loss: 0.007070974912494421\n",
            "Epoch [7/10], Step [640/897], Loss: 0.17652828991413116\n",
            "Epoch [7/10], Step [650/897], Loss: 0.009656026028096676\n",
            "Epoch [7/10], Step [660/897], Loss: 0.0068359654396772385\n",
            "Epoch [7/10], Step [670/897], Loss: 0.10727299004793167\n",
            "Epoch [7/10], Step [680/897], Loss: 0.008880041539669037\n",
            "Epoch [7/10], Step [690/897], Loss: 0.023974644020199776\n",
            "Epoch [7/10], Step [700/897], Loss: 0.035216283053159714\n",
            "Epoch [7/10], Step [710/897], Loss: 0.06000145897269249\n",
            "Epoch [7/10], Step [720/897], Loss: 0.04880282282829285\n",
            "Epoch [7/10], Step [730/897], Loss: 0.013273140415549278\n",
            "Epoch [7/10], Step [740/897], Loss: 0.06477601826190948\n",
            "Epoch [7/10], Step [750/897], Loss: 0.0474502332508564\n",
            "Epoch [7/10], Step [760/897], Loss: 0.09477446973323822\n",
            "Epoch [7/10], Step [770/897], Loss: 0.1225246787071228\n",
            "Epoch [7/10], Step [780/897], Loss: 0.008423936553299427\n",
            "Epoch [7/10], Step [790/897], Loss: 0.006077042315155268\n",
            "Epoch [7/10], Step [800/897], Loss: 0.0036919801495969296\n",
            "Epoch [7/10], Step [810/897], Loss: 0.08509120345115662\n",
            "Epoch [7/10], Step [820/897], Loss: 0.016526084393262863\n",
            "Epoch [7/10], Step [830/897], Loss: 0.07689941674470901\n",
            "Epoch [7/10], Step [840/897], Loss: 0.002608145587146282\n",
            "Epoch [7/10], Step [850/897], Loss: 0.1739618480205536\n",
            "Epoch [7/10], Step [860/897], Loss: 0.09828400611877441\n",
            "Epoch [7/10], Step [870/897], Loss: 0.14554354548454285\n",
            "Epoch [7/10], Step [880/897], Loss: 0.06788277626037598\n",
            "Epoch [7/10], Step [890/897], Loss: 0.005602315533906221\n",
            "Epoch [7/10], Train Loss: 0.041995691874250354, Train Accuracy: 0.9869957814733465\n",
            "Epoch [7/10], Validation Loss: 0.05552244460055243, Validation Accuracy: 0.9819075507216064\n",
            "Epoch [8/10], Step [10/897], Loss: 0.012412581592798233\n",
            "Epoch [8/10], Step [20/897], Loss: 0.09032227843999863\n",
            "Epoch [8/10], Step [30/897], Loss: 0.001927048433572054\n",
            "Epoch [8/10], Step [40/897], Loss: 0.06404541432857513\n",
            "Epoch [8/10], Step [50/897], Loss: 0.051324594765901566\n",
            "Epoch [8/10], Step [60/897], Loss: 0.01250521745532751\n",
            "Epoch [8/10], Step [70/897], Loss: 0.0063134608790278435\n",
            "Epoch [8/10], Step [80/897], Loss: 0.009014623239636421\n",
            "Epoch [8/10], Step [90/897], Loss: 0.020197903737425804\n",
            "Epoch [8/10], Step [100/897], Loss: 0.007961724884808064\n",
            "Epoch [8/10], Step [110/897], Loss: 0.007109738886356354\n",
            "Epoch [8/10], Step [120/897], Loss: 0.01638529635965824\n",
            "Epoch [8/10], Step [130/897], Loss: 0.018566850572824478\n",
            "Epoch [8/10], Step [140/897], Loss: 0.004329613875597715\n",
            "Epoch [8/10], Step [150/897], Loss: 0.0053552016615867615\n",
            "Epoch [8/10], Step [160/897], Loss: 0.025115201249718666\n",
            "Epoch [8/10], Step [170/897], Loss: 0.0028342544101178646\n",
            "Epoch [8/10], Step [180/897], Loss: 0.04744477942585945\n",
            "Epoch [8/10], Step [190/897], Loss: 0.005785547662526369\n",
            "Epoch [8/10], Step [200/897], Loss: 0.14992684125900269\n",
            "Epoch [8/10], Step [210/897], Loss: 0.06916335970163345\n",
            "Epoch [8/10], Step [220/897], Loss: 0.029125535860657692\n",
            "Epoch [8/10], Step [230/897], Loss: 0.0072790649719536304\n",
            "Epoch [8/10], Step [240/897], Loss: 0.020236164331436157\n",
            "Epoch [8/10], Step [250/897], Loss: 0.044013578444719315\n",
            "Epoch [8/10], Step [260/897], Loss: 0.07041065394878387\n",
            "Epoch [8/10], Step [270/897], Loss: 0.040858346968889236\n",
            "Epoch [8/10], Step [280/897], Loss: 0.010929537005722523\n",
            "Epoch [8/10], Step [290/897], Loss: 0.004065957851707935\n",
            "Epoch [8/10], Step [300/897], Loss: 0.09891621023416519\n",
            "Epoch [8/10], Step [310/897], Loss: 0.06600534170866013\n",
            "Epoch [8/10], Step [320/897], Loss: 0.29575487971305847\n",
            "Epoch [8/10], Step [330/897], Loss: 0.01931927353143692\n",
            "Epoch [8/10], Step [340/897], Loss: 0.009110229089856148\n",
            "Epoch [8/10], Step [350/897], Loss: 0.11251962184906006\n",
            "Epoch [8/10], Step [360/897], Loss: 0.004625432193279266\n",
            "Epoch [8/10], Step [370/897], Loss: 0.1026216521859169\n",
            "Epoch [8/10], Step [380/897], Loss: 0.049842674285173416\n",
            "Epoch [8/10], Step [390/897], Loss: 0.016895994544029236\n",
            "Epoch [8/10], Step [400/897], Loss: 0.027440445497632027\n",
            "Epoch [8/10], Step [410/897], Loss: 0.05062645673751831\n",
            "Epoch [8/10], Step [420/897], Loss: 0.03246111050248146\n",
            "Epoch [8/10], Step [430/897], Loss: 0.011042239144444466\n",
            "Epoch [8/10], Step [440/897], Loss: 0.12043291330337524\n",
            "Epoch [8/10], Step [450/897], Loss: 0.021144336089491844\n",
            "Epoch [8/10], Step [460/897], Loss: 0.004995477385818958\n",
            "Epoch [8/10], Step [470/897], Loss: 0.028471024706959724\n",
            "Epoch [8/10], Step [480/897], Loss: 0.02477293461561203\n",
            "Epoch [8/10], Step [490/897], Loss: 0.03107602335512638\n",
            "Epoch [8/10], Step [500/897], Loss: 0.030825087800621986\n",
            "Epoch [8/10], Step [510/897], Loss: 0.026746336370706558\n",
            "Epoch [8/10], Step [520/897], Loss: 0.023565402254462242\n",
            "Epoch [8/10], Step [530/897], Loss: 0.010299975983798504\n",
            "Epoch [8/10], Step [540/897], Loss: 0.2151106894016266\n",
            "Epoch [8/10], Step [550/897], Loss: 0.09203647822141647\n",
            "Epoch [8/10], Step [560/897], Loss: 0.29396963119506836\n",
            "Epoch [8/10], Step [570/897], Loss: 0.03650621324777603\n",
            "Epoch [8/10], Step [580/897], Loss: 0.012132180854678154\n",
            "Epoch [8/10], Step [590/897], Loss: 0.11197478324174881\n",
            "Epoch [8/10], Step [600/897], Loss: 0.0900498479604721\n",
            "Epoch [8/10], Step [610/897], Loss: 0.026798347011208534\n",
            "Epoch [8/10], Step [620/897], Loss: 0.008189606480300426\n",
            "Epoch [8/10], Step [630/897], Loss: 0.006094308570027351\n",
            "Epoch [8/10], Step [640/897], Loss: 0.005625480320304632\n",
            "Epoch [8/10], Step [650/897], Loss: 0.008019449189305305\n",
            "Epoch [8/10], Step [660/897], Loss: 0.05575626343488693\n",
            "Epoch [8/10], Step [670/897], Loss: 0.0019654342904686928\n",
            "Epoch [8/10], Step [680/897], Loss: 0.005672428756952286\n",
            "Epoch [8/10], Step [690/897], Loss: 0.013781481422483921\n",
            "Epoch [8/10], Step [700/897], Loss: 0.06333330273628235\n",
            "Epoch [8/10], Step [710/897], Loss: 0.006495443172752857\n",
            "Epoch [8/10], Step [720/897], Loss: 0.016080308705568314\n",
            "Epoch [8/10], Step [730/897], Loss: 0.031015250831842422\n",
            "Epoch [8/10], Step [740/897], Loss: 0.010579789988696575\n",
            "Epoch [8/10], Step [750/897], Loss: 0.001612439751625061\n",
            "Epoch [8/10], Step [760/897], Loss: 0.139908567070961\n",
            "Epoch [8/10], Step [770/897], Loss: 0.059655480086803436\n",
            "Epoch [8/10], Step [780/897], Loss: 0.05302378162741661\n",
            "Epoch [8/10], Step [790/897], Loss: 0.06327556073665619\n",
            "Epoch [8/10], Step [800/897], Loss: 0.02216676063835621\n",
            "Epoch [8/10], Step [810/897], Loss: 0.013295776210725307\n",
            "Epoch [8/10], Step [820/897], Loss: 0.06261222064495087\n",
            "Epoch [8/10], Step [830/897], Loss: 0.007730261422693729\n",
            "Epoch [8/10], Step [840/897], Loss: 0.08247110247612\n",
            "Epoch [8/10], Step [850/897], Loss: 0.06287875771522522\n",
            "Epoch [8/10], Step [860/897], Loss: 0.020181871950626373\n",
            "Epoch [8/10], Step [870/897], Loss: 0.018113555386662483\n",
            "Epoch [8/10], Step [880/897], Loss: 0.012574403546750546\n",
            "Epoch [8/10], Step [890/897], Loss: 0.02149537205696106\n",
            "Epoch [8/10], Train Loss: 0.04121897013424096, Train Accuracy: 0.9868214621901474\n",
            "Epoch [8/10], Validation Loss: 0.04574095792788078, Validation Accuracy: 0.9852541309349508\n",
            "Epoch [9/10], Step [10/897], Loss: 0.1543855220079422\n",
            "Epoch [9/10], Step [20/897], Loss: 0.06574509292840958\n",
            "Epoch [9/10], Step [30/897], Loss: 0.08439809828996658\n",
            "Epoch [9/10], Step [40/897], Loss: 0.07371175289154053\n",
            "Epoch [9/10], Step [50/897], Loss: 0.01774396002292633\n",
            "Epoch [9/10], Step [60/897], Loss: 0.003977720160037279\n",
            "Epoch [9/10], Step [70/897], Loss: 0.023544732481241226\n",
            "Epoch [9/10], Step [80/897], Loss: 0.01330962311476469\n",
            "Epoch [9/10], Step [90/897], Loss: 0.11339572072029114\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-87625716df52>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the RBF model with initialized centers and sigmas\n",
        "model = RBFModel(10000, 1000, 2, all_centers, [sigma])\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "best_val_accuracy = 0.0  # Track the best validation accuracy\n",
        "best_model_state = None  # Track the state of the best model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
        "        batch_x, batch_y = batch_x.to(torch.float32).to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_x)\n",
        "        loss = criterion(output, batch_y.squeeze().to(torch.long))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        correct_predictions += (predicted == batch_y.squeeze()).sum().item()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct_predictions / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss}, Train Accuracy: {accuracy}')\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "    # Save the model if validation accuracy improves\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "# Save the best model state\n",
        "torch.save(best_model_state, 'best_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVIUzsOeEUJm",
        "outputId": "33d2f090-ce09-493e-e7db-a600434d99f3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/897], Loss: 0.07019159197807312\n",
            "Epoch [1/10], Step [20/897], Loss: 0.02855178713798523\n",
            "Epoch [1/10], Step [30/897], Loss: 0.009279455058276653\n",
            "Epoch [1/10], Step [40/897], Loss: 0.0691031813621521\n",
            "Epoch [1/10], Step [50/897], Loss: 0.0025888867676258087\n",
            "Epoch [1/10], Step [60/897], Loss: 0.057388562709093094\n",
            "Epoch [1/10], Step [70/897], Loss: 0.008315368555486202\n",
            "Epoch [1/10], Step [80/897], Loss: 0.006657800637185574\n",
            "Epoch [1/10], Step [90/897], Loss: 0.04611688107252121\n",
            "Epoch [1/10], Step [100/897], Loss: 0.03640985116362572\n",
            "Epoch [1/10], Step [110/897], Loss: 0.015569888055324554\n",
            "Epoch [1/10], Step [120/897], Loss: 0.16186471283435822\n",
            "Epoch [1/10], Step [130/897], Loss: 0.01802576333284378\n",
            "Epoch [1/10], Step [140/897], Loss: 0.006723491940647364\n",
            "Epoch [1/10], Step [150/897], Loss: 0.013975323177874088\n",
            "Epoch [1/10], Step [160/897], Loss: 0.007512892130762339\n",
            "Epoch [1/10], Step [170/897], Loss: 0.021436745300889015\n",
            "Epoch [1/10], Step [180/897], Loss: 0.03661678731441498\n",
            "Epoch [1/10], Step [190/897], Loss: 0.020933248102664948\n",
            "Epoch [1/10], Step [200/897], Loss: 0.012555993162095547\n",
            "Epoch [1/10], Step [210/897], Loss: 0.07169017940759659\n",
            "Epoch [1/10], Step [220/897], Loss: 0.08636916428804398\n",
            "Epoch [1/10], Step [230/897], Loss: 0.02139074169099331\n",
            "Epoch [1/10], Step [240/897], Loss: 0.01694037765264511\n",
            "Epoch [1/10], Step [250/897], Loss: 0.03719272091984749\n",
            "Epoch [1/10], Step [260/897], Loss: 0.011106238700449467\n",
            "Epoch [1/10], Step [270/897], Loss: 0.009663928300142288\n",
            "Epoch [1/10], Step [280/897], Loss: 0.0477760024368763\n",
            "Epoch [1/10], Step [290/897], Loss: 0.010079463943839073\n",
            "Epoch [1/10], Step [300/897], Loss: 0.013519740663468838\n",
            "Epoch [1/10], Step [310/897], Loss: 0.03768320009112358\n",
            "Epoch [1/10], Step [320/897], Loss: 0.004180232994258404\n",
            "Epoch [1/10], Step [330/897], Loss: 0.09628773480653763\n",
            "Epoch [1/10], Step [340/897], Loss: 0.008976256474852562\n",
            "Epoch [1/10], Step [350/897], Loss: 0.004325528629124165\n",
            "Epoch [1/10], Step [360/897], Loss: 0.006094359327107668\n",
            "Epoch [1/10], Step [370/897], Loss: 0.08332530409097672\n",
            "Epoch [1/10], Step [380/897], Loss: 0.009195470251142979\n",
            "Epoch [1/10], Step [390/897], Loss: 0.008731585927307606\n",
            "Epoch [1/10], Step [400/897], Loss: 0.07888292521238327\n",
            "Epoch [1/10], Step [410/897], Loss: 0.1604161262512207\n",
            "Epoch [1/10], Step [420/897], Loss: 0.023586371913552284\n",
            "Epoch [1/10], Step [430/897], Loss: 0.025932492688298225\n",
            "Epoch [1/10], Step [440/897], Loss: 0.08775931596755981\n",
            "Epoch [1/10], Step [450/897], Loss: 0.07398655265569687\n",
            "Epoch [1/10], Step [460/897], Loss: 0.024462616071105003\n",
            "Epoch [1/10], Step [470/897], Loss: 0.033608317375183105\n",
            "Epoch [1/10], Step [480/897], Loss: 0.014482333324849606\n",
            "Epoch [1/10], Step [490/897], Loss: 0.01910521648824215\n",
            "Epoch [1/10], Step [500/897], Loss: 0.05303049832582474\n",
            "Epoch [1/10], Step [510/897], Loss: 0.01066990289837122\n",
            "Epoch [1/10], Step [520/897], Loss: 0.011593291535973549\n",
            "Epoch [1/10], Step [530/897], Loss: 0.0920531377196312\n",
            "Epoch [1/10], Step [540/897], Loss: 0.07326201349496841\n",
            "Epoch [1/10], Step [550/897], Loss: 0.00498028052970767\n",
            "Epoch [1/10], Step [560/897], Loss: 0.00506958132609725\n",
            "Epoch [1/10], Step [570/897], Loss: 0.010817819274961948\n",
            "Epoch [1/10], Step [580/897], Loss: 0.008541923016309738\n",
            "Epoch [1/10], Step [590/897], Loss: 0.028328463435173035\n",
            "Epoch [1/10], Step [600/897], Loss: 0.005728611722588539\n",
            "Epoch [1/10], Step [610/897], Loss: 0.005321172531694174\n",
            "Epoch [1/10], Step [620/897], Loss: 0.15241527557373047\n",
            "Epoch [1/10], Step [630/897], Loss: 0.005558730103075504\n",
            "Epoch [1/10], Step [640/897], Loss: 0.13410943746566772\n",
            "Epoch [1/10], Step [650/897], Loss: 0.026594702154397964\n",
            "Epoch [1/10], Step [660/897], Loss: 0.01791973225772381\n",
            "Epoch [1/10], Step [670/897], Loss: 0.00667159678414464\n",
            "Epoch [1/10], Step [680/897], Loss: 0.032459523528814316\n",
            "Epoch [1/10], Step [690/897], Loss: 0.1524059772491455\n",
            "Epoch [1/10], Step [700/897], Loss: 0.010506391525268555\n",
            "Epoch [1/10], Step [710/897], Loss: 0.015612639486789703\n",
            "Epoch [1/10], Step [720/897], Loss: 0.006036804057657719\n",
            "Epoch [1/10], Step [730/897], Loss: 0.150996133685112\n",
            "Epoch [1/10], Step [740/897], Loss: 0.0020996963139623404\n",
            "Epoch [1/10], Step [750/897], Loss: 0.11298125982284546\n",
            "Epoch [1/10], Step [760/897], Loss: 0.01915932446718216\n",
            "Epoch [1/10], Step [770/897], Loss: 0.047996774315834045\n",
            "Epoch [1/10], Step [780/897], Loss: 0.024310193955898285\n",
            "Epoch [1/10], Step [790/897], Loss: 0.00950651429593563\n",
            "Epoch [1/10], Step [800/897], Loss: 0.014373362064361572\n",
            "Epoch [1/10], Step [810/897], Loss: 0.07289007306098938\n",
            "Epoch [1/10], Step [820/897], Loss: 0.0067763980478048325\n",
            "Epoch [1/10], Step [830/897], Loss: 0.005461027845740318\n",
            "Epoch [1/10], Step [840/897], Loss: 0.012046265415847301\n",
            "Epoch [1/10], Step [850/897], Loss: 0.013996942900121212\n",
            "Epoch [1/10], Step [860/897], Loss: 0.0042328485287725925\n",
            "Epoch [1/10], Step [870/897], Loss: 0.005046274978667498\n",
            "Epoch [1/10], Step [880/897], Loss: 0.020900215953588486\n",
            "Epoch [1/10], Step [890/897], Loss: 0.008766370825469494\n",
            "Epoch [1/10], Train Loss: 0.03449770090156707, Train Accuracy: 0.9896802984346128\n",
            "Epoch [1/10], Validation Loss: 0.042541657735665254, Validation Accuracy: 0.986509098514955\n",
            "Epoch [2/10], Step [10/897], Loss: 0.010419083759188652\n",
            "Epoch [2/10], Step [20/897], Loss: 0.012828292325139046\n",
            "Epoch [2/10], Step [30/897], Loss: 0.012728003785014153\n",
            "Epoch [2/10], Step [40/897], Loss: 0.006052893586456776\n",
            "Epoch [2/10], Step [50/897], Loss: 0.027476411312818527\n",
            "Epoch [2/10], Step [60/897], Loss: 0.011563627980649471\n",
            "Epoch [2/10], Step [70/897], Loss: 0.07273122668266296\n",
            "Epoch [2/10], Step [80/897], Loss: 0.023877624422311783\n",
            "Epoch [2/10], Step [90/897], Loss: 0.008368630893528461\n",
            "Epoch [2/10], Step [100/897], Loss: 0.050606388598680496\n",
            "Epoch [2/10], Step [110/897], Loss: 0.01209788117557764\n",
            "Epoch [2/10], Step [120/897], Loss: 0.06819713860750198\n",
            "Epoch [2/10], Step [130/897], Loss: 0.07735607028007507\n",
            "Epoch [2/10], Step [140/897], Loss: 0.012850451283156872\n",
            "Epoch [2/10], Step [150/897], Loss: 0.01165772695094347\n",
            "Epoch [2/10], Step [160/897], Loss: 0.045586489140987396\n",
            "Epoch [2/10], Step [170/897], Loss: 0.028929444029927254\n",
            "Epoch [2/10], Step [180/897], Loss: 0.01909095235168934\n",
            "Epoch [2/10], Step [190/897], Loss: 0.02247544936835766\n",
            "Epoch [2/10], Step [200/897], Loss: 0.028601979836821556\n",
            "Epoch [2/10], Step [210/897], Loss: 0.005738620180636644\n",
            "Epoch [2/10], Step [220/897], Loss: 0.009450091049075127\n",
            "Epoch [2/10], Step [230/897], Loss: 0.023349545896053314\n",
            "Epoch [2/10], Step [240/897], Loss: 0.042934488505125046\n",
            "Epoch [2/10], Step [250/897], Loss: 0.0034229648299515247\n",
            "Epoch [2/10], Step [260/897], Loss: 0.012024621479213238\n",
            "Epoch [2/10], Step [270/897], Loss: 0.08569745719432831\n",
            "Epoch [2/10], Step [280/897], Loss: 0.06494229286909103\n",
            "Epoch [2/10], Step [290/897], Loss: 0.008090659976005554\n",
            "Epoch [2/10], Step [300/897], Loss: 0.015572763979434967\n",
            "Epoch [2/10], Step [310/897], Loss: 0.003055984154343605\n",
            "Epoch [2/10], Step [320/897], Loss: 0.015820473432540894\n",
            "Epoch [2/10], Step [330/897], Loss: 0.004342706874012947\n",
            "Epoch [2/10], Step [340/897], Loss: 0.03864561766386032\n",
            "Epoch [2/10], Step [350/897], Loss: 0.23013022541999817\n",
            "Epoch [2/10], Step [360/897], Loss: 0.011939957737922668\n",
            "Epoch [2/10], Step [370/897], Loss: 0.00925415474921465\n",
            "Epoch [2/10], Step [380/897], Loss: 0.007347764912992716\n",
            "Epoch [2/10], Step [390/897], Loss: 0.15910442173480988\n",
            "Epoch [2/10], Step [400/897], Loss: 0.010008176788687706\n",
            "Epoch [2/10], Step [410/897], Loss: 0.004889853298664093\n",
            "Epoch [2/10], Step [420/897], Loss: 0.012303879484534264\n",
            "Epoch [2/10], Step [430/897], Loss: 0.04995308816432953\n",
            "Epoch [2/10], Step [440/897], Loss: 0.009675318375229836\n",
            "Epoch [2/10], Step [450/897], Loss: 0.06621694564819336\n",
            "Epoch [2/10], Step [460/897], Loss: 0.020280202850699425\n",
            "Epoch [2/10], Step [470/897], Loss: 0.37969908118247986\n",
            "Epoch [2/10], Step [480/897], Loss: 0.018979284912347794\n",
            "Epoch [2/10], Step [490/897], Loss: 0.005440195091068745\n",
            "Epoch [2/10], Step [500/897], Loss: 0.06753802299499512\n",
            "Epoch [2/10], Step [510/897], Loss: 0.013493327423930168\n",
            "Epoch [2/10], Step [520/897], Loss: 0.012595737352967262\n",
            "Epoch [2/10], Step [530/897], Loss: 0.005328269209712744\n",
            "Epoch [2/10], Step [540/897], Loss: 0.008312730118632317\n",
            "Epoch [2/10], Step [550/897], Loss: 0.07443325966596603\n",
            "Epoch [2/10], Step [560/897], Loss: 0.045572299510240555\n",
            "Epoch [2/10], Step [570/897], Loss: 0.026395194232463837\n",
            "Epoch [2/10], Step [580/897], Loss: 0.05179854854941368\n",
            "Epoch [2/10], Step [590/897], Loss: 0.0647992193698883\n",
            "Epoch [2/10], Step [600/897], Loss: 0.019764339551329613\n",
            "Epoch [2/10], Step [610/897], Loss: 0.0077763875015079975\n",
            "Epoch [2/10], Step [620/897], Loss: 0.003161613829433918\n",
            "Epoch [2/10], Step [630/897], Loss: 0.01657133735716343\n",
            "Epoch [2/10], Step [640/897], Loss: 0.037496648728847504\n",
            "Epoch [2/10], Step [650/897], Loss: 0.04422663897275925\n",
            "Epoch [2/10], Step [660/897], Loss: 0.004666857421398163\n",
            "Epoch [2/10], Step [670/897], Loss: 0.13350841403007507\n",
            "Epoch [2/10], Step [680/897], Loss: 0.29745298624038696\n",
            "Epoch [2/10], Step [690/897], Loss: 0.007135797291994095\n",
            "Epoch [2/10], Step [700/897], Loss: 0.032963335514068604\n",
            "Epoch [2/10], Step [710/897], Loss: 0.2275046706199646\n",
            "Epoch [2/10], Step [720/897], Loss: 0.007847183384001255\n",
            "Epoch [2/10], Step [730/897], Loss: 0.0026652670931071043\n",
            "Epoch [2/10], Step [740/897], Loss: 0.045892901718616486\n",
            "Epoch [2/10], Step [750/897], Loss: 0.0034708755556493998\n",
            "Epoch [2/10], Step [760/897], Loss: 0.0535183809697628\n",
            "Epoch [2/10], Step [770/897], Loss: 0.11188248544931412\n",
            "Epoch [2/10], Step [780/897], Loss: 0.015857553109526634\n",
            "Epoch [2/10], Step [790/897], Loss: 0.0028625691775232553\n",
            "Epoch [2/10], Step [800/897], Loss: 0.044990554451942444\n",
            "Epoch [2/10], Step [810/897], Loss: 0.023411747068166733\n",
            "Epoch [2/10], Step [820/897], Loss: 0.0034706240985542536\n",
            "Epoch [2/10], Step [830/897], Loss: 0.004330810159444809\n",
            "Epoch [2/10], Step [840/897], Loss: 0.06092429906129837\n",
            "Epoch [2/10], Step [850/897], Loss: 0.015331721864640713\n",
            "Epoch [2/10], Step [860/897], Loss: 0.05713191628456116\n",
            "Epoch [2/10], Step [870/897], Loss: 0.046208564192056656\n",
            "Epoch [2/10], Step [880/897], Loss: 0.006077032070606947\n",
            "Epoch [2/10], Step [890/897], Loss: 0.004436994902789593\n",
            "Epoch [2/10], Train Loss: 0.03403940256604553, Train Accuracy: 0.9896802984346128\n",
            "Epoch [2/10], Validation Loss: 0.04205734664371804, Validation Accuracy: 0.9869274210416231\n",
            "Epoch [3/10], Step [10/897], Loss: 0.008629253134131432\n",
            "Epoch [3/10], Step [20/897], Loss: 0.006480237934738398\n",
            "Epoch [3/10], Step [30/897], Loss: 0.002428532112389803\n",
            "Epoch [3/10], Step [40/897], Loss: 0.0039027866441756487\n",
            "Epoch [3/10], Step [50/897], Loss: 0.015851955860853195\n",
            "Epoch [3/10], Step [60/897], Loss: 0.17940941452980042\n",
            "Epoch [3/10], Step [70/897], Loss: 0.002916284603998065\n",
            "Epoch [3/10], Step [80/897], Loss: 0.09645746648311615\n",
            "Epoch [3/10], Step [90/897], Loss: 0.03589608520269394\n",
            "Epoch [3/10], Step [100/897], Loss: 0.06895390152931213\n",
            "Epoch [3/10], Step [110/897], Loss: 0.022786276414990425\n",
            "Epoch [3/10], Step [120/897], Loss: 0.03936908394098282\n",
            "Epoch [3/10], Step [130/897], Loss: 0.0306404922157526\n",
            "Epoch [3/10], Step [140/897], Loss: 0.0029073506593704224\n",
            "Epoch [3/10], Step [150/897], Loss: 0.005881724413484335\n",
            "Epoch [3/10], Step [160/897], Loss: 0.019960511475801468\n",
            "Epoch [3/10], Step [170/897], Loss: 0.007556495722383261\n",
            "Epoch [3/10], Step [180/897], Loss: 0.01816830411553383\n",
            "Epoch [3/10], Step [190/897], Loss: 0.008409721776843071\n",
            "Epoch [3/10], Step [200/897], Loss: 0.04879964143037796\n",
            "Epoch [3/10], Step [210/897], Loss: 0.0074071986600756645\n",
            "Epoch [3/10], Step [220/897], Loss: 0.009749634191393852\n",
            "Epoch [3/10], Step [230/897], Loss: 0.011082079261541367\n",
            "Epoch [3/10], Step [240/897], Loss: 0.00804104283452034\n",
            "Epoch [3/10], Step [250/897], Loss: 0.028482135385274887\n",
            "Epoch [3/10], Step [260/897], Loss: 0.018774516880512238\n",
            "Epoch [3/10], Step [270/897], Loss: 0.0059203715063631535\n",
            "Epoch [3/10], Step [280/897], Loss: 0.19887681305408478\n",
            "Epoch [3/10], Step [290/897], Loss: 0.012474125251173973\n",
            "Epoch [3/10], Step [300/897], Loss: 0.002210168866440654\n",
            "Epoch [3/10], Step [310/897], Loss: 0.025803856551647186\n",
            "Epoch [3/10], Step [320/897], Loss: 0.019438086077570915\n",
            "Epoch [3/10], Step [330/897], Loss: 0.005812306888401508\n",
            "Epoch [3/10], Step [340/897], Loss: 0.009148751385509968\n",
            "Epoch [3/10], Step [350/897], Loss: 0.04745911434292793\n",
            "Epoch [3/10], Step [360/897], Loss: 0.04203173890709877\n",
            "Epoch [3/10], Step [370/897], Loss: 0.020192543044686317\n",
            "Epoch [3/10], Step [380/897], Loss: 0.05658230930566788\n",
            "Epoch [3/10], Step [390/897], Loss: 0.007037568837404251\n",
            "Epoch [3/10], Step [400/897], Loss: 0.003881132695823908\n",
            "Epoch [3/10], Step [410/897], Loss: 0.05237743631005287\n",
            "Epoch [3/10], Step [420/897], Loss: 0.04845815151929855\n",
            "Epoch [3/10], Step [430/897], Loss: 0.004062626976519823\n",
            "Epoch [3/10], Step [440/897], Loss: 0.01916811242699623\n",
            "Epoch [3/10], Step [450/897], Loss: 0.016194701194763184\n",
            "Epoch [3/10], Step [460/897], Loss: 0.05039706453680992\n",
            "Epoch [3/10], Step [470/897], Loss: 0.04287980496883392\n",
            "Epoch [3/10], Step [480/897], Loss: 0.007160678505897522\n",
            "Epoch [3/10], Step [490/897], Loss: 0.054790228605270386\n",
            "Epoch [3/10], Step [500/897], Loss: 0.114566870033741\n",
            "Epoch [3/10], Step [510/897], Loss: 0.012834800407290459\n",
            "Epoch [3/10], Step [520/897], Loss: 0.012666510418057442\n",
            "Epoch [3/10], Step [530/897], Loss: 0.06296300143003464\n",
            "Epoch [3/10], Step [540/897], Loss: 0.026914231479167938\n",
            "Epoch [3/10], Step [550/897], Loss: 0.02529548667371273\n",
            "Epoch [3/10], Step [560/897], Loss: 0.011591489426791668\n",
            "Epoch [3/10], Step [570/897], Loss: 0.08183693140745163\n",
            "Epoch [3/10], Step [580/897], Loss: 0.013030779547989368\n",
            "Epoch [3/10], Step [590/897], Loss: 0.03930588439106941\n",
            "Epoch [3/10], Step [600/897], Loss: 0.017146393656730652\n",
            "Epoch [3/10], Step [610/897], Loss: 0.014187989756464958\n",
            "Epoch [3/10], Step [620/897], Loss: 0.022061314433813095\n",
            "Epoch [3/10], Step [630/897], Loss: 0.0041370270773768425\n",
            "Epoch [3/10], Step [640/897], Loss: 0.010273593477904797\n",
            "Epoch [3/10], Step [650/897], Loss: 0.01879410818219185\n",
            "Epoch [3/10], Step [660/897], Loss: 0.1134704127907753\n",
            "Epoch [3/10], Step [670/897], Loss: 0.006946318317204714\n",
            "Epoch [3/10], Step [680/897], Loss: 0.0443943627178669\n",
            "Epoch [3/10], Step [690/897], Loss: 0.1612839698791504\n",
            "Epoch [3/10], Step [700/897], Loss: 0.15035678446292877\n",
            "Epoch [3/10], Step [710/897], Loss: 0.05976817384362221\n",
            "Epoch [3/10], Step [720/897], Loss: 0.05209466442465782\n",
            "Epoch [3/10], Step [730/897], Loss: 0.0270763598382473\n",
            "Epoch [3/10], Step [740/897], Loss: 0.03895246982574463\n",
            "Epoch [3/10], Step [750/897], Loss: 0.020590422675013542\n",
            "Epoch [3/10], Step [760/897], Loss: 0.13383762538433075\n",
            "Epoch [3/10], Step [770/897], Loss: 0.001724786008708179\n",
            "Epoch [3/10], Step [780/897], Loss: 0.028798911720514297\n",
            "Epoch [3/10], Step [790/897], Loss: 0.013168170116841793\n",
            "Epoch [3/10], Step [800/897], Loss: 0.02006102167069912\n",
            "Epoch [3/10], Step [810/897], Loss: 0.010857855901122093\n",
            "Epoch [3/10], Step [820/897], Loss: 0.004333697725087404\n",
            "Epoch [3/10], Step [830/897], Loss: 0.005786624737083912\n",
            "Epoch [3/10], Step [840/897], Loss: 0.028874898329377174\n",
            "Epoch [3/10], Step [850/897], Loss: 0.05069500580430031\n",
            "Epoch [3/10], Step [860/897], Loss: 0.05990507826209068\n",
            "Epoch [3/10], Step [870/897], Loss: 0.009044788777828217\n",
            "Epoch [3/10], Step [880/897], Loss: 0.007502446882426739\n",
            "Epoch [3/10], Step [890/897], Loss: 0.01723298616707325\n",
            "Epoch [3/10], Train Loss: 0.03340469800268581, Train Accuracy: 0.9899940731443713\n",
            "Epoch [3/10], Validation Loss: 0.04194851447189582, Validation Accuracy: 0.9869274210416231\n",
            "Epoch [4/10], Step [10/897], Loss: 0.06854625046253204\n",
            "Epoch [4/10], Step [20/897], Loss: 0.004419189877808094\n",
            "Epoch [4/10], Step [30/897], Loss: 0.011464101262390614\n",
            "Epoch [4/10], Step [40/897], Loss: 0.023787442594766617\n",
            "Epoch [4/10], Step [50/897], Loss: 0.046579327434301376\n",
            "Epoch [4/10], Step [60/897], Loss: 0.021773669868707657\n",
            "Epoch [4/10], Step [70/897], Loss: 0.01874353736639023\n",
            "Epoch [4/10], Step [80/897], Loss: 0.0122818099334836\n",
            "Epoch [4/10], Step [90/897], Loss: 0.004616708029061556\n",
            "Epoch [4/10], Step [100/897], Loss: 0.12783895432949066\n",
            "Epoch [4/10], Step [110/897], Loss: 0.025726625695824623\n",
            "Epoch [4/10], Step [120/897], Loss: 0.016183774918317795\n",
            "Epoch [4/10], Step [130/897], Loss: 0.004253678023815155\n",
            "Epoch [4/10], Step [140/897], Loss: 0.016749119386076927\n",
            "Epoch [4/10], Step [150/897], Loss: 0.026698242872953415\n",
            "Epoch [4/10], Step [160/897], Loss: 0.009227195754647255\n",
            "Epoch [4/10], Step [170/897], Loss: 0.0036846662405878305\n",
            "Epoch [4/10], Step [180/897], Loss: 0.006484470330178738\n",
            "Epoch [4/10], Step [190/897], Loss: 0.008480515331029892\n",
            "Epoch [4/10], Step [200/897], Loss: 0.024300968274474144\n",
            "Epoch [4/10], Step [210/897], Loss: 0.04151370748877525\n",
            "Epoch [4/10], Step [220/897], Loss: 0.01824442483484745\n",
            "Epoch [4/10], Step [230/897], Loss: 0.006094659678637981\n",
            "Epoch [4/10], Step [240/897], Loss: 0.006464919075369835\n",
            "Epoch [4/10], Step [250/897], Loss: 0.022536130622029305\n",
            "Epoch [4/10], Step [260/897], Loss: 0.016995670273900032\n",
            "Epoch [4/10], Step [270/897], Loss: 0.03891792148351669\n",
            "Epoch [4/10], Step [280/897], Loss: 0.006695265416055918\n",
            "Epoch [4/10], Step [290/897], Loss: 0.07565172761678696\n",
            "Epoch [4/10], Step [300/897], Loss: 0.004257897846400738\n",
            "Epoch [4/10], Step [310/897], Loss: 0.1641152799129486\n",
            "Epoch [4/10], Step [320/897], Loss: 0.007574956398457289\n",
            "Epoch [4/10], Step [330/897], Loss: 0.01882234588265419\n",
            "Epoch [4/10], Step [340/897], Loss: 0.23117107152938843\n",
            "Epoch [4/10], Step [350/897], Loss: 0.07330476492643356\n",
            "Epoch [4/10], Step [360/897], Loss: 0.01653650775551796\n",
            "Epoch [4/10], Step [370/897], Loss: 0.010994854383170605\n",
            "Epoch [4/10], Step [380/897], Loss: 0.010094891302287579\n",
            "Epoch [4/10], Step [390/897], Loss: 0.013734688982367516\n",
            "Epoch [4/10], Step [400/897], Loss: 0.018134141340851784\n",
            "Epoch [4/10], Step [410/897], Loss: 0.024924980476498604\n",
            "Epoch [4/10], Step [420/897], Loss: 0.007252746727317572\n",
            "Epoch [4/10], Step [430/897], Loss: 0.05364714190363884\n",
            "Epoch [4/10], Step [440/897], Loss: 0.13471883535385132\n",
            "Epoch [4/10], Step [450/897], Loss: 0.020098911598324776\n",
            "Epoch [4/10], Step [460/897], Loss: 0.0496172197163105\n",
            "Epoch [4/10], Step [470/897], Loss: 0.03199995681643486\n",
            "Epoch [4/10], Step [480/897], Loss: 0.027462629601359367\n",
            "Epoch [4/10], Step [490/897], Loss: 0.024466652423143387\n",
            "Epoch [4/10], Step [500/897], Loss: 0.029651494696736336\n",
            "Epoch [4/10], Step [510/897], Loss: 0.012277178466320038\n",
            "Epoch [4/10], Step [520/897], Loss: 0.008097135461866856\n",
            "Epoch [4/10], Step [530/897], Loss: 0.011247504502534866\n",
            "Epoch [4/10], Step [540/897], Loss: 0.008780661970376968\n",
            "Epoch [4/10], Step [550/897], Loss: 0.01517391949892044\n",
            "Epoch [4/10], Step [560/897], Loss: 0.019358966499567032\n",
            "Epoch [4/10], Step [570/897], Loss: 0.08047552406787872\n",
            "Epoch [4/10], Step [580/897], Loss: 0.012391879223287106\n",
            "Epoch [4/10], Step [590/897], Loss: 0.009097926318645477\n",
            "Epoch [4/10], Step [600/897], Loss: 0.02260063774883747\n",
            "Epoch [4/10], Step [610/897], Loss: 0.019038930535316467\n",
            "Epoch [4/10], Step [620/897], Loss: 0.011533801443874836\n",
            "Epoch [4/10], Step [630/897], Loss: 0.07828202098608017\n",
            "Epoch [4/10], Step [640/897], Loss: 0.052480388432741165\n",
            "Epoch [4/10], Step [650/897], Loss: 0.01747922971844673\n",
            "Epoch [4/10], Step [660/897], Loss: 0.003584527876228094\n",
            "Epoch [4/10], Step [670/897], Loss: 0.03208713233470917\n",
            "Epoch [4/10], Step [680/897], Loss: 0.011389554478228092\n",
            "Epoch [4/10], Step [690/897], Loss: 0.012865553610026836\n",
            "Epoch [4/10], Step [700/897], Loss: 0.016565604135394096\n",
            "Epoch [4/10], Step [710/897], Loss: 0.011889315210282803\n",
            "Epoch [4/10], Step [720/897], Loss: 0.1226869449019432\n",
            "Epoch [4/10], Step [730/897], Loss: 0.018475886434316635\n",
            "Epoch [4/10], Step [740/897], Loss: 0.0037890304811298847\n",
            "Epoch [4/10], Step [750/897], Loss: 0.016251981258392334\n",
            "Epoch [4/10], Step [760/897], Loss: 0.01716730184853077\n",
            "Epoch [4/10], Step [770/897], Loss: 0.015822438523173332\n",
            "Epoch [4/10], Step [780/897], Loss: 0.014441724866628647\n",
            "Epoch [4/10], Step [790/897], Loss: 0.008884034119546413\n",
            "Epoch [4/10], Step [800/897], Loss: 0.005871970672160387\n",
            "Epoch [4/10], Step [810/897], Loss: 0.04202214255928993\n",
            "Epoch [4/10], Step [820/897], Loss: 0.02061147801578045\n",
            "Epoch [4/10], Step [830/897], Loss: 0.00748998299241066\n",
            "Epoch [4/10], Step [840/897], Loss: 0.07426910847425461\n",
            "Epoch [4/10], Step [850/897], Loss: 0.005032502114772797\n",
            "Epoch [4/10], Step [860/897], Loss: 0.014017047360539436\n",
            "Epoch [4/10], Step [870/897], Loss: 0.016592204570770264\n",
            "Epoch [4/10], Step [880/897], Loss: 0.0049675991758704185\n",
            "Epoch [4/10], Step [890/897], Loss: 0.005085631273686886\n",
            "Epoch [4/10], Train Loss: 0.03323111901191606, Train Accuracy: 0.9897848900045323\n",
            "Epoch [4/10], Validation Loss: 0.04157550160361946, Validation Accuracy: 0.9871365823049572\n",
            "Epoch [5/10], Step [10/897], Loss: 0.14091521501541138\n",
            "Epoch [5/10], Step [20/897], Loss: 0.022975048050284386\n",
            "Epoch [5/10], Step [30/897], Loss: 0.04431205242872238\n",
            "Epoch [5/10], Step [40/897], Loss: 0.00880617555230856\n",
            "Epoch [5/10], Step [50/897], Loss: 0.03294587507843971\n",
            "Epoch [5/10], Step [60/897], Loss: 0.005215606186538935\n",
            "Epoch [5/10], Step [70/897], Loss: 0.012735344469547272\n",
            "Epoch [5/10], Step [80/897], Loss: 0.07130569964647293\n",
            "Epoch [5/10], Step [90/897], Loss: 0.01249284390360117\n",
            "Epoch [5/10], Step [100/897], Loss: 0.009758922271430492\n",
            "Epoch [5/10], Step [110/897], Loss: 0.014896086417138577\n",
            "Epoch [5/10], Step [120/897], Loss: 0.01359160989522934\n",
            "Epoch [5/10], Step [130/897], Loss: 0.010235457681119442\n",
            "Epoch [5/10], Step [140/897], Loss: 0.022328834980726242\n",
            "Epoch [5/10], Step [150/897], Loss: 0.07906851917505264\n",
            "Epoch [5/10], Step [160/897], Loss: 0.00960572250187397\n",
            "Epoch [5/10], Step [170/897], Loss: 0.013930744491517544\n",
            "Epoch [5/10], Step [180/897], Loss: 0.024344539269804955\n",
            "Epoch [5/10], Step [190/897], Loss: 0.006200965028256178\n",
            "Epoch [5/10], Step [200/897], Loss: 0.005082303192466497\n",
            "Epoch [5/10], Step [210/897], Loss: 0.07221594452857971\n",
            "Epoch [5/10], Step [220/897], Loss: 0.021328261122107506\n",
            "Epoch [5/10], Step [230/897], Loss: 0.08622054010629654\n",
            "Epoch [5/10], Step [240/897], Loss: 0.07942478358745575\n",
            "Epoch [5/10], Step [250/897], Loss: 0.009819034487009048\n",
            "Epoch [5/10], Step [260/897], Loss: 0.1771250069141388\n",
            "Epoch [5/10], Step [270/897], Loss: 0.00883484072983265\n",
            "Epoch [5/10], Step [280/897], Loss: 0.020082181319594383\n",
            "Epoch [5/10], Step [290/897], Loss: 0.10085688531398773\n",
            "Epoch [5/10], Step [300/897], Loss: 0.008285039104521275\n",
            "Epoch [5/10], Step [310/897], Loss: 0.12458007782697678\n",
            "Epoch [5/10], Step [320/897], Loss: 0.05600835755467415\n",
            "Epoch [5/10], Step [330/897], Loss: 0.012359815649688244\n",
            "Epoch [5/10], Step [340/897], Loss: 0.15086190402507782\n",
            "Epoch [5/10], Step [350/897], Loss: 0.0026363241486251354\n",
            "Epoch [5/10], Step [360/897], Loss: 0.30454933643341064\n",
            "Epoch [5/10], Step [370/897], Loss: 0.022198185324668884\n",
            "Epoch [5/10], Step [380/897], Loss: 0.007444010116159916\n",
            "Epoch [5/10], Step [390/897], Loss: 0.11887412518262863\n",
            "Epoch [5/10], Step [400/897], Loss: 0.0250107329338789\n",
            "Epoch [5/10], Step [410/897], Loss: 0.054301973432302475\n",
            "Epoch [5/10], Step [420/897], Loss: 0.028182853013277054\n",
            "Epoch [5/10], Step [430/897], Loss: 0.07976838201284409\n",
            "Epoch [5/10], Step [440/897], Loss: 0.029454737901687622\n",
            "Epoch [5/10], Step [450/897], Loss: 0.011478090658783913\n",
            "Epoch [5/10], Step [460/897], Loss: 0.03550732880830765\n",
            "Epoch [5/10], Step [470/897], Loss: 0.008583190850913525\n",
            "Epoch [5/10], Step [480/897], Loss: 0.005408014170825481\n",
            "Epoch [5/10], Step [490/897], Loss: 0.005530822090804577\n",
            "Epoch [5/10], Step [500/897], Loss: 0.002488519763574004\n",
            "Epoch [5/10], Step [510/897], Loss: 0.024301311001181602\n",
            "Epoch [5/10], Step [520/897], Loss: 0.08169960975646973\n",
            "Epoch [5/10], Step [530/897], Loss: 0.09356904029846191\n",
            "Epoch [5/10], Step [540/897], Loss: 0.035267915576696396\n",
            "Epoch [5/10], Step [550/897], Loss: 0.06215067207813263\n",
            "Epoch [5/10], Step [560/897], Loss: 0.014417294412851334\n",
            "Epoch [5/10], Step [570/897], Loss: 0.01942809857428074\n",
            "Epoch [5/10], Step [580/897], Loss: 0.01566176861524582\n",
            "Epoch [5/10], Step [590/897], Loss: 0.03660808131098747\n",
            "Epoch [5/10], Step [600/897], Loss: 0.015534917823970318\n",
            "Epoch [5/10], Step [610/897], Loss: 0.030811499804258347\n",
            "Epoch [5/10], Step [620/897], Loss: 0.03351818397641182\n",
            "Epoch [5/10], Step [630/897], Loss: 0.021215425804257393\n",
            "Epoch [5/10], Step [640/897], Loss: 0.008421837352216244\n",
            "Epoch [5/10], Step [650/897], Loss: 0.0033737183548510075\n",
            "Epoch [5/10], Step [660/897], Loss: 0.05495979264378548\n",
            "Epoch [5/10], Step [670/897], Loss: 0.026948092505335808\n",
            "Epoch [5/10], Step [680/897], Loss: 0.005945142358541489\n",
            "Epoch [5/10], Step [690/897], Loss: 0.031039033085107803\n",
            "Epoch [5/10], Step [700/897], Loss: 0.042320650070905685\n",
            "Epoch [5/10], Step [710/897], Loss: 0.026430584490299225\n",
            "Epoch [5/10], Step [720/897], Loss: 0.026932457461953163\n",
            "Epoch [5/10], Step [730/897], Loss: 0.015665961429476738\n",
            "Epoch [5/10], Step [740/897], Loss: 0.03508945181965828\n",
            "Epoch [5/10], Step [750/897], Loss: 0.010958393104374409\n",
            "Epoch [5/10], Step [760/897], Loss: 0.00508998753502965\n",
            "Epoch [5/10], Step [770/897], Loss: 0.016251713037490845\n",
            "Epoch [5/10], Step [780/897], Loss: 0.05690591409802437\n",
            "Epoch [5/10], Step [790/897], Loss: 0.10238786041736603\n",
            "Epoch [5/10], Step [800/897], Loss: 0.03525429591536522\n",
            "Epoch [5/10], Step [810/897], Loss: 0.026296494528651237\n",
            "Epoch [5/10], Step [820/897], Loss: 0.01249037217348814\n",
            "Epoch [5/10], Step [830/897], Loss: 0.21933917701244354\n",
            "Epoch [5/10], Step [840/897], Loss: 0.0028466421645134687\n",
            "Epoch [5/10], Step [850/897], Loss: 0.009825415909290314\n",
            "Epoch [5/10], Step [860/897], Loss: 0.02438448742032051\n",
            "Epoch [5/10], Step [870/897], Loss: 0.013979675248265266\n",
            "Epoch [5/10], Step [880/897], Loss: 0.004055870696902275\n",
            "Epoch [5/10], Step [890/897], Loss: 0.14718660712242126\n",
            "Epoch [5/10], Train Loss: 0.033563587826509506, Train Accuracy: 0.9895059791514137\n",
            "Epoch [5/10], Validation Loss: 0.04177348489793684, Validation Accuracy: 0.9867182597782891\n",
            "Epoch [6/10], Step [10/897], Loss: 0.018574867397546768\n",
            "Epoch [6/10], Step [20/897], Loss: 0.01943819597363472\n",
            "Epoch [6/10], Step [30/897], Loss: 0.00621960312128067\n",
            "Epoch [6/10], Step [40/897], Loss: 0.0459265299141407\n",
            "Epoch [6/10], Step [50/897], Loss: 0.21838536858558655\n",
            "Epoch [6/10], Step [60/897], Loss: 0.022947899997234344\n",
            "Epoch [6/10], Step [70/897], Loss: 0.013478410430252552\n",
            "Epoch [6/10], Step [80/897], Loss: 0.03012136183679104\n",
            "Epoch [6/10], Step [90/897], Loss: 0.08301141858100891\n",
            "Epoch [6/10], Step [100/897], Loss: 0.01178421638906002\n",
            "Epoch [6/10], Step [110/897], Loss: 0.05924426019191742\n",
            "Epoch [6/10], Step [120/897], Loss: 0.025161199271678925\n",
            "Epoch [6/10], Step [130/897], Loss: 0.00836966373026371\n",
            "Epoch [6/10], Step [140/897], Loss: 0.006500472314655781\n",
            "Epoch [6/10], Step [150/897], Loss: 0.016247360035777092\n",
            "Epoch [6/10], Step [160/897], Loss: 0.07919972389936447\n",
            "Epoch [6/10], Step [170/897], Loss: 0.0288323275744915\n",
            "Epoch [6/10], Step [180/897], Loss: 0.10862558335065842\n",
            "Epoch [6/10], Step [190/897], Loss: 0.025791967287659645\n",
            "Epoch [6/10], Step [200/897], Loss: 0.010044554248452187\n",
            "Epoch [6/10], Step [210/897], Loss: 0.006274516694247723\n",
            "Epoch [6/10], Step [220/897], Loss: 0.023283617570996284\n",
            "Epoch [6/10], Step [230/897], Loss: 0.005060950759798288\n",
            "Epoch [6/10], Step [240/897], Loss: 0.003496580757200718\n",
            "Epoch [6/10], Step [250/897], Loss: 0.08718668669462204\n",
            "Epoch [6/10], Step [260/897], Loss: 0.004677194636315107\n",
            "Epoch [6/10], Step [270/897], Loss: 0.02014954388141632\n",
            "Epoch [6/10], Step [280/897], Loss: 0.05545797571539879\n",
            "Epoch [6/10], Step [290/897], Loss: 0.018908025696873665\n",
            "Epoch [6/10], Step [300/897], Loss: 0.009418830275535583\n",
            "Epoch [6/10], Step [310/897], Loss: 0.016341501846909523\n",
            "Epoch [6/10], Step [320/897], Loss: 0.020037760958075523\n",
            "Epoch [6/10], Step [330/897], Loss: 0.031602002680301666\n",
            "Epoch [6/10], Step [340/897], Loss: 0.012850628234446049\n",
            "Epoch [6/10], Step [350/897], Loss: 0.024350697174668312\n",
            "Epoch [6/10], Step [360/897], Loss: 0.042190201580524445\n",
            "Epoch [6/10], Step [370/897], Loss: 0.004688051994889975\n",
            "Epoch [6/10], Step [380/897], Loss: 0.00834860373288393\n",
            "Epoch [6/10], Step [390/897], Loss: 0.010825150646269321\n",
            "Epoch [6/10], Step [400/897], Loss: 0.059093136340379715\n",
            "Epoch [6/10], Step [410/897], Loss: 0.0029555887449532747\n",
            "Epoch [6/10], Step [420/897], Loss: 0.02554956078529358\n",
            "Epoch [6/10], Step [430/897], Loss: 0.06319187581539154\n",
            "Epoch [6/10], Step [440/897], Loss: 0.019917892292141914\n",
            "Epoch [6/10], Step [450/897], Loss: 0.03907542675733566\n",
            "Epoch [6/10], Step [460/897], Loss: 0.007629291154444218\n",
            "Epoch [6/10], Step [470/897], Loss: 0.009830087423324585\n",
            "Epoch [6/10], Step [480/897], Loss: 0.04421764239668846\n",
            "Epoch [6/10], Step [490/897], Loss: 0.007494582328945398\n",
            "Epoch [6/10], Step [500/897], Loss: 0.007542329840362072\n",
            "Epoch [6/10], Step [510/897], Loss: 0.09322698414325714\n",
            "Epoch [6/10], Step [520/897], Loss: 0.0064018103294074535\n",
            "Epoch [6/10], Step [530/897], Loss: 0.1459522396326065\n",
            "Epoch [6/10], Step [540/897], Loss: 0.05562928318977356\n",
            "Epoch [6/10], Step [550/897], Loss: 0.0024514147080481052\n",
            "Epoch [6/10], Step [560/897], Loss: 0.059419699013233185\n",
            "Epoch [6/10], Step [570/897], Loss: 0.003487926907837391\n",
            "Epoch [6/10], Step [580/897], Loss: 0.06946568191051483\n",
            "Epoch [6/10], Step [590/897], Loss: 0.03144514933228493\n",
            "Epoch [6/10], Step [600/897], Loss: 0.012720744125545025\n",
            "Epoch [6/10], Step [610/897], Loss: 0.020639918744564056\n",
            "Epoch [6/10], Step [620/897], Loss: 0.006306866649538279\n",
            "Epoch [6/10], Step [630/897], Loss: 0.007568816188722849\n",
            "Epoch [6/10], Step [640/897], Loss: 0.007911855354905128\n",
            "Epoch [6/10], Step [650/897], Loss: 0.11307009309530258\n",
            "Epoch [6/10], Step [660/897], Loss: 0.009965525940060616\n",
            "Epoch [6/10], Step [670/897], Loss: 0.02589740790426731\n",
            "Epoch [6/10], Step [680/897], Loss: 0.049743860960006714\n",
            "Epoch [6/10], Step [690/897], Loss: 0.04020126909017563\n",
            "Epoch [6/10], Step [700/897], Loss: 0.09935982525348663\n",
            "Epoch [6/10], Step [710/897], Loss: 0.0346091166138649\n",
            "Epoch [6/10], Step [720/897], Loss: 0.01242165919393301\n",
            "Epoch [6/10], Step [730/897], Loss: 0.0039060069248080254\n",
            "Epoch [6/10], Step [740/897], Loss: 0.017507264390587807\n",
            "Epoch [6/10], Step [750/897], Loss: 0.004297328181564808\n",
            "Epoch [6/10], Step [760/897], Loss: 0.0028800880536437035\n",
            "Epoch [6/10], Step [770/897], Loss: 0.031679537147283554\n",
            "Epoch [6/10], Step [780/897], Loss: 0.019755078479647636\n",
            "Epoch [6/10], Step [790/897], Loss: 0.031142519786953926\n",
            "Epoch [6/10], Step [800/897], Loss: 0.10399723798036575\n",
            "Epoch [6/10], Step [810/897], Loss: 0.0411379411816597\n",
            "Epoch [6/10], Step [820/897], Loss: 0.055157531052827835\n",
            "Epoch [6/10], Step [830/897], Loss: 0.00383810605853796\n",
            "Epoch [6/10], Step [840/897], Loss: 0.028695594519376755\n",
            "Epoch [6/10], Step [850/897], Loss: 0.0037835598923265934\n",
            "Epoch [6/10], Step [860/897], Loss: 0.006517276167869568\n",
            "Epoch [6/10], Step [870/897], Loss: 0.003449518932029605\n",
            "Epoch [6/10], Step [880/897], Loss: 0.04652832821011543\n",
            "Epoch [6/10], Step [890/897], Loss: 0.013032092712819576\n",
            "Epoch [6/10], Train Loss: 0.03280769285970184, Train Accuracy: 0.9901683924275704\n",
            "Epoch [6/10], Validation Loss: 0.04311696046778004, Validation Accuracy: 0.986613679146622\n",
            "Epoch [7/10], Step [10/897], Loss: 0.014550779014825821\n",
            "Epoch [7/10], Step [20/897], Loss: 0.0018451372161507607\n",
            "Epoch [7/10], Step [30/897], Loss: 0.028865724802017212\n",
            "Epoch [7/10], Step [40/897], Loss: 0.009151800535619259\n",
            "Epoch [7/10], Step [50/897], Loss: 0.011650136671960354\n",
            "Epoch [7/10], Step [60/897], Loss: 0.06124086305499077\n",
            "Epoch [7/10], Step [70/897], Loss: 0.06237681582570076\n",
            "Epoch [7/10], Step [80/897], Loss: 0.006398824974894524\n",
            "Epoch [7/10], Step [90/897], Loss: 0.021731670945882797\n",
            "Epoch [7/10], Step [100/897], Loss: 0.19746536016464233\n",
            "Epoch [7/10], Step [110/897], Loss: 0.004278680309653282\n",
            "Epoch [7/10], Step [120/897], Loss: 0.0052749766036868095\n",
            "Epoch [7/10], Step [130/897], Loss: 0.006260485388338566\n",
            "Epoch [7/10], Step [140/897], Loss: 0.0273005161434412\n",
            "Epoch [7/10], Step [150/897], Loss: 0.08770288527011871\n",
            "Epoch [7/10], Step [160/897], Loss: 0.006932961754500866\n",
            "Epoch [7/10], Step [170/897], Loss: 0.1317443996667862\n",
            "Epoch [7/10], Step [180/897], Loss: 0.07985658198595047\n",
            "Epoch [7/10], Step [190/897], Loss: 0.004755030386149883\n",
            "Epoch [7/10], Step [200/897], Loss: 0.006826719269156456\n",
            "Epoch [7/10], Step [210/897], Loss: 0.011037986725568771\n",
            "Epoch [7/10], Step [220/897], Loss: 0.008612891659140587\n",
            "Epoch [7/10], Step [230/897], Loss: 0.02661040984094143\n",
            "Epoch [7/10], Step [240/897], Loss: 0.11878260970115662\n",
            "Epoch [7/10], Step [250/897], Loss: 0.017894791439175606\n",
            "Epoch [7/10], Step [260/897], Loss: 0.04637577384710312\n",
            "Epoch [7/10], Step [270/897], Loss: 0.07825105637311935\n",
            "Epoch [7/10], Step [280/897], Loss: 0.10270637273788452\n",
            "Epoch [7/10], Step [290/897], Loss: 0.08359727263450623\n",
            "Epoch [7/10], Step [300/897], Loss: 0.02586238458752632\n",
            "Epoch [7/10], Step [310/897], Loss: 0.011211716569960117\n",
            "Epoch [7/10], Step [320/897], Loss: 0.15695983171463013\n",
            "Epoch [7/10], Step [330/897], Loss: 0.004026664420962334\n",
            "Epoch [7/10], Step [340/897], Loss: 0.01826685108244419\n",
            "Epoch [7/10], Step [350/897], Loss: 0.04892277345061302\n",
            "Epoch [7/10], Step [360/897], Loss: 0.014395135454833508\n",
            "Epoch [7/10], Step [370/897], Loss: 0.009150034748017788\n",
            "Epoch [7/10], Step [380/897], Loss: 0.0627521350979805\n",
            "Epoch [7/10], Step [390/897], Loss: 0.013269016519188881\n",
            "Epoch [7/10], Step [400/897], Loss: 0.11454812437295914\n",
            "Epoch [7/10], Step [410/897], Loss: 0.00846229586750269\n",
            "Epoch [7/10], Step [420/897], Loss: 0.029353659600019455\n",
            "Epoch [7/10], Step [430/897], Loss: 0.05565542355179787\n",
            "Epoch [7/10], Step [440/897], Loss: 0.00641919020563364\n",
            "Epoch [7/10], Step [450/897], Loss: 0.005345610901713371\n",
            "Epoch [7/10], Step [460/897], Loss: 0.007201572880148888\n",
            "Epoch [7/10], Step [470/897], Loss: 0.08509911596775055\n",
            "Epoch [7/10], Step [480/897], Loss: 0.012532999739050865\n",
            "Epoch [7/10], Step [490/897], Loss: 0.004924865905195475\n",
            "Epoch [7/10], Step [500/897], Loss: 0.025919344276189804\n",
            "Epoch [7/10], Step [510/897], Loss: 0.004627515096217394\n",
            "Epoch [7/10], Step [520/897], Loss: 0.20049116015434265\n",
            "Epoch [7/10], Step [530/897], Loss: 0.011185749433934689\n",
            "Epoch [7/10], Step [540/897], Loss: 0.008140003308653831\n",
            "Epoch [7/10], Step [550/897], Loss: 0.09842512011528015\n",
            "Epoch [7/10], Step [560/897], Loss: 0.011896890588104725\n",
            "Epoch [7/10], Step [570/897], Loss: 0.004227085504680872\n",
            "Epoch [7/10], Step [580/897], Loss: 0.017438409850001335\n",
            "Epoch [7/10], Step [590/897], Loss: 0.007669581566005945\n",
            "Epoch [7/10], Step [600/897], Loss: 0.03835245966911316\n",
            "Epoch [7/10], Step [610/897], Loss: 0.22582915425300598\n",
            "Epoch [7/10], Step [620/897], Loss: 0.008665097877383232\n",
            "Epoch [7/10], Step [630/897], Loss: 0.008448254317045212\n",
            "Epoch [7/10], Step [640/897], Loss: 0.017409788444638252\n",
            "Epoch [7/10], Step [650/897], Loss: 0.0071387640200555325\n",
            "Epoch [7/10], Step [660/897], Loss: 0.07948508113622665\n",
            "Epoch [7/10], Step [670/897], Loss: 0.016610458493232727\n",
            "Epoch [7/10], Step [680/897], Loss: 0.011297093704342842\n",
            "Epoch [7/10], Step [690/897], Loss: 0.007184954360127449\n",
            "Epoch [7/10], Step [700/897], Loss: 0.045517824590206146\n",
            "Epoch [7/10], Step [710/897], Loss: 0.05853794887661934\n",
            "Epoch [7/10], Step [720/897], Loss: 0.009438084438443184\n",
            "Epoch [7/10], Step [730/897], Loss: 0.12886972725391388\n",
            "Epoch [7/10], Step [740/897], Loss: 0.026743005961179733\n",
            "Epoch [7/10], Step [750/897], Loss: 0.034118689596652985\n",
            "Epoch [7/10], Step [760/897], Loss: 0.04072849452495575\n",
            "Epoch [7/10], Step [770/897], Loss: 0.09813883900642395\n",
            "Epoch [7/10], Step [780/897], Loss: 0.0052739838138222694\n",
            "Epoch [7/10], Step [790/897], Loss: 0.012215320020914078\n",
            "Epoch [7/10], Step [800/897], Loss: 0.009654411114752293\n",
            "Epoch [7/10], Step [810/897], Loss: 0.040501177310943604\n",
            "Epoch [7/10], Step [820/897], Loss: 0.01924963854253292\n",
            "Epoch [7/10], Step [830/897], Loss: 0.1505851000547409\n",
            "Epoch [7/10], Step [840/897], Loss: 0.03216170892119408\n",
            "Epoch [7/10], Step [850/897], Loss: 0.013284538872539997\n",
            "Epoch [7/10], Step [860/897], Loss: 0.00924957450479269\n",
            "Epoch [7/10], Step [870/897], Loss: 0.009216590784490108\n",
            "Epoch [7/10], Step [880/897], Loss: 0.005587304942309856\n",
            "Epoch [7/10], Step [890/897], Loss: 0.07547374069690704\n",
            "Epoch [7/10], Train Loss: 0.0324725481231698, Train Accuracy: 0.9903775755674092\n",
            "Epoch [7/10], Validation Loss: 0.040970318753702134, Validation Accuracy: 0.9872411629366241\n",
            "Epoch [8/10], Step [10/897], Loss: 0.03452885523438454\n",
            "Epoch [8/10], Step [20/897], Loss: 0.05022666975855827\n",
            "Epoch [8/10], Step [30/897], Loss: 0.012255441397428513\n",
            "Epoch [8/10], Step [40/897], Loss: 0.05630100890994072\n",
            "Epoch [8/10], Step [50/897], Loss: 0.019495736807584763\n",
            "Epoch [8/10], Step [60/897], Loss: 0.10447077453136444\n",
            "Epoch [8/10], Step [70/897], Loss: 0.02327544055879116\n",
            "Epoch [8/10], Step [80/897], Loss: 0.025433747097849846\n",
            "Epoch [8/10], Step [90/897], Loss: 0.008937414735555649\n",
            "Epoch [8/10], Step [100/897], Loss: 0.004539260640740395\n",
            "Epoch [8/10], Step [110/897], Loss: 0.010377855971455574\n",
            "Epoch [8/10], Step [120/897], Loss: 0.028324836865067482\n",
            "Epoch [8/10], Step [130/897], Loss: 0.005559972487390041\n",
            "Epoch [8/10], Step [140/897], Loss: 0.022585969418287277\n",
            "Epoch [8/10], Step [150/897], Loss: 0.014207870699465275\n",
            "Epoch [8/10], Step [160/897], Loss: 0.03514380007982254\n",
            "Epoch [8/10], Step [170/897], Loss: 0.011536313220858574\n",
            "Epoch [8/10], Step [180/897], Loss: 0.014890684746205807\n",
            "Epoch [8/10], Step [190/897], Loss: 0.04390237480401993\n",
            "Epoch [8/10], Step [200/897], Loss: 0.004896399565041065\n",
            "Epoch [8/10], Step [210/897], Loss: 0.030117282643914223\n",
            "Epoch [8/10], Step [220/897], Loss: 0.026220623403787613\n",
            "Epoch [8/10], Step [230/897], Loss: 0.002241874346509576\n",
            "Epoch [8/10], Step [240/897], Loss: 0.0031999116763472557\n",
            "Epoch [8/10], Step [250/897], Loss: 0.011713866144418716\n",
            "Epoch [8/10], Step [260/897], Loss: 0.05800134316086769\n",
            "Epoch [8/10], Step [270/897], Loss: 0.02946327067911625\n",
            "Epoch [8/10], Step [280/897], Loss: 0.08790169656276703\n",
            "Epoch [8/10], Step [290/897], Loss: 0.01786552555859089\n",
            "Epoch [8/10], Step [300/897], Loss: 0.012076309882104397\n",
            "Epoch [8/10], Step [310/897], Loss: 0.05322679132223129\n",
            "Epoch [8/10], Step [320/897], Loss: 0.006430921144783497\n",
            "Epoch [8/10], Step [330/897], Loss: 0.009332015179097652\n",
            "Epoch [8/10], Step [340/897], Loss: 0.007016118615865707\n",
            "Epoch [8/10], Step [350/897], Loss: 0.018384497612714767\n",
            "Epoch [8/10], Step [360/897], Loss: 0.031105412170290947\n",
            "Epoch [8/10], Step [370/897], Loss: 0.10812170058488846\n",
            "Epoch [8/10], Step [380/897], Loss: 0.0112064890563488\n",
            "Epoch [8/10], Step [390/897], Loss: 0.004347269888967276\n",
            "Epoch [8/10], Step [400/897], Loss: 0.0025602763053029776\n",
            "Epoch [8/10], Step [410/897], Loss: 0.008558106608688831\n",
            "Epoch [8/10], Step [420/897], Loss: 0.06370040774345398\n",
            "Epoch [8/10], Step [430/897], Loss: 0.09920933842658997\n",
            "Epoch [8/10], Step [440/897], Loss: 0.07815977931022644\n",
            "Epoch [8/10], Step [450/897], Loss: 0.06333590298891068\n",
            "Epoch [8/10], Step [460/897], Loss: 0.002460247138515115\n",
            "Epoch [8/10], Step [470/897], Loss: 0.0641847625374794\n",
            "Epoch [8/10], Step [480/897], Loss: 0.01297038048505783\n",
            "Epoch [8/10], Step [490/897], Loss: 0.0033709171693772078\n",
            "Epoch [8/10], Step [500/897], Loss: 0.11875717341899872\n",
            "Epoch [8/10], Step [510/897], Loss: 0.04021260142326355\n",
            "Epoch [8/10], Step [520/897], Loss: 0.016515986993908882\n",
            "Epoch [8/10], Step [530/897], Loss: 0.04859306290745735\n",
            "Epoch [8/10], Step [540/897], Loss: 0.048735857009887695\n",
            "Epoch [8/10], Step [550/897], Loss: 0.023774808272719383\n",
            "Epoch [8/10], Step [560/897], Loss: 0.005082228686660528\n",
            "Epoch [8/10], Step [570/897], Loss: 0.009708670899271965\n",
            "Epoch [8/10], Step [580/897], Loss: 0.014130312949419022\n",
            "Epoch [8/10], Step [590/897], Loss: 0.049577973783016205\n",
            "Epoch [8/10], Step [600/897], Loss: 0.01174888014793396\n",
            "Epoch [8/10], Step [610/897], Loss: 0.01135648600757122\n",
            "Epoch [8/10], Step [620/897], Loss: 0.012012943625450134\n",
            "Epoch [8/10], Step [630/897], Loss: 0.0382169671356678\n",
            "Epoch [8/10], Step [640/897], Loss: 0.039237115532159805\n",
            "Epoch [8/10], Step [650/897], Loss: 0.009407876059412956\n",
            "Epoch [8/10], Step [660/897], Loss: 0.005665337201207876\n",
            "Epoch [8/10], Step [670/897], Loss: 0.007478299085050821\n",
            "Epoch [8/10], Step [680/897], Loss: 0.01705324836075306\n",
            "Epoch [8/10], Step [690/897], Loss: 0.009758977219462395\n",
            "Epoch [8/10], Step [700/897], Loss: 0.006070919334888458\n",
            "Epoch [8/10], Step [710/897], Loss: 0.009633072651922703\n",
            "Epoch [8/10], Step [720/897], Loss: 0.039220310747623444\n",
            "Epoch [8/10], Step [730/897], Loss: 0.009067095816135406\n",
            "Epoch [8/10], Step [740/897], Loss: 0.025364331901073456\n",
            "Epoch [8/10], Step [750/897], Loss: 0.005723549053072929\n",
            "Epoch [8/10], Step [760/897], Loss: 0.005272736307233572\n",
            "Epoch [8/10], Step [770/897], Loss: 0.013242187909781933\n",
            "Epoch [8/10], Step [780/897], Loss: 0.010544491931796074\n",
            "Epoch [8/10], Step [790/897], Loss: 0.021379930898547173\n",
            "Epoch [8/10], Step [800/897], Loss: 0.17138056457042694\n",
            "Epoch [8/10], Step [810/897], Loss: 0.011647838167846203\n",
            "Epoch [8/10], Step [820/897], Loss: 0.014523793943226337\n",
            "Epoch [8/10], Step [830/897], Loss: 0.020884251222014427\n",
            "Epoch [8/10], Step [840/897], Loss: 0.004331095609813929\n",
            "Epoch [8/10], Step [850/897], Loss: 0.005451359786093235\n",
            "Epoch [8/10], Step [860/897], Loss: 0.009579947218298912\n",
            "Epoch [8/10], Step [870/897], Loss: 0.04284211993217468\n",
            "Epoch [8/10], Step [880/897], Loss: 0.008708148263394833\n",
            "Epoch [8/10], Step [890/897], Loss: 0.0075942943803966045\n",
            "Epoch [8/10], Train Loss: 0.0321786561268335, Train Accuracy: 0.9899592092877314\n",
            "Epoch [8/10], Validation Loss: 0.040765545948449035, Validation Accuracy: 0.9874503241999582\n",
            "Epoch [9/10], Step [10/897], Loss: 0.006174287758767605\n",
            "Epoch [9/10], Step [20/897], Loss: 0.11733818799257278\n",
            "Epoch [9/10], Step [30/897], Loss: 0.0030293636955320835\n",
            "Epoch [9/10], Step [40/897], Loss: 0.03511165454983711\n",
            "Epoch [9/10], Step [50/897], Loss: 0.016499878838658333\n",
            "Epoch [9/10], Step [60/897], Loss: 0.03036228008568287\n",
            "Epoch [9/10], Step [70/897], Loss: 0.014166411012411118\n",
            "Epoch [9/10], Step [80/897], Loss: 0.0063017369247972965\n",
            "Epoch [9/10], Step [90/897], Loss: 0.017713341861963272\n",
            "Epoch [9/10], Step [100/897], Loss: 0.0641218051314354\n",
            "Epoch [9/10], Step [110/897], Loss: 0.041840825229883194\n",
            "Epoch [9/10], Step [120/897], Loss: 0.04068354144692421\n",
            "Epoch [9/10], Step [130/897], Loss: 0.003499586135149002\n",
            "Epoch [9/10], Step [140/897], Loss: 0.0034721989650279284\n",
            "Epoch [9/10], Step [150/897], Loss: 0.07098039984703064\n",
            "Epoch [9/10], Step [160/897], Loss: 0.003280709031969309\n",
            "Epoch [9/10], Step [170/897], Loss: 0.004429967142641544\n",
            "Epoch [9/10], Step [180/897], Loss: 0.10132435709238052\n",
            "Epoch [9/10], Step [190/897], Loss: 0.07626323401927948\n",
            "Epoch [9/10], Step [200/897], Loss: 0.025501562282443047\n",
            "Epoch [9/10], Step [210/897], Loss: 0.1676124483346939\n",
            "Epoch [9/10], Step [220/897], Loss: 0.005851542577147484\n",
            "Epoch [9/10], Step [230/897], Loss: 0.007669578772038221\n",
            "Epoch [9/10], Step [240/897], Loss: 0.13982775807380676\n",
            "Epoch [9/10], Step [250/897], Loss: 0.014732176437973976\n",
            "Epoch [9/10], Step [260/897], Loss: 0.005451074801385403\n",
            "Epoch [9/10], Step [270/897], Loss: 0.015456157736480236\n",
            "Epoch [9/10], Step [280/897], Loss: 0.01785960979759693\n",
            "Epoch [9/10], Step [290/897], Loss: 0.009958184324204922\n",
            "Epoch [9/10], Step [300/897], Loss: 0.02156279794871807\n",
            "Epoch [9/10], Step [310/897], Loss: 0.0115051856264472\n",
            "Epoch [9/10], Step [320/897], Loss: 0.10065270960330963\n",
            "Epoch [9/10], Step [340/897], Loss: 0.05126737430691719\n",
            "Epoch [9/10], Step [350/897], Loss: 0.017214590683579445\n",
            "Epoch [9/10], Step [360/897], Loss: 0.06228036433458328\n",
            "Epoch [9/10], Step [370/897], Loss: 0.009945286437869072\n",
            "Epoch [9/10], Step [380/897], Loss: 0.005390909966081381\n",
            "Epoch [9/10], Step [390/897], Loss: 0.005630864296108484\n",
            "Epoch [9/10], Step [400/897], Loss: 0.008412179537117481\n",
            "Epoch [9/10], Step [410/897], Loss: 0.04782303795218468\n",
            "Epoch [9/10], Step [420/897], Loss: 0.007612321060150862\n",
            "Epoch [9/10], Step [430/897], Loss: 0.09347199648618698\n",
            "Epoch [9/10], Step [440/897], Loss: 0.01318550668656826\n",
            "Epoch [9/10], Step [450/897], Loss: 0.01605403982102871\n",
            "Epoch [9/10], Step [460/897], Loss: 0.017904004082083702\n",
            "Epoch [9/10], Step [470/897], Loss: 0.01941727101802826\n",
            "Epoch [9/10], Step [480/897], Loss: 0.01130867563188076\n",
            "Epoch [9/10], Step [490/897], Loss: 0.06102539971470833\n",
            "Epoch [9/10], Step [500/897], Loss: 0.01053463015705347\n",
            "Epoch [9/10], Step [510/897], Loss: 0.006333568599075079\n",
            "Epoch [9/10], Step [520/897], Loss: 0.0315699465572834\n",
            "Epoch [9/10], Step [530/897], Loss: 0.01383266318589449\n",
            "Epoch [9/10], Step [540/897], Loss: 0.057249002158641815\n",
            "Epoch [9/10], Step [550/897], Loss: 0.019743381068110466\n",
            "Epoch [9/10], Step [560/897], Loss: 0.03653467446565628\n",
            "Epoch [9/10], Step [570/897], Loss: 0.012127457186579704\n",
            "Epoch [9/10], Step [580/897], Loss: 0.06696303933858871\n",
            "Epoch [9/10], Step [590/897], Loss: 0.012255925685167313\n",
            "Epoch [9/10], Step [600/897], Loss: 0.010341182351112366\n",
            "Epoch [9/10], Step [610/897], Loss: 0.30500417947769165\n",
            "Epoch [9/10], Step [620/897], Loss: 0.005233179312199354\n",
            "Epoch [9/10], Step [630/897], Loss: 0.006501541938632727\n",
            "Epoch [9/10], Step [640/897], Loss: 0.01684538833796978\n",
            "Epoch [9/10], Step [650/897], Loss: 0.03652983531355858\n",
            "Epoch [9/10], Step [660/897], Loss: 0.006084189284592867\n",
            "Epoch [9/10], Step [670/897], Loss: 0.05966461822390556\n",
            "Epoch [9/10], Step [680/897], Loss: 0.015638301149010658\n",
            "Epoch [9/10], Step [690/897], Loss: 0.005378098227083683\n",
            "Epoch [9/10], Step [700/897], Loss: 0.01593475416302681\n",
            "Epoch [9/10], Step [710/897], Loss: 0.00882047414779663\n",
            "Epoch [9/10], Step [720/897], Loss: 0.020944984629750252\n",
            "Epoch [9/10], Step [730/897], Loss: 0.21306124329566956\n",
            "Epoch [9/10], Step [740/897], Loss: 0.010479919612407684\n",
            "Epoch [9/10], Step [750/897], Loss: 0.01371784694492817\n",
            "Epoch [9/10], Step [760/897], Loss: 0.00920886266976595\n",
            "Epoch [9/10], Step [770/897], Loss: 0.009142926894128323\n",
            "Epoch [9/10], Step [780/897], Loss: 0.010689868591725826\n",
            "Epoch [9/10], Step [790/897], Loss: 0.05477992817759514\n",
            "Epoch [9/10], Step [800/897], Loss: 0.005465470254421234\n",
            "Epoch [9/10], Step [810/897], Loss: 0.020633738487958908\n",
            "Epoch [9/10], Step [820/897], Loss: 0.058547984808683395\n",
            "Epoch [9/10], Step [830/897], Loss: 0.023381555452942848\n",
            "Epoch [9/10], Step [840/897], Loss: 0.005635484587401152\n",
            "Epoch [9/10], Step [850/897], Loss: 0.008393092080950737\n",
            "Epoch [9/10], Step [860/897], Loss: 0.025612054392695427\n",
            "Epoch [9/10], Step [870/897], Loss: 0.0014901834074407816\n",
            "Epoch [9/10], Step [880/897], Loss: 0.045156750828027725\n",
            "Epoch [9/10], Step [890/897], Loss: 0.03707192465662956\n",
            "Epoch [9/10], Train Loss: 0.03184873391778095, Train Accuracy: 0.9901335285709305\n",
            "Epoch [9/10], Validation Loss: 0.04413577605172817, Validation Accuracy: 0.9857770340932859\n",
            "Epoch [10/10], Step [10/897], Loss: 0.024631232023239136\n",
            "Epoch [10/10], Step [20/897], Loss: 0.022500472143292427\n",
            "Epoch [10/10], Step [30/897], Loss: 0.0029530897736549377\n",
            "Epoch [10/10], Step [40/897], Loss: 0.016927221789956093\n",
            "Epoch [10/10], Step [50/897], Loss: 0.009182378649711609\n",
            "Epoch [10/10], Step [60/897], Loss: 0.09516852349042892\n",
            "Epoch [10/10], Step [70/897], Loss: 0.11596433818340302\n",
            "Epoch [10/10], Step [80/897], Loss: 0.05243849381804466\n",
            "Epoch [10/10], Step [90/897], Loss: 0.011648283340036869\n",
            "Epoch [10/10], Step [100/897], Loss: 0.09758821874856949\n",
            "Epoch [10/10], Step [110/897], Loss: 0.04797489568591118\n",
            "Epoch [10/10], Step [120/897], Loss: 0.0041311620734632015\n",
            "Epoch [10/10], Step [130/897], Loss: 0.05072410777211189\n",
            "Epoch [10/10], Step [140/897], Loss: 0.084626205265522\n",
            "Epoch [10/10], Step [150/897], Loss: 0.010154586285352707\n",
            "Epoch [10/10], Step [160/897], Loss: 0.008245151489973068\n",
            "Epoch [10/10], Step [170/897], Loss: 0.009884770028293133\n",
            "Epoch [10/10], Step [180/897], Loss: 0.02091110683977604\n",
            "Epoch [10/10], Step [190/897], Loss: 0.015485335141420364\n",
            "Epoch [10/10], Step [200/897], Loss: 0.006625533103942871\n",
            "Epoch [10/10], Step [210/897], Loss: 0.007462590467184782\n",
            "Epoch [10/10], Step [220/897], Loss: 0.009700527414679527\n",
            "Epoch [10/10], Step [230/897], Loss: 0.08257603645324707\n",
            "Epoch [10/10], Step [240/897], Loss: 0.011438461020588875\n",
            "Epoch [10/10], Step [250/897], Loss: 0.11043668538331985\n",
            "Epoch [10/10], Step [260/897], Loss: 0.07503139972686768\n",
            "Epoch [10/10], Step [270/897], Loss: 0.06590066105127335\n",
            "Epoch [10/10], Step [280/897], Loss: 0.01876150630414486\n",
            "Epoch [10/10], Step [290/897], Loss: 0.05762054771184921\n",
            "Epoch [10/10], Step [300/897], Loss: 0.013938573189079762\n",
            "Epoch [10/10], Step [310/897], Loss: 0.06377998739480972\n",
            "Epoch [10/10], Step [320/897], Loss: 0.006666176952421665\n",
            "Epoch [10/10], Step [330/897], Loss: 0.014334945939481258\n",
            "Epoch [10/10], Step [340/897], Loss: 0.03356283903121948\n",
            "Epoch [10/10], Step [350/897], Loss: 0.00709251593798399\n",
            "Epoch [10/10], Step [360/897], Loss: 0.014727966859936714\n",
            "Epoch [10/10], Step [370/897], Loss: 0.036368560045957565\n",
            "Epoch [10/10], Step [380/897], Loss: 0.009203857742249966\n",
            "Epoch [10/10], Step [390/897], Loss: 0.024593418464064598\n",
            "Epoch [10/10], Step [400/897], Loss: 0.004890578333288431\n",
            "Epoch [10/10], Step [410/897], Loss: 0.04409630596637726\n",
            "Epoch [10/10], Step [420/897], Loss: 0.006800881586968899\n",
            "Epoch [10/10], Step [430/897], Loss: 0.038477156311273575\n",
            "Epoch [10/10], Step [440/897], Loss: 0.007383342366665602\n",
            "Epoch [10/10], Step [450/897], Loss: 0.012025321833789349\n",
            "Epoch [10/10], Step [460/897], Loss: 0.006003479473292828\n",
            "Epoch [10/10], Step [470/897], Loss: 0.02330024540424347\n",
            "Epoch [10/10], Step [480/897], Loss: 0.0062031676061451435\n",
            "Epoch [10/10], Step [490/897], Loss: 0.046080149710178375\n",
            "Epoch [10/10], Step [500/897], Loss: 0.004156170878559351\n",
            "Epoch [10/10], Step [510/897], Loss: 0.041487205773591995\n",
            "Epoch [10/10], Step [520/897], Loss: 0.0057641672901809216\n",
            "Epoch [10/10], Step [530/897], Loss: 0.016384176909923553\n",
            "Epoch [10/10], Step [540/897], Loss: 0.03453483805060387\n",
            "Epoch [10/10], Step [550/897], Loss: 0.010489780455827713\n",
            "Epoch [10/10], Step [560/897], Loss: 0.02288428135216236\n",
            "Epoch [10/10], Step [570/897], Loss: 0.03528722748160362\n",
            "Epoch [10/10], Step [580/897], Loss: 0.009576931595802307\n",
            "Epoch [10/10], Step [590/897], Loss: 0.009668664075434208\n",
            "Epoch [10/10], Step [600/897], Loss: 0.030528834089636803\n",
            "Epoch [10/10], Step [610/897], Loss: 0.09035733342170715\n",
            "Epoch [10/10], Step [620/897], Loss: 0.020773857831954956\n",
            "Epoch [10/10], Step [630/897], Loss: 0.029377324506640434\n",
            "Epoch [10/10], Step [640/897], Loss: 0.009065497666597366\n",
            "Epoch [10/10], Step [650/897], Loss: 0.006064251996576786\n",
            "Epoch [10/10], Step [660/897], Loss: 0.009916981682181358\n",
            "Epoch [10/10], Step [670/897], Loss: 0.023888112977147102\n",
            "Epoch [10/10], Step [680/897], Loss: 0.019380245357751846\n",
            "Epoch [10/10], Step [690/897], Loss: 0.0010504494421184063\n",
            "Epoch [10/10], Step [700/897], Loss: 0.027875537052750587\n",
            "Epoch [10/10], Step [710/897], Loss: 0.19571936130523682\n",
            "Epoch [10/10], Step [720/897], Loss: 0.02739240974187851\n",
            "Epoch [10/10], Step [730/897], Loss: 0.016850365325808525\n",
            "Epoch [10/10], Step [740/897], Loss: 0.0063640824519097805\n",
            "Epoch [10/10], Step [750/897], Loss: 0.012600153684616089\n",
            "Epoch [10/10], Step [760/897], Loss: 0.1081719771027565\n",
            "Epoch [10/10], Step [770/897], Loss: 0.05169946327805519\n",
            "Epoch [10/10], Step [780/897], Loss: 0.057462941855192184\n",
            "Epoch [10/10], Step [790/897], Loss: 0.008151398971676826\n",
            "Epoch [10/10], Step [800/897], Loss: 0.00929140206426382\n",
            "Epoch [10/10], Step [810/897], Loss: 0.07305657118558884\n",
            "Epoch [10/10], Step [820/897], Loss: 0.0014037860091775656\n",
            "Epoch [10/10], Step [830/897], Loss: 0.00753503292798996\n",
            "Epoch [10/10], Step [840/897], Loss: 0.15910719335079193\n",
            "Epoch [10/10], Step [850/897], Loss: 0.07875008881092072\n",
            "Epoch [10/10], Step [860/897], Loss: 0.003910616505891085\n",
            "Epoch [10/10], Step [870/897], Loss: 0.07309570163488388\n",
            "Epoch [10/10], Step [880/897], Loss: 0.011261477135121822\n",
            "Epoch [10/10], Step [890/897], Loss: 0.00842871330678463\n",
            "Epoch [10/10], Train Loss: 0.031624547950689835, Train Accuracy: 0.9904473032806889\n",
            "Epoch [10/10], Validation Loss: 0.04074269944543399, Validation Accuracy: 0.9872411629366241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-instantiate the model\n",
        "model = RBFModel(10000, 1000, 2, all_centers, [sigma])\n",
        "\n",
        "# Load the saved model state dictionary\n",
        "model_state_dict = torch.load('best_model.pth')\n",
        "\n",
        "# Load the model state dictionary into the model\n",
        "model.load_state_dict(model_state_dict)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-rclGF3LlSh",
        "outputId": "cede9801-cb25-4c07-e82a-23f455423c29"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RBFModel(\n",
              "  (linear): Linear(in_features=1000, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (batch_x, batch_y) in enumerate(train_loader):\n",
        "        batch_x, batch_y = batch_x.to(torch.float32).to(device), batch_y.to(device)\n",
        "        output = model(batch_x)\n",
        "        break"
      ],
      "metadata": {
        "id": "UxJgq6lJK_Gi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31lEhP2pMDiO",
        "outputId": "1b1b9353-b281-47d9-cabe-45a5e025f0a0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0]], device='cuda:0', dtype=torch.int8)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZePCnjZLFBE",
        "outputId": "f4de6f35-9e8b-45a0-c666-34e85bd79a09"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.5949, -3.7890],\n",
              "        [ 2.3963, -2.5447],\n",
              "        [ 4.7550, -4.9008],\n",
              "        [-2.0959,  1.8883],\n",
              "        [ 1.0827, -1.2903],\n",
              "        [ 4.2781, -4.4563],\n",
              "        [ 1.8530, -2.1296],\n",
              "        [ 2.4713, -2.7093],\n",
              "        [ 3.5167, -3.6290],\n",
              "        [ 3.1558, -3.3632],\n",
              "        [ 2.7086, -2.9140],\n",
              "        [ 4.2223, -4.3793],\n",
              "        [ 2.4756, -2.7242],\n",
              "        [ 5.2013, -5.3457],\n",
              "        [ 3.3582, -3.5315],\n",
              "        [ 4.8625, -5.0373],\n",
              "        [ 2.8663, -3.1130],\n",
              "        [ 2.2492, -2.4581],\n",
              "        [ 2.9067, -3.1698],\n",
              "        [ 3.5118, -3.6695],\n",
              "        [ 3.2226, -3.4617],\n",
              "        [ 1.3615, -1.5182],\n",
              "        [ 2.4652, -2.7245],\n",
              "        [ 3.8473, -4.0772],\n",
              "        [ 2.7479, -2.9086],\n",
              "        [ 4.8274, -4.9394],\n",
              "        [ 4.1073, -4.3304],\n",
              "        [ 2.5318, -2.7993],\n",
              "        [ 3.7883, -4.0139],\n",
              "        [ 3.2683, -3.5091],\n",
              "        [ 2.5497, -2.8065],\n",
              "        [ 3.9403, -4.1609]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, you can extract the trained parameters\n",
        "trained_centers = model.centers.detach().cpu().numpy()\n",
        "trained_sigmas = model.sigmas.detach().cpu().numpy()\n",
        "\n",
        "# Print or use the trained parameters\n",
        "print(\"Trained Centers:\", trained_centers)\n",
        "print(\"Trained Sigmas:\", trained_sigmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvj498VR87gn",
        "outputId": "93c8d761-f1e9-4604-a72d-61b49eb99ce9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Centers: [[ 6.82626901e-01 -1.57842796e-01 -9.10633264e-01 ... -1.35908069e-03\n",
            "  -7.18771835e-06 -3.75721239e-07]\n",
            " [-6.69717064e-02 -1.14904760e-01 -9.74693168e-01 ... -7.95601256e-04\n",
            "  -1.61720848e-06  5.57291111e-08]\n",
            " [ 1.13970780e-01  1.01621359e-02 -8.41641331e-01 ... -1.29463659e-03\n",
            "  -5.19032333e-06 -9.45904902e-07]\n",
            " ...\n",
            " [ 3.28155071e-02 -8.84417520e-02  1.41652337e+00 ...  8.22687592e-04\n",
            "  -1.71252737e-04  1.12640569e-07]\n",
            " [-5.62768074e-02 -8.98881165e-02 -9.67118275e-01 ... -7.32456894e-04\n",
            "  -8.73512269e-06 -3.86629888e-08]\n",
            " [ 1.01287085e+00  2.35785213e-01  1.07589317e+00 ...  1.32226698e-03\n",
            "  -1.63619668e-04  2.84134646e-07]]\n",
            "Trained Sigmas: [4.9973483]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_centers.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJGskhNEM-UM",
        "outputId": "38c69ba7-7763-48c5-f050-e32ddbbd06f0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_centers[0][:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHc5HGHRMSQi",
        "outputId": "2ce1d659-c206-442f-8c6d-14c01c07a070"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6.82626901e-01, -1.57842796e-01, -9.10633264e-01,  2.37577198e-01,\n",
              "       -6.26838652e-02,  6.70036206e-01, -1.60009342e-01, -2.23739003e-01,\n",
              "        1.18476455e-01, -3.87529193e-01,  2.73201868e-02, -4.53082282e-01,\n",
              "       -4.94938135e-01,  9.96039044e-01,  4.91933296e-02,  8.45646006e-02,\n",
              "       -3.00290185e-01, -6.18974304e-02,  8.16441501e-01,  7.67651765e-01,\n",
              "        4.17308083e-01,  3.52254245e-02,  1.71979274e-01,  1.04967470e-01,\n",
              "        1.13975330e-01,  1.22958523e+00, -6.97449781e-02, -3.97829565e-01,\n",
              "        2.14242215e-01,  4.87425539e-01, -2.96133903e-01, -3.67946295e-02,\n",
              "        7.64374631e-02, -1.10759236e-01,  9.25623384e-02, -2.34933596e-01,\n",
              "        4.64434705e-01,  1.00638761e+00, -5.27856739e-01,  8.17454471e-02,\n",
              "       -2.76793184e-02,  1.77565128e-01, -3.23893320e-01, -5.36100208e-01,\n",
              "       -2.74831020e-01, -2.64936380e-01, -2.19044896e-01, -2.59677026e-01,\n",
              "       -3.90212805e-01,  2.23359263e-01,  2.52186492e-01, -2.46078962e-01,\n",
              "        7.84815601e-01, -2.45909099e-01, -4.76057581e-01, -2.29403669e-01,\n",
              "       -2.43877811e-01, -2.07786805e-01, -2.93487542e-01, -2.92132099e-01,\n",
              "       -2.92132099e-01, -2.92132099e-01, -2.92132099e-01, -1.75710201e-01,\n",
              "       -6.37137946e-02,  1.16532086e-01,  4.10407359e-01,  1.61695316e-01,\n",
              "       -2.36734832e-01, -2.76807038e-01, -1.00817390e-03, -2.35789674e-01,\n",
              "        9.22065593e-01,  1.10533786e-01,  1.43627853e-01, -7.00624959e-02,\n",
              "       -2.70299651e-01,  2.67549966e-01,  1.02795251e+00,  2.15815789e-01,\n",
              "       -1.79046458e-01, -2.65419197e-01,  8.99715689e-02,  1.05557532e+00,\n",
              "        2.60867759e-02,  3.55341875e-01, -3.18106643e-02,  4.81192107e-02,\n",
              "       -1.24726429e-01, -1.24726429e-01, -1.24726429e-01, -1.49117785e-01,\n",
              "       -7.27853148e-02, -7.21400559e-02, -4.95310067e-02,  1.89280473e-01,\n",
              "       -1.78665013e-01,  6.84531162e-01, -5.56174291e-01, -1.39422240e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9JL7fSGAMiO",
        "outputId": "ab339fb3-9a2c-4fa9-a947-21be5661a531"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}