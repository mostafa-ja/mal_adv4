{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO5ACkIZxWo7aG6BjrmESm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/mal_adv4/blob/main/adverserial_attacks_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,balanced_accuracy_score\n",
        "from scipy import sparse\n",
        "import gdown\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "G-8aXXZ9Be6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MalwareDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size=10000, hidden_1_size=200, hidden_2_size=200, num_labels=2, dropout_prob=0.6):\n",
        "        super(MalwareDetectionModel, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_1_size = hidden_1_size\n",
        "        self.hidden_2_size = hidden_2_size\n",
        "        self.num_labels = num_labels\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_1_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        #self.dropout1 = nn.Dropout(dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_1_size, hidden_2_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_prob)\n",
        "        self.fc3 = nn.Linear(hidden_2_size, num_labels)\n",
        "        #self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        #x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        #x = self.log_softmax(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "xdNbTvxTTqyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWbPG4QvZxtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, lr_step=(20,30,40), lr_decay_ratio=0.2):\n",
        "    \"\"\"Adjust the learning rate based on the epoch number.\"\"\"\n",
        "    if epoch == 0:\n",
        "        optimizer.param_groups[0]['lr'] /= 8\n",
        "    elif epoch in [1, 2, 3]:  # in step five , we finish warm up ,and start normal learning rate\n",
        "        optimizer.param_groups[0]['lr'] *= 2\n",
        "    if epoch in lr_step: # in these steps , we are geting close to optimal point so we need to have shorter step\n",
        "        optimizer.param_groups[0]['lr'] *= lr_decay_ratio\n",
        "    return optimizer\n"
      ],
      "metadata": {
        "id": "5QhoGvdY_w4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adversarial_training(model, train_loader, val_loader, attack, adv_epochs=50, lr=0.001, weight_decay=0., device=device, verbose=True, **kwargs):\n",
        "\n",
        "    # Assuming positive class (malware) is label 1\n",
        "    #class_weights = torch.tensor([0.11, 0.89]).to(device)  # Adjust the weights based on the class distribution, higher weight for positive class\n",
        "\n",
        "    # Define Loss Function and Optimizer\n",
        "    #criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    total_time = 0.\n",
        "    nbatches = len(train_loader)\n",
        "    best_acc_val = 0.\n",
        "    acc_val_adv_be = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(adv_epochs):\n",
        "        epoch_losses = []\n",
        "        epoch_accuracies = []\n",
        "        optimizer = adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "        for idx_batch, (x_batch, y_batch) in enumerate(train_loader):\n",
        "            x_batch, y_batch = x_batch.to(torch.float32).to(device), y_batch.to(device)\n",
        "            batch_size = x_batch.shape[0]\n",
        "\n",
        "            # Separate malicious and benign samples\n",
        "            mal_x_batch, ben_x_batch = x_batch[y_batch.squeeze() == 1], x_batch[y_batch.squeeze() == 0]\n",
        "            mal_y_batch, ben_y_batch = y_batch[y_batch.squeeze() == 1], y_batch[y_batch.squeeze() == 0]\n",
        "\n",
        "            # Generate adversarial examples\n",
        "            model.eval()\n",
        "            pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "            x_batch = torch.cat([ben_x_batch, pertb_mal_x], dim=0)\n",
        "            y_batch = torch.cat([ben_y_batch, mal_y_batch])\n",
        "            model.train()\n",
        "\n",
        "            # Forward pass and backward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            loss_train = criterion(outputs, y_batch.view(-1).long())\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate metrics\n",
        "            epoch_losses.append(loss_train.item())\n",
        "            predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            acc_train = (predicted == y_batch).sum().item() / len(y_batch)\n",
        "            epoch_accuracies.append(acc_train)\n",
        "\n",
        "            # Print batch level information\n",
        "            #if verbose:\n",
        "                #print(f'Mini batch: {idx_batch + 1}/{nbatches} | Epoch: {epoch + 1}/{adv_epochs} | Batch Loss: {loss_train.item():.4f} | Batch Accuracy: {acc_train * 100:.2f}%')\n",
        "\n",
        "        # Calculate epoch level metrics\n",
        "        mean_loss = np.mean(epoch_losses)\n",
        "        mean_accuracy = np.mean(epoch_accuracies) * 100\n",
        "\n",
        "        # Print epoch level information\n",
        "        if verbose:\n",
        "            print(f'Epoch: {epoch+1}/{adv_epochs} | Training loss (epoch level): {mean_loss:.4f} | Train accuracy: {mean_accuracy:.2f}%')\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        model.eval()\n",
        "        cor_val = 0\n",
        "        cor_ad_val = 0\n",
        "        n_samples = 0\n",
        "        n_ad_samples = 0\n",
        "        for x_val, y_val in val_loader:\n",
        "            x_val, y_val = x_val.to(torch.float32).to(device), y_val.to(device)\n",
        "            n_samples += len(x_val)\n",
        "            outputs = model(x_val)\n",
        "            predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            cor_val += (predicted == y_val).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for validation set\n",
        "            mal_x_batch, mal_y_batch = x_val[y_val.squeeze() == 1], y_val[y_val.squeeze() == 1]\n",
        "            n_ad_samples += len(mal_x_batch)\n",
        "            pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_val += (y_pred == 1.).sum().item()\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        assert n_ad_samples > 0\n",
        "        avg_acc_val = (cor_val / n_samples)\n",
        "        avg_acc_ad_val = (cor_ad_val / n_ad_samples)\n",
        "        acc_all = (avg_acc_val + avg_acc_ad_val) / 2.\n",
        "\n",
        "        # Update best validation accuracy\n",
        "        if acc_all >= best_acc_val:\n",
        "            best_acc_val = acc_all\n",
        "            acc_val_adv_be = avg_acc_ad_val\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), 'model_AT_rFGSM.pth')\n",
        "\n",
        "        # Print validation results\n",
        "        if verbose:\n",
        "            print(f\"\\tVal accuracy(without attack) {(avg_acc_val) * 100:.4}% and accuracy(with attack) {(avg_acc_ad_val) * 100:.4}% under attack and overall accuracy {acc_all * 100:.4}%.\")\n",
        "            print(f\"\\tModel select at epoch {best_epoch} with validation accuracy {best_acc_val * 100:.4}% and accuracy {acc_val_adv_be * 100:.4}% under attack.\")\n"
      ],
      "metadata": {
        "id": "B0v7V8W7lidM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jhWKL4dZcwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    if attack == mimicry:\n",
        "      # Pre-select benign samples\n",
        "      benign_samples = []\n",
        "      for x_batch, y_batch in test_loader:\n",
        "        benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "      ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "      del benign_samples\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            if attack == mimicry:\n",
        "                pertb_mal_x = mimicry(ben_x, mal_x_batch, model, **kwargs)\n",
        "            else :\n",
        "                with torch.enable_grad():\n",
        "                    pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n",
        "    if attack == mimicry:\n",
        "      del ben_x\n"
      ],
      "metadata": {
        "id": "B2S5U28YIGzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def round_x(x, round_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Rounds x by thresholding it according to round_threshold.\n",
        "    :param x: input tensor\n",
        "    :param round_threshold: threshold parameter\n",
        "    :return: a tensor of 0s and 1s\n",
        "    \"\"\"\n",
        "    return (x >= round_threshold).float()\n",
        "\n",
        "def get_x0(x, initial_rounding_threshold=0.5, is_sample=False):\n",
        "    \"\"\"\n",
        "    Helper function to randomly initialize the inner maximizer algorithm.\n",
        "    Randomizes the input tensor while preserving its functionality.\n",
        "    :param x: input tensor\n",
        "    :param rounding_threshold: threshold for rounding\n",
        "    :param is_sample: flag to sample randomly from feasible area\n",
        "    :return: randomly sampled feasible version of x\n",
        "    \"\"\"\n",
        "    if is_sample:\n",
        "        rand_x = round_x(torch.rand(x.size()), initial_rounding_threshold=initial_rounding_threshold)\n",
        "        return (rand_x.byte() | x.byte()).float()\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "def or_float_tensors(x_1, x_2):\n",
        "    \"\"\"\n",
        "    ORs two float tensors by converting them to byte and back.\n",
        "    :param x_1: tensor one\n",
        "    :param x_2: tensor two\n",
        "    :return: float tensor of 0s and 1s\n",
        "    \"\"\"\n",
        "    return (x_1.byte() | x_2.byte()).float()\n",
        "\n",
        "\n",
        "def xor_float_tensors(x_1, x_2):\n",
        "    \"\"\"\n",
        "    XORs two float tensors by converting them to byte and back\n",
        "    Note that byte() takes the first 8 bit after the decimal point of the float\n",
        "    e.g., 0.0 ==> 0\n",
        "          0.1 ==> 0\n",
        "          1.1 ==> 1\n",
        "        255.1 ==> 255\n",
        "        256.1 ==> 0\n",
        "    Subsequently the purpose of this function is to map 1s float tensors to 1\n",
        "    and those of 0s to 0. I.e., it is meant to be used on tensors of 0s and 1s.\n",
        "\n",
        "    :param x_1: tensor one\n",
        "    :param x_2: tensor two\n",
        "    :return: float tensor of 0s and 1s.\n",
        "    \"\"\"\n",
        "    return (x_1.byte() ^ x_2.byte()).float()\n",
        "\n",
        "def get_loss(x,y,model):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(x)\n",
        "    loss = criterion(outputs, y.view(-1).long())\n",
        "    _, predicted = torch.topk(outputs, k=1)\n",
        "    done = (predicted != y).squeeze()\n",
        "\n",
        "    return loss, done\n",
        "\n"
      ],
      "metadata": {
        "id": "XCbqeJ84gfDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, labels_batch in test_loader:\n",
        "            X_batch, labels_batch = X_batch.to(torch.float32).to(device), labels_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.topk(outputs, k=1)\n",
        "            predictions.extend(predicted.tolist())\n",
        "            true_labels.extend(labels_batch.tolist())\n",
        "\n",
        "    # Convert predictions and true labels to numpy arrays\n",
        "    predictions = np.array(predictions)\n",
        "    true_labels = np.array(true_labels)\n",
        "\n",
        "    # Calculate and print test accuracy\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    balanced_acc = balanced_accuracy_score(true_labels, predictions)\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Test balanced Accuracy: {balanced_acc:.4f}')\n",
        "\n",
        "    # Calculate and print precision, recall, and F1-score\n",
        "    precision = precision_score(true_labels, predictions)\n",
        "    recall = recall_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions)\n",
        "\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-score: {f1:.4f}')\n",
        "\n",
        "    # Calculate and print true positives, true negatives, false positives, and false negatives\n",
        "    TP = ((predictions == 1) & (true_labels == 1)).sum()\n",
        "    TN = ((predictions == 0) & (true_labels == 0)).sum()\n",
        "    FP = ((predictions == 1) & (true_labels == 0)).sum()\n",
        "    FN = ((predictions == 0) & (true_labels == 1)).sum()\n",
        "\n",
        "    print(f'True Positives (TP): {TP}')\n",
        "    print(f'True Negatives (TN): {TN}')\n",
        "    print(f'False Positives (FP): {FP}')\n",
        "    print(f'False Negatives (FN): {FN}')\n",
        "\n",
        "    # Calculate and print False Negative Rate (FNR) and False Positive Rate (FPR)\n",
        "    FNR = (FN / (FN + TP)) * 100\n",
        "    FPR = (FP / (FP + TN)) * 100\n",
        "\n",
        "    print(f'False Negative Rate (FNR): {FNR:.4f}')\n",
        "    print(f'False Positive Rate (FPR): {FPR:.4f}')"
      ],
      "metadata": {
        "id": "yX_2SRH7_PnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dfgsm_k(x, y, model, k=25, epsilon=0.02, alpha=1., initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    FGSM^k with deterministic rounding\n",
        "    :param y: ground truth labels\n",
        "    :param x: feature vector\n",
        "    :param model: neural network model\n",
        "    :param k: number of steps\n",
        "    :param epsilon: update value in each direction\n",
        "    :param alpha: hyperparameter for controlling the portionate of rounding\n",
        "    :param initial_rounding_threshold: threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: threshold parameter for rounding\n",
        "    :param is_report_loss_diff: flag to report loss difference\n",
        "    :param is_sample: flag to sample randomly from the feasible area\n",
        "    :return: the adversarial version of x according to dfgsm_k (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step\n",
        "    for t in range(k):\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        # Find the next sample\n",
        "        x_next = x_next + epsilon * torch.sign(grad_vars[0].data)\n",
        "\n",
        "        # Projection\n",
        "        x_next = torch.clamp(x_next, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = (torch.rand(x_next.size()) * alpha).to(device=x.device)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        #print(f\"Natural loss: {loss_natural.mean():.4f}, Adversarial loss: {loss_adv.mean():.4f}, Difference: {(loss_adv.mean() - loss_natural.mean()):.4f}\")\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"dFGSM: attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next\n"
      ],
      "metadata": {
        "id": "H6BiF-vCHLlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bga_k(x, y, model, k=25, alpha=1., is_report_loss_diff=True, use_sample=False):\n",
        "    \"\"\"\n",
        "    Multi-step bit gradient ascent\n",
        "    :param x: feature vector\n",
        "    :param y: ground truth labels\n",
        "    :param model: neural network model\n",
        "    :param k: number of steps\n",
        "    :param alpha: hyperparameter for controlling updates\n",
        "    :param is_report_loss_diff: flag to report loss difference\n",
        "    :param use_sample: flag to sample randomly from the feasible area\n",
        "    :return: the adversarial version of x according to bga_k (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize worst loss and corresponding adversarial samples\n",
        "    loss_worst = loss_natural.clone()\n",
        "    x_worst = x.clone()\n",
        "\n",
        "    # Book-keeping\n",
        "    sqrt_m = (torch.sqrt(torch.tensor([x.size()[1]], dtype=torch.float))).to(x.device)\n",
        "\n",
        "    # Multi-step with gradients\n",
        "    for t in range(k):\n",
        "        if t == 0:\n",
        "            # Initialize starting point\n",
        "            x_next = get_x0(x, use_sample)\n",
        "        else:\n",
        "            # Compute gradient\n",
        "            grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "            grad_data = grad_vars[0].data\n",
        "\n",
        "            # Compute the updates\n",
        "            # torch.norm(grad_data, 2, 1), 2:the L2-norm , 1:the norm along dimension 1\n",
        "            x_update = (sqrt_m * (1. - 2. * x_next) * grad_data >= (alpha * torch.norm(grad_data, 2, 1).unsqueeze(1))).float()\n",
        "\n",
        "            # Find the next sample with projection to the feasible set\n",
        "            x_next = xor_float_tensors(x_update, x_next)\n",
        "            x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "        # Update worst loss and adversarial samples\n",
        "        replace_flag = (loss.data > loss_worst)\n",
        "        loss_worst[replace_flag] = loss.data[replace_flag]\n",
        "        x_worst[replace_flag] = x_next[replace_flag]\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        #print(f\"Natural loss: {loss_natural.mean():.4f}, Adversarial loss: {loss_worst.mean():.4f}, Difference: {(loss_worst.mean() - loss_natural.mean()):.4f}\")\n",
        "        outputs = model(x_worst)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"bga_k: attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    return x_worst\n"
      ],
      "metadata": {
        "id": "V_1x56e5UYFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bca_k(x, y, model, k=25, is_report_loss_diff=True, use_sample=False):\n",
        "    \"\"\"\n",
        "    Multi-step bit coordinate ascent\n",
        "    :param use_sample:\n",
        "    :param is_report_loss_diff:\n",
        "    :param y:\n",
        "    :param x: (tensor) feature vector\n",
        "    :param model: nn model\n",
        "    :param k: num of steps\n",
        "    :return: the adversarial version of x according to bca_k (tensor)\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # keeping worst loss\n",
        "    loss_worst = loss_natural.clone()\n",
        "    x_worst = x.clone()\n",
        "\n",
        "    # multi-step with gradients\n",
        "    loss = None\n",
        "    x_var = None\n",
        "    x_next = None\n",
        "    for t in range(k):\n",
        "        if t == 0:\n",
        "            # initialize starting point\n",
        "            x_next = get_x0(x, use_sample)\n",
        "        else:\n",
        "            # compute gradient\n",
        "            grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "            grad_data = grad_vars[0].data\n",
        "\n",
        "            # compute the updates (can be made more efficient than this)\n",
        "            #aug_grad = (1. - 2. * x_next) * grad_data #this line is wrong because the grad_data can be negative\n",
        "            aug_grad = (x_next < 0.5) * grad_data # the correct version\n",
        "            val, _ = torch.topk(aug_grad, 1)\n",
        "            x_update = (aug_grad >= val.expand_as(aug_grad)).float()\n",
        "            # find the next sample with projection to the feasible set\n",
        "            x_next = xor_float_tensors(x_update, x_next)\n",
        "            x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "        # forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "        # update worst loss and adversarial samples\n",
        "        replace_flag = (loss.data > loss_worst)\n",
        "        loss_worst[replace_flag] = loss.data[replace_flag]\n",
        "        x_worst[replace_flag] = x_next[replace_flag]\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        #print(\"Natural loss (%.4f) vs Adversarial loss (%.4f), Difference: (%.4f)\" %(loss_natural.mean(), loss_worst.mean(), loss_worst.mean() - loss_natural.mean()))\n",
        "        outputs = model(x_worst)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"bca_k: attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "\n",
        "\n",
        "    return x_worst"
      ],
      "metadata": {
        "id": "l5w-UPPVeDe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grosse_k(x, y, model, k=25, is_report_loss_diff=True, use_sample=False):\n",
        "    \"\"\"\n",
        "    Multi-step bit coordinate ascent\n",
        "    :param use_sample:\n",
        "    :param is_report_loss_diff:\n",
        "    :param y:\n",
        "    :param x: (tensor) feature vector\n",
        "    :param model: nn model\n",
        "    :param k: num of steps\n",
        "    :return: the adversarial version of x according to bca_k (tensor)\n",
        "    \"\"\"\n",
        "    epsilon = 1e-10 # avoid gradient less than epsilon\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # keeping worst loss\n",
        "    loss_worst = loss_natural.clone()\n",
        "    x_worst = x.clone()\n",
        "\n",
        "    # multi-step with gradients\n",
        "    output = None\n",
        "    x_var = None\n",
        "    x_next = None\n",
        "    for t in range(k):\n",
        "        if t == 0:\n",
        "            # initialize starting point\n",
        "            x_next = get_x0(x, use_sample)\n",
        "        else:\n",
        "            # compute gradient\n",
        "            # ouput.shape=([batch_size, 2]) because of 2 neoruns, so we just use the output of the first neorun(benign)\n",
        "            grad_vars = torch.autograd.grad(output[:, 0].mean(), x_var)\n",
        "            grad_data = grad_vars[0].data\n",
        "\n",
        "            # compute the updates (can be made more efficient than this)\n",
        "            #aug_grad = (1. - x_next) * grad_data\n",
        "            aug_grad = (x_next < 0.5) * grad_data\n",
        "            val, _ = torch.topk(aug_grad, 1)\n",
        "            x_update = ((aug_grad >= val.expand_as(aug_grad)).float()) * (aug_grad > epsilon)\n",
        "\n",
        "            # find the next sample with projection to the feasible set\n",
        "            x_next = xor_float_tensors(x_update, x_next)\n",
        "            x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "        # forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        output = model(x_var)\n",
        "        loss = criterion(output, y.view(-1).long())\n",
        "\n",
        "        # update worst loss and adversarial samples\n",
        "        replace_flag = (loss.data > loss_worst)\n",
        "        loss_worst[replace_flag] = loss.data[replace_flag]\n",
        "        x_worst[replace_flag] = x_next[replace_flag]\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        #print(\"Natural loss (%.4f) vs Adversarial loss (%.4f), Difference: (%.4f)\" %(loss_natural.mean(), loss_worst.mean(), loss_worst.mean() - loss_natural.mean()))\n",
        "        outputs = model(x_worst)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"grosse_k: attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "\n",
        "\n",
        "    return x_worst"
      ],
      "metadata": {
        "id": "2jjbi-wHwYKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bKnpWI--7Tum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        #pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            #perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            #perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            #perturbation[torch.isnan(perturbation)] = 0.\n",
        "            #perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x_next - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "\n",
        "            val, _ = torch.topk(torch.abs(gradients), 1)\n",
        "            perturbation = (torch.abs(gradients) >= val.expand_as(gradients)).float() * torch.sign(gradients).float()\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            outputs = model(x_next)\n",
        "            _, predicted = torch.topk(outputs, k=1)\n",
        "            done = (predicted != y).squeeze()\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "XdQ-5RbP6oDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "def mimic_attack_effectiveness_optimized(test_loader, model, seed, trials=1000, device=\"cuda:0\"):\n",
        "  \"\"\"\n",
        "  Calculates the effectiveness of the mimic attack on the given model.\n",
        "\n",
        "  Args:\n",
        "      test_loader: A PyTorch dataloader containing the test data.\n",
        "      model: The PyTorch model to be attacked.\n",
        "      seed: The random seed for reproducibility.\n",
        "      trials: The number of random samples to use from the benign class (default: 1000).\n",
        "      device: The device to use for computations (default: \"cuda:0\" if available, otherwise \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "      The effectiveness of the mimic attack as a percentage (float).\n",
        "  \"\"\"\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  model.eval()\n",
        "\n",
        "  # Initialize counters\n",
        "  successful_attacks = 0\n",
        "  total_malicious_samples = 0\n",
        "\n",
        "  # Pre-select benign samples for efficiency\n",
        "  benign_samples = []\n",
        "  for x_batch, y_batch in test_loader:\n",
        "    benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "  ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "  # Clear unnecessary variables\n",
        "  del benign_samples\n",
        "\n",
        "  trials = min(trials, len(ben_x))\n",
        "\n",
        "\n",
        "  for x_batch, y_batch in test_loader:\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    malicious_samples = x_batch[y_batch.squeeze() == 1]\n",
        "\n",
        "    if len(malicious_samples) > 0:\n",
        "      # Expand dimensions for efficient broadcasting\n",
        "      malicious_samples_expanded = malicious_samples.unsqueeze(1).expand(-1, trials, -1)\n",
        "\n",
        "      # Generate random indices outside the loop\n",
        "      seed += 1\n",
        "      torch.manual_seed(seed)\n",
        "      indices = torch.randperm(len(ben_x), device=device)[:trials]\n",
        "      trial_vectors_expanded = ben_x[indices].unsqueeze(0)\n",
        "\n",
        "      # Perform the mimic attack and update counters\n",
        "      modified_x = torch.clamp(malicious_samples_expanded + trial_vectors_expanded, min=0., max=1.)\n",
        "      _, done = get_loss(modified_x.view(-1, modified_x.shape[-1]), torch.ones(trials * malicious_samples.shape[0], 1, device=device), model)\n",
        "      successful_attacks += (done.view(malicious_samples.shape[0], trials).sum(dim=1) > 0).sum().item()\n",
        "      total_malicious_samples += malicious_samples.shape[0]\n",
        "\n",
        "  # Calculate and print attack effectiveness\n",
        "  attack_effectiveness = (successful_attacks / total_malicious_samples) * 100 if total_malicious_samples > 0 else 0\n",
        "  print(f\"Mimic attack effectiveness: {attack_effectiveness:.3f}%.\")\n",
        "\n",
        "  return attack_effectiveness  # Added return statement for clarity\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "kVA4nOM0YUVa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwzxIE8UFGsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mimicry(ben_x, malicious_samples, model, removal_array, trials=30, seed=230, is_report_loss_diff=False):\n",
        "    \"\"\"\n",
        "    Perform a mimicry attack.\n",
        "\n",
        "    Args:\n",
        "    - ben_x (torch.Tensor): Benign samples tensor.\n",
        "    - malicious_samples (torch.Tensor): Malicious samples tensor.\n",
        "    - model (torch.nn.Module): PyTorch model used for the attack.\n",
        "    - trials (int): Number of trials for the attack.\n",
        "    - seed (int): Random seed for reproducibility.\n",
        "    - is_report_loss_diff (bool): Flag to indicate whether to report attack effectiveness.\n",
        "\n",
        "    Returns:\n",
        "    - adv_x (torch.Tensor): Adversarial examples tensor.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    seed += 1\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Ensure trials do not exceed the length of ben_x\n",
        "    trials = min(trials, len(ben_x))\n",
        "\n",
        "    # Get the number of malicious samples\n",
        "    n_samples = len(malicious_samples)\n",
        "\n",
        "    if n_samples > 0:\n",
        "        # Expand dimensions for efficient broadcasting\n",
        "        malicious_samples_expanded = malicious_samples.unsqueeze(1).expand(-1, trials, -1)\n",
        "        fixed_malicious_samples_expanded = malicious_samples_expanded * (1 - removal_array)\n",
        "\n",
        "        # Generate random indices for sampling from ben_x\n",
        "        indices = torch.randperm(len(ben_x), device=ben_x.device)[:trials]\n",
        "        trial_vectors_expanded = ben_x[indices].unsqueeze(0)\n",
        "\n",
        "        # Perform the mimic attack\n",
        "        pertbx = torch.clamp(fixed_malicious_samples_expanded + trial_vectors_expanded, min=0., max=1.).to(torch.float32)\n",
        "\n",
        "        # Compute the loss and check if adversarial examples are successful\n",
        "        loss, done = get_loss(pertbx.view(-1, pertbx.shape[-1]), torch.ones(n_samples * trials, 1, device=ben_x.device,dtype=torch.float32), model)\n",
        "\n",
        "        # Add maximum loss to successful attacks to differentiate\n",
        "        max_v = loss.max()\n",
        "        loss[done] += max_v\n",
        "\n",
        "        # Reshape the loss and done tensors\n",
        "        loss = loss.view(n_samples, trials)\n",
        "        done = done.view(n_samples, trials)\n",
        "\n",
        "        # Report attack effectiveness if required\n",
        "        if is_report_loss_diff:\n",
        "            n_done = torch.any(done, dim=-1).sum()\n",
        "            print(f\"Mimicry*{trials}: Attack effectiveness {n_done / n_samples * 100:.3f}%.\")\n",
        "\n",
        "        # Get the index of the maximum loss for each sample\n",
        "        _, indices = loss.max(dim=-1)\n",
        "        adv_x = pertbx[torch.arange(n_samples), indices]\n",
        "\n",
        "        del pertbx, loss, done, malicious_samples_expanded, trial_vectors_expanded\n",
        "\n",
        "        return adv_x\n",
        "    else:\n",
        "        print(\"No malicious samples found.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "I10dA9oPZ40X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PGD_Max(x,y, model, attack_list = ['linf', 'l2', 'l1'],steps_max=5, is_sample = False, varepsilon = 1e-20):\n",
        "    \"\"\"\n",
        "    PGD_Max adversarial attack.\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [samples, features])\n",
        "        y: Ground truth labels tensor (shape: [samples])\n",
        "        model: Neural network model\n",
        "        attack_list: List of norms for attacks (default: ['linf', 'l2', 'l1'])\n",
        "        steps_max: Maximum number of steps (default: 5)\n",
        "        is_sample: Flag to sample randomly from the feasible area (default: False)\n",
        "        vaρεpsilon: Tolerance for stopping condition (default: 1e-20)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial version of input data (tensor)\n",
        "    \"\"\"\n",
        "    batch_size = x.shape[0]\n",
        "    norm_params = {\n",
        "        'l1': {'k': 50, 'step_length': 1.0},\n",
        "        'l2': {'k': 200, 'step_length': 0.05},\n",
        "        'linf': {'k': 500, 'step_length': 0.002}\n",
        "    }\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss, done = get_loss(x,y,model) #shape:[samples],[samples]\n",
        "\n",
        "    pre_loss = loss\n",
        "    n = x.shape[0]\n",
        "    adv_x = x.detach().clone()\n",
        "    stop_flag = torch.zeros(n, dtype=torch.bool) #[samples]\n",
        "\n",
        "    for t in range(steps_max):\n",
        "      num_remaining  = (~stop_flag).sum().item()\n",
        "      print('number of remaining samples : ',num_remaining )\n",
        "      if num_remaining  <= 0:\n",
        "          break\n",
        "\n",
        "      remaining_label = y[~stop_flag]\n",
        "      pertbx = []\n",
        "\n",
        "      for norm in attack_list:\n",
        "          if norm in norm_params:\n",
        "              params = norm_params[norm]\n",
        "              perturbation = pgd(adv_x[~stop_flag], remaining_label, model, norm=norm, is_sample=is_sample, **params)\n",
        "              print(\"the number of added features : \", (perturbation.sum() - adv_x[~stop_flag].sum())/len(adv_x[~stop_flag]))\n",
        "              pertbx.append(perturbation)\n",
        "          else:\n",
        "              raise ValueError(\"Expected 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "\n",
        "      # here pertbx.shape = a list of (number of attacks  ,(num_remaining ,features))\n",
        "      pertbx = torch.vstack(pertbx)\n",
        "      # here pertbx.shape = a tensor (num_remaining *number of attacks samples, features)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        remaining_label_ext = torch.cat([remaining_label] * len(attack_list)) #(labels*number of attacks )\n",
        "        loss, done = get_loss(pertbx, remaining_label_ext,model) #(labels*number of attacks )\n",
        "        loss = loss.reshape(len(attack_list), num_remaining ).permute(1, 0) #(num_remaining ,number of attacks)\n",
        "        done = done.reshape(len(attack_list), num_remaining ).permute(1, 0) #(num_remaining ,number of attacks)\n",
        "\n",
        "        success_flag = torch.any(done, dim=-1) #(num_remaining )\n",
        "        # for a sample, if there is at least one successful attack, we will select the one with maximum loss;\n",
        "        # while if no attacks evade the victim successful, all perturbed examples are reminded for selection\n",
        "\n",
        "        done[~torch.any(done, dim=-1)] = 1 #loss.shape=done.shape=(samples,number of attacks)\n",
        "        loss = (loss * done.to(torch.float)) + torch.min(loss) * (~done).to(torch.float) #(num_remaining ,number of attacks)\n",
        "        pertbx = pertbx.reshape(len(attack_list), num_remaining , x.shape[1]).permute([1, 0, 2])#(num_remaining ,attacks,features)\n",
        "        _, indices = loss.max(dim=-1) # ans:(samples), max loss among attacks which worked, and max loss among all attacks for sample , none of them worked\n",
        "        adv_x[~stop_flag] = pertbx[torch.arange(num_remaining ), indices]\n",
        "        a_loss = loss[torch.arange(num_remaining ), indices]\n",
        "        pre_stop_flag = stop_flag.clone()\n",
        "        stop_flag[~stop_flag] = (torch.abs(pre_loss[~stop_flag] - a_loss) < varepsilon) | success_flag\n",
        "        pre_loss[~pre_stop_flag] = a_loss\n",
        "\n",
        "    return adv_x"
      ],
      "metadata": {
        "id": "c589J_H1GcGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PGD_Max2(x,y, model, attack_list = ['linf', 'l2', 'l1'],steps_max=5, is_sample = False, varepsilon = 1e-20, is_report_loss_diff = False):\n",
        "    \"\"\"\n",
        "    PGD_Max adversarial attack.\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [samples, features])\n",
        "        y: Ground truth labels tensor (shape: [samples])\n",
        "        model: Neural network model\n",
        "        attack_list: List of norms for attacks (default: ['linf', 'l2', 'l1'])\n",
        "        steps_max: Maximum number of steps (default: 5)\n",
        "        is_sample: Flag to sample randomly from the feasible area (default: False)\n",
        "        vaρεpsilon: Tolerance for stopping condition (default: 1e-20)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial version of input data (tensor)\n",
        "    \"\"\"\n",
        "    batch_size = x.shape[0]\n",
        "    norm_params = {\n",
        "        'l1': {'k': 50, 'step_length': 1.0, 'is_report_loss_diff':is_report_loss_diff},\n",
        "        'l2': {'k': 200, 'step_length': 0.05, 'is_report_loss_diff':is_report_loss_diff},\n",
        "        'linf': {'k': 500, 'step_length': 0.002, 'is_report_loss_diff':is_report_loss_diff}\n",
        "    }\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss, done = get_loss(x,y,model) #shape:[samples],[samples]\n",
        "\n",
        "    pre_loss = loss\n",
        "    n = x.shape[0]\n",
        "    adv_x = x.detach().clone().to(x.device)\n",
        "    stop_flag = torch.zeros(n, dtype=torch.bool).to(x.device) #[samples]\n",
        "\n",
        "    for t in range(steps_max):\n",
        "      num_remaining  = (~stop_flag).sum().item()\n",
        "      #print('number of remaining samples : ',num_remaining )\n",
        "      if num_remaining  <= 0:\n",
        "          break\n",
        "\n",
        "      remaining_label = y[~stop_flag]\n",
        "      pertbx = []\n",
        "\n",
        "      for norm in attack_list:\n",
        "          if norm in norm_params:\n",
        "              params = norm_params[norm]\n",
        "              perturbation = pgd(adv_x[~stop_flag], remaining_label, model, norm=norm, is_sample=is_sample, **params)\n",
        "              #print(\"the number of added features : \", (perturbation.sum() - adv_x[~stop_flag].sum())/len(adv_x[~stop_flag]))\n",
        "              pertbx.append(perturbation)\n",
        "          else:\n",
        "              raise ValueError(\"Expected 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "\n",
        "      # here pertbx.shape = a list of (number of attacks  ,(num_remaining ,features))\n",
        "      pertbx = torch.vstack(pertbx)\n",
        "      # here pertbx.shape = a tensor (num_remaining *number of attacks samples, features)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        remaining_label_ext = torch.cat([remaining_label] * len(attack_list)) #(labels*number of attacks )\n",
        "        loss, done = get_loss(pertbx, remaining_label_ext,model) #(labels*number of attacks )\n",
        "\n",
        "        # for a sample, if there is at least one successful attack, we will select the one with maximum loss;\n",
        "        # while if no attacks evade the victim successful, all perturbed examples are reminded for selection\n",
        "        max_v = loss.amax()\n",
        "        loss[done] += max_v\n",
        "\n",
        "        loss = loss.reshape(len(attack_list), num_remaining ).permute(1, 0) #(num_remaining ,number of attacks)\n",
        "        done = done.reshape(len(attack_list), num_remaining ).permute(1, 0) #(num_remaining ,number of attacks)\n",
        "\n",
        "        success_flag = torch.any(done, dim=-1) #(num_remaining )\n",
        "\n",
        "        pertbx = pertbx.reshape(len(attack_list), num_remaining , x.shape[1]).permute([1, 0, 2])#(num_remaining ,attacks,features)\n",
        "        _, indices = loss.max(dim=-1) # ans:(samples), max loss among attacks which worked, and max loss among all attacks for sample , none of them worked\n",
        "        adv_x[~stop_flag] = pertbx[torch.arange(num_remaining ), indices]\n",
        "        a_loss = loss[torch.arange(num_remaining ), indices]\n",
        "        pre_stop_flag = stop_flag.clone()\n",
        "        stop_flag[~stop_flag] = (torch.abs(pre_loss[~stop_flag] - a_loss) < varepsilon) | success_flag\n",
        "        pre_loss[~pre_stop_flag] = a_loss\n",
        "\n",
        "    return adv_x"
      ],
      "metadata": {
        "id": "d2Rw-5_fJU4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_step(x, y, model, norm, k, step_length):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack for stepwise.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :return: The adversarial version of x (tensor)(not rounded)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        grad_data = grad_vars[0].data\n",
        "        gradients = grad_data * (x < 0.5)\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients)\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm)\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "        elif norm == 'l1':\n",
        "            #ignore the gradient of indice which is updated\n",
        "            gradients = gradients * (x_next < 0.5)\n",
        "            val, _ = torch.topk(gradients, 1)\n",
        "            perturbation = torch.sign(gradients >= val.expand_as(gradients))\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            outputs = model(x_next)\n",
        "            _, predicted = torch.topk(outputs, k=1)\n",
        "            done = (predicted != y).squeeze()\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    #remove negative pertubations, we cant use OR function because we have values between (0,1) like 0.2 which we want to keep\n",
        "    x_adv = (((x_next - x) >= 0) * x_next) + (((x_next - x) < 0) * x)\n",
        "    return x_adv"
      ],
      "metadata": {
        "id": "GP2Fx9BOd-zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def StepwiseMax(\n",
        "    x,\n",
        "    label,\n",
        "    model,\n",
        "    attack_list=[\"linf\", \"l2\", \"l1\"],\n",
        "    step_lengths={\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002},\n",
        "    steps=100,\n",
        "    step_check = 1,\n",
        "    random_start=False,\n",
        "    round_threshold=0.5,\n",
        "    is_attacker=False,\n",
        "    is_score_round = False\n",
        "):\n",
        "  \"\"\"\n",
        "    Stepwise max attack (mixture of pgd-l1, pgd-l2, pgd-linf).\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [batch_size, feature_dim])\n",
        "        label: Ground truth labels tensor (shape: [batch_size])\n",
        "        model: Victim model\n",
        "        attack_list: List of attack norms (default: [\"linf\", \"l2\", \"l1\"])\n",
        "        step_lengths: Dictionary mapping norm to its step length (default: {\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002})\n",
        "        steps: Maximum number of iterations (default: 100)\n",
        "        random_start: Use random starting point (default: False)\n",
        "        round_threshold: Threshold for rounding real scalars (default: 0.5)\n",
        "        is_attacker: Play the role of attacker (default: False)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial examples tensor (same shape as x)\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  step_check = 1\n",
        "  if not is_attacker:\n",
        "      step_checks = [1, 10, 25, 50]\n",
        "      step_check = random.choice(step_checks)\n",
        "\n",
        "  print(f\"Step check: {step_check}\")\n",
        "  mini_steps = [step_check] * (steps // step_check)\n",
        "  if steps % step_check != 0:\n",
        "      mini_steps.append(steps % step_check)\n",
        "\n",
        "  n, red_n = x.shape\n",
        "  adv_x = x.detach().clone()\n",
        "  pert_x_cont = None\n",
        "  prev_done = None\n",
        "  for i, mini_step in enumerate(mini_steps):\n",
        "      with torch.no_grad():\n",
        "          if i == 0 :\n",
        "              adv_x = get_x0(adv_x, initial_rounding_threshold=round_threshold, is_sample=random_start)\n",
        "\n",
        "          _, done = get_loss(adv_x, label, model)\n",
        "      if torch.all(done):\n",
        "          break\n",
        "      if i == 0:\n",
        "          adv_x[~done] = x[~done]  # recompute the perturbation under other penalty factors\n",
        "          prev_done = done\n",
        "      else:\n",
        "          adv_x[~done] = pert_x_cont[~done[~prev_done]]\n",
        "          prev_done = done\n",
        "\n",
        "      num_sample_red = torch.sum(~done).item()\n",
        "      pertbx = []\n",
        "      for norm in attack_list:\n",
        "          step_length = step_lengths.get(norm, step_lengths[\"l1\"])\n",
        "          perturbation = pgd_step(adv_x[~done], label[~done], model, norm, mini_step, step_length)\n",
        "          #print(\"the number of added features(not rounded) \", norm,\": \", perturbation.sum()/len(adv_x[~done]) - adv_x[~done].sum()/len(adv_x[~done]))\n",
        "          #print(\"the number of added features(rounded) \", norm,\" : \",(round_x(perturbation, round_threshold).sum() - round_x(adv_x[~done], round_threshold).sum())/len(adv_x[~done]))\n",
        "          pertbx.append(perturbation)\n",
        "      with torch.no_grad():\n",
        "          pertbx = torch.vstack(pertbx)\n",
        "\n",
        "          n_attacks = len(attack_list)\n",
        "          label_ext = torch.cat([label[~done]] * n_attacks)\n",
        "\n",
        "          if (not is_attacker) and (not is_score_round):\n",
        "              scores, _done = get_loss(pertbx, label_ext,model)\n",
        "          else:\n",
        "              scores, _done = get_loss(round_x(pertbx, round_threshold), label_ext, model)\n",
        "          max_v = scores.amax() if scores.amax() > 0 else 0.\n",
        "          scores[_done] += max_v\n",
        "\n",
        "          pertbx = pertbx.reshape(n_attacks, num_sample_red, red_n).permute([1, 0, 2])\n",
        "          scores = scores.reshape(n_attacks, num_sample_red).permute(1, 0)\n",
        "          _2, s_idx = scores.max(dim=-1)\n",
        "          pert_x_cont = pertbx[torch.arange(num_sample_red), s_idx]\n",
        "          adv_x[~done] = pert_x_cont if not is_attacker else round_x(pert_x_cont, round_threshold)\n",
        "\n",
        "  print(i)\n",
        "  if is_attacker:\n",
        "      adv_x = round_x(adv_x, round_threshold)\n",
        "  with torch.no_grad():\n",
        "      _, done = get_loss(adv_x, label, model)\n",
        "      print(f\"step-wise max: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "  return adv_x"
      ],
      "metadata": {
        "id": "1x4bXgnCqSxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def StepwiseMax2(\n",
        "    x,\n",
        "    label,\n",
        "    model,\n",
        "    attack_list=[\"linf\", \"l2\", \"l1\"],\n",
        "    step_lengths={\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002},\n",
        "    steps=100,\n",
        "    step_check = 1,\n",
        "    random_start=False,\n",
        "    round_threshold=0.5,\n",
        "    is_attacker=False,\n",
        "    is_score_round = False\n",
        "):\n",
        "  \"\"\"\n",
        "    Stepwise max attack (mixture of pgd-l1, pgd-l2, pgd-linf).\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [batch_size, feature_dim])\n",
        "        label: Ground truth labels tensor (shape: [batch_size])\n",
        "        model: Victim model\n",
        "        attack_list: List of attack norms (default: [\"linf\", \"l2\", \"l1\"])\n",
        "        step_lengths: Dictionary mapping norm to its step length (default: {\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002})\n",
        "        steps: Maximum number of iterations (default: 100)\n",
        "        random_start: Use random starting point (default: False)\n",
        "        round_threshold: Threshold for rounding real scalars (default: 0.5)\n",
        "        is_attacker: Play the role of attacker (default: False)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial examples tensor (same shape as x)\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  step_check = 1\n",
        "  if not is_attacker:\n",
        "      step_checks = [1, 10, 25, 50]\n",
        "      step_check = random.choice(step_checks)\n",
        "\n",
        "  print(f\"Step check: {step_check}\")\n",
        "  mini_steps = [step_check] * (steps // step_check)\n",
        "  if steps % step_check != 0:\n",
        "      mini_steps.append(steps % step_check)\n",
        "  n, red_n = x.shape\n",
        "  adv_x = x.detach().clone()\n",
        "  pert_x_cont = None\n",
        "  prev_done = None\n",
        "  for i, mini_step in enumerate(mini_steps):\n",
        "      with torch.no_grad():\n",
        "          if i == 0 :\n",
        "              adv_x = get_x0(adv_x, initial_rounding_threshold=round_threshold, is_sample=random_start)\n",
        "\n",
        "          if is_attacker:\n",
        "            _, done = get_loss(round_x(adv_x, round_threshold), label, model)\n",
        "          else :\n",
        "            _, done = get_loss(adv_x, label, model)\n",
        "      if torch.all(done):\n",
        "          break\n",
        "      if i == 0:\n",
        "          adv_x[~done] = x[~done]  # recompute the perturbation under other penalty factors\n",
        "          prev_done = done\n",
        "      else:\n",
        "          adv_x[~done] = pert_x_cont[~done[~prev_done]]\n",
        "          prev_done = done\n",
        "\n",
        "      print('len(adv_x[~done]) : ',len(adv_x[~done]))\n",
        "      num_sample_red = torch.sum(~done).item()\n",
        "      pertbx = []\n",
        "      for norm in attack_list:\n",
        "          step_length = step_lengths.get(norm, step_lengths[\"l1\"])\n",
        "          perturbation = pgd_step(adv_x[~done], label[~done], model, norm, mini_step, step_length)\n",
        "          #print(\"the number of added features(not rounded) \", norm,\": \", perturbation.sum()/len(adv_x[~done]) - adv_x[~done].sum()/len(adv_x[~done]))\n",
        "          #print(\"the number of added features(rounded) \", norm,\" : \",(round_x(perturbation, round_threshold).sum() - round_x(adv_x[~done], round_threshold).sum())/len(adv_x[~done]))\n",
        "          pertbx.append(perturbation)\n",
        "      with torch.no_grad():\n",
        "          pertbx = torch.vstack(pertbx)\n",
        "\n",
        "          n_attacks = len(attack_list)\n",
        "          label_ext = torch.cat([label[~done]] * n_attacks)\n",
        "\n",
        "          if (is_score_round):\n",
        "              scores, _done = get_loss(round_x(pertbx, round_threshold), label_ext, model)\n",
        "          elif is_attacker:\n",
        "              scores, _ = get_loss(pertbx, label_ext,model)\n",
        "              _, _done = get_loss(round_x(pertbx, round_threshold), label_ext, model)\n",
        "          else:\n",
        "              scores, _done = get_loss(pertbx, label_ext,model)\n",
        "\n",
        "          max_v = scores.amax() if scores.amax() > 0 else 0.\n",
        "          scores[_done] += max_v\n",
        "\n",
        "          pertbx = pertbx.reshape(n_attacks, num_sample_red, red_n).permute([1, 0, 2])\n",
        "          scores = scores.reshape(n_attacks, num_sample_red).permute(1, 0)\n",
        "          _2, s_idx = scores.max(dim=-1)\n",
        "          pert_x_cont = pertbx[torch.arange(num_sample_red), s_idx]\n",
        "          adv_x[~done] = pert_x_cont\n",
        "\n",
        "  print(i)\n",
        "  if is_attacker:\n",
        "      adv_x = round_x(adv_x, round_threshold)\n",
        "  with torch.no_grad():\n",
        "      _, done = get_loss(adv_x, label, model)\n",
        "      print(f\"step-wise max: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "  return adv_x"
      ],
      "metadata": {
        "id": "YF48Kp2unI4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_one_step(x, y, model, step_lengths):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack for stepwise.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :return: The adversarial version of x (tensor)(not rounded)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "\n",
        "    # one-step PGD\n",
        "\n",
        "    # Forward pass\n",
        "    x_var = x_next.clone().detach().requires_grad_(True)\n",
        "    y_model = model(x_var)\n",
        "    loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "    # Compute gradient\n",
        "    grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "    grad_data = grad_vars[0].data\n",
        "    gradients = grad_data * (x < 0.5)\n",
        "\n",
        "\n",
        "    # Norms\n",
        "    pertbx = []\n",
        "    # norm = linf\n",
        "    step_length = step_lengths.get(\"linf\", step_lengths[\"l1\"])\n",
        "    perturbation_linf = torch.sign(gradients)\n",
        "    x_next_linf = torch.clamp(x_next + perturbation_linf * step_length, min=0., max=1.)\n",
        "    #remove negative pertubations, we cant use OR function because we have values between (0,1) like 0.2 which we want to keep\n",
        "    x_adv_linf = (((x_next_linf - x) >= 0) * x_next_linf) + (((x_next_linf - x) < 0) * x)\n",
        "    pertbx.append(x_adv_linf)\n",
        "    # norm = l2\n",
        "    step_length = step_lengths.get(\"l2\", step_lengths[\"l1\"])\n",
        "    l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "    perturbation_l2 = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm)\n",
        "    perturbation_l2[torch.isnan(perturbation_l2)] = 0.\n",
        "    perturbation_l2[torch.isinf(perturbation_l2)] = 1.\n",
        "    x_next_l2 = torch.clamp(x_next + perturbation_l2 * step_length, min=0., max=1.)\n",
        "    x_adv_l2 = (((x_next_l2 - x) >= 0) * x_next_l2) + (((x_next_l2 - x) < 0) * x)\n",
        "    pertbx.append(x_adv_l2)\n",
        "    # norm = l1\n",
        "    step_length = step_lengths.get(\"l1\", step_lengths[\"l1\"])\n",
        "    #ignore the gradient of indice which is updated\n",
        "    gradients = gradients * (x_next < 0.5)\n",
        "    val, _ = torch.topk(gradients, 1)\n",
        "    perturbation_l1 = torch.sign(gradients >= val.expand_as(gradients))\n",
        "    x_next_l1 = torch.clamp(x_next + perturbation_l1 * step_length, min=0., max=1.)\n",
        "    x_adv_l1 = (((x_next_l1 - x) >= 0) * x_next_l1) + (((x_next_l1 - x) < 0) * x)\n",
        "    pertbx.append(x_adv_l1)\n",
        "\n",
        "    return pertbx"
      ],
      "metadata": {
        "id": "De3vBz1IHpY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uh6L653jYA1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def StepwiseMax_onestep(\n",
        "    x,\n",
        "    label,\n",
        "    model,\n",
        "    attack_list=[\"linf\", \"l2\", \"l1\"],\n",
        "    step_lengths={\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002},\n",
        "    steps=100,\n",
        "    random_start=False,\n",
        "    round_threshold=0.5,\n",
        "):\n",
        "  \"\"\"\n",
        "    Stepwise max attack (mixture of pgd-l1, pgd-l2, pgd-linf).\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [batch_size, feature_dim])\n",
        "        label: Ground truth labels tensor (shape: [batch_size])\n",
        "        model: Victim model\n",
        "        attack_list: List of attack norms (default: [\"linf\", \"l2\", \"l1\"])\n",
        "        step_lengths: Dictionary mapping norm to its step length (default: {\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002})\n",
        "        steps: Maximum number of iterations (default: 100)\n",
        "        random_start: Use random starting point (default: False)\n",
        "        round_threshold: Threshold for rounding real scalars (default: 0.5)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial examples tensor (same shape as x)\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  n, red_n = x.shape\n",
        "  adv_x = x.detach().clone()\n",
        "  pert_x_cont = None\n",
        "  prev_done = None\n",
        "  for step in range(steps):\n",
        "      with torch.no_grad():\n",
        "          if step == 0 :\n",
        "              adv_x = get_x0(adv_x, initial_rounding_threshold=round_threshold, is_sample=random_start)\n",
        "\n",
        "          _, done = get_loss(round_x(adv_x, round_threshold), label, model)\n",
        "\n",
        "      if torch.all(done):\n",
        "          break\n",
        "      if step == 0:\n",
        "          adv_x[~done] = x[~done]  # recompute the perturbation under other penalty factors\n",
        "          prev_done = done\n",
        "      else:\n",
        "          adv_x[~done] = pert_x_cont[~done[~prev_done]]\n",
        "          prev_done = done\n",
        "\n",
        "      #print('len(adv_x[~done]) : ',len(adv_x[~done]))\n",
        "      num_sample_red = torch.sum(~done).item()\n",
        "\n",
        "      pertbx = pgd_one_step(adv_x[~done], label[~done], model, step_lengths)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          pertbx = torch.vstack(pertbx)\n",
        "\n",
        "          n_attacks = len(attack_list)\n",
        "          label_ext = torch.cat([label[~done]] * n_attacks)\n",
        "\n",
        "          scores, _ = get_loss(pertbx, label_ext,model)\n",
        "          _, _done = get_loss(round_x(pertbx, round_threshold), label_ext, model)\n",
        "\n",
        "          max_v = scores.amax() if scores.amax() > 0 else 0.\n",
        "          scores[_done] += max_v\n",
        "\n",
        "          pertbx = pertbx.reshape(n_attacks, num_sample_red, red_n).permute([1, 0, 2])\n",
        "          scores = scores.reshape(n_attacks, num_sample_red).permute(1, 0)\n",
        "          _2, s_idx = scores.max(dim=-1)\n",
        "          pert_x_cont = pertbx[torch.arange(num_sample_red), s_idx]\n",
        "          adv_x[~done] = pert_x_cont\n",
        "\n",
        "  #print(step)\n",
        "  adv_x = round_x(adv_x, round_threshold)\n",
        "  with torch.no_grad():\n",
        "      _, done = get_loss(adv_x, label, model)\n",
        "      print(f\"step-wise max: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "  return adv_x"
      ],
      "metadata": {
        "id": "SSiNK4V_PaFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def around_x(x, std_deviation=0.1, random_start=False):\n",
        "    \"\"\"\n",
        "    Helper function to randomly initialize the inner maximizer algorithm.\n",
        "    Randomizes the input tensor while preserving its functionality.\n",
        "    :param x: input tensor\n",
        "    :param std_deviation: std_deviation for domain\n",
        "    :param random_start: flag to sample randomly from feasible area\n",
        "    :return: randomly sampled feasible version of x\n",
        "    \"\"\"\n",
        "    if random_start:\n",
        "\n",
        "        # Generate random tensor from a Gaussian distribution centered around zero\n",
        "        random_tensor = abs(torch.randn(x.size()))\n",
        "\n",
        "        # Scale the values to control the spread of the distribution\n",
        "        random_tensor *= std_deviation\n",
        "\n",
        "        return torch.clamp(x + random_tensor, min=0., max=1.)\n",
        "    else:\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Bfshb8AgqPtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_one_step2(x, y, model, step_lengths,x_initial):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack for stepwise.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :return: The adversarial version of x (tensor)(not rounded)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "\n",
        "    # one-step PGD\n",
        "\n",
        "    # Forward pass\n",
        "    x_var = x_next.clone().detach().requires_grad_(True)\n",
        "    y_model = model(x_var)\n",
        "    loss = criterion(y_model, y.view(-1).long())\n",
        "\n",
        "    # Compute gradient\n",
        "    grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "    grad_data = grad_vars[0].data\n",
        "    gradients = grad_data * (x_initial < 0.5)\n",
        "\n",
        "\n",
        "    # Norms\n",
        "    pertbx = []\n",
        "    # norm = linf\n",
        "    step_length = step_lengths.get(\"linf\", step_lengths[\"l1\"])\n",
        "    perturbation_linf = torch.sign(gradients)\n",
        "    x_next_linf = torch.clamp(x_next + perturbation_linf * step_length, min=0., max=1.)\n",
        "    #remove negative pertubations, we cant use OR function because we have values between (0,1) like 0.2 which we want to keep\n",
        "    x_adv_linf = (((x_next_linf - x_initial) >= 0) * x_next_linf) + (((x_next_linf - x_initial) < 0) * x_initial)\n",
        "    pertbx.append(x_adv_linf)\n",
        "    # norm = l2\n",
        "    step_length = step_lengths.get(\"l2\", step_lengths[\"l1\"])\n",
        "    l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "    perturbation_l2 = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm)\n",
        "    perturbation_l2[torch.isnan(perturbation_l2)] = 0.\n",
        "    perturbation_l2[torch.isinf(perturbation_l2)] = 1.\n",
        "    x_next_l2 = torch.clamp(x_next + perturbation_l2 * step_length, min=0., max=1.)\n",
        "    x_adv_l2 = (((x_next_l2 - x_initial) >= 0) * x_next_l2) + (((x_next_l2 - x_initial) < 0) * x_initial)\n",
        "    pertbx.append(x_adv_l2)\n",
        "    # norm = l1\n",
        "    step_length = step_lengths.get(\"l1\", step_lengths[\"l1\"])\n",
        "    #ignore the gradient of indice which is updated\n",
        "    gradients_l1 = gradients * (x_next < 0.5)\n",
        "    val, _ = torch.topk(gradients_l1, 1)\n",
        "\n",
        "    perturbation_l1 = torch.sign(gradients >= val.expand_as(gradients)) * (val > 1e-10)\n",
        "\n",
        "    x_next_l1 = torch.clamp(x_next + perturbation_l1 * step_length, min=0., max=1.)\n",
        "    x_adv_l1 = (((x_next_l1 - x_initial) >= 0) * x_next_l1) + (((x_next_l1 - x_initial) < 0) * x_initial)\n",
        "    pertbx.append(x_adv_l1)\n",
        "\n",
        "    return pertbx"
      ],
      "metadata": {
        "id": "VIny0BjsYBFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def StepwiseMax_onestep2(\n",
        "    x,\n",
        "    label,\n",
        "    model,\n",
        "    attack_list=[\"linf\", \"l2\", \"l1\"],\n",
        "    step_lengths={\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002},\n",
        "    steps=100,\n",
        "    random_start=False,\n",
        "    round_threshold=0.5,\n",
        "):\n",
        "  \"\"\"\n",
        "    Stepwise max attack (mixture of pgd-l1, pgd-l2, pgd-linf).\n",
        "\n",
        "    Args:\n",
        "        x: Input data tensor (shape: [batch_size, feature_dim])\n",
        "        label: Ground truth labels tensor (shape: [batch_size])\n",
        "        model: Victim model\n",
        "        attack_list: List of attack norms (default: [\"linf\", \"l2\", \"l1\"])\n",
        "        step_lengths: Dictionary mapping norm to its step length (default: {\"l1\": 1.0, \"l2\": 0.05, \"linf\": 0.002})\n",
        "        steps: Maximum number of iterations (default: 100)\n",
        "        random_start: Use random starting point (default: False)\n",
        "        round_threshold: Threshold for rounding real scalars (default: 0.5)\n",
        "\n",
        "    Returns:\n",
        "        Adversarial examples tensor (same shape as x)\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  n, red_n = x.shape\n",
        "  adv_x = x.detach().clone()\n",
        "  pert_x_cont = None\n",
        "  prev_done = None\n",
        "  for step in range(steps):\n",
        "      with torch.no_grad():\n",
        "          if step == 0 :\n",
        "              adv_x = around_x(adv_x, std_deviation=0.1, random_start=random_start)\n",
        "\n",
        "          _, done = get_loss(round_x(adv_x, round_threshold), label, model)\n",
        "\n",
        "      if torch.all(done):\n",
        "          break\n",
        "      if step == 0:\n",
        "          adv_x[~done] = x[~done]  # recompute the perturbation under other penalty factors\n",
        "          prev_done = done\n",
        "      else:\n",
        "          adv_x[~done] = pert_x_cont[~done[~prev_done]]\n",
        "          prev_done = done\n",
        "\n",
        "      #print('remaining samples : ',len(adv_x[~done]))\n",
        "      num_sample_red = torch.sum(~done).item()\n",
        "\n",
        "      pertbx = pgd_one_step2(adv_x[~done], label[~done], model, step_lengths,x[~done])\n",
        "\n",
        "      with torch.no_grad():\n",
        "          pertbx = torch.vstack(pertbx)\n",
        "\n",
        "          n_attacks = len(attack_list)\n",
        "          label_ext = torch.cat([label[~done]] * n_attacks)\n",
        "\n",
        "          scores, _ = get_loss(pertbx, label_ext,model)\n",
        "          _, _done = get_loss(round_x(pertbx, round_threshold), label_ext, model)\n",
        "\n",
        "          max_v = scores.amax() if scores.amax() > 0 else 0.\n",
        "          scores[_done] += max_v\n",
        "\n",
        "          pertbx = pertbx.reshape(n_attacks, num_sample_red, red_n).permute([1, 0, 2])\n",
        "          scores = scores.reshape(n_attacks, num_sample_red).permute(1, 0)\n",
        "          _2, s_idx = scores.max(dim=-1)\n",
        "          print('best attack : ',s_idx)\n",
        "          pert_x_cont = pertbx[torch.arange(num_sample_red), s_idx]\n",
        "          adv_x[~done] = pert_x_cont\n",
        "\n",
        "  #print(step)\n",
        "  adv_x = round_x(adv_x, round_threshold)\n",
        "  with torch.no_grad():\n",
        "      _, done = get_loss(adv_x, label, model)\n",
        "      print(f\"step-wise max: attack effectiveness {done.sum().item() / done.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "  return adv_x"
      ],
      "metadata": {
        "id": "zRBrmMyWYHn4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}