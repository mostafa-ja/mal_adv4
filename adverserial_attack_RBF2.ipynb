{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/mal_adv4/blob/main/adverserial_attack_RBF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RBF_models\n",
        "\n",
        "download_links = [\n",
        "                  'https://drive.google.com/uc?id=1-8lJXLdAl_4NdDwzw9kFML0aiOCTrI9f',\n",
        "                  'https://drive.google.com/uc?id=1-OHACrNCt0yKBbdqQPVfNZcjKt5_jxKD',\n",
        "                  'https://drive.google.com/uc?id=1-KeXJXtU1_6m9JOhormeVwigy0myX3HL',\n",
        "                  'https://drive.google.com/uc?id=1-13RDdZqnrNkdHg3D8PC5KI0CZREwlsz',\n",
        "                  'https://drive.google.com/uc?id=1-8LjsCdzKh6asxCFsYLiQZbSEXXKSQBP',\n",
        "\n",
        "]\n",
        "\n",
        "import gdown\n",
        "output_filepath = '/content/'\n",
        "for link in download_links:\n",
        "  gdown.download(link, output_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM_3KjraHnkn",
        "outputId": "b5b87530-5725-4c48-dee5-0eeda05cd54a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-8lJXLdAl_4NdDwzw9kFML0aiOCTrI9f\n",
            "From (redirected): https://drive.google.com/uc?id=1-8lJXLdAl_4NdDwzw9kFML0aiOCTrI9f&confirm=t&uuid=f18b454e-e76b-44cd-b956-8dab617e9628\n",
            "To: /content/best_model_gaussian_400.pth\n",
            "100%|██████████| 32.0M/32.0M [00:00<00:00, 55.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-OHACrNCt0yKBbdqQPVfNZcjKt5_jxKD\n",
            "To: /content/best_model_gaussian_600_nonremoval.pth\n",
            "100%|██████████| 5.50M/5.50M [00:00<00:00, 38.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-KeXJXtU1_6m9JOhormeVwigy0myX3HL\n",
            "To: /content/best_model_gaussian_600.pth\n",
            "100%|██████████| 24.0M/24.0M [00:00<00:00, 41.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-13RDdZqnrNkdHg3D8PC5KI0CZREwlsz\n",
            "To: /content/best_model_gaussian_1000_nonremoval.pth\n",
            "100%|██████████| 9.16M/9.16M [00:00<00:00, 46.8MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-8LjsCdzKh6asxCFsYLiQZbSEXXKSQBP\n",
            "From (redirected): https://drive.google.com/uc?id=1-8LjsCdzKh6asxCFsYLiQZbSEXXKSQBP&confirm=t&uuid=835445db-62e2-4853-92d9-6d61052e8ad6\n",
            "To: /content/best_model_gaussian_1000.pth\n",
            "100%|██████████| 40.0M/40.0M [00:00<00:00, 64.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "download_links = ['https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_0.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_1.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_2.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y0.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y1.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y2.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_DNN_drebin_best.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM_weightedLoss.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/insertion_array.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/removal_array.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/adverserial_attacks_functions.py'\n",
        "]"
      ],
      "metadata": {
        "id": "1IW4pHac9VLq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "output_filepath = '/content/'\n",
        "for link in download_links:\n",
        "  gdown.download(link, output_filepath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kzSbjaXGVeG",
        "outputId": "57965244-ccf5-465a-c6dd-adc2c70a5666"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_0.npz\n",
            "To: /content/sparse_matrix_0.npz\n",
            "100%|██████████| 461k/461k [00:00<00:00, 4.61MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_1.npz\n",
            "To: /content/sparse_matrix_1.npz\n",
            "100%|██████████| 148k/148k [00:00<00:00, 2.40MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_2.npz\n",
            "To: /content/sparse_matrix_2.npz\n",
            "100%|██████████| 150k/150k [00:00<00:00, 2.33MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y0.npz\n",
            "To: /content/sparse_matrix_y0.npz\n",
            "100%|██████████| 5.79k/5.79k [00:00<00:00, 11.2MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y1.npz\n",
            "To: /content/sparse_matrix_y1.npz\n",
            "100%|██████████| 2.64k/2.64k [00:00<00:00, 7.65MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y2.npz\n",
            "To: /content/sparse_matrix_y2.npz\n",
            "100%|██████████| 2.71k/2.71k [00:00<00:00, 8.22MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_DNN_drebin_best.pth\n",
            "To: /content/model_DNN_drebin_best.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 53.5MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM_weightedLoss.pth\n",
            "To: /content/model_AT_rFGSM_weightedLoss.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 50.0MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM.pth\n",
            "To: /content/model_AT_rFGSM.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 16.0MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/insertion_array.pkl\n",
            "To: /content/insertion_array.pkl\n",
            "100%|██████████| 80.2k/80.2k [00:00<00:00, 2.69MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/removal_array.pkl\n",
            "To: /content/removal_array.pkl\n",
            "100%|██████████| 80.2k/80.2k [00:00<00:00, 2.56MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/adverserial_attacks_functions.py\n",
            "To: /content/adverserial_attacks_functions.py\n",
            "67.1kB [00:00, 13.0MB/s]                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,balanced_accuracy_score\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "from adverserial_attacks_functions import *\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "JKDdI3K9LrlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3da86a-8c5c-4388-b2b1-f90e7dadab3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78c711acded0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    print(f\"Random seed set as {seed}\")\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX4ncRLLFDnN",
        "outputId": "00260fe6-5ae5-4ec2-f07c-e6c30d16138f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set as 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the .pkl file\n",
        "with open('/content/insertion_array.pkl', 'rb') as f:\n",
        "    # Load the object\n",
        "    insertion_array = pickle.load(f)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n",
        "\n",
        "insertion_array = torch.tensor(insertion_array, dtype=torch.uint8).to(device)\n",
        "print(len(insertion_array))\n",
        "\n",
        "# Open the .pkl file\n",
        "with open('/content/removal_array.pkl', 'rb') as f:\n",
        "    # Load the object\n",
        "    removal_array = pickle.load(f)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n",
        "\n",
        "removal_array = torch.tensor(removal_array, dtype=torch.uint8).to(device)\n",
        "print(len(removal_array))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXV0WIjsJG_F",
        "outputId": "6ca13e12-3b6c-4714-d162-765c5ce38fe2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset\n",
        "X_train = sparse.load_npz(\"/content/sparse_matrix_0.npz\").toarray()\n",
        "X_val = sparse.load_npz(\"/content/sparse_matrix_1.npz\").toarray()\n",
        "X_test = sparse.load_npz(\"/content/sparse_matrix_2.npz\").toarray()\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.int8)\n",
        "X_val = torch.tensor(X_val, dtype=torch.int8)\n",
        "X_test = torch.tensor(X_test, dtype=torch.int8)\n",
        "\n",
        "\n",
        "y_train = sparse.load_npz(\"/content/sparse_matrix_y0.npz\").toarray().reshape((-1, 1))\n",
        "y_val = sparse.load_npz(\"/content/sparse_matrix_y1.npz\").toarray().reshape((-1, 1))\n",
        "y_test = sparse.load_npz(\"/content/sparse_matrix_y2.npz\").toarray().reshape((-1, 1))\n",
        "\n",
        "y_train = torch.tensor(y_train, dtype=torch.int8)\n",
        "y_val = torch.tensor(y_val, dtype=torch.int8)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int8)\n",
        "\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"x_train:\", X_train.shape)\n",
        "print(\"x_val:\", X_val.shape)\n",
        "print(\"x_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_val:\", y_val.shape)\n",
        "print(\"y_test:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5blmEg4h-GKy",
        "outputId": "0dc9bed1-ca1c-40bd-b577-facde4127261"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "x_train: torch.Size([28683, 10000])\n",
            "x_val: torch.Size([9562, 10000])\n",
            "x_test: torch.Size([9562, 10000])\n",
            "y_train: torch.Size([28683, 1])\n",
            "y_val: torch.Size([9562, 1])\n",
            "y_test: torch.Size([9562, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of benigns and malicious sample in training dataset\n",
        "n_ben = (y_train.squeeze()== 0).sum().item()\n",
        "n_mal = (y_train.squeeze()== 1).sum().item()\n",
        "print('the proportion of malwares : ', n_mal/(n_mal+n_ben))\n",
        "\n",
        "# Combine features and labels into datasets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Define the DataLoader for training, validation, and test sets\n",
        "batch_size = 1024\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del train_dataset, val_dataset, test_dataset, y_train, y_val, y_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81AZSXOV-HoW",
        "outputId": "7d789684-d6ca-487d-93d1-b6060613d0fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the proportion of malwares :  0.11386535578565701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your model\n",
        "model_AT_rFGSM = MalwareDetectionModel().to(device)\n",
        "\n",
        "# Load model parameters\n",
        "model_AT_rFGSM.load_state_dict(torch.load('model_AT_rFGSM.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE8WMAUgSCms",
        "outputId": "a367d0a8-6044-4e19-89d9-cded016b86cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RBFModel(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, init_centers, init_sigmas, kernel):\n",
        "        super(RBFModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.centers = nn.Parameter(torch.Tensor(init_centers))\n",
        "        self.sigmas = nn.Parameter(torch.Tensor(init_sigmas))\n",
        "        self.kernel = kernel\n",
        "        # Linear layer for output\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def gaussian(self, x, c, sigma):\n",
        "        return torch.exp(-torch.sum((x[:, None, :] - c) ** 2, dim=-1) / (2 * sigma ** 2))\n",
        "\n",
        "    def laplacian(self, x, c, sigma):\n",
        "        return torch.exp(-torch.sum(torch.abs(x[:, None, :] - c) , dim=-1) / sigma)\n",
        "\n",
        "    def forward(self, x):\n",
        "      if self.kernel == 'gaussian':\n",
        "        radial_out = self.gaussian(x, self.centers, self.sigmas)\n",
        "      elif self.kernel == 'laplacian':\n",
        "        radial_out = self.laplacian(x, self.centers, self.sigmas)\n",
        "      else:\n",
        "        raise ValueError(\"Invalid kernel type. Choose 'gaussian' or 'laplacian'.\")\n",
        "\n",
        "      output = self.linear(radial_out.to(torch.float32))\n",
        "      return output\n"
      ],
      "metadata": {
        "id": "WHAI-VGJSGa2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_removal_features = False\n",
        "non_removal_mask = torch.logical_not(removal_array).to('cpu')\n",
        "sigma = 4.15\n",
        "kernel = 'gaussian'\n",
        "all_centers = torch.rand((1000, 10000))\n",
        "model_gaussian_1000 = RBFModel(1000, 2, all_centers, [sigma], kernel)\n",
        "model_gaussian_1000 = model_gaussian_1000.to(device)\n",
        "\n",
        "# Load the model state dictionary\n",
        "model_gaussian_1000.load_state_dict(torch.load('/content/best_model_gaussian_1000.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "id": "W9qJYeK0bbnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b5d1c6-0bdd-48c0-999f-47b7f8874ac0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_removal_features = True\n",
        "non_removal_mask = torch.logical_not(removal_array).to('cpu')\n",
        "sigma = 4.15\n",
        "kernel = 'gaussian'\n",
        "all_centers = torch.rand((1000, 1144))\n",
        "model_gaussian_1000_nonremoval = RBFModel(1000, 2, all_centers, [sigma], kernel)\n",
        "model_gaussian_1000_nonremoval = model_gaussian_1000_nonremoval.to(device)\n",
        "\n",
        "# Load the model state dictionary\n",
        "model_gaussian_1000_nonremoval.load_state_dict(torch.load('/content/best_model_gaussian_1000_nonremoval.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMA7rzpTPhv7",
        "outputId": "a346260a-a737-471c-da6a-05960e44b325"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QcyXy3kSd6QV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in test_loader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "bens = x[y.squeeze()==0]\n",
        "bens_y = y[y.squeeze()==0]\n",
        "print(bens.shape)\n",
        "\n",
        "mals = x[y.squeeze()==1]\n",
        "mals_y = y[y.squeeze()==1]\n",
        "print(mals.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StL135L1JUiE",
        "outputId": "28d36cbd-0ba7-4237-a62c-f5e9b2d39f0c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 10000])\n",
            "torch.Size([1024, 1])\n",
            "torch.Size([909, 10000])\n",
            "torch.Size([115, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`analysis`**"
      ],
      "metadata": {
        "id": "lR06hA6gn_by"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PGD VS PGD2"
      ],
      "metadata": {
        "id": "lBVu5_O4xF4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = mals.to(device)\n",
        "\n",
        "# Expand insertion_array and removal_array to match the batch size\n",
        "expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "# Update insertion and removal arrays based on input x\n",
        "insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))"
      ],
      "metadata": {
        "id": "5Ybdj5iAOo_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(insertion_array.sum())\n",
        "print(insertion_array_updated.sum(dim=-1))\n",
        "\n",
        "print(removal_array.sum())\n",
        "print(removal_array_updated.sum(dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk3PeXfviDL7",
        "outputId": "197ac01e-11d2-4654-8570-7f1d13aa1736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9943, device='cuda:0')\n",
            "tensor([9943, 9944, 9943, 9949, 9944, 9944, 9944, 9945, 9946, 9947, 9943, 9945,\n",
            "        9944, 9943, 9943, 9944, 9946, 9945, 9947, 9945, 9945, 9946, 9944, 9945,\n",
            "        9945, 9945, 9943, 9944, 9944], device='cuda:0')\n",
            "tensor(8856, device='cuda:0')\n",
            "tensor([9996, 9993, 9986, 9962, 9993, 9986, 9973, 9989, 9967, 9980, 9996, 9989,\n",
            "        9993, 9992, 9985, 9989, 9977, 9968, 9984, 9968, 9976, 9989, 9993, 9965,\n",
            "        9984, 9989, 9994, 9989, 9997], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((x.to(torch.uint8)[0]).sum())\n",
        "print((removal_array_updated[0]*x.to(torch.uint8)[0]).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGD5H7qPqL3s",
        "outputId": "e223f1fc-6408-4e8e-c810-0d2cbb0c60aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10, device='cuda:0')\n",
            "tensor(6, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((x.to(torch.uint8)[1]).sum())\n",
        "print((removal_array_updated[1]*x.to(torch.uint8)[1]).sum())"
      ],
      "metadata": {
        "id": "2uCejvFCpYLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c42626-3ec1-44f2-e0c4-1bc4d2638ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(18, device='cuda:0')\n",
            "tensor(11, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        #pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            #perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            #perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            #perturbation[torch.isnan(perturbation)] = 0.\n",
        "            #perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "gCxNx4sfOr-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd2(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    #insertion_array_updated = torch.bitwise_or(insertion_array.to(torch.uint8), x.squeeze().to(torch.uint8) )\n",
        "    #removal_array_updated = torch.bitwise_or(removal_array.to(torch.uint8), (1 - x.squeeze().to(torch.uint8)) )\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        #pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "            #perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            #perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            #perturbation[torch.isnan(perturbation)] = 0.\n",
        "            #perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "ljfazeC6MOcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=13\n",
        "adv3 = pgd(mals[i:i+1].to(torch.float32).to(device), mals_y[i:i+1].to(device), model_AT_rFGSM, insertion_array, removal_array, k=200, step_length=.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfb42f2-39b7-4d36-d6f3-851143bc691e",
        "id": "wno-FScyQc7r"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********  0\n",
            "loss_mal :  tensor([38.1857], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  1\n",
            "loss_mal :  tensor([34.2699], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  2\n",
            "loss_mal :  tensor([29.9703], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  3\n",
            "loss_mal :  tensor([25.4033], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  4\n",
            "loss_mal :  tensor([21.0827], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  5\n",
            "loss_mal :  tensor([18.6158], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  6\n",
            "loss_mal :  tensor([19.1730], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  7\n",
            "loss_mal :  tensor([18.5722], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  8\n",
            "loss_mal :  tensor([17.9622], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  9\n",
            "loss_mal :  tensor([18.2092], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  10\n",
            "loss_mal :  tensor([17.6264], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  11\n",
            "loss_mal :  tensor([17.2231], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  12\n",
            "loss_mal :  tensor([17.6461], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  13\n",
            "loss_mal :  tensor([17.0438], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  14\n",
            "loss_mal :  tensor([16.5450], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  15\n",
            "loss_mal :  tensor([16.6668], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  16\n",
            "loss_mal :  tensor([16.7365], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  17\n",
            "loss_mal :  tensor([16.2144], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  18\n",
            "loss_mal :  tensor([15.8460], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  19\n",
            "loss_mal :  tensor([16.2541], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  20\n",
            "loss_mal :  tensor([15.6365], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  21\n",
            "loss_mal :  tensor([15.1847], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  22\n",
            "loss_mal :  tensor([15.2408], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  23\n",
            "loss_mal :  tensor([15.3970], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  24\n",
            "loss_mal :  tensor([14.9240], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  25\n",
            "loss_mal :  tensor([14.4805], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([4.7684e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  26\n",
            "loss_mal :  tensor([15.1770], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  27\n",
            "loss_mal :  tensor([15.3370], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  28\n",
            "loss_mal :  tensor([18.0399], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  29\n",
            "loss_mal :  tensor([17.5291], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  30\n",
            "loss_mal :  tensor([18.5759], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  31\n",
            "loss_mal :  tensor([18.5073], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  32\n",
            "loss_mal :  tensor([19.3361], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  33\n",
            "loss_mal :  tensor([19.5283], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  34\n",
            "loss_mal :  tensor([19.8090], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  35\n",
            "loss_mal :  tensor([20.6668], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  36\n",
            "loss_mal :  tensor([20.5820], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  37\n",
            "loss_mal :  tensor([21.0349], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  38\n",
            "loss_mal :  tensor([22.0950], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  39\n",
            "loss_mal :  tensor([25.7003], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  40\n",
            "loss_mal :  tensor([25.0341], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  41\n",
            "loss_mal :  tensor([25.9631], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  42\n",
            "loss_mal :  tensor([26.0358], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  43\n",
            "loss_mal :  tensor([26.2545], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  44\n",
            "loss_mal :  tensor([27.0234], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  45\n",
            "loss_mal :  tensor([27.2249], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  46\n",
            "loss_mal :  tensor([27.8303], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  47\n",
            "loss_mal :  tensor([28.4456], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  48\n",
            "loss_mal :  tensor([28.5112], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  49\n",
            "loss_mal :  tensor([28.7157], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  50\n",
            "loss_mal :  tensor([29.7865], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  51\n",
            "loss_mal :  tensor([29.8639], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  52\n",
            "loss_mal :  tensor([30.0937], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  53\n",
            "loss_mal :  tensor([30.4724], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  54\n",
            "loss_mal :  tensor([31.2638], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  55\n",
            "loss_mal :  tensor([31.3466], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  56\n",
            "loss_mal :  tensor([31.4016], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  57\n",
            "loss_mal :  tensor([32.3835], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  58\n",
            "loss_mal :  tensor([32.7490], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  59\n",
            "loss_mal :  tensor([32.6407], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  60\n",
            "loss_mal :  tensor([32.9704], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  61\n",
            "loss_mal :  tensor([34.3624], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  62\n",
            "loss_mal :  tensor([36.8631], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  63\n",
            "loss_mal :  tensor([37.2833], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  64\n",
            "loss_mal :  tensor([37.5473], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  65\n",
            "loss_mal :  tensor([38.0170], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  66\n",
            "loss_mal :  tensor([38.4765], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  67\n",
            "loss_mal :  tensor([38.8761], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  68\n",
            "loss_mal :  tensor([39.2470], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  69\n",
            "loss_mal :  tensor([39.7409], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  70\n",
            "loss_mal :  tensor([39.2557], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  71\n",
            "loss_mal :  tensor([39.6956], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  72\n",
            "loss_mal :  tensor([40.5667], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  73\n",
            "loss_mal :  tensor([40.7395], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  74\n",
            "loss_mal :  tensor([41.2648], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  75\n",
            "loss_mal :  tensor([40.6510], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  76\n",
            "loss_mal :  tensor([41.3032], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  77\n",
            "loss_mal :  tensor([42.1344], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  78\n",
            "loss_mal :  tensor([42.0639], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  79\n",
            "loss_mal :  tensor([42.6217], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  80\n",
            "loss_mal :  tensor([42.4742], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  81\n",
            "loss_mal :  tensor([43.2104], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  82\n",
            "loss_mal :  tensor([42.8373], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  83\n",
            "loss_mal :  tensor([43.8001], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  84\n",
            "loss_mal :  tensor([43.6212], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  85\n",
            "loss_mal :  tensor([44.2113], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  86\n",
            "loss_mal :  tensor([44.4069], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  87\n",
            "loss_mal :  tensor([44.8563], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  88\n",
            "loss_mal :  tensor([44.4067], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  89\n",
            "loss_mal :  tensor([45.6805], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  90\n",
            "loss_mal :  tensor([45.2139], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  91\n",
            "loss_mal :  tensor([46.1938], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  92\n",
            "loss_mal :  tensor([46.2202], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  93\n",
            "loss_mal :  tensor([46.6961], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  94\n",
            "loss_mal :  tensor([46.2663], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  95\n",
            "loss_mal :  tensor([47.0882], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  96\n",
            "loss_mal :  tensor([47.0736], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  97\n",
            "loss_mal :  tensor([47.5637], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  98\n",
            "loss_mal :  tensor([46.9779], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  99\n",
            "loss_mal :  tensor([47.8359], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  100\n",
            "loss_mal :  tensor([47.7958], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  101\n",
            "loss_mal :  tensor([48.2153], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  102\n",
            "loss_mal :  tensor([48.4137], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  103\n",
            "loss_mal :  tensor([48.6403], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  104\n",
            "loss_mal :  tensor([48.9003], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  105\n",
            "loss_mal :  tensor([48.8218], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  106\n",
            "loss_mal :  tensor([49.4160], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  107\n",
            "loss_mal :  tensor([49.1052], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  108\n",
            "loss_mal :  tensor([49.3387], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  109\n",
            "loss_mal :  tensor([49.3371], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  110\n",
            "loss_mal :  tensor([50.3853], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  111\n",
            "loss_mal :  tensor([49.5553], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  112\n",
            "loss_mal :  tensor([50.2185], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  113\n",
            "loss_mal :  tensor([49.6733], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  114\n",
            "loss_mal :  tensor([50.8431], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  115\n",
            "loss_mal :  tensor([50.0224], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  116\n",
            "loss_mal :  tensor([50.2238], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  117\n",
            "loss_mal :  tensor([51.2280], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  118\n",
            "loss_mal :  tensor([50.3908], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  119\n",
            "loss_mal :  tensor([51.5084], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  120\n",
            "loss_mal :  tensor([50.5488], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  121\n",
            "loss_mal :  tensor([50.7055], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  122\n",
            "loss_mal :  tensor([51.7984], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  123\n",
            "loss_mal :  tensor([50.8001], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  124\n",
            "loss_mal :  tensor([52.6957], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  125\n",
            "loss_mal :  tensor([51.2729], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  126\n",
            "loss_mal :  tensor([52.1815], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  127\n",
            "loss_mal :  tensor([51.9248], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  128\n",
            "loss_mal :  tensor([51.9660], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  129\n",
            "loss_mal :  tensor([53.9478], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  130\n",
            "loss_mal :  tensor([52.9872], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  131\n",
            "loss_mal :  tensor([52.9911], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  132\n",
            "loss_mal :  tensor([53.0953], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  133\n",
            "loss_mal :  tensor([53.1235], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  134\n",
            "loss_mal :  tensor([53.1636], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  135\n",
            "loss_mal :  tensor([53.1505], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  136\n",
            "loss_mal :  tensor([53.2048], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  137\n",
            "loss_mal :  tensor([53.1772], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  138\n",
            "loss_mal :  tensor([53.2403], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  139\n",
            "loss_mal :  tensor([53.2744], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  140\n",
            "loss_mal :  tensor([53.2884], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  141\n",
            "loss_mal :  tensor([53.2570], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  142\n",
            "loss_mal :  tensor([53.2793], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  143\n",
            "loss_mal :  tensor([53.2307], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  144\n",
            "loss_mal :  tensor([53.3362], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  145\n",
            "loss_mal :  tensor([54.3056], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  146\n",
            "loss_mal :  tensor([53.5258], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  147\n",
            "loss_mal :  tensor([54.6378], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  148\n",
            "loss_mal :  tensor([53.6328], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  149\n",
            "loss_mal :  tensor([53.7278], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  150\n",
            "loss_mal :  tensor([54.0571], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  151\n",
            "loss_mal :  tensor([54.0202], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  152\n",
            "loss_mal :  tensor([55.1657], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  153\n",
            "loss_mal :  tensor([54.2403], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  154\n",
            "loss_mal :  tensor([55.3521], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  155\n",
            "loss_mal :  tensor([54.3920], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  156\n",
            "loss_mal :  tensor([54.3931], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  157\n",
            "loss_mal :  tensor([54.5385], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  158\n",
            "loss_mal :  tensor([54.5983], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  159\n",
            "loss_mal :  tensor([54.6775], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  160\n",
            "loss_mal :  tensor([55.6373], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  161\n",
            "loss_mal :  tensor([54.7709], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  162\n",
            "loss_mal :  tensor([55.8183], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  163\n",
            "loss_mal :  tensor([54.8311], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  164\n",
            "loss_mal :  tensor([56.0166], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  165\n",
            "loss_mal :  tensor([54.8923], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  166\n",
            "loss_mal :  tensor([55.0505], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  167\n",
            "loss_mal :  tensor([54.9815], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  168\n",
            "loss_mal :  tensor([56.0288], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  169\n",
            "loss_mal :  tensor([55.0874], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  170\n",
            "loss_mal :  tensor([56.0652], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  171\n",
            "loss_mal :  tensor([55.1147], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  172\n",
            "loss_mal :  tensor([55.2418], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  173\n",
            "loss_mal :  tensor([56.3768], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  174\n",
            "loss_mal :  tensor([55.3652], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  175\n",
            "loss_mal :  tensor([55.3756], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  176\n",
            "loss_mal :  tensor([55.4923], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  177\n",
            "loss_mal :  tensor([55.5233], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  178\n",
            "loss_mal :  tensor([56.4636], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  179\n",
            "loss_mal :  tensor([55.5444], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  180\n",
            "loss_mal :  tensor([56.5000], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  181\n",
            "loss_mal :  tensor([55.5681], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  182\n",
            "loss_mal :  tensor([57.3156], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  183\n",
            "loss_mal :  tensor([55.7465], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  184\n",
            "loss_mal :  tensor([55.8201], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  185\n",
            "loss_mal :  tensor([56.1730], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  186\n",
            "loss_mal :  tensor([55.9438], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  187\n",
            "loss_mal :  tensor([59.8039], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  188\n",
            "loss_mal :  tensor([58.7559], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  189\n",
            "loss_mal :  tensor([58.7960], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  190\n",
            "loss_mal :  tensor([58.7328], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  191\n",
            "loss_mal :  tensor([58.7429], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  192\n",
            "loss_mal :  tensor([58.8203], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  193\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  194\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  195\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  196\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  197\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  198\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  199\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "PGD linf: Attack effectiveness 0.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=13\n",
        "adv3 = pgd2(mals[i:i+1].to(torch.float32).to(device), mals_y[i:i+1].to(device), model_AT_rFGSM, insertion_array, removal_array, k=200, step_length=.01, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6ffcbd-df80-4a08-c94c-d2ee834981fb",
        "id": "VA7I64WAQlzh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_mal :  tensor([38.1857], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([36.2778], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([34.1459], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([32.0459], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([29.7434], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([27.4553], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([25.1175], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([22.7606], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([20.7420], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([18.7855], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([17.7333], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([17.0531], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([16.1223], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([15.3524], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([14.7656], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([13.8007], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.0729e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([13.0112], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.2650e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([12.5444], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.5763e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([11.6671], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([8.5830e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([11.3863], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1325e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([10.2367], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.5881e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.2161], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([9.9416e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.4727], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.6887e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([8.7340], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.7916], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0004], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.4897], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0006], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.3333], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([8.8449e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.3538], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0006], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([5.9726], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0026], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([6.3044], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0018], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.2304], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0007], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([6.0834], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0023], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.7234], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0089], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.3221], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0134], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([5.8951], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0028], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.3293], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0133], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.2786], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0384], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.0669], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0173], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.9127], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0202], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.9162], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0557], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.9034], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0204], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.6429], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0265], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.9277], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0550], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.9581], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1521], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.9436], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1545], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.6260], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0751], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.8590], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0590], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.4482], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0904], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.2558], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.3352], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.9197], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.5085], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.7572], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.6330], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.9694], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.4769], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.3177], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.3116], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.5444], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.8679], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.5643], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.8410], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3931], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0958], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.8096], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.5888], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.3027], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.3425], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.7227], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.6645], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.3755], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1613], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0892], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.4609], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0557], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.9151], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0451], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.1222], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0605], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.8352], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0731], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.6521], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0147], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([4.2298], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.1553], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.9392], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0211], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.8668], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0074], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([4.9123], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0599], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.8451], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0096], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([4.6477], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0040], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.5347], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0038], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.5780], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0036], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.6367], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0025], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([6.0047], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0015], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([6.4953], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0008], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.1494], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0004], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.7841], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0004], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.8195], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([8.4549], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([8.6143], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.7866e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([9.2318], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0005], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.5747], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.3669e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([9.5154], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.3736e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.2967], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.5868e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.5637], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.7789e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.1824], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.7404e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.9576], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.6451e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.0120], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1086e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.4133], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.0371e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.4801], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([5.9604e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.0398], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([6.0797e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.0062], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.2915e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.3569], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.6955e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.4986], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.2650e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.9914], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.1458e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.0478], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.4305e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.4899], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.5497e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.3926], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.5367e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.9123], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.5367e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.8642], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([5.9605e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.2797], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([8.3446e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.9929], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.7684e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.4852], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.1526e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.1059], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.7666], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.8205], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.2218], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.1785], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.5289], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.0717], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.6906], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.8573], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.8415], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.0516], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.9979], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.1779], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.2113], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.1807], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.5920], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.8915], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.7072], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.9245], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.1821], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.9369], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.8058], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.1525], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.2827], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.3518], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.3100], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.4926], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.9303], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.5889], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.6794], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.7684], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.8297], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.5572], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.3395], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.0459], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.0215], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.0777], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2277], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2953], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2866], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.4161], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.6764], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.5151], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.5406], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.6368], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.4993], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.6868], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.8509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.6713], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.8222], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.3812], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.8683], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.5569], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.9306], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.8948], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.1698], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.0208], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.5820], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.1567], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.9064], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.2704], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.6482], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.3006], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.3837], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.7682], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.7323], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.7673], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.4611], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.9545], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.9501], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.1487], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.1800], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.3351], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.5411], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.6605], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.2835], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.6128], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.8182], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.0505], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.3736], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.3705], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.3748], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.5194], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.6121], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.6836], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.0233], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.2959], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.2906], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.0599], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.3548], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.6085], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.4280], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.2590], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.6373], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.5472], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.7872], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zc-0yhEwDPT",
        "outputId": "b372804d-2ebb-43d6-aa4f-74e7dbeaa5ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 6.897%.\n",
            "PGD linf: Attack effectiveness 8.000%.\n",
            "PGD linf: Attack effectiveness 7.692%.\n",
            "PGD linf: Attack effectiveness 2.857%.\n",
            "PGD linf: Attack effectiveness 8.000%.\n",
            "PGD linf: Attack effectiveness 8.000%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 6.061%.\n",
            "PGD linf: Attack effectiveness 9.524%.\n",
            "PGD linf: Attack effectiveness 10.345%.\n",
            "PGD linf: Attack effectiveness 10.000%.\n",
            "PGD linf: Attack effectiveness 11.765%.\n",
            "PGD linf: Attack effectiveness 10.000%.\n",
            "PGD linf: Attack effectiveness 14.815%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 3.226%.\n",
            "PGD linf: Attack effectiveness 3.333%.\n",
            "PGD linf: Attack effectiveness 8.000%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 14.815%.\n",
            "PGD linf: Attack effectiveness 16.667%.\n",
            "PGD linf: Attack effectiveness 10.000%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 17.241%.\n",
            "PGD linf: Attack effectiveness 18.182%.\n",
            "PGD linf: Attack effectiveness 3.846%.\n",
            "PGD linf: Attack effectiveness 10.714%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 3.125%.\n",
            "PGD linf: Attack effectiveness 6.250%.\n",
            "PGD linf: Attack effectiveness 8.333%.\n",
            "PGD linf: Attack effectiveness 8.108%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 3.226%.\n",
            "PGD linf: Attack effectiveness 7.143%.\n",
            "PGD linf: Attack effectiveness 3.571%.\n",
            "PGD linf: Attack effectiveness 2.941%.\n",
            "PGD linf: Attack effectiveness 6.667%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7TiMkblwKWx",
        "outputId": "af65e2b9-876f-4810-eb13-f219664addf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 52.000%.\n",
            "PGD linf: Attack effectiveness 52.941%.\n",
            "PGD linf: Attack effectiveness 48.485%.\n",
            "PGD linf: Attack effectiveness 73.810%.\n",
            "PGD linf: Attack effectiveness 55.172%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 55.882%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 56.000%.\n",
            "PGD linf: Attack effectiveness 64.516%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 58.065%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 63.333%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 45.455%.\n",
            "PGD linf: Attack effectiveness 65.517%.\n",
            "PGD linf: Attack effectiveness 61.364%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 70.270%.\n",
            "PGD linf: Attack effectiveness 43.750%.\n",
            "PGD linf: Attack effectiveness 46.875%.\n",
            "PGD linf: Attack effectiveness 61.111%.\n",
            "PGD linf: Attack effectiveness 64.865%.\n",
            "PGD linf: Attack effectiveness 68.421%.\n",
            "PGD linf: Attack effectiveness 61.290%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 64.286%.\n",
            "PGD linf: Attack effectiveness 76.471%.\n",
            "PGD linf: Attack effectiveness 46.667%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.26%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PG2 VS PGD_min"
      ],
      "metadata": {
        "id": "ZPY-dkCCw36O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "\n",
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        #pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(torch.abs(x_next - torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "bbCZIZiQlS30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mkFFSOXLuzWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=13\n",
        "adv3 = pgd2(mals[i:i+1].to(torch.float32).to(device), mals_y[i:i+1].to(device), model_AT_rFGSM, insertion_array, removal_array, k=100, step_length=.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55e6e04-edac-48fb-c943-d3704baebed4",
        "id": "p4HK-RBquzoT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********  0\n",
            "loss_mal :  tensor([38.1857], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  1\n",
            "loss_mal :  tensor([34.2699], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  2\n",
            "loss_mal :  tensor([29.9657], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  3\n",
            "loss_mal :  tensor([25.3872], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  4\n",
            "loss_mal :  tensor([20.8934], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  5\n",
            "loss_mal :  tensor([17.8123], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  6\n",
            "loss_mal :  tensor([18.1594], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  7\n",
            "loss_mal :  tensor([15.2948], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  8\n",
            "loss_mal :  tensor([15.7294], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  9\n",
            "loss_mal :  tensor([12.9326], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  10\n",
            "loss_mal :  tensor([12.9426], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  11\n",
            "loss_mal :  tensor([10.3893], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.0756e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  12\n",
            "loss_mal :  tensor([8.0923], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0003], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  13\n",
            "loss_mal :  tensor([9.6567], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([6.4013e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  14\n",
            "loss_mal :  tensor([7.6484], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0005], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  15\n",
            "loss_mal :  tensor([6.4177], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0016], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  16\n",
            "loss_mal :  tensor([7.7147], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0004], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  17\n",
            "loss_mal :  tensor([6.7666], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0012], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  18\n",
            "loss_mal :  tensor([5.8183], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0030], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  19\n",
            "loss_mal :  tensor([3.7913], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0228], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  20\n",
            "loss_mal :  tensor([3.6471], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0264], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  21\n",
            "loss_mal :  tensor([6.5451], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0014], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  22\n",
            "loss_mal :  tensor([3.4312], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0329], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  23\n",
            "loss_mal :  tensor([1.7684], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1871], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  24\n",
            "loss_mal :  tensor([2.7752], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0644], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  25\n",
            "loss_mal :  tensor([3.6827], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0255], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  26\n",
            "loss_mal :  tensor([0.7988], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.5976], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  27\n",
            "loss_mal :  tensor([4.7537], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0087], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  28\n",
            "loss_mal :  tensor([3.3431], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0360], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  29\n",
            "loss_mal :  tensor([1.8469], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1717], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  30\n",
            "loss_mal :  tensor([0.3787], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1545], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  31\n",
            "loss_mal :  tensor([1.0695], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.4204], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  32\n",
            "loss_mal :  tensor([1.9833], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1481], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  33\n",
            "loss_mal :  tensor([1.8189], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1770], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  34\n",
            "loss_mal :  tensor([1.7494], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1910], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  35\n",
            "loss_mal :  tensor([0.1565], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.9320], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  36\n",
            "loss_mal :  tensor([0.2397], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.5459], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  37\n",
            "loss_mal :  tensor([0.9653], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.4794], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  38\n",
            "loss_mal :  tensor([0.0746], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.6333], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  39\n",
            "loss_mal :  tensor([0.3412], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.2410], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  40\n",
            "loss_mal :  tensor([0.3077], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.3284], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  41\n",
            "loss_mal :  tensor([0.0294], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.5420], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  42\n",
            "loss_mal :  tensor([0.3282], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.2736], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  43\n",
            "loss_mal :  tensor([0.0365], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.3295], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  44\n",
            "loss_mal :  tensor([0.8298], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.5729], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  45\n",
            "loss_mal :  tensor([0.0195], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.9491], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  46\n",
            "loss_mal :  tensor([0.0035], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.6541], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  47\n",
            "loss_mal :  tensor([0.0026], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.9476], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  48\n",
            "loss_mal :  tensor([0.0009], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([6.9745], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  49\n",
            "loss_mal :  tensor([0.0007], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.2607], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  50\n",
            "loss_mal :  tensor([0.0006], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.3728], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  51\n",
            "loss_mal :  tensor([0.0005], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.5356], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  52\n",
            "loss_mal :  tensor([5.0543e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([9.8933], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  53\n",
            "loss_mal :  tensor([2.6703e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.5321], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  54\n",
            "loss_mal :  tensor([3.8742e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.1599], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  55\n",
            "loss_mal :  tensor([1.2278e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.3117], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  56\n",
            "loss_mal :  tensor([2.7179e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.5147], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  57\n",
            "loss_mal :  tensor([9.2983e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.5798], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  58\n",
            "loss_mal :  tensor([2.0027e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.8165], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  59\n",
            "loss_mal :  tensor([5.0068e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.2088], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  60\n",
            "loss_mal :  tensor([3.5047e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.2593], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  61\n",
            "loss_mal :  tensor([3.2186e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.6607], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  62\n",
            "loss_mal :  tensor([1.3113e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.5381], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  63\n",
            "loss_mal :  tensor([9.5367e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.8148], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  64\n",
            "loss_mal :  tensor([1.3113e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.5161], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  65\n",
            "loss_mal :  tensor([1.6689e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.3080], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  66\n",
            "loss_mal :  tensor([1.3113e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.5208], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  67\n",
            "loss_mal :  tensor([7.1526e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.2034], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  68\n",
            "loss_mal :  tensor([2.7418e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.8268], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  69\n",
            "loss_mal :  tensor([4.7684e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.6124], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  70\n",
            "loss_mal :  tensor([1.7881e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.2622], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  71\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.0045], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  72\n",
            "loss_mal :  tensor([8.3446e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.9858], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  73\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.3753], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  74\n",
            "loss_mal :  tensor([7.1526e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.2143], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  75\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.7451], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  76\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.0896], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  77\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.0098], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  78\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.4239], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  79\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.0536], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  80\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.5323], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  81\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.1801], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  82\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.4030], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  83\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.2882], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  84\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.0552], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  85\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.1186], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  86\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.5041], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  87\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.9173], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  88\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.2590], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  89\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.7290], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  90\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.7429], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  91\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.4080], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  92\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2161], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  93\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2731], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  94\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.4290], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  95\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.0085], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  96\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.4040], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  97\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.8233], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  98\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.7385], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  99\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.1252], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=13\n",
        "adv3 = pgd_min(mals[i:i+1].to(torch.float32).to(device), mals_y[i:i+1].to(device), model_AT_rFGSM, insertion_array, removal_array, k=100, step_length=.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ad855a-1a8e-416f-d870-1889cb1de5d1",
        "id": "LVWC9nF7aZia"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********  0\n",
            "loss_mal :  tensor([38.1857], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  1\n",
            "loss_mal :  tensor([34.2345], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  2\n",
            "loss_mal :  tensor([29.9309], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  3\n",
            "loss_mal :  tensor([25.2920], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  4\n",
            "loss_mal :  tensor([20.6400], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  5\n",
            "loss_mal :  tensor([17.5564], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  6\n",
            "loss_mal :  tensor([17.9629], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  7\n",
            "loss_mal :  tensor([15.0592], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  8\n",
            "loss_mal :  tensor([15.8962], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  9\n",
            "loss_mal :  tensor([12.6979], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.0994e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  10\n",
            "loss_mal :  tensor([12.9943], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.2650e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  11\n",
            "loss_mal :  tensor([11.0808], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.5378e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  12\n",
            "loss_mal :  tensor([8.7707], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  13\n",
            "loss_mal :  tensor([9.1304], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0001], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  14\n",
            "loss_mal :  tensor([8.6260], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  15\n",
            "loss_mal :  tensor([8.2149], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0003], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  16\n",
            "loss_mal :  tensor([6.0652], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0023], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  17\n",
            "loss_mal :  tensor([4.6464], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0096], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  18\n",
            "loss_mal :  tensor([7.6485], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0005], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  19\n",
            "loss_mal :  tensor([5.5824], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0038], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  20\n",
            "loss_mal :  tensor([3.1646], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0432], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  21\n",
            "loss_mal :  tensor([3.1386], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0443], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  22\n",
            "loss_mal :  tensor([5.6555], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0035], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  23\n",
            "loss_mal :  tensor([3.6233], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0271], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  24\n",
            "loss_mal :  tensor([3.2767], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0385], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  25\n",
            "loss_mal :  tensor([1.4218], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.2761], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  26\n",
            "loss_mal :  tensor([0.9565], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.4849], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  27\n",
            "loss_mal :  tensor([4.6454], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0097], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  28\n",
            "loss_mal :  tensor([1.7336], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1944], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  29\n",
            "loss_mal :  tensor([2.0485], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1380], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  30\n",
            "loss_mal :  tensor([0.4938], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.9424], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  31\n",
            "loss_mal :  tensor([0.2037], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.6913], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  32\n",
            "loss_mal :  tensor([0.1557], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.9365], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  33\n",
            "loss_mal :  tensor([0.0924], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.4275], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  34\n",
            "loss_mal :  tensor([1.2486], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.3382], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  35\n",
            "loss_mal :  tensor([0.1078], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.2809], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  36\n",
            "loss_mal :  tensor([0.0104], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([4.5724], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  37\n",
            "loss_mal :  tensor([0.0039], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.5394], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  38\n",
            "loss_mal :  tensor([0.0020], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([6.1924], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  39\n",
            "loss_mal :  tensor([0.0006], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.4991], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  40\n",
            "loss_mal :  tensor([0.0007], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.2758], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  41\n",
            "loss_mal :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([8.4886], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  42\n",
            "loss_mal :  tensor([0.0001], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([9.0697], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  43\n",
            "loss_mal :  tensor([4.1365e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.0920], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  44\n",
            "loss_mal :  tensor([2.1457e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.7516], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  45\n",
            "loss_mal :  tensor([1.5378e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.0796], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  46\n",
            "loss_mal :  tensor([6.6757e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.9121], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  47\n",
            "loss_mal :  tensor([4.1723e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.3956], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  48\n",
            "loss_mal :  tensor([2.9802e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.7136], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  49\n",
            "loss_mal :  tensor([3.2186e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.6321], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  50\n",
            "loss_mal :  tensor([1.3113e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.5583], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  51\n",
            "loss_mal :  tensor([2.7895e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.4868], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  52\n",
            "loss_mal :  tensor([9.5367e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.8692], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  53\n",
            "loss_mal :  tensor([0.0009], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.0662], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  54\n",
            "loss_mal :  tensor([8.3446e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.0070], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  55\n",
            "loss_mal :  tensor([3.4571e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.5824], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  56\n",
            "loss_mal :  tensor([5.9605e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.3068], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  57\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.7389], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  58\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.0653], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  59\n",
            "loss_mal :  tensor([7.3909e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.8143], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  60\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.1435], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  61\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.9123], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  62\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.5133], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  63\n",
            "loss_mal :  tensor([9.5367e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.5583], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  64\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.7569], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  65\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.4895], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  66\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.2097], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  67\n",
            "loss_mal :  tensor([1.1921e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.6698], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  68\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.2498], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  69\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.6732], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  70\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.7017], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  71\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.6343], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  72\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.6715], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  73\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.2091], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  74\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.8232], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  75\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.5095], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  76\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.5486], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  77\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.6970], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  78\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.7999], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  79\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.0975], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  80\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.7299], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  81\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.4905], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  82\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.8879], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  83\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2015], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  84\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.0201], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  85\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2666], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  86\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.9117], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  87\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2953], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  88\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.0091], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  89\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.7422], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  90\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.6773], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  91\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.6948], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  92\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.1372], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  93\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.4145], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  94\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.9832], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  95\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.4932], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  96\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.3465], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  97\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.7350], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  98\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.3766], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  99\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.5599], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K6yI8cjxbARk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYqfSitVY0z6",
        "outputId": "52fb3a24-204b-4ea4-be27-267a8085f9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 52.000%.\n",
            "PGD linf: Attack effectiveness 52.941%.\n",
            "PGD linf: Attack effectiveness 48.485%.\n",
            "PGD linf: Attack effectiveness 73.810%.\n",
            "PGD linf: Attack effectiveness 55.172%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 55.882%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 56.000%.\n",
            "PGD linf: Attack effectiveness 64.516%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 58.065%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 63.333%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 45.455%.\n",
            "PGD linf: Attack effectiveness 65.517%.\n",
            "PGD linf: Attack effectiveness 61.364%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 70.270%.\n",
            "PGD linf: Attack effectiveness 43.750%.\n",
            "PGD linf: Attack effectiveness 46.875%.\n",
            "PGD linf: Attack effectiveness 61.111%.\n",
            "PGD linf: Attack effectiveness 64.865%.\n",
            "PGD linf: Attack effectiveness 68.421%.\n",
            "PGD linf: Attack effectiveness 61.290%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 64.286%.\n",
            "PGD linf: Attack effectiveness 76.471%.\n",
            "PGD linf: Attack effectiveness 46.667%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.26%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weBpa3AzZt0B",
        "outputId": "01b53b9a-24c5-4539-da24-13ffeff109ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 46.154%.\n",
            "PGD linf: Attack effectiveness 82.857%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 70.588%.\n",
            "PGD linf: Attack effectiveness 63.636%.\n",
            "PGD linf: Attack effectiveness 71.429%.\n",
            "PGD linf: Attack effectiveness 62.069%.\n",
            "PGD linf: Attack effectiveness 85.000%.\n",
            "PGD linf: Attack effectiveness 61.765%.\n",
            "PGD linf: Attack effectiveness 55.000%.\n",
            "PGD linf: Attack effectiveness 74.074%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 70.968%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 67.742%.\n",
            "PGD linf: Attack effectiveness 77.778%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 40.909%.\n",
            "PGD linf: Attack effectiveness 65.517%.\n",
            "PGD linf: Attack effectiveness 65.909%.\n",
            "PGD linf: Attack effectiveness 61.538%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 59.375%.\n",
            "PGD linf: Attack effectiveness 62.500%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 73.684%.\n",
            "PGD linf: Attack effectiveness 70.968%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 67.857%.\n",
            "PGD linf: Attack effectiveness 79.412%.\n",
            "PGD linf: Attack effectiveness 46.667%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.33%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 1000, 'step_length': 0.01, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "id": "tqEgfkUMbAJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f02e812-de50-4bf6-89f6-928f75c9eb1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 53.846%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 58.824%.\n",
            "PGD linf: Attack effectiveness 54.545%.\n",
            "PGD linf: Attack effectiveness 73.810%.\n",
            "PGD linf: Attack effectiveness 58.621%.\n",
            "PGD linf: Attack effectiveness 85.000%.\n",
            "PGD linf: Attack effectiveness 64.706%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 77.778%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 74.194%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 70.968%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 83.333%.\n",
            "PGD linf: Attack effectiveness 54.545%.\n",
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 53.846%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 75.676%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 62.500%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 78.947%.\n",
            "PGD linf: Attack effectiveness 70.968%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 76.471%.\n",
            "PGD linf: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 1000, 'step_length': 0.01, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GABElx1ezzPg",
        "outputId": "cf6dc2d9-d88c-435f-f2ae-147faaf0c425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 76.000%.\n",
            "PGD linf: Attack effectiveness 57.692%.\n",
            "PGD linf: Attack effectiveness 82.857%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 76.000%.\n",
            "PGD linf: Attack effectiveness 82.353%.\n",
            "PGD linf: Attack effectiveness 69.697%.\n",
            "PGD linf: Attack effectiveness 73.810%.\n",
            "PGD linf: Attack effectiveness 75.862%.\n",
            "PGD linf: Attack effectiveness 95.000%.\n",
            "PGD linf: Attack effectiveness 67.647%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 85.185%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 77.419%.\n",
            "PGD linf: Attack effectiveness 76.667%.\n",
            "PGD linf: Attack effectiveness 84.000%.\n",
            "PGD linf: Attack effectiveness 80.645%.\n",
            "PGD linf: Attack effectiveness 77.778%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 93.333%.\n",
            "PGD linf: Attack effectiveness 59.091%.\n",
            "PGD linf: Attack effectiveness 79.310%.\n",
            "PGD linf: Attack effectiveness 81.818%.\n",
            "PGD linf: Attack effectiveness 65.385%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 81.081%.\n",
            "PGD linf: Attack effectiveness 65.625%.\n",
            "PGD linf: Attack effectiveness 81.250%.\n",
            "PGD linf: Attack effectiveness 80.556%.\n",
            "PGD linf: Attack effectiveness 75.676%.\n",
            "PGD linf: Attack effectiveness 84.211%.\n",
            "PGD linf: Attack effectiveness 77.419%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 82.143%.\n",
            "PGD linf: Attack effectiveness 82.353%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.83%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGjwR_talp00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJQQUoXLmZ0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bmCcaJuelpx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sgq8ZyPrlpvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RvdTlE5V1MU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tsWSFfB8nWHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# validating updated insertion and removal array\n",
        "wirh considering insertion array on the run\n",
        "\n",
        "\n",
        "```\n",
        "pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "E5Dpa5hpuLu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(torch.abs(x_next - torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "bJasPS-vuzPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = mals[13:14].to(torch.float32).to(device)\n",
        "y = mals_y[13:14].to(device)\n",
        "adv = pgd_min(x, y, model_AT_rFGSM, insertion_array, removal_array, k=1000, step_length=.01, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e869a9d3-a196-4e72-d54e-8389a4f4c3d0",
        "id": "QI_tKs-1oNBs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand insertion_array and removal_array to match the batch size\n",
        "expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "# Update insertion and removal arrays based on input x\n",
        "insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))"
      ],
      "metadata": {
        "id": "5xTLkqYbrAHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.sum())\n",
        "print(adv.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j59hRfeaoo1W",
        "outputId": "a5940c9d-97b8-4c28-b67d-6e2992750db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(14., device='cuda:0')\n",
            "tensor(42., device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "insert = ((adv-x) > 0)\n",
        "remove =  ((adv-x) < 0)\n",
        "same = (adv != x)\n",
        "print(insert.sum())\n",
        "print(remove.sum())\n",
        "print(same.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8HRqvvFoysS",
        "outputId": "ac618f87-64de-4d4c-dfe5-5c8672bf1698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(31, device='cuda:0')\n",
            "tensor(3, device='cuda:0')\n",
            "tensor(34, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(insert * insertion_array_updated).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deR6NC6jqzGu",
        "outputId": "c411dd1f-6c8e-43c3-da11-826cd3045c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(31, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(remove * removal_array_updated).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Am53CKBrKg8",
        "outputId": "291fe368-cd9b-4c91-f6b8-32f0c8111e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OhpF6QCUsn0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2K2YsavZtVAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = mals[3:4].to(torch.float32).to(device)\n",
        "y = mals_y[3:4].to(device)\n",
        "adv = pgd_min(x, y, model_AT_rFGSM, insertion_array, removal_array, k=1000, step_length=.01, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7044388-9d7c-4431-b1f9-d009ddaef69c",
        "id": "CfU7y5qitYVh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 0.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand insertion_array and removal_array to match the batch size\n",
        "expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "# Update insertion and removal arrays based on input x\n",
        "insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))"
      ],
      "metadata": {
        "id": "4dhqP9iJtYWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.sum())\n",
        "print(adv.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83877cad-2f9c-462b-b560-71f9042992e6",
        "id": "QdkfvZxetYWA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(128., device='cuda:0')\n",
            "tensor(93., device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "insert = ((adv-x) > 0)\n",
        "remove =  ((adv-x) < 0)\n",
        "same = (adv != x)\n",
        "print(insert.sum())\n",
        "print(remove.sum())\n",
        "print(same.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d750612-1eda-479f-f66d-16327a48e08d",
        "id": "xO-SInkGtYWA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(29, device='cuda:0')\n",
            "tensor(64, device='cuda:0')\n",
            "tensor(93, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(insert * insertion_array_updated).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c18f1f-eb60-47f7-e52a-43570fd77360",
        "id": "FM7EBlUXtYWA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(29, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(remove * removal_array_updated).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f68c97-725c-4bbe-d947-b970a29da642",
        "id": "cQCN8JCztYWB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(64, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 1000, 'step_length': 0.01, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu4Q10JJtoPD",
        "outputId": "52015930-78c6-44cf-c5e3-ae380ce9ed80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 53.846%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 76.471%.\n",
            "PGD linf: Attack effectiveness 69.697%.\n",
            "PGD linf: Attack effectiveness 73.810%.\n",
            "PGD linf: Attack effectiveness 65.517%.\n",
            "PGD linf: Attack effectiveness 90.000%.\n",
            "PGD linf: Attack effectiveness 67.647%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 85.185%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 80.645%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 84.000%.\n",
            "PGD linf: Attack effectiveness 77.419%.\n",
            "PGD linf: Attack effectiveness 77.778%.\n",
            "PGD linf: Attack effectiveness 76.667%.\n",
            "PGD linf: Attack effectiveness 90.000%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 77.273%.\n",
            "PGD linf: Attack effectiveness 65.385%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 75.676%.\n",
            "PGD linf: Attack effectiveness 62.500%.\n",
            "PGD linf: Attack effectiveness 71.875%.\n",
            "PGD linf: Attack effectiveness 77.778%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 81.579%.\n",
            "PGD linf: Attack effectiveness 77.419%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 82.143%.\n",
            "PGD linf: Attack effectiveness 82.353%.\n",
            "PGD linf: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.84%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlcFrE-At2Es",
        "outputId": "7e575a64-f991-4b96-d0f5-11f540ee7b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 68.966%.\n",
            "PGD linf: Attack effectiveness 52.000%.\n",
            "PGD linf: Attack effectiveness 38.462%.\n",
            "PGD linf: Attack effectiveness 68.571%.\n",
            "PGD linf: Attack effectiveness 56.000%.\n",
            "PGD linf: Attack effectiveness 48.000%.\n",
            "PGD linf: Attack effectiveness 55.882%.\n",
            "PGD linf: Attack effectiveness 57.576%.\n",
            "PGD linf: Attack effectiveness 57.143%.\n",
            "PGD linf: Attack effectiveness 58.621%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 35.000%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 48.000%.\n",
            "PGD linf: Attack effectiveness 58.065%.\n",
            "PGD linf: Attack effectiveness 53.333%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 54.839%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 63.333%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 36.364%.\n",
            "PGD linf: Attack effectiveness 51.724%.\n",
            "PGD linf: Attack effectiveness 54.545%.\n",
            "PGD linf: Attack effectiveness 53.846%.\n",
            "PGD linf: Attack effectiveness 71.429%.\n",
            "PGD linf: Attack effectiveness 59.459%.\n",
            "PGD linf: Attack effectiveness 37.500%.\n",
            "PGD linf: Attack effectiveness 53.125%.\n",
            "PGD linf: Attack effectiveness 63.889%.\n",
            "PGD linf: Attack effectiveness 54.054%.\n",
            "PGD linf: Attack effectiveness 65.789%.\n",
            "PGD linf: Attack effectiveness 51.613%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 53.571%.\n",
            "PGD linf: Attack effectiveness 79.412%.\n",
            "PGD linf: Attack effectiveness 33.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.48%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11GKRtxxvDS7",
        "outputId": "6ba75f1a-a08d-44c4-bfc0-82609ca77e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l2: Attack effectiveness 72.414%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 53.846%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 68.000%.\n",
            "PGD l2: Attack effectiveness 76.471%.\n",
            "PGD l2: Attack effectiveness 72.727%.\n",
            "PGD l2: Attack effectiveness 71.429%.\n",
            "PGD l2: Attack effectiveness 68.966%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 64.706%.\n",
            "PGD l2: Attack effectiveness 65.000%.\n",
            "PGD l2: Attack effectiveness 81.481%.\n",
            "PGD l2: Attack effectiveness 68.000%.\n",
            "PGD l2: Attack effectiveness 83.871%.\n",
            "PGD l2: Attack effectiveness 70.000%.\n",
            "PGD l2: Attack effectiveness 92.000%.\n",
            "PGD l2: Attack effectiveness 77.419%.\n",
            "PGD l2: Attack effectiveness 77.778%.\n",
            "PGD l2: Attack effectiveness 80.000%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 50.000%.\n",
            "PGD l2: Attack effectiveness 68.966%.\n",
            "PGD l2: Attack effectiveness 72.727%.\n",
            "PGD l2: Attack effectiveness 65.385%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 75.676%.\n",
            "PGD l2: Attack effectiveness 62.500%.\n",
            "PGD l2: Attack effectiveness 75.000%.\n",
            "PGD l2: Attack effectiveness 80.556%.\n",
            "PGD l2: Attack effectiveness 72.973%.\n",
            "PGD l2: Attack effectiveness 81.579%.\n",
            "PGD l2: Attack effectiveness 74.194%.\n",
            "PGD l2: Attack effectiveness 78.571%.\n",
            "PGD l2: Attack effectiveness 82.143%.\n",
            "PGD l2: Attack effectiveness 82.353%.\n",
            "PGD l2: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.4%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQlv0R8cy8ei",
        "outputId": "3b586d1b-bb6f-4ca6-e86b-68e7b14c180d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 46.154%.\n",
            "PGD l1: Attack effectiveness 77.143%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 70.588%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 85.000%.\n",
            "PGD l1: Attack effectiveness 61.765%.\n",
            "PGD l1: Attack effectiveness 55.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 64.516%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 76.667%.\n",
            "PGD l1: Attack effectiveness 45.455%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 63.636%.\n",
            "PGD l1: Attack effectiveness 61.538%.\n",
            "PGD l1: Attack effectiveness 82.143%.\n",
            "PGD l1: Attack effectiveness 70.270%.\n",
            "PGD l1: Attack effectiveness 56.250%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 72.222%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 78.947%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "conclusion"
      ],
      "metadata": {
        "id": "Ul_o4rkOlGbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        #pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "QlY-uEEslagY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd2(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    #insertion_array_updated = torch.bitwise_or(insertion_array.to(torch.uint8), x.squeeze().to(torch.uint8) )\n",
        "    #removal_array_updated = torch.bitwise_or(removal_array.to(torch.uint8), (1 - x.squeeze().to(torch.uint8)) )\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "            #perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            #perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            #perturbation[torch.isnan(perturbation)] = 0.\n",
        "            #perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "SpW_1QIilagZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "\n",
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(torch.abs(x_next - torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "NBZMVbXDlJHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWM9Rk7rlFGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a1360c-0121-4b4a-af74-29cbfcf2e145",
        "id": "BALrCnWXopyU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03975ba4-44f7-44dc-9378-190d4f61177c",
        "id": "44nIZ-BOopyV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3482cec-ca56-4635-e3a6-40df6cdcd869",
        "id": "_LtUf4DpopyV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDOzv4PEoxCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910cc077-94db-40d8-f9c7-5089fe7bdb6c",
        "id": "TZXatuR9opyW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 80.18%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3a4611-2a25-42f2-cf5c-1659a16d4a68",
        "id": "DPKydw6TopyW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 69.12%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb97212-bdf4-4f26-bfc6-8e75f1caec39",
        "id": "nSywqfscopyW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.4%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MHc85REZo0wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9e502c-6dc1-4574-f730-823817f99c5a",
        "id": "ynb613QSopyX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 78.58%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3788fbe-aba1-4625-f0e5-fa1bf9848105",
        "id": "8sYbWremopyX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4087e04b-bcd7-413b-8b0f-a3a353adacb5",
        "id": "_Skn51BBopyY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.48%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hEGhljhiopKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db551d4d-5554-4691-ab91-ee604468179b",
        "id": "jNJ8MZZ9l44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PbugNsKmK4E",
        "outputId": "ac165f7d-6fcb-49fa-bcc0-7fa07b3104ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cak-U6IRmKGe",
        "outputId": "4fab09b7-8e2e-4fd2-f694-cc48a5a5daa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JaqDhY32mRVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc77889-61c3-45e2-9c3e-1c3a6579bc54",
        "id": "ZQT0WTE4l44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 86.46%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5IFBSEtmPtm",
        "outputId": "0fb02373-d479-469a-8c4e-325c53dc0301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 68.32%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpBKzLPNmQPy",
        "outputId": "ba7c0bfb-f8c8-4c0f-ad3a-f9d7f4218b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_hA66l_mYj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9f2fa6-79d4-48e9-dc56-84c65b36deb1",
        "id": "AfSDAvhYl44L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvZZq6OPmaEg",
        "outputId": "c25277f0-f675-4ec0-f2ef-387d5de4af5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.75%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2pkuMd_marG",
        "outputId": "040366b5-5500-4231-eb0d-b49d5e904266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ytAbwnAmfA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Study GKDE gradients"
      ],
      "metadata": {
        "id": "F5OlODkKX4jK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I8VdyzW2Xerd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde(adv_x,y,model,benigns, bandwidth, penalty_factor):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y.view(-1).long())\n",
        "    #print('ce: ', ce)\n",
        "    kde = KDE(adv_x, benigns, bandwidth)\n",
        "    #print('kde : ', kde)\n",
        "    loss_no_reduction = ce + penalty_factor * kde\n",
        "    _, predicted = torch.topk(outputs, k=1)\n",
        "    done = (predicted != y).squeeze()\n",
        "\n",
        "    return loss_no_reduction, done\n",
        "\n",
        "\n",
        "def gkde(x, y, model,bens, bandwidth, penalty_factor, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural, _ = get_loss_kde(x,y,model,bens, bandwidth, penalty_factor)\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        print('************** t ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        #y_model = model(x_var)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        outputs = model(x_var)\n",
        "        ce_loss = criterion(outputs, y.view(-1).long())\n",
        "        print('ce_loss: ', ce_loss)\n",
        "        kde_loss = KDE(x_var, bens, bandwidth)\n",
        "        print('kde_loss: ', kde_loss)\n",
        "        ce_grad = torch.autograd.grad(ce_loss.mean(), x_var, retain_graph=True)[0].data\n",
        "        kde_grad = torch.autograd.grad(kde_loss.mean(), x_var)[0].data\n",
        "        print('ce_grad ',torch.abs(ce_grad).sum(dim=-1).detach())\n",
        "        print('kde_grad ',torch.abs(kde_grad).sum(dim=-1).detach())\n",
        "        penalty_factor = torch.abs(ce_grad).sum(dim=-1).detach()/(torch.abs(kde_grad).sum(dim=-1).detach()+ 1e-20)\n",
        "        print('penalty_factor ',penalty_factor)\n",
        "\n",
        "        if t > 5:\n",
        "          decayed_penalty_factor = penalty_factor * (1 - t / k)\n",
        "        else:\n",
        "          decayed_penalty_factor = penalty_factor\n",
        "\n",
        "        # Compute loss\n",
        "        loss, _ = get_loss_kde(x_var,y,model,bens, bandwidth, decayed_penalty_factor)\n",
        "        #loss,_ = get_loss_kde(x_var,y,model,bens, bandwidth, penalty_factor)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "        pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        #pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x_next - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "\n",
        "            val, _ = torch.topk(torch.abs(gradients), 1)\n",
        "            perturbation = (torch.abs(gradients) >= val.expand_as(gradients)).float() * torch.sign(gradients).float()\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            _, done = get_loss_kde(x_next,y,model,bens, bandwidth, penalty_factor)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(x_next)\n",
        "    loss_adv = criterion(outputs, y.view(-1).long())\n",
        "    _, predicted = torch.topk(outputs, k=1)\n",
        "    done = (predicted != y).squeeze()\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "uV0R8nbaXfAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-select benign samples\n",
        "benign_samples = []\n",
        "\n",
        "for x_batch, y_batch in test_loader:\n",
        "  benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "# Forward pass to get logits for benign samples\n",
        "with torch.no_grad():  # No need for gradients\n",
        "    outputs = model_AT_rFGSM(ben_x.to(torch.float32))\n",
        "\n",
        "# Calculate softmax probabilities\n",
        "probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "# Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "# Select the top 500 high confidence benign samples\n",
        "top_500_high_confidence_benign_samples = ben_x[sorted_indices[:500]]\n",
        "\n",
        "del benign_samples, outputs, probabilities, ben_x  # Free up memory"
      ],
      "metadata": {
        "id": "U-eZopKFjIGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv = gkde(mals.to(torch.float32).to(device), mals_y.to(device), model_AT_rFGSM, top_500_high_confidence_benign_samples,0.6,1., insertion_array, removal_array, k=100, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krs1slxHjjin",
        "outputId": "3cde2f43-6cfb-463c-8502-43b750dcde8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************** t  0\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 9.2741e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1325e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.3344e-05, 2.2177e-10, 1.1448e-15, 0.0000e+00, 2.2177e-10, 3.4137e-18,\n",
            "        1.3935e-26, 6.6971e-06, 1.2336e-39, 1.8229e-36, 1.3344e-05, 5.0246e-11,\n",
            "        2.2177e-10, 5.7728e-08, 3.5037e-15, 4.1197e-12, 1.2445e-38, 3.1590e-40,\n",
            "        8.5677e-20, 2.3542e-38, 7.7047e-27, 2.7702e-10, 2.2177e-10, 4.0407e-39,\n",
            "        1.8901e-17, 1.1674e-15, 2.3014e-07, 4.1197e-12, 7.4781e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.0327e-15, 1.5589e-35, 3.6246e-23, 0.0000e+00, 1.5589e-35, 3.7404e-14,\n",
            "        0.0000e+00, 8.7472e-02, 0.0000e+00, 0.0000e+00, 1.0327e-15, 5.3577e+02,\n",
            "        1.5589e-35, 1.6707e-14, 1.1631e-37, 1.0270e-30, 0.0000e+00, 2.1986e-10,\n",
            "        1.8128e-25, 0.0000e+00, 0.0000e+00, 1.4061e-06, 1.5589e-35, 1.8835e-19,\n",
            "        3.6079e-13, 8.1566e-07, 1.8215e-31, 1.0270e-30, 1.0897e-14])\n",
            "kde_grad  tensor([1.1016e-05, 3.5169e-10, 2.7524e-15, 0.0000e+00, 3.5169e-10, 9.5880e-18,\n",
            "        5.7000e-26, 5.5288e-06, 7.5966e-39, 1.0206e-35, 1.1016e-05, 7.7543e-11,\n",
            "        3.5169e-10, 6.9480e-08, 7.9209e-15, 7.6794e-12, 7.4443e-38, 1.9407e-39,\n",
            "        2.6343e-19, 1.4114e-37, 3.1644e-26, 4.2795e-10, 3.5169e-10, 2.4811e-38,\n",
            "        5.0840e-17, 2.8093e-15, 2.5477e-07, 7.6794e-12, 2.9783e-03])\n",
            "penalty_factor  tensor([9.3744e-11, 4.4326e-26, 1.3169e-08, 0.0000e+00, 4.4326e-26, 3.8971e+03,\n",
            "        0.0000e+00, 1.5821e+04, 0.0000e+00, 0.0000e+00, 9.3744e-11, 6.9094e+12,\n",
            "        4.4326e-26, 2.4046e-07, 1.4684e-23, 1.3374e-19, 0.0000e+00, 2.1986e+10,\n",
            "        6.6298e-07, 0.0000e+00, 0.0000e+00, 3.2857e+03, 4.4326e-26, 1.8835e+01,\n",
            "        7.0952e+03, 2.9034e+08, 7.1495e-25, 1.3374e-19, 3.6588e-12])\n",
            "************** t  1\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 9.6215e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1673e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.7364e-05, 3.5889e-10, 2.0438e-15, 0.0000e+00, 3.5889e-10, 7.1692e-18,\n",
            "        1.3935e-26, 6.9177e-06, 1.2336e-39, 1.8229e-36, 1.7364e-05, 9.2329e-11,\n",
            "        3.5889e-10, 7.3458e-08, 5.6621e-15, 6.4538e-12, 1.2445e-38, 1.0376e-39,\n",
            "        2.0702e-19, 2.3542e-38, 7.7047e-27, 4.3162e-10, 3.5889e-10, 9.6569e-39,\n",
            "        3.9067e-17, 2.5587e-15, 2.8502e-07, 6.4538e-12, 7.6893e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([3.5289e-15, 1.4416e-33, 7.3212e-22, 0.0000e+00, 1.4416e-33, 1.7363e-11,\n",
            "        0.0000e+00, 8.1916e-01, 0.0000e+00, 0.0000e+00, 3.5289e-15, 6.2337e+02,\n",
            "        1.4416e-33, 2.8014e-13, 2.0506e-35, 8.6054e-29, 0.0000e+00, 6.6531e-09,\n",
            "        9.6210e-25, 0.0000e+00, 0.0000e+00, 4.9014e-06, 1.4416e-33, 2.6390e-18,\n",
            "        1.0897e-12, 5.1903e-06, 1.0666e-28, 8.6054e-29, 1.5972e-13])\n",
            "kde_grad  tensor([1.8160e-05, 7.0789e-10, 5.7878e-15, 0.0000e+00, 7.0789e-10, 2.4803e-17,\n",
            "        5.7000e-26, 7.5852e-06, 7.5966e-39, 1.0206e-35, 1.8160e-05, 1.5668e-10,\n",
            "        7.0789e-10, 1.0875e-07, 1.4909e-14, 1.4090e-11, 7.4443e-38, 6.7503e-39,\n",
            "        6.9992e-19, 1.4114e-37, 3.1644e-26, 7.4803e-10, 7.0789e-10, 6.1113e-38,\n",
            "        1.0927e-16, 6.8721e-15, 4.7459e-07, 1.4090e-11, 5.2694e-03])\n",
            "penalty_factor  tensor([1.9432e-10, 2.0365e-24, 1.2649e-07, 0.0000e+00, 2.0365e-24, 6.9973e+05,\n",
            "        0.0000e+00, 1.0799e+05, 0.0000e+00, 0.0000e+00, 1.9432e-10, 3.9787e+12,\n",
            "        2.0365e-24, 2.5760e-06, 1.3754e-21, 6.1073e-18, 0.0000e+00, 6.6531e+11,\n",
            "        1.3552e-06, 0.0000e+00, 0.0000e+00, 6.5524e+03, 2.0365e-24, 2.6390e+02,\n",
            "        9.9721e+03, 7.5526e+08, 2.2474e-22, 6.1073e-18, 3.0310e-11])\n",
            "************** t  2\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.3584e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1650e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.3439e-05, 6.2128e-10, 4.0406e-15, 0.0000e+00, 6.2128e-10, 1.7692e-17,\n",
            "        1.3935e-26, 7.5188e-06, 1.2336e-39, 1.8229e-36, 2.3439e-05, 1.7606e-10,\n",
            "        6.2128e-10, 9.8817e-08, 1.0016e-14, 1.0751e-11, 1.2445e-38, 2.4089e-39,\n",
            "        5.3579e-19, 2.3542e-38, 7.7047e-27, 7.0119e-10, 6.2128e-10, 2.1177e-38,\n",
            "        8.2252e-17, 5.9727e-15, 4.0464e-07, 1.0751e-11, 8.2887e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([7.4656e-15, 1.1246e-32, 1.7981e-21, 0.0000e+00, 1.1246e-32, 3.3273e-11,\n",
            "        0.0000e+00, 1.0796e+00, 0.0000e+00, 0.0000e+00, 7.4656e-15, 6.2854e+02,\n",
            "        1.1246e-32, 9.4188e-13, 8.2866e-35, 3.4244e-28, 0.0000e+00, 1.0506e-07,\n",
            "        1.2217e-24, 0.0000e+00, 0.0000e+00, 1.1980e-05, 1.1246e-32, 2.9213e-17,\n",
            "        1.8892e-12, 1.1184e-05, 5.3788e-28, 3.4244e-28, 5.5975e-13])\n",
            "kde_grad  tensor([2.6128e-05, 1.2949e-09, 1.1551e-14, 0.0000e+00, 1.2949e-09, 6.1039e-17,\n",
            "        5.7000e-26, 8.8794e-06, 7.5966e-39, 1.0206e-35, 2.6128e-05, 2.9445e-10,\n",
            "        1.2949e-09, 1.5242e-07, 2.6504e-14, 2.4106e-11, 7.4443e-38, 1.6271e-38,\n",
            "        1.8109e-18, 1.4114e-37, 3.1644e-26, 1.2027e-09, 1.2949e-09, 1.3819e-37,\n",
            "        2.2895e-16, 1.6048e-14, 6.9808e-07, 2.4106e-11, 6.6194e-03])\n",
            "penalty_factor  tensor([2.8574e-10, 8.6852e-24, 1.5568e-07, 0.0000e+00, 8.6852e-24, 5.4501e+05,\n",
            "        0.0000e+00, 1.2159e+05, 0.0000e+00, 0.0000e+00, 2.8574e-10, 2.1346e+12,\n",
            "        8.6852e-24, 6.1794e-06, 3.1266e-21, 1.4205e-17, 0.0000e+00, 1.0506e+13,\n",
            "        6.7096e-07, 0.0000e+00, 0.0000e+00, 9.9610e+03, 8.6852e-24, 2.9213e+03,\n",
            "        8.2511e+03, 6.9690e+08, 7.7051e-22, 1.4205e-17, 8.4562e-11])\n",
            "************** t  3\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.7025e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1608e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.2010e-05, 1.0892e-09, 7.9941e-15, 0.0000e+00, 1.0892e-09, 4.2879e-17,\n",
            "        1.3935e-26, 8.1964e-06, 1.2336e-39, 1.8229e-36, 3.2010e-05, 3.3198e-10,\n",
            "        1.0892e-09, 1.3365e-07, 1.7660e-14, 1.7798e-11, 1.2445e-38, 4.9348e-39,\n",
            "        1.3681e-18, 2.3542e-38, 7.7047e-27, 1.1198e-09, 1.0892e-09, 4.7752e-38,\n",
            "        1.7110e-16, 1.3825e-14, 5.8241e-07, 1.7798e-11, 9.0112e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.1756e-14, 6.8905e-32, 4.0493e-21, 0.0000e+00, 6.8905e-32, 6.2882e-11,\n",
            "        0.0000e+00, 1.3587e+00, 0.0000e+00, 0.0000e+00, 2.1756e-14, 6.8419e+02,\n",
            "        6.8905e-32, 2.9358e-12, 3.3108e-34, 1.3261e-27, 0.0000e+00, 1.2461e-06,\n",
            "        1.5574e-24, 0.0000e+00, 0.0000e+00, 2.3855e-05, 6.8905e-32, 2.6250e-16,\n",
            "        3.2374e-12, 2.3583e-05, 2.6036e-27, 1.3261e-27, 1.5457e-12])\n",
            "kde_grad  tensor([3.5867e-05, 2.2996e-09, 2.2653e-14, 0.0000e+00, 2.2996e-09, 1.4590e-16,\n",
            "        5.7000e-26, 1.0199e-05, 7.5966e-39, 1.0206e-35, 3.5867e-05, 5.4258e-10,\n",
            "        2.2996e-09, 2.0900e-07, 4.6321e-14, 4.0487e-11, 7.4443e-38, 3.4378e-38,\n",
            "        4.5750e-18, 1.4114e-37, 3.1644e-26, 1.9005e-09, 2.2996e-09, 3.1902e-37,\n",
            "        4.7136e-16, 3.6733e-14, 9.9710e-07, 4.0487e-11, 7.8553e-03])\n",
            "penalty_factor  tensor([6.0657e-10, 2.9964e-23, 1.7875e-07, 0.0000e+00, 2.9964e-23, 4.3097e+05,\n",
            "        0.0000e+00, 1.3322e+05, 0.0000e+00, 0.0000e+00, 6.0657e-10, 1.2610e+12,\n",
            "        2.9964e-23, 1.4047e-05, 7.1475e-21, 3.2754e-17, 0.0000e+00, 1.2461e+14,\n",
            "        3.3967e-07, 0.0000e+00, 0.0000e+00, 1.2552e+04, 2.9964e-23, 2.6250e+04,\n",
            "        6.8682e+03, 6.4203e+08, 2.6112e-21, 3.2754e-17, 1.9678e-10])\n",
            "************** t  4\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.2715e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1565e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-07, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.3643e-05, 1.8847e-09, 1.5592e-14, 0.0000e+00, 1.8847e-09, 1.0099e-16,\n",
            "        1.3935e-26, 8.9815e-06, 1.2336e-39, 1.8229e-36, 4.3643e-05, 6.1583e-10,\n",
            "        1.8847e-09, 1.8032e-07, 3.0713e-14, 2.8450e-11, 1.2445e-38, 1.0078e-38,\n",
            "        3.4176e-18, 2.3542e-38, 7.7047e-27, 1.7539e-09, 1.8847e-09, 9.7802e-38,\n",
            "        3.5012e-16, 3.1429e-14, 8.3200e-07, 2.8450e-11, 9.9633e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([6.2167e-14, 4.3156e-31, 9.1189e-21, 0.0000e+00, 4.3156e-31, 1.1911e-10,\n",
            "        0.0000e+00, 1.5993e+00, 0.0000e+00, 0.0000e+00, 6.2167e-14, 6.7873e+02,\n",
            "        4.3156e-31, 8.9621e-12, 1.3228e-33, 3.3705e-27, 0.0000e+00, 1.2298e-05,\n",
            "        2.3477e-24, 0.0000e+00, 0.0000e+00, 1.1821e-04, 4.3156e-31, 1.9823e-15,\n",
            "        5.5479e-12, 1.0407e-04, 1.2603e-26, 3.3705e-27, 3.9053e-12])\n",
            "kde_grad  tensor([4.8400e-05, 4.0373e-09, 4.3796e-14, 0.0000e+00, 4.0373e-09, 3.3865e-16,\n",
            "        5.7000e-26, 1.1508e-05, 7.5966e-39, 1.0206e-35, 4.8400e-05, 9.8416e-10,\n",
            "        4.0373e-09, 2.8340e-07, 7.9848e-14, 6.7716e-11, 7.4443e-38, 7.1722e-38,\n",
            "        1.1324e-17, 1.4114e-37, 3.1644e-26, 2.9657e-09, 4.0373e-09, 6.7114e-37,\n",
            "        9.5516e-16, 8.2624e-14, 1.4134e-06, 6.7716e-11, 8.7465e-03])\n",
            "penalty_factor  tensor([1.2844e-09, 1.0689e-22, 2.0821e-07, 0.0000e+00, 1.0689e-22, 3.5173e+05,\n",
            "        0.0000e+00, 1.3897e+05, 0.0000e+00, 0.0000e+00, 1.2844e-09, 6.8965e+11,\n",
            "        1.0689e-22, 3.1624e-05, 1.6567e-20, 4.9773e-17, 0.0000e+00, 1.2298e+15,\n",
            "        2.0714e-07, 0.0000e+00, 0.0000e+00, 3.9858e+04, 1.0689e-22, 1.9823e+05,\n",
            "        5.8083e+03, 1.2595e+09, 8.9162e-21, 4.9773e-17, 4.4650e-10])\n",
            "************** t  5\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.7973e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1572e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-07, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.3842e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.9109e-05, 3.2439e-09, 2.9981e-14, 0.0000e+00, 3.2439e-09, 2.3105e-16,\n",
            "        1.3935e-26, 9.9041e-06, 1.2336e-39, 1.8229e-36, 5.9109e-05, 1.1235e-09,\n",
            "        3.2439e-09, 2.4199e-07, 5.2683e-14, 4.6266e-11, 1.2445e-38, 1.8472e-38,\n",
            "        8.3749e-18, 2.3542e-38, 7.7047e-27, 2.7205e-09, 3.2439e-09, 1.8805e-37,\n",
            "        7.0558e-16, 7.0214e-14, 1.1796e-06, 4.6266e-11, 1.1023e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.7764e-13, 2.0958e-30, 2.0493e-20, 0.0000e+00, 2.0958e-30, 2.2579e-10,\n",
            "        0.0000e+00, 2.1647e+00, 0.0000e+00, 0.0000e+00, 1.7764e-13, 6.8874e+02,\n",
            "        2.0958e-30, 2.7023e-11, 5.2853e-33, 1.2417e-26, 0.0000e+00, 7.0598e-05,\n",
            "        3.8284e-24, 0.0000e+00, 0.0000e+00, 2.6931e-04, 2.0958e-30, 1.2279e-14,\n",
            "        9.8367e-12, 2.1428e-04, 6.1003e-26, 1.2417e-26, 9.3877e-12])\n",
            "kde_grad  tensor([6.4872e-05, 6.9745e-09, 8.3465e-14, 0.0000e+00, 6.9745e-09, 7.6330e-16,\n",
            "        5.7000e-26, 1.2766e-05, 7.5966e-39, 1.0206e-35, 6.4872e-05, 1.7547e-09,\n",
            "        6.9745e-09, 3.8130e-07, 1.3575e-13, 1.1018e-10, 7.4443e-38, 1.3453e-37,\n",
            "        2.7499e-17, 1.4114e-37, 3.1644e-26, 4.5359e-09, 6.9745e-09, 1.3167e-36,\n",
            "        1.9064e-15, 1.8260e-13, 1.9885e-06, 1.1018e-10, 9.6349e-03])\n",
            "penalty_factor  tensor([2.7384e-09, 3.0049e-22, 2.4553e-07, 0.0000e+00, 3.0049e-22, 2.9581e+05,\n",
            "        0.0000e+00, 1.6957e+05, 0.0000e+00, 0.0000e+00, 2.7384e-09, 3.9252e+11,\n",
            "        3.0049e-22, 7.0872e-05, 3.8933e-20, 1.1270e-16, 0.0000e+00, 7.0598e+15,\n",
            "        1.3917e-07, 0.0000e+00, 0.0000e+00, 5.9372e+04, 3.0049e-22, 1.2279e+06,\n",
            "        5.1598e+03, 1.1735e+09, 3.0678e-20, 1.1270e-16, 9.7434e-10])\n",
            "************** t  6\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 3.4756e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1589e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.3842e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 4.7684e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.9524e-05, 5.5630e-09, 5.6800e-14, 0.0000e+00, 5.5630e-09, 5.1339e-16,\n",
            "        1.3935e-26, 1.0927e-05, 1.2336e-39, 1.8229e-36, 7.9524e-05, 2.0154e-09,\n",
            "        5.5630e-09, 3.2410e-07, 8.9097e-14, 7.4817e-11, 1.2445e-38, 4.2564e-38,\n",
            "        2.0134e-17, 2.3542e-38, 7.7047e-27, 4.1390e-09, 5.5630e-09, 3.2551e-37,\n",
            "        1.4004e-15, 1.5422e-13, 1.6600e-06, 7.4817e-11, 1.2156e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([5.0761e-13, 1.1923e-29, 4.5796e-20, 0.0000e+00, 1.1923e-29, 4.2856e-10,\n",
            "        0.0000e+00, 2.6888e+00, 0.0000e+00, 0.0000e+00, 5.0761e-13, 6.8874e+02,\n",
            "        1.1923e-29, 8.1428e-11, 1.9879e-32, 4.5203e-26, 0.0000e+00, 1.7382e-04,\n",
            "        6.2429e-24, 0.0000e+00, 0.0000e+00, 5.6683e-04, 1.1923e-29, 6.8372e-14,\n",
            "        1.7545e-11, 4.3569e-04, 2.9529e-25, 4.5203e-26, 2.2721e-11])\n",
            "kde_grad  tensor([8.6362e-05, 1.1845e-08, 1.5682e-13, 0.0000e+00, 1.1845e-08, 1.6707e-15,\n",
            "        5.7000e-26, 1.4064e-05, 7.5966e-39, 1.0206e-35, 8.6362e-05, 3.0702e-09,\n",
            "        1.1845e-08, 5.0702e-07, 2.2767e-13, 1.7659e-10, 7.4443e-38, 3.1597e-37,\n",
            "        6.5492e-17, 1.4114e-37, 3.1644e-26, 6.8824e-09, 1.1845e-08, 2.3220e-36,\n",
            "        3.7472e-15, 3.9641e-13, 2.7763e-06, 1.7659e-10, 1.0625e-02])\n",
            "penalty_factor  tensor([5.8777e-09, 1.0066e-21, 2.9203e-07, 0.0000e+00, 1.0066e-21, 2.5652e+05,\n",
            "        0.0000e+00, 1.9118e+05, 0.0000e+00, 0.0000e+00, 5.8777e-09, 2.2433e+11,\n",
            "        1.0066e-21, 1.6060e-04, 8.7313e-20, 2.5598e-16, 0.0000e+00, 1.7382e+16,\n",
            "        9.5309e-08, 0.0000e+00, 0.0000e+00, 8.2359e+04, 1.0066e-21, 6.8372e+06,\n",
            "        4.6821e+03, 1.0991e+09, 1.0636e-19, 2.5598e-16, 2.1385e-09])\n",
            "************** t  7\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 4.2848e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1601e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4305e-06, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.0729e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.0628e-04, 9.4263e-09, 1.0608e-13, 0.0000e+00, 9.4263e-09, 1.1080e-15,\n",
            "        1.3935e-26, 1.2037e-05, 1.2336e-39, 1.8229e-36, 1.0628e-04, 3.5552e-09,\n",
            "        9.4263e-09, 4.3129e-07, 1.4868e-13, 1.1955e-10, 1.2445e-38, 1.0787e-37,\n",
            "        4.7480e-17, 2.3542e-38, 7.7047e-27, 6.2459e-09, 9.4263e-09, 4.9218e-37,\n",
            "        2.7346e-15, 3.3286e-13, 2.3183e-06, 1.1955e-10, 1.3389e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.4505e-12, 6.8402e-29, 1.0251e-19, 0.0000e+00, 6.8402e-29, 8.1343e-10,\n",
            "        0.0000e+00, 3.2077e+00, 0.0000e+00, 0.0000e+00, 1.4505e-12, 6.8874e+02,\n",
            "        6.8402e-29, 2.4536e-10, 7.9423e-32, 1.7351e-25, 0.0000e+00, 8.2211e-04,\n",
            "        1.0180e-23, 0.0000e+00, 0.0000e+00, 1.1842e-03, 6.8402e-29, 3.2599e-13,\n",
            "        3.1381e-11, 9.5006e-04, 1.4367e-24, 1.7351e-25, 5.5322e-11])\n",
            "kde_grad  tensor([1.1419e-04, 1.9874e-08, 2.9045e-13, 0.0000e+00, 1.9874e-08, 3.5508e-15,\n",
            "        5.7000e-26, 1.5445e-05, 7.5966e-39, 1.0206e-35, 1.1419e-04, 5.3065e-09,\n",
            "        1.9874e-08, 6.6984e-07, 3.7653e-13, 2.8007e-10, 7.4443e-38, 8.1377e-37,\n",
            "        1.5296e-16, 1.4114e-37, 3.1644e-26, 1.0238e-08, 1.9874e-08, 3.5794e-36,\n",
            "        7.2556e-15, 8.4546e-13, 3.8467e-06, 2.8007e-10, 1.1652e-02])\n",
            "penalty_factor  tensor([1.2702e-08, 3.4418e-21, 3.5292e-07, 0.0000e+00, 3.4418e-21, 2.2908e+05,\n",
            "        0.0000e+00, 2.0768e+05, 0.0000e+00, 0.0000e+00, 1.2702e-08, 1.2979e+11,\n",
            "        3.4418e-21, 3.6630e-04, 2.1093e-19, 6.1953e-16, 0.0000e+00, 8.2211e+16,\n",
            "        6.6551e-08, 0.0000e+00, 0.0000e+00, 1.1567e+05, 3.4418e-21, 3.2599e+07,\n",
            "        4.3251e+03, 1.1237e+09, 3.7348e-19, 6.1953e-16, 4.7480e-09])\n",
            "************** t  8\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 5.2738e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1634e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.3842e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.2186e-06, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.3842e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.4109e-04, 1.5773e-08, 1.9543e-13, 0.0000e+00, 1.5773e-08, 2.3227e-15,\n",
            "        1.3935e-26, 1.3229e-05, 1.2336e-39, 1.8229e-36, 1.4109e-04, 6.2076e-09,\n",
            "        1.5773e-08, 5.7024e-07, 2.4473e-13, 1.8863e-10, 1.2445e-38, 2.5987e-37,\n",
            "        1.0982e-16, 2.3542e-38, 7.7047e-27, 9.3026e-09, 1.5773e-08, 6.8872e-37,\n",
            "        5.2354e-15, 7.0592e-13, 3.2080e-06, 1.8863e-10, 1.4707e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([4.2057e-12, 3.9267e-28, 2.2939e-19, 0.0000e+00, 3.9267e-28, 1.5439e-09,\n",
            "        0.0000e+00, 3.9058e+00, 0.0000e+00, 0.0000e+00, 4.2057e-12, 7.2351e+02,\n",
            "        3.9267e-28, 7.4628e-10, 3.1733e-31, 6.6947e-25, 0.0000e+00, 1.4471e-03,\n",
            "        1.6601e-23, 0.0000e+00, 0.0000e+00, 2.6792e-03, 3.9267e-28, 1.6372e-12,\n",
            "        5.7497e-11, 2.0282e-03, 6.9817e-24, 6.6947e-25, 1.3459e-10])\n",
            "kde_grad  tensor([1.4998e-04, 3.2956e-08, 5.3021e-13, 0.0000e+00, 3.2956e-08, 7.3285e-15,\n",
            "        5.7000e-26, 1.6925e-05, 7.5966e-39, 1.0206e-35, 1.4998e-04, 9.1385e-09,\n",
            "        3.2956e-08, 8.7919e-07, 6.1417e-13, 4.4027e-10, 7.4443e-38, 1.9921e-36,\n",
            "        3.5031e-16, 1.4114e-37, 3.1644e-26, 1.5011e-08, 3.2956e-08, 5.0994e-36,\n",
            "        1.3854e-14, 1.7715e-12, 5.2990e-06, 4.4027e-10, 1.2770e-02])\n",
            "penalty_factor  tensor([2.8042e-08, 1.1915e-20, 4.3265e-07, 0.0000e+00, 1.1915e-20, 2.1067e+05,\n",
            "        0.0000e+00, 2.3078e+05, 0.0000e+00, 0.0000e+00, 2.8042e-08, 7.9171e+10,\n",
            "        1.1915e-20, 8.4883e-04, 5.1668e-19, 1.5206e-15, 0.0000e+00, 1.4471e+17,\n",
            "        4.7387e-08, 0.0000e+00, 0.0000e+00, 1.7849e+05, 1.1915e-20, 1.6372e+08,\n",
            "        4.1502e+03, 1.1449e+09, 1.3176e-18, 1.5206e-15, 1.0539e-08])\n",
            "************** t  9\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.4359e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1659e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.0068e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1525e-06, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 5.0068e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.8606e-04, 2.6093e-08, 3.5491e-13, 0.0000e+00, 2.6093e-08, 4.7292e-15,\n",
            "        1.3935e-26, 1.4507e-05, 1.2336e-39, 1.8229e-36, 1.8606e-04, 1.0723e-08,\n",
            "        2.6093e-08, 7.4910e-07, 3.9731e-13, 2.9515e-10, 1.2445e-38, 5.7132e-37,\n",
            "        2.4915e-16, 2.3542e-38, 7.7047e-27, 1.3669e-08, 2.6093e-08, 8.5414e-37,\n",
            "        9.9268e-15, 1.4694e-12, 4.4129e-06, 2.9515e-10, 1.6128e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.2110e-11, 2.2532e-27, 5.1334e-19, 0.0000e+00, 2.2532e-27, 2.9304e-09,\n",
            "        0.0000e+00, 4.4885e+00, 0.0000e+00, 0.0000e+00, 1.2110e-11, 8.0757e+02,\n",
            "        2.2532e-27, 2.2748e-09, 1.2919e-30, 2.5709e-24, 0.0000e+00, 2.9371e-03,\n",
            "        2.7070e-23, 0.0000e+00, 0.0000e+00, 5.9480e-03, 2.2532e-27, 8.4685e-12,\n",
            "        1.0324e-10, 4.2652e-03, 3.3973e-23, 2.5709e-24, 3.2731e-10])\n",
            "kde_grad  tensor([1.9564e-04, 5.3972e-08, 9.5406e-13, 0.0000e+00, 5.3972e-08, 1.4687e-14,\n",
            "        5.7000e-26, 1.8504e-05, 7.5966e-39, 1.0206e-35, 1.9564e-04, 1.5559e-08,\n",
            "        5.3972e-08, 1.1465e-06, 9.8803e-13, 6.8409e-10, 7.4443e-38, 4.4199e-36,\n",
            "        7.8671e-16, 1.4114e-37, 3.1644e-26, 2.1708e-08, 5.3972e-08, 6.4365e-36,\n",
            "        2.6007e-14, 3.6484e-12, 7.2310e-06, 6.8409e-10, 1.3942e-02])\n",
            "penalty_factor  tensor([6.1900e-08, 4.1748e-20, 5.3806e-07, 0.0000e+00, 4.1748e-20, 1.9952e+05,\n",
            "        0.0000e+00, 2.4257e+05, 0.0000e+00, 0.0000e+00, 6.1900e-08, 5.1902e+10,\n",
            "        4.1748e-20, 1.9842e-03, 1.3075e-18, 3.7581e-15, 0.0000e+00, 2.9371e+17,\n",
            "        3.4409e-08, 0.0000e+00, 0.0000e+00, 2.7400e+05, 4.1748e-20, 8.4685e+08,\n",
            "        3.9698e+03, 1.1691e+09, 4.6982e-18, 3.7581e-15, 2.3476e-08])\n",
            "************** t  10\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 7.8545e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1676e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0967e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.6451e-05, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.0490e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.4373e-04, 4.2647e-08, 6.3539e-13, 0.0000e+00, 4.2647e-08, 9.3527e-15,\n",
            "        1.3935e-26, 1.5873e-05, 1.2336e-39, 1.8229e-36, 2.4373e-04, 1.8265e-08,\n",
            "        4.2647e-08, 9.7771e-07, 6.3489e-13, 4.5699e-10, 1.2445e-38, 1.4313e-36,\n",
            "        5.5432e-16, 2.3542e-38, 7.7047e-27, 1.9819e-08, 4.2647e-08, 9.2592e-37,\n",
            "        1.8536e-14, 3.0086e-12, 6.0246e-06, 4.5699e-10, 1.7647e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([3.4871e-11, 1.2930e-26, 1.1488e-18, 0.0000e+00, 1.2930e-26, 5.5621e-09,\n",
            "        0.0000e+00, 5.6970e+00, 0.0000e+00, 0.0000e+00, 3.4871e-11, 8.0761e+02,\n",
            "        1.2930e-26, 6.9140e-09, 4.8563e-30, 9.8724e-24, 0.0000e+00, 6.4069e-03,\n",
            "        4.4143e-23, 0.0000e+00, 0.0000e+00, 1.4238e-02, 1.2930e-26, 4.3973e-11,\n",
            "        1.8508e-10, 8.7869e-03, 1.6531e-22, 9.8724e-24, 7.9603e-10])\n",
            "kde_grad  tensor([2.5348e-04, 8.7325e-08, 1.6922e-12, 0.0000e+00, 8.7325e-08, 2.8583e-14,\n",
            "        5.7000e-26, 2.0186e-05, 7.5966e-39, 1.0206e-35, 2.5348e-04, 2.7023e-08,\n",
            "        8.7325e-08, 1.4853e-06, 1.5689e-12, 1.0517e-09, 7.4443e-38, 1.1163e-35,\n",
            "        1.7324e-15, 1.4114e-37, 3.1644e-26, 3.0969e-08, 8.7325e-08, 7.1079e-36,\n",
            "        4.8076e-14, 7.3783e-12, 9.7922e-06, 1.0517e-09, 1.5188e-02])\n",
            "penalty_factor  tensor([1.3757e-07, 1.4807e-19, 6.7886e-07, 0.0000e+00, 1.4807e-19, 1.9459e+05,\n",
            "        0.0000e+00, 2.8223e+05, 0.0000e+00, 0.0000e+00, 1.3757e-07, 2.9886e+10,\n",
            "        1.4807e-19, 4.6550e-03, 3.0954e-18, 9.3873e-15, 0.0000e+00, 6.4069e+17,\n",
            "        2.5480e-08, 0.0000e+00, 0.0000e+00, 4.5976e+05, 1.4807e-19, 4.3973e+09,\n",
            "        3.8498e+03, 1.1909e+09, 1.6882e-17, 9.3873e-15, 5.2413e-08])\n",
            "************** t  11\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 9.6846e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1653e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.3484e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.8742e-05, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.2053e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.1715e-04, 6.8872e-08, 1.1214e-12, 0.0000e+00, 6.8872e-08, 1.7965e-14,\n",
            "        1.3935e-26, 1.7329e-05, 1.2336e-39, 1.8229e-36, 3.1715e-04, 3.1652e-08,\n",
            "        6.8872e-08, 1.2678e-06, 1.0027e-12, 7.0012e-10, 1.2445e-38, 3.2395e-36,\n",
            "        1.2095e-15, 2.3542e-38, 7.7047e-27, 2.8324e-08, 6.8872e-08, 8.8475e-37,\n",
            "        3.4088e-14, 6.0391e-12, 8.1628e-06, 7.0012e-10, 1.9266e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.0041e-10, 7.4198e-26, 2.5707e-18, 0.0000e+00, 7.4198e-26, 1.0557e-08,\n",
            "        0.0000e+00, 7.0794e+00, 0.0000e+00, 0.0000e+00, 1.0041e-10, 8.6126e+02,\n",
            "        7.4198e-26, 2.1014e-08, 1.9657e-29, 3.7912e-23, 0.0000e+00, 1.3550e-02,\n",
            "        7.1983e-23, 0.0000e+00, 0.0000e+00, 3.3975e-02, 7.4198e-26, 1.0527e-10,\n",
            "        3.3274e-10, 1.8490e-02, 8.0440e-22, 3.7912e-23, 1.9798e-09])\n",
            "kde_grad  tensor([3.2620e-04, 1.3958e-07, 2.9585e-12, 0.0000e+00, 1.3958e-07, 5.4014e-14,\n",
            "        5.7000e-26, 2.1971e-05, 7.5966e-39, 1.0206e-35, 3.2620e-04, 4.8205e-08,\n",
            "        1.3958e-07, 1.9117e-06, 2.4576e-12, 1.5996e-09, 7.4443e-38, 2.5468e-35,\n",
            "        3.7406e-15, 1.4114e-37, 3.1644e-26, 4.3647e-08, 1.3958e-07, 6.9165e-36,\n",
            "        8.7515e-14, 1.4673e-11, 1.3159e-05, 1.5996e-09, 1.6507e-02])\n",
            "penalty_factor  tensor([3.0782e-07, 5.3157e-19, 8.6891e-07, 0.0000e+00, 5.3157e-19, 1.9545e+05,\n",
            "        0.0000e+00, 3.2221e+05, 0.0000e+00, 0.0000e+00, 3.0782e-07, 1.7866e+10,\n",
            "        5.3157e-19, 1.0992e-02, 7.9986e-18, 2.3700e-14, 0.0000e+00, 1.3550e+18,\n",
            "        1.9243e-08, 0.0000e+00, 0.0000e+00, 7.7841e+05, 5.3157e-19, 1.0527e+10,\n",
            "        3.8021e+03, 1.2601e+09, 6.1127e-17, 2.3700e-14, 1.1994e-07])\n",
            "************** t  12\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.1958e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1680e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.2914e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.3098e-05, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 4.5895e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.0996e-04, 1.0989e-07, 1.9509e-12, 0.0000e+00, 1.0989e-07, 3.3518e-14,\n",
            "        1.3935e-26, 1.8867e-05, 1.2336e-39, 1.8229e-36, 4.0996e-04, 5.5146e-08,\n",
            "        1.0989e-07, 1.6334e-06, 1.5654e-12, 1.0613e-09, 1.2445e-38, 6.6719e-36,\n",
            "        2.5880e-15, 2.3542e-38, 7.7047e-27, 3.9996e-08, 1.0989e-07, 9.2804e-37,\n",
            "        6.1737e-14, 1.1931e-11, 1.0976e-05, 1.0613e-09, 2.0964e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.9738e-10, 4.2564e-25, 5.7382e-18, 0.0000e+00, 4.2564e-25, 2.0299e-08,\n",
            "        0.0000e+00, 8.9003e+00, 0.0000e+00, 0.0000e+00, 2.9738e-10, 8.5971e+02,\n",
            "        4.2564e-25, 6.3868e-08, 8.5612e-29, 1.4894e-22, 0.0000e+00, 3.1191e-02,\n",
            "        1.1738e-22, 0.0000e+00, 0.0000e+00, 8.6752e-02, 4.2564e-25, 3.4541e-10,\n",
            "        5.9819e-10, 3.8384e-02, 3.9142e-21, 1.4894e-22, 4.8686e-09])\n",
            "kde_grad  tensor([4.1694e-04, 2.2042e-07, 5.0985e-12, 0.0000e+00, 2.2042e-07, 9.9114e-14,\n",
            "        5.7000e-26, 2.3884e-05, 7.5966e-39, 1.0206e-35, 4.1694e-04, 8.5672e-08,\n",
            "        2.2042e-07, 2.4444e-06, 3.8124e-12, 2.4073e-09, 7.4443e-38, 5.2857e-35,\n",
            "        7.9194e-15, 1.4114e-37, 3.1644e-26, 6.0617e-08, 2.2042e-07, 7.3480e-36,\n",
            "        1.5687e-13, 2.8646e-11, 1.7549e-05, 2.4073e-09, 1.7962e-02])\n",
            "penalty_factor  tensor([7.1325e-07, 1.9311e-18, 1.1255e-06, 0.0000e+00, 1.9311e-18, 2.0481e+05,\n",
            "        0.0000e+00, 3.7264e+05, 0.0000e+00, 0.0000e+00, 7.1325e-07, 1.0035e+10,\n",
            "        1.9311e-18, 2.6129e-02, 2.2456e-17, 6.1869e-14, 0.0000e+00, 3.1191e+18,\n",
            "        1.4822e-08, 0.0000e+00, 0.0000e+00, 1.4312e+06, 1.9311e-18, 3.4541e+10,\n",
            "        3.8133e+03, 1.3400e+09, 2.2304e-16, 6.1869e-14, 2.7105e-07])\n",
            "************** t  13\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.4786e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1731e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.9007e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.3100e-04, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 9.5720e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.2610e-04, 1.7324e-07, 3.3347e-12, 0.0000e+00, 1.7324e-07, 6.0738e-14,\n",
            "        1.3935e-26, 2.0506e-05, 1.2336e-39, 1.8229e-36, 5.2610e-04, 9.6354e-08,\n",
            "        1.7324e-07, 2.0908e-06, 2.4173e-12, 1.5892e-09, 1.2445e-38, 1.0904e-35,\n",
            "        5.4303e-15, 2.3542e-38, 7.7047e-27, 5.5748e-08, 1.7324e-07, 8.9664e-37,\n",
            "        1.1011e-13, 2.3171e-11, 1.4647e-05, 1.5892e-09, 2.2786e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([8.6840e-10, 2.4426e-24, 1.2986e-17, 0.0000e+00, 2.4426e-24, 3.8968e-08,\n",
            "        0.0000e+00, 1.0989e+01, 0.0000e+00, 0.0000e+00, 8.6840e-10, 8.4578e+02,\n",
            "        2.4426e-24, 1.9190e-07, 3.7744e-28, 5.8492e-22, 0.0000e+00, 3.0702e-02,\n",
            "        1.9163e-22, 0.0000e+00, 0.0000e+00, 2.1778e-01, 2.4426e-24, 1.1206e-09,\n",
            "        1.0748e-09, 8.0050e-02, 1.9046e-20, 5.8492e-22, 1.2201e-08])\n",
            "kde_grad  tensor([5.3002e-04, 3.4386e-07, 8.6703e-12, 0.0000e+00, 3.4386e-07, 1.7660e-13,\n",
            "        5.7000e-26, 2.5882e-05, 7.5966e-39, 1.0206e-35, 5.3002e-04, 1.5007e-07,\n",
            "        3.4386e-07, 3.1051e-06, 5.8486e-12, 3.5873e-09, 7.4443e-38, 8.7043e-35,\n",
            "        1.6439e-14, 1.4114e-37, 3.1644e-26, 8.3073e-08, 3.4386e-07, 7.1876e-36,\n",
            "        2.7688e-13, 5.4927e-11, 2.3224e-05, 3.5873e-09, 1.9435e-02])\n",
            "penalty_factor  tensor([1.6384e-06, 7.1036e-18, 1.4977e-06, 0.0000e+00, 7.1036e-18, 2.2066e+05,\n",
            "        0.0000e+00, 4.2460e+05, 0.0000e+00, 0.0000e+00, 1.6384e-06, 5.6360e+09,\n",
            "        7.1036e-18, 6.1800e-02, 6.4536e-17, 1.6305e-13, 0.0000e+00, 3.0702e+18,\n",
            "        1.1657e-08, 0.0000e+00, 0.0000e+00, 2.6216e+06, 7.1036e-18, 1.1206e+11,\n",
            "        3.8818e+03, 1.4574e+09, 8.2010e-16, 1.6305e-13, 6.2779e-07])\n",
            "************** t  14\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.8276e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1802e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0514e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.8717e-04, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.9954e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([6.7104e-04, 2.6983e-07, 5.6377e-12, 0.0000e+00, 2.6983e-07, 1.0691e-13,\n",
            "        1.3935e-26, 2.2239e-05, 1.2336e-39, 1.8229e-36, 6.7104e-04, 1.6729e-07,\n",
            "        2.6983e-07, 2.6575e-06, 3.6856e-12, 2.3585e-09, 1.2445e-38, 1.9248e-35,\n",
            "        1.1130e-14, 2.3542e-38, 7.7047e-27, 7.6707e-08, 2.6983e-07, 7.8965e-37,\n",
            "        1.9308e-13, 4.4215e-11, 1.9397e-05, 2.3585e-09, 2.4711e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.3759e-09, 1.3978e-23, 2.9203e-17, 0.0000e+00, 1.3978e-23, 7.4804e-08,\n",
            "        0.0000e+00, 1.3560e+01, 0.0000e+00, 0.0000e+00, 2.3759e-09, 8.4578e+02,\n",
            "        1.3978e-23, 5.8047e-07, 1.6764e-27, 2.2809e-21, 0.0000e+00, 4.8473e-02,\n",
            "        3.1503e-22, 0.0000e+00, 0.0000e+00, 5.7562e-01, 1.3978e-23, 3.6952e-09,\n",
            "        1.9412e-09, 1.6687e-01, 9.2678e-20, 2.2809e-21, 3.0250e-08])\n",
            "kde_grad  tensor([6.6832e-04, 5.2993e-07, 1.4518e-11, 0.0000e+00, 5.2993e-07, 3.0553e-13,\n",
            "        5.7000e-26, 2.7983e-05, 7.5966e-39, 1.0206e-35, 6.6832e-04, 2.5965e-07,\n",
            "        5.2993e-07, 3.9215e-06, 8.8778e-12, 5.2843e-09, 7.4443e-38, 1.5471e-34,\n",
            "        3.3477e-14, 1.4114e-37, 3.1644e-26, 1.1236e-07, 5.2993e-07, 6.4075e-36,\n",
            "        4.8153e-13, 1.0346e-10, 3.0499e-05, 5.2843e-09, 2.0983e-02])\n",
            "penalty_factor  tensor([3.5550e-06, 2.6376e-17, 2.0115e-06, 0.0000e+00, 2.6376e-17, 2.4483e+05,\n",
            "        0.0000e+00, 4.8457e+05, 0.0000e+00, 0.0000e+00, 3.5550e-06, 3.2573e+09,\n",
            "        2.6376e-17, 1.4802e-01, 1.8883e-16, 4.3164e-13, 0.0000e+00, 4.8473e+18,\n",
            "        9.4103e-09, 0.0000e+00, 0.0000e+00, 5.1229e+06, 2.6376e-17, 3.6952e+11,\n",
            "        4.0313e+03, 1.6129e+09, 3.0387e-15, 4.3164e-13, 1.4416e-06])\n",
            "************** t  15\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.2574e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1892e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7773e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5096e-03, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 4.1619e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([8.4833e-04, 4.1523e-07, 9.3956e-12, 0.0000e+00, 4.1523e-07, 1.8276e-13,\n",
            "        1.3935e-26, 2.4064e-05, 1.2336e-39, 1.8229e-36, 8.4833e-04, 2.8799e-07,\n",
            "        4.1523e-07, 3.3576e-06, 5.5626e-12, 3.4632e-09, 1.2445e-38, 3.9479e-35,\n",
            "        2.2457e-14, 2.3542e-38, 7.7047e-27, 1.0421e-07, 4.1523e-07, 6.2834e-37,\n",
            "        3.3342e-13, 8.2898e-11, 2.5493e-05, 3.4632e-09, 2.6739e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([7.1261e-09, 8.0236e-23, 6.7292e-17, 0.0000e+00, 8.0236e-23, 1.4360e-07,\n",
            "        0.0000e+00, 1.6808e+01, 0.0000e+00, 0.0000e+00, 7.1261e-09, 8.5183e+02,\n",
            "        8.0236e-23, 1.7541e-06, 7.1136e-27, 9.1390e-21, 0.0000e+00, 7.7008e-02,\n",
            "        5.1731e-22, 0.0000e+00, 0.0000e+00, 1.4793e+00, 8.0236e-23, 1.2407e-08,\n",
            "        3.5163e-09, 3.4798e-01, 4.5097e-19, 9.1390e-21, 7.4994e-08])\n",
            "kde_grad  tensor([8.4165e-04, 8.0681e-07, 2.3961e-11, 0.0000e+00, 8.0681e-07, 5.1654e-13,\n",
            "        5.7000e-26, 3.0188e-05, 7.5966e-39, 1.0206e-35, 8.4165e-04, 4.4235e-07,\n",
            "        8.0681e-07, 4.9165e-06, 1.3315e-11, 7.7010e-09, 7.4443e-38, 3.1870e-34,\n",
            "        6.6805e-14, 1.4114e-37, 3.1644e-26, 1.5000e-07, 8.0681e-07, 5.1626e-36,\n",
            "        8.2463e-13, 1.9144e-10, 3.9746e-05, 7.7010e-09, 2.2603e-02])\n",
            "penalty_factor  tensor([8.4668e-06, 9.9449e-17, 2.8084e-06, 0.0000e+00, 9.9449e-17, 2.7800e+05,\n",
            "        0.0000e+00, 5.5677e+05, 0.0000e+00, 0.0000e+00, 8.4668e-06, 1.9257e+09,\n",
            "        9.9449e-17, 3.5677e-01, 5.3427e-16, 1.1867e-12, 0.0000e+00, 7.7008e+18,\n",
            "        7.7436e-09, 0.0000e+00, 0.0000e+00, 9.8615e+06, 9.9449e-17, 1.2407e+12,\n",
            "        4.2641e+03, 1.8176e+09, 1.1346e-14, 1.1867e-12, 3.3179e-06])\n",
            "************** t  16\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.7881e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2003e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.6449e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.9350e-03, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 8.6771e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.0671e-03, 6.3132e-07, 1.5393e-11, 0.0000e+00, 6.3132e-07, 3.0767e-13,\n",
            "        1.3935e-26, 2.5982e-05, 1.2336e-39, 1.8229e-36, 1.0671e-03, 4.8964e-07,\n",
            "        6.3132e-07, 4.2146e-06, 8.3033e-12, 5.0313e-09, 1.2445e-38, 7.8333e-35,\n",
            "        4.4430e-14, 2.3542e-38, 7.7047e-27, 1.3978e-07, 6.3132e-07, 4.5382e-37,\n",
            "        5.6795e-13, 1.5270e-10, 3.3250e-05, 5.0313e-09, 2.8870e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9924e-08, 4.6059e-22, 1.5674e-16, 0.0000e+00, 4.6059e-22, 2.7896e-07,\n",
            "        0.0000e+00, 2.0280e+01, 0.0000e+00, 0.0000e+00, 1.9924e-08, 8.4538e+02,\n",
            "        4.6059e-22, 5.3006e-06, 3.1616e-26, 3.6071e-20, 0.0000e+00, 1.5679e-01,\n",
            "        8.4947e-22, 0.0000e+00, 0.0000e+00, 3.7263e+00, 4.6059e-22, 4.1067e-08,\n",
            "        6.3555e-09, 7.2541e-01, 2.2046e-18, 3.6071e-20, 1.8631e-07])\n",
            "kde_grad  tensor([1.0485e-03, 1.2135e-06, 3.9018e-11, 0.0000e+00, 1.2135e-06, 8.7041e-13,\n",
            "        5.7000e-26, 3.2494e-05, 7.5966e-39, 1.0206e-35, 1.0485e-03, 7.4300e-07,\n",
            "        1.2135e-06, 6.1235e-06, 1.9731e-11, 1.1103e-08, 7.4443e-38, 6.3461e-34,\n",
            "        1.3070e-13, 1.4114e-37, 3.1644e-26, 1.9767e-07, 1.2135e-06, 3.7747e-36,\n",
            "        1.3897e-12, 3.4799e-10, 5.1398e-05, 1.1103e-08, 2.4293e-02])\n",
            "penalty_factor  tensor([1.9002e-05, 3.7956e-16, 4.0172e-06, 0.0000e+00, 3.7956e-16, 3.2049e+05,\n",
            "        0.0000e+00, 6.2413e+05, 0.0000e+00, 0.0000e+00, 1.9002e-05, 1.1378e+09,\n",
            "        3.7956e-16, 8.6562e-01, 1.6023e-15, 3.2488e-12, 0.0000e+00, 1.5679e+19,\n",
            "        6.4992e-09, 0.0000e+00, 0.0000e+00, 1.8851e+07, 3.7956e-16, 4.1067e+12,\n",
            "        4.5734e+03, 2.0846e+09, 4.2893e-14, 3.2488e-12, 7.6690e-06])\n",
            "************** t  17\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 3.4449e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2130e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.0227e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0129e-02, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.8176e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.3327e-03, 9.4834e-07, 2.4927e-11, 0.0000e+00, 9.4834e-07, 5.1093e-13,\n",
            "        1.3935e-26, 2.7974e-05, 1.2336e-39, 1.8229e-36, 1.3327e-03, 8.2139e-07,\n",
            "        9.4834e-07, 5.2559e-06, 1.2246e-11, 7.2278e-09, 1.2445e-38, 1.0968e-34,\n",
            "        8.6193e-14, 2.3542e-38, 7.7047e-27, 1.8505e-07, 9.4834e-07, 2.9821e-37,\n",
            "        9.5269e-13, 2.7637e-10, 4.3013e-05, 7.2278e-09, 3.1101e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([5.7026e-08, 2.7031e-21, 3.6255e-16, 0.0000e+00, 2.7031e-21, 5.2643e-07,\n",
            "        0.0000e+00, 2.4626e+01, 0.0000e+00, 0.0000e+00, 5.7026e-08, 8.8683e+02,\n",
            "        2.7031e-21, 1.6178e-05, 1.2815e-25, 1.4425e-19, 0.0000e+00, 1.3535e-01,\n",
            "        1.3949e-21, 0.0000e+00, 0.0000e+00, 9.3261e+00, 2.7031e-21, 1.4985e-07,\n",
            "        1.1878e-08, 1.5049e+00, 1.0778e-17, 1.4425e-19, 4.7795e-07])\n",
            "kde_grad  tensor([1.2992e-03, 1.8029e-06, 6.2567e-11, 0.0000e+00, 1.8029e-06, 1.4448e-12,\n",
            "        5.7000e-26, 3.4932e-05, 7.5966e-39, 1.0206e-35, 1.2992e-03, 1.2291e-06,\n",
            "        1.8029e-06, 7.5767e-06, 2.8898e-11, 1.5841e-08, 7.4443e-38, 8.9565e-34,\n",
            "        2.5071e-13, 1.4114e-37, 3.1644e-26, 2.5737e-07, 1.8029e-06, 2.5106e-36,\n",
            "        2.3059e-12, 6.2135e-10, 6.6001e-05, 1.5841e-08, 2.6052e-02])\n",
            "penalty_factor  tensor([4.3894e-05, 1.4993e-15, 5.7946e-06, 0.0000e+00, 1.4993e-15, 3.6436e+05,\n",
            "        0.0000e+00, 7.0496e+05, 0.0000e+00, 0.0000e+00, 4.3894e-05, 7.2153e+08,\n",
            "        1.4993e-15, 2.1353e+00, 4.4346e-15, 9.1061e-12, 0.0000e+00, 1.3535e+19,\n",
            "        5.5639e-09, 0.0000e+00, 0.0000e+00, 3.6236e+07, 1.4993e-15, 1.4985e+13,\n",
            "        5.1513e+03, 2.4220e+09, 1.6329e-13, 9.1061e-12, 1.8346e-05])\n",
            "************** t  18\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 4.2234e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2254e+01,\n",
            "        -0.0000e+00, 1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.0401e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.5038e-02, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 3.7916e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.6551e-03, 1.4020e-06, 3.9793e-11, 0.0000e+00, 1.4020e-06, 8.3654e-13,\n",
            "        1.3935e-26, 3.0069e-05, 1.2336e-39, 1.8229e-36, 1.6551e-03, 1.3592e-06,\n",
            "        1.4020e-06, 6.5118e-06, 1.7852e-11, 1.0245e-08, 1.2445e-38, 1.8071e-34,\n",
            "        1.6396e-13, 2.3542e-38, 7.7047e-27, 2.4194e-07, 1.4020e-06, 1.7506e-37,\n",
            "        1.5737e-12, 4.9143e-10, 5.5250e-05, 1.0245e-08, 3.3430e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.6369e-07, 1.6110e-20, 8.3779e-16, 0.0000e+00, 1.6110e-20, 9.9394e-07,\n",
            "        0.0000e+00, 2.9451e+01, 0.0000e+00, 0.0000e+00, 1.6369e-07, 8.9632e+02,\n",
            "        1.6110e-20, 1.2621e-04, 5.6956e-25, 5.8129e-19, 0.0000e+00, 1.9682e-01,\n",
            "        2.2835e-21, 0.0000e+00, 0.0000e+00, 2.2981e+01, 1.6110e-20, 2.5517e-07,\n",
            "        2.2375e-08, 3.1271e+00, 5.2655e-17, 5.8129e-19, 1.1918e-06])\n",
            "kde_grad  tensor([1.5945e-03, 2.6548e-06, 9.8888e-11, 0.0000e+00, 2.6548e-06, 2.3704e-12,\n",
            "        5.7000e-26, 3.7433e-05, 7.5966e-39, 1.0206e-35, 1.5945e-03, 2.0004e-06,\n",
            "        2.6548e-06, 9.3130e-06, 4.1823e-11, 2.2396e-08, 7.4443e-38, 1.4826e-33,\n",
            "        4.7147e-13, 1.4114e-37, 3.1644e-26, 3.3085e-07, 2.6548e-06, 1.4960e-36,\n",
            "        3.7673e-12, 1.0899e-09, 8.4043e-05, 2.2396e-08, 2.7875e-02])\n",
            "penalty_factor  tensor([1.0266e-04, 6.0683e-15, 8.4720e-06, 0.0000e+00, 6.0683e-15, 4.1932e+05,\n",
            "        0.0000e+00, 7.8676e+05, 0.0000e+00, 0.0000e+00, 1.0266e-04, 4.4806e+08,\n",
            "        6.0683e-15, 1.3552e+01, 1.3618e-14, 2.5955e-11, 0.0000e+00, 1.9682e+19,\n",
            "        4.8433e-09, 0.0000e+00, 0.0000e+00, 6.9460e+07, 6.0683e-15, 2.5517e+13,\n",
            "        5.9393e+03, 2.8693e+09, 6.2653e-13, 2.5955e-11, 4.2754e-05])\n",
            "************** t  19\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 5.1572e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2373e+01,\n",
            "        -0.0000e+00, 2.3842e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.8707e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.2633e-02, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 7.9675e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.0418e-03, 2.0556e-06, 6.2619e-11, 0.0000e+00, 2.0556e-06, 1.3801e-12,\n",
            "        1.3935e-26, 3.2249e-05, 1.2336e-39, 1.8229e-36, 2.0418e-03, 2.2124e-06,\n",
            "        2.0556e-06, 8.0152e-06, 2.5754e-11, 1.4414e-08, 1.2445e-38, 3.0400e-34,\n",
            "        3.0160e-13, 2.3542e-38, 7.7047e-27, 3.1242e-07, 2.0556e-06, 9.7096e-38,\n",
            "        2.5204e-12, 8.5854e-10, 7.0427e-05, 1.4414e-08, 3.5854e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([4.7506e-07, 9.4556e-20, 1.9363e-15, 0.0000e+00, 9.4556e-20, 2.1254e-06,\n",
            "        0.0000e+00, 3.4943e+01, 0.0000e+00, 0.0000e+00, 4.7506e-07, 8.6847e+02,\n",
            "        9.4556e-20, 3.0404e-04, 2.7990e-24, 2.3341e-18, 0.0000e+00, 2.9214e-01,\n",
            "        3.9691e-21, 0.0000e+00, 0.0000e+00, 5.5863e+01, 9.4556e-20, 7.8681e-07,\n",
            "        3.3459e-08, 6.0946e+00, 2.5726e-16, 2.3341e-18, 2.9911e-06])\n",
            "kde_grad  tensor([1.9436e-03, 3.8494e-06, 1.5405e-10, 0.0000e+00, 3.8494e-06, 4.0496e-12,\n",
            "        5.7000e-26, 4.0024e-05, 7.5966e-39, 1.0206e-35, 1.9436e-03, 3.2125e-06,\n",
            "        3.8494e-06, 1.1372e-05, 6.0063e-11, 3.1263e-08, 7.4443e-38, 2.5034e-33,\n",
            "        8.7169e-13, 1.4114e-37, 3.1644e-26, 4.1993e-07, 3.8494e-06, 8.3951e-37,\n",
            "        6.1020e-12, 1.8778e-09, 1.0619e-04, 3.1263e-08, 2.9758e-02])\n",
            "penalty_factor  tensor([2.4442e-04, 2.4563e-14, 1.2569e-05, 0.0000e+00, 2.4563e-14, 5.2485e+05,\n",
            "        0.0000e+00, 8.7305e+05, 0.0000e+00, 0.0000e+00, 2.4442e-04, 2.7034e+08,\n",
            "        2.4563e-14, 2.6736e+01, 4.6600e-14, 7.4661e-11, 0.0000e+00, 2.9214e+19,\n",
            "        4.5534e-09, 0.0000e+00, 0.0000e+00, 1.3303e+08, 2.4563e-14, 7.8681e+13,\n",
            "        5.4833e+03, 3.2457e+09, 2.4225e-12, 7.4661e-11, 1.0051e-04])\n",
            "************** t  20\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.2250e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2471e+01,\n",
            "        -0.0000e+00, 7.1526e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1544e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4812e-01, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.5113e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.5022e-03, 2.9778e-06, 9.7133e-11, 0.0000e+00, 2.9778e-06, 2.3126e-12,\n",
            "        1.3935e-26, 3.4492e-05, 1.2336e-39, 1.8229e-36, 2.5022e-03, 3.5531e-06,\n",
            "        2.9778e-06, 9.7958e-06, 3.6799e-11, 2.0064e-08, 1.2445e-38, 5.0474e-34,\n",
            "        5.5159e-13, 2.3542e-38, 7.7047e-27, 3.9713e-07, 2.9778e-06, 5.7717e-38,\n",
            "        4.0283e-12, 1.4616e-09, 8.9088e-05, 2.0064e-08, 3.8368e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.3797e-06, 5.5499e-19, 4.4404e-15, 0.0000e+00, 5.5499e-19, 4.2915e-06,\n",
            "        0.0000e+00, 3.8607e+01, 0.0000e+00, 0.0000e+00, 1.3797e-06, 8.4467e+02,\n",
            "        5.5499e-19, 9.2140e-04, 1.3689e-23, 9.3725e-18, 0.0000e+00, 5.5687e-01,\n",
            "        6.7238e-21, 0.0000e+00, 0.0000e+00, 1.1442e+02, 5.5499e-19, 2.4000e-06,\n",
            "        6.2570e-08, 1.2828e+01, 1.2569e-15, 9.3725e-18, 8.0025e-06])\n",
            "kde_grad  tensor([2.3531e-03, 5.5139e-06, 2.3654e-10, 0.0000e+00, 5.5139e-06, 6.9474e-12,\n",
            "        5.7000e-26, 4.2740e-05, 7.5966e-39, 1.0206e-35, 2.3531e-03, 5.0669e-06,\n",
            "        5.5139e-06, 1.3805e-05, 8.5422e-11, 4.3171e-08, 7.4443e-38, 4.1739e-33,\n",
            "        1.5758e-12, 1.4114e-37, 3.1644e-26, 5.2910e-07, 5.5139e-06, 5.0391e-37,\n",
            "        9.6769e-12, 3.1934e-09, 1.3315e-04, 4.3171e-08, 3.1698e-02])\n",
            "penalty_factor  tensor([5.8633e-04, 1.0065e-13, 1.8772e-05, 0.0000e+00, 1.0065e-13, 6.1771e+05,\n",
            "        0.0000e+00, 9.0329e+05, 0.0000e+00, 0.0000e+00, 5.8633e-04, 1.6670e+08,\n",
            "        1.0065e-13, 6.6742e+01, 1.6025e-13, 2.1710e-10, 0.0000e+00, 5.5687e+19,\n",
            "        4.2669e-09, 0.0000e+00, 0.0000e+00, 2.1626e+08, 1.0065e-13, 2.4000e+14,\n",
            "        6.4659e+03, 4.0170e+09, 9.4397e-12, 2.1710e-10, 2.5246e-04])\n",
            "************** t  21\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 7.4942e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2569e+01,\n",
            "        -0.0000e+00, 2.3842e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.3085e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.2735e-01, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 3.0934e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.0461e-03, 4.2617e-06, 1.4844e-10, 0.0000e+00, 4.2617e-06, 3.9019e-12,\n",
            "        1.3935e-26, 3.6767e-05, 1.2336e-39, 1.8229e-36, 3.0461e-03, 5.6199e-06,\n",
            "        4.2617e-06, 1.1900e-05, 5.2042e-11, 2.7630e-08, 1.2445e-38, 6.8836e-34,\n",
            "        9.8855e-13, 2.3542e-38, 7.7047e-27, 5.0002e-07, 4.2617e-06, 3.1516e-38,\n",
            "        6.3542e-12, 2.4634e-09, 1.1183e-04, 2.7630e-08, 4.0968e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([3.9859e-06, 3.2635e-18, 1.0252e-14, 0.0000e+00, 3.2635e-18, 9.1512e-06,\n",
            "        0.0000e+00, 4.6466e+01, 0.0000e+00, 0.0000e+00, 3.9859e-06, 8.4467e+02,\n",
            "        3.2635e-18, 2.9924e-03, 5.4114e-23, 3.7568e-17, 0.0000e+00, 2.4101e-01,\n",
            "        1.1393e-20, 0.0000e+00, 0.0000e+00, 2.3711e+02, 3.2635e-18, 6.9404e-06,\n",
            "        1.1563e-07, 2.2832e+01, 6.1421e-15, 3.7568e-17, 2.1127e-05])\n",
            "kde_grad  tensor([2.8295e-03, 7.8020e-06, 3.5808e-10, 0.0000e+00, 7.8020e-06, 1.1887e-11,\n",
            "        5.7000e-26, 4.5631e-05, 7.5966e-39, 1.0206e-35, 2.8295e-03, 7.8661e-06,\n",
            "        7.8020e-06, 1.6636e-05, 1.2032e-10, 5.8974e-08, 7.4443e-38, 5.7966e-33,\n",
            "        2.7931e-12, 1.4114e-37, 3.1644e-26, 6.5546e-07, 7.8020e-06, 2.7778e-37,\n",
            "        1.5096e-11, 5.3117e-09, 1.6565e-04, 5.8974e-08, 3.3689e-02])\n",
            "penalty_factor  tensor([1.4087e-03, 4.1829e-13, 2.8630e-05, 0.0000e+00, 4.1829e-13, 7.6985e+05,\n",
            "        0.0000e+00, 1.0183e+06, 0.0000e+00, 0.0000e+00, 1.4087e-03, 1.0738e+08,\n",
            "        4.1829e-13, 1.7988e+02, 4.4974e-13, 6.3702e-10, 0.0000e+00, 2.4101e+19,\n",
            "        4.0789e-09, 0.0000e+00, 0.0000e+00, 3.6175e+08, 4.1829e-13, 6.9404e+14,\n",
            "        7.6601e+03, 4.2985e+09, 3.7078e-11, 6.3702e-10, 6.2713e-04])\n",
            "************** t  22\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 9.1079e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2667e+01,\n",
            "        -0.0000e+00, 7.2717e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0371e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.7009e-01, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0830e-02, -0.0000e+00, -0.0000e+00, 1.1921e-07],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.6835e-03, 6.0225e-06, 2.2374e-10, 0.0000e+00, 6.0225e-06, 6.5839e-12,\n",
            "        1.3935e-26, 3.9171e-05, 1.2336e-39, 1.8229e-36, 3.6835e-03, 8.7530e-06,\n",
            "        6.0225e-06, 1.4363e-05, 7.1434e-11, 3.7643e-08, 1.2445e-38, 8.4426e-34,\n",
            "        1.7381e-12, 2.3542e-38, 7.7047e-27, 6.2218e-07, 6.0225e-06, 1.6268e-38,\n",
            "        9.8699e-12, 4.0724e-09, 1.3931e-04, 3.7643e-08, 4.3647e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.1515e-05, 1.9682e-17, 2.3669e-14, 0.0000e+00, 1.9682e-17, 1.8054e-05,\n",
            "        0.0000e+00, 5.6946e+01, 0.0000e+00, 0.0000e+00, 1.1515e-05, 8.4467e+02,\n",
            "        1.9682e-17, 9.2084e-03, 1.7500e-22, 1.5072e-16, 0.0000e+00, 6.1792e-01,\n",
            "        1.9295e-20, 0.0000e+00, 0.0000e+00, 4.1425e+02, 1.9682e-17, 7.8554e-06,\n",
            "        2.1346e-07, 4.6475e+01, 2.9789e-14, 1.5072e-16, 1.4015e-04])\n",
            "kde_grad  tensor([3.3792e-03, 1.0911e-05, 5.3414e-10, 0.0000e+00, 1.0911e-05, 2.0178e-11,\n",
            "        5.7000e-26, 4.8464e-05, 7.5966e-39, 1.0206e-35, 3.3792e-03, 1.2019e-05,\n",
            "        1.0911e-05, 1.9914e-05, 1.6939e-10, 7.9691e-08, 7.4443e-38, 7.1492e-33,\n",
            "        4.8528e-12, 1.4114e-37, 3.1644e-26, 8.0107e-07, 1.0911e-05, 1.4471e-37,\n",
            "        2.3186e-11, 8.6876e-09, 2.0450e-04, 7.9691e-08, 3.5724e-02])\n",
            "penalty_factor  tensor([3.4076e-03, 1.8040e-12, 4.4312e-05, 0.0000e+00, 1.8040e-12, 8.9475e+05,\n",
            "        0.0000e+00, 1.1750e+06, 0.0000e+00, 0.0000e+00, 3.4076e-03, 7.0277e+07,\n",
            "        1.8040e-12, 4.6241e+02, 1.0331e-12, 1.8912e-09, 0.0000e+00, 6.1792e+19,\n",
            "        3.9761e-09, 0.0000e+00, 0.0000e+00, 5.1712e+08, 1.8040e-12, 7.8554e+14,\n",
            "        9.2066e+03, 5.3496e+09, 1.4567e-10, 1.8912e-09, 3.9232e-03])\n",
            "************** t  23\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 1.1084e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2755e+01,\n",
            "        -0.0000e+00, 2.2292e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2432e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2218e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.1798e-01, -0.0000e+00, -0.0000e+00, 2.3842e-07],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.4246e-03, 8.3941e-06, 3.3241e-10, 0.0000e+00, 8.3941e-06, 1.0930e-11,\n",
            "        1.3935e-26, 4.1640e-05, 1.2336e-39, 1.8229e-36, 4.4246e-03, 1.3424e-05,\n",
            "        8.3941e-06, 1.7221e-05, 9.9090e-11, 5.0736e-08, 1.2445e-38, 8.6693e-34,\n",
            "        2.9962e-12, 2.3542e-38, 7.7047e-27, 7.6513e-07, 8.3941e-06, 1.2415e-38,\n",
            "        1.5096e-11, 6.6253e-09, 1.7184e-04, 5.0736e-08, 4.6397e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([3.3129e-05, 1.1721e-16, 5.4644e-14, 0.0000e+00, 1.1721e-16, 9.0756e-05,\n",
            "        0.0000e+00, 7.3485e+01, 0.0000e+00, 0.0000e+00, 3.3129e-05, 8.6119e+02,\n",
            "        1.1721e-16, 2.8367e-02, 8.3935e-22, 6.0465e-16, 0.0000e+00, 5.4442e-01,\n",
            "        3.4457e-20, 0.0000e+00, 0.0000e+00, 6.2200e+02, 1.1721e-16, 1.4841e-05,\n",
            "        3.9501e-07, 7.8930e+01, 1.4303e-13, 6.0465e-16, 3.0610e-04])\n",
            "kde_grad  tensor([4.0083e-03, 1.5096e-05, 7.8531e-10, 0.0000e+00, 1.5096e-05, 3.3896e-11,\n",
            "        5.7000e-26, 5.1359e-05, 7.5966e-39, 1.0206e-35, 4.0083e-03, 1.8075e-05,\n",
            "        1.5096e-05, 2.3681e-05, 2.3377e-10, 1.0652e-07, 7.4443e-38, 7.3896e-33,\n",
            "        8.2653e-12, 1.4114e-37, 3.1644e-26, 9.6711e-07, 1.5096e-05, 1.1133e-37,\n",
            "        3.5063e-11, 1.3944e-08, 2.5127e-04, 1.0652e-07, 3.7798e-02])\n",
            "penalty_factor  tensor([8.2650e-03, 7.7645e-12, 6.9583e-05, 0.0000e+00, 7.7645e-12, 2.6775e+06,\n",
            "        0.0000e+00, 1.4308e+06, 0.0000e+00, 0.0000e+00, 8.2650e-03, 4.7645e+07,\n",
            "        7.7645e-12, 1.1979e+03, 3.5905e-12, 5.6762e-09, 0.0000e+00, 5.4442e+19,\n",
            "        4.1689e-09, 0.0000e+00, 0.0000e+00, 6.4315e+08, 7.7645e-12, 1.4841e+15,\n",
            "        1.1266e+04, 5.6606e+09, 5.6922e-10, 5.6762e-09, 8.0984e-03])\n",
            "************** t  24\n",
            "ce_loss:  tensor([1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 1.3565e-01, -0.0000e+00, -0.0000e+00, 1.1921e-07, 1.2850e+01,\n",
            "        -0.0000e+00, 6.9735e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.0638e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.0220e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 2.1319e-01, -0.0000e+00, -0.0000e+00, 5.9605e-07],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.2796e-03, 1.1584e-05, 4.8683e-10, 0.0000e+00, 1.1584e-05, 1.8138e-11,\n",
            "        1.3935e-26, 4.4165e-05, 1.2336e-39, 1.8229e-36, 5.2796e-03, 2.0259e-05,\n",
            "        1.1584e-05, 2.0502e-05, 1.3606e-10, 6.7649e-08, 1.2445e-38, 1.0510e-33,\n",
            "        4.9430e-12, 2.3542e-38, 7.7047e-27, 9.3154e-07, 1.1584e-05, 8.7284e-39,\n",
            "        2.2725e-11, 1.0584e-08, 2.1080e-04, 6.7649e-08, 4.9129e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.7267e-04, 6.9966e-16, 1.2616e-13, 0.0000e+00, 6.9966e-16, 1.3321e-04,\n",
            "        0.0000e+00, 8.6611e+01, 0.0000e+00, 0.0000e+00, 1.7267e-04, 9.1898e+02,\n",
            "        6.9966e-16, 9.0538e-02, 4.0176e-21, 2.4257e-15, 0.0000e+00, 8.6326e-01,\n",
            "        3.5221e-20, 0.0000e+00, 0.0000e+00, 7.7761e+02, 6.9966e-16, 7.2180e-05,\n",
            "        7.3705e-07, 1.4121e+02, 6.8612e-13, 2.4257e-15, 7.8157e-04])\n",
            "kde_grad  tensor([4.7222e-03, 2.0590e-05, 1.1380e-09, 0.0000e+00, 2.0590e-05, 5.6073e-11,\n",
            "        5.7000e-26, 5.4305e-05, 7.5966e-39, 1.0206e-35, 4.7222e-03, 2.6776e-05,\n",
            "        2.0590e-05, 2.7997e-05, 3.1929e-10, 1.4085e-07, 7.4443e-38, 8.9934e-33,\n",
            "        1.3883e-11, 1.4114e-37, 3.1644e-26, 1.1849e-06, 2.0590e-05, 7.8908e-38,\n",
            "        5.2221e-11, 2.1991e-08, 3.0543e-04, 1.4085e-07, 4.0118e-02])\n",
            "penalty_factor  tensor([3.6567e-02, 3.3981e-11, 1.1086e-04, 0.0000e+00, 3.3981e-11, 2.3756e+06,\n",
            "        0.0000e+00, 1.5949e+06, 0.0000e+00, 0.0000e+00, 3.6567e-02, 3.4321e+07,\n",
            "        3.3981e-11, 3.2339e+03, 1.2583e-11, 1.7222e-08, 0.0000e+00, 8.6326e+19,\n",
            "        2.5370e-09, 0.0000e+00, 0.0000e+00, 6.5624e+08, 3.3981e-11, 7.2180e+15,\n",
            "        1.4114e+04, 6.4213e+09, 2.2464e-09, 1.7222e-08, 1.9482e-02])\n",
            "************** t  25\n",
            "ce_loss:  tensor([4.7684e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-07,\n",
            "        -0.0000e+00, 1.6666e-01, -0.0000e+00, -0.0000e+00, 4.7684e-07, 1.2994e+01,\n",
            "        -0.0000e+00, 2.2337e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.3695e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.8549e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 3.7503e-01, -0.0000e+00, -0.0000e+00, 1.5497e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([6.2579e-03, 1.5795e-05, 7.0280e-10, 0.0000e+00, 1.5795e-05, 2.9581e-11,\n",
            "        1.3935e-26, 4.6740e-05, 1.2336e-39, 1.8229e-36, 6.2579e-03, 3.0088e-05,\n",
            "        1.5795e-05, 2.4260e-05, 1.8490e-10, 8.9233e-08, 1.2445e-38, 1.0822e-33,\n",
            "        8.1912e-12, 2.3542e-38, 7.7047e-27, 1.1411e-06, 1.5795e-05, 5.8047e-39,\n",
            "        3.3703e-11, 1.6629e-08, 2.5661e-04, 8.9233e-08, 5.1994e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([5.9639e-04, 4.0390e-15, 2.9606e-13, 0.0000e+00, 4.0390e-15, 3.0305e-04,\n",
            "        0.0000e+00, 1.0732e+02, 0.0000e+00, 0.0000e+00, 5.9639e-04, 9.0729e+02,\n",
            "        4.0390e-15, 2.9412e-01, 1.9236e-20, 9.7317e-15, 0.0000e+00, 1.3613e+00,\n",
            "        5.9651e-20, 0.0000e+00, 0.0000e+00, 8.0062e+02, 4.0390e-15, 1.0615e-04,\n",
            "        1.3940e-06, 2.2193e+02, 3.2639e-12, 9.7317e-15, 2.0345e-03])\n",
            "kde_grad  tensor([5.5253e-03, 2.7741e-05, 1.6253e-09, 0.0000e+00, 2.7741e-05, 9.1334e-11,\n",
            "        5.7000e-26, 5.7292e-05, 7.5966e-39, 1.0206e-35, 5.5253e-03, 3.9077e-05,\n",
            "        2.7741e-05, 3.2853e-05, 4.3150e-10, 1.8422e-07, 7.4443e-38, 9.3034e-33,\n",
            "        2.2731e-11, 1.4114e-37, 3.1644e-26, 1.4739e-06, 2.7741e-05, 5.2897e-38,\n",
            "        7.6554e-11, 3.4043e-08, 3.6840e-04, 1.8422e-07, 4.2258e-02])\n",
            "penalty_factor  tensor([1.0794e-01, 1.4560e-10, 1.8216e-04, 0.0000e+00, 1.4560e-10, 3.3181e+06,\n",
            "        0.0000e+00, 1.8733e+06, 0.0000e+00, 0.0000e+00, 1.0794e-01, 2.3218e+07,\n",
            "        1.4560e-10, 8.9524e+03, 4.4580e-11, 5.2827e-08, 0.0000e+00, 1.3613e+20,\n",
            "        2.6242e-09, 0.0000e+00, 0.0000e+00, 5.4320e+08, 1.4560e-10, 1.0615e+16,\n",
            "        1.8210e+04, 6.5190e+09, 8.8598e-09, 5.2827e-08, 4.8144e-02])\n",
            "************** t  26\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.4692e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.1914e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1644e-07, 1.2445e-38, 1.1125e-33,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.9838e-39,\n",
            "        4.9220e-11, 2.5654e-08, 3.0965e-04, 1.1644e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9864e-14, 0.0000e+00, 2.5096e+00,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 8.7417e-05,\n",
            "        2.5566e-06, 3.3728e+02, 1.5691e-11, 3.9864e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3833e-07, 7.4443e-38, 9.6021e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 4.5553e-38,\n",
            "        1.1049e-10, 5.1785e-08, 4.4160e-04, 2.3833e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6726e-07, 0.0000e+00, 2.5096e+20,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 8.7417e+15,\n",
            "        2.3139e+04, 6.5131e+09, 3.5533e-08, 1.6726e-07, 1.1778e-01])\n",
            "************** t  27\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.9865e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 2.3842e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 7.1086e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.5859e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.5815e+00,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.2689e-04,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 6.1962e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 4.2131e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.5815e+20,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.2689e+16,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  28\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.6794e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 5.6759e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.5072e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.0004e+00,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.2443e-04,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 5.0309e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 4.1589e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.0004e+20,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.2443e+16,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  29\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.1640e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.4072e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.6789e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 9.7035e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 7.3542e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.0465e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.4936e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 9.7035e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 7.3542e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  30\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7962e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.5663e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.6105e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.0698e+00,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 9.0641e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.3085e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.4403e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.0698e+20,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 9.0641e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  31\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.0034e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.5639e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.9591e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.0163e+00,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 7.2719e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.4180e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.8396e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.0163e+20,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 7.2719e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  32\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.3307e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.1227e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.9938e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 7.9304e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.0902e-04,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.0265e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.8822e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 7.9304e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.0902e+16,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  33\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5438e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 6.8834e-35,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.2836e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 8.1842e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.5713e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 6.3396e-34,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.2234e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 8.1842e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.5713e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  34\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.1881e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 4.8070e-35,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.2986e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.8436e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.7493e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 4.4980e-34,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.2417e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.8436e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.7493e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  35\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.4785e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.5604e-35,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 9.4980e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 3.3244e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.5860e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.4177e-34,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 9.1228e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 3.3244e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.5860e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  36\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.1187e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.7664e-35,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 7.9826e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.5997e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.6605e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.6765e-34,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 7.7197e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.5997e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.6605e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  37\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.1452e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.4933e-35,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.9125e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.5155e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.0303e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.4249e-34,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 4.7795e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.5155e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.0303e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  38\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.2954e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 9.8673e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.1829e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.1815e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 9.0513e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 9.4625e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 4.0971e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.1815e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 9.0513e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  39\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.1633e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 8.5839e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 3.9184e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.8860e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 8.1751e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 8.2704e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 3.8542e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.8860e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 8.1751e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  40\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.2157e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 5.0023e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.8384e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.9298e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 7.6821e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 4.8489e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.7943e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.9298e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 7.6821e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  41\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.9165e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.4534e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.0632e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 5.6079e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.4378e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.3973e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.0435e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 5.6079e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.4378e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  42\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2135e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.1779e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.9885e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 7.3854e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.4543e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.1530e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.9907e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 7.3854e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.4543e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  43\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2242e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.4900e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.6184e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 6.2385e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 5.5908e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.4787e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.6278e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 6.2385e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 5.5908e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  44\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3920e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.1646e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.1288e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 5.2783e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.2583e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.1611e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.1295e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 5.2783e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.2583e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  45\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.9416e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 7.3051e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.1310e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 5.0666e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.6702e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 7.3158e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.1369e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 5.0666e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.6702e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  46\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.2225e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 5.9391e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.0618e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 4.4532e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.1188e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 5.9721e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.0567e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 4.4532e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.1188e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  47\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.5299e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.8936e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 8.0262e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.1090e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.9738e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.9535e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 8.0093e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.1090e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.9738e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  48\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.2424e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.1564e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 5.7592e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.0390e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.1837e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.2160e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 5.7781e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.0390e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.1837e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  49\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.8623e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.9490e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 5.0612e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.0337e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.6624e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.9939e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 5.1532e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.0337e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.6624e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  50\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.0265e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.3856e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 3.5115e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.2733e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.1835e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.4243e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 3.6834e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.2733e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.1835e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  51\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.5391e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 8.3671e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.3197e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.8458e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.5875e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 8.6358e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.4167e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.8458e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.5875e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  52\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7762e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 6.3329e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.3333e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 9.3821e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.7196e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 6.5826e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.3294e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 9.3821e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.7196e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  53\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.2983e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 4.3516e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.2591e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 6.2664e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.2667e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 4.5421e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.3142e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 6.2664e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.2667e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  54\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1206e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.4183e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 9.4209e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 5.7960e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.2587e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.5391e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.0819e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 5.7960e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.2587e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  55\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.9141e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.1644e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 6.4880e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 4.6549e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.8145e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.2787e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 6.7442e-41,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 4.6549e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.8145e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  56\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.6757e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.1720e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 5.0166e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 4.2098e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.1241e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.2411e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 5.3978e-41,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 4.2098e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.1241e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  57\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4305e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 8.4371e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.7690e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 9.9183e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.0377e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 9.0401e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.2414e-41,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 9.9183e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.0377e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  58\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.1458e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 4.3123e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.8147e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.3847e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 8.4882e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 4.6509e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.1279e-41,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.3847e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 8.4882e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  59\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7881e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.5643e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.9072e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 7.6007e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 5.5465e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.8596e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.2871e-41,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 7.6007e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 5.5465e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  60\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3446e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0729e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7349e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.3532e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.1827e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 7.4752e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 6.5045e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.4465e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4593e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.6427e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.6485e-42,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.1226e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 6.5045e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.4465e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  61\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3446e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.3113e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7349e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.2390e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 9.1365e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 7.4752e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 6.9707e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.7952e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4593e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.4436e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.7404e-42,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.1226e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 6.9707e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.7952e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  62\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3446e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.7684e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7349e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.4871e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 8.0014e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 7.4752e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 3.2369e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.2270e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4593e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.6332e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.7488e-42,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.1226e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 3.2369e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.2270e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  63\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3446e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.9605e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7349e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 8.3216e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.7224e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 7.4752e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 3.0920e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.7796e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4593e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 9.2109e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 8.7021e-43,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.1226e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 3.0920e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.7796e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  64\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3446e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7349e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 6.9701e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.7644e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 7.4752e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.4260e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.5582e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4593e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 7.7439e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 8.7301e-43,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.1226e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.4260e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.5582e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  65\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 4.7736e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 3.0408e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.6126e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.6042e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 5.3343e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.6126e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.6042e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  66\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.5788e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.9198e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 7.6144e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.9122e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.9398e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 7.6144e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.9122e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  67\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.5860e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.4854e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.3604e-05,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.7867e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.8137e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.3604e+15,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.7867e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  68\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.1483e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.3593e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.5624e-05,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.9895e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.3282e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.5624e+15,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.9895e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  69\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 7.4636e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 9.6690e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 5.8181e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 2.4066e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 8.6682e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 5.8181e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 2.4066e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  70\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 7.0352e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 7.2868e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 4.9077e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 1.9063e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 8.2299e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 4.9077e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 1.9063e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  71\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 4.4854e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 5.3249e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 4.4979e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 8.8757e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 5.1948e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 4.4979e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 8.8757e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  72\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 4.3366e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 2.5223e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 3.4402e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 4.9902e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 5.0223e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 3.4402e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 4.9902e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  73\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 2.9310e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 2.1019e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 3.6347e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 2.0767e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 3.4628e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 3.6347e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 2.0767e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  74\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 1.7991e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 1.4013e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 1.2155e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 3.4184e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 2.1240e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 1.2155e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 3.4184e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  75\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 1.1883e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 9.8091e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 1.2257e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 1.8058e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 1.3389e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 1.2257e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 1.8058e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  76\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1592e-07, 1.2445e-38, 1.0248e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 9.8091e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 5.8213e-14, 0.0000e+00, 6.9531e-07,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 1.3291e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3816e-07, 7.4443e-38, 1.1524e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.4443e-07, 0.0000e+00, 6.9531e+13,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 1.3291e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  77\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4854e-10, 1.1592e-07, 1.2445e-38, 6.0788e-42,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 4.2039e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0811e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 8.9618e-20, 5.8213e-14, 0.0000e+00, 1.3419e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 1.2939e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.9245e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7699e-10, 2.3816e-07, 7.4443e-38, 7.0275e-41,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4352e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 1.5532e-10, 2.4443e-07, 0.0000e+00, 1.3419e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 1.2939e+10,\n",
            "        2.3116e+04, 6.4160e+09, 4.3391e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  78\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4812e-10, 1.1592e-07, 1.2445e-38, 3.5341e-42,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 2.8026e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0811e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.0539e-19, 5.8213e-14, 0.0000e+00, 5.1062e-07,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 9.9462e-11,\n",
            "        2.5549e-06, 3.3248e+02, 1.9245e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7651e-10, 2.3816e-07, 7.4443e-38, 4.0412e-41,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4352e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 1.8281e-10, 2.4443e-07, 0.0000e+00, 5.1062e+13,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 9.9462e+09,\n",
            "        2.3116e+04, 6.4160e+09, 4.3391e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  79\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4812e-10, 1.1592e-07, 1.2445e-38, 2.9974e-42,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 2.8026e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0811e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.0539e-19, 5.8213e-14, 0.0000e+00, 3.9513e-07,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 3.5979e-11,\n",
            "        2.5549e-06, 3.3248e+02, 1.9245e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7651e-10, 2.3816e-07, 7.4443e-38, 3.4629e-41,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4352e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 1.8281e-10, 2.4443e-07, 0.0000e+00, 3.9513e+13,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 3.5979e+09,\n",
            "        2.3116e+04, 6.4160e+09, 4.3391e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  80\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4771e-10, 1.1592e-07, 1.2445e-38, 1.7026e-42,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 1.4013e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0811e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.2169e-19, 5.8213e-14, 0.0000e+00, 3.0875e-07,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 3.9535e-11,\n",
            "        2.5549e-06, 3.3248e+02, 1.9245e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7602e-10, 2.3816e-07, 7.4443e-38, 1.8158e-41,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4352e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 2.1126e-10, 2.4443e-07, 0.0000e+00, 3.0875e+13,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 3.9535e+09,\n",
            "        2.3116e+04, 6.4160e+09, 4.3391e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  81\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.7008e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4771e-10, 1.1592e-07, 1.2445e-38, 1.4181e-42,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 0.0000e+00,\n",
            "        4.9193e-11, 2.5629e-08, 3.0811e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 9.5206e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.2169e-19, 5.8213e-14, 0.0000e+00, 2.6122e-07,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 2.3322e-11,\n",
            "        2.5549e-06, 3.3248e+02, 1.9245e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4515e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7602e-10, 2.3816e-07, 7.4443e-38, 1.5112e-41,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4352e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 6.5593e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 2.1126e-10, 2.4443e-07, 0.0000e+00, 2.6122e+13,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 2.3322e+09,\n",
            "        2.3116e+04, 6.4160e+09, 4.3391e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  82\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.6670e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4771e-10, 1.1592e-07, 1.2445e-38, 8.8422e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 0.0000e+00,\n",
            "        4.9193e-11, 2.5629e-08, 3.0759e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 1.0261e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.2169e-19, 5.8213e-14, 0.0000e+00, 2.1329e-07,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 3.6155e-11,\n",
            "        2.5549e-06, 3.3248e+02, 2.2617e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4526e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7602e-10, 2.3816e-07, 7.4443e-38, 8.1387e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4337e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 7.0637e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 2.1126e-10, 2.4443e-07, 0.0000e+00, 2.1329e+13,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 3.6155e+09,\n",
            "        2.3116e+04, 6.4160e+09, 5.1012e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  83\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.3113e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9788e-10, 0.0000e+00, 2.1157e-05, 4.6540e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4771e-10, 1.1592e-07, 1.2445e-38, 6.1657e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 0.0000e+00,\n",
            "        4.9193e-11, 2.5629e-08, 3.0759e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 7.7334e-13, 0.0000e+00, 3.5665e-14, 1.1348e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.2169e-19, 5.8213e-14, 0.0000e+00, 1.3439e-07,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 1.4856e-11,\n",
            "        2.5549e-06, 3.3248e+02, 2.2617e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2867e-09, 0.0000e+00, 3.6959e-05, 1.4495e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7602e-10, 2.3816e-07, 7.4443e-38, 7.1088e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4337e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.3819e-04, 0.0000e+00, 9.6498e-10, 7.8286e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 2.1126e-10, 2.4443e-07, 0.0000e+00, 1.3439e+13,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 1.4856e+09,\n",
            "        2.3116e+04, 6.4160e+09, 5.1012e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  84\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.3113e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9788e-10, 0.0000e+00, 2.1157e-05, 4.6463e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4771e-10, 1.1592e-07, 1.2445e-38, 4.0357e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 0.0000e+00,\n",
            "        4.9193e-11, 2.5629e-08, 3.0759e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 7.7334e-13, 0.0000e+00, 3.5665e-14, 1.1652e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.2169e-19, 5.8213e-14, 0.0000e+00, 1.5055e-07,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 1.3647e-11,\n",
            "        2.5549e-06, 3.3248e+02, 2.2617e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2867e-09, 0.0000e+00, 3.6959e-05, 1.4480e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7602e-10, 2.3816e-07, 7.4443e-38, 4.0189e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4337e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.3819e-04, 0.0000e+00, 9.6498e-10, 8.0473e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 2.1126e-10, 2.4443e-07, 0.0000e+00, 1.5055e+13,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 1.3647e+09,\n",
            "        2.3116e+04, 6.4160e+09, 5.1012e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  85\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4305e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1099e-05, 9.9788e-10, 0.0000e+00, 2.1099e-05, 4.6385e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1099e-05, 2.8520e-05, 2.4771e-10, 1.1560e-07, 1.2445e-38, 3.2790e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1099e-05, 0.0000e+00,\n",
            "        4.9193e-11, 2.5629e-08, 3.0759e-04, 1.1560e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 4.5095e-14, 7.7334e-13, 0.0000e+00, 4.5095e-14, 1.2494e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        4.5095e-14, 9.1241e-01, 1.2169e-19, 7.1940e-14, 0.0000e+00, 9.4592e-08,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 4.5095e-14, 1.0531e-11,\n",
            "        2.5549e-06, 3.3248e+02, 2.2617e-11, 7.1940e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6897e-05, 2.2867e-09, 0.0000e+00, 3.6897e-05, 1.4465e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6897e-05, 3.8296e-05, 5.7602e-10, 2.3772e-07, 7.4443e-38, 4.0217e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6897e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4337e-04, 2.3772e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.2222e-09, 3.3819e-04, 0.0000e+00, 1.2222e-09, 8.6373e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.2222e-09, 2.3825e+04, 2.1126e-10, 3.0262e-07, 0.0000e+00, 9.4592e+12,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.2222e-09, 1.0531e+09,\n",
            "        2.3116e+04, 6.4160e+09, 5.1012e-08, 3.0262e-07, 1.1778e-01])\n",
            "************** t  86\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5497e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1099e-05, 9.9788e-10, 0.0000e+00, 2.1099e-05, 4.6231e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1099e-05, 2.8520e-05, 2.4771e-10, 1.1560e-07, 1.2445e-38, 2.2141e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1099e-05, 0.0000e+00,\n",
            "        4.9128e-11, 2.5629e-08, 3.0759e-04, 1.1560e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 4.5095e-14, 7.7334e-13, 0.0000e+00, 4.5095e-14, 1.2218e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        4.5095e-14, 9.1241e-01, 1.2169e-19, 7.1940e-14, 0.0000e+00, 7.8410e-08,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 4.5095e-14, 9.2384e-12,\n",
            "        2.7097e-06, 3.3248e+02, 2.2617e-11, 7.1940e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6897e-05, 2.2867e-09, 0.0000e+00, 3.6897e-05, 1.4434e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6897e-05, 3.8296e-05, 5.7602e-10, 2.3772e-07, 7.4443e-38, 2.0123e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6897e-05, 0.0000e+00,\n",
            "        1.1047e-10, 5.1820e-08, 4.4337e-04, 2.3772e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.2222e-09, 3.3819e-04, 0.0000e+00, 1.2222e-09, 8.4643e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.2222e-09, 2.3825e+04, 2.1126e-10, 3.0262e-07, 0.0000e+00, 7.8410e+12,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.2222e-09, 9.2384e+08,\n",
            "        2.4529e+04, 6.4160e+09, 5.1012e-08, 3.0262e-07, 1.1778e-01])\n",
            "************** t  87\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5497e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1099e-05, 9.9788e-10, 0.0000e+00, 2.1099e-05, 4.6205e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1099e-05, 2.8520e-05, 2.4771e-10, 1.1560e-07, 1.2445e-38, 1.8777e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1099e-05, 0.0000e+00,\n",
            "        4.9128e-11, 2.5629e-08, 3.0759e-04, 1.1560e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 4.5095e-14, 7.7334e-13, 0.0000e+00, 4.5095e-14, 1.2220e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        4.5095e-14, 9.1241e-01, 1.2169e-19, 7.1940e-14, 0.0000e+00, 4.9371e-08,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 4.5095e-14, 6.6160e-13,\n",
            "        2.7097e-06, 3.3248e+02, 2.2617e-11, 7.1940e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6897e-05, 2.2867e-09, 0.0000e+00, 3.6897e-05, 1.4435e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6897e-05, 3.8296e-05, 5.7602e-10, 2.3772e-07, 7.4443e-38, 2.0207e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6897e-05, 0.0000e+00,\n",
            "        1.1047e-10, 5.1820e-08, 4.4337e-04, 2.3772e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.2222e-09, 3.3819e-04, 0.0000e+00, 1.2222e-09, 8.4653e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.2222e-09, 2.3825e+04, 2.1126e-10, 3.0262e-07, 0.0000e+00, 4.9371e+12,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.2222e-09, 6.6160e+07,\n",
            "        2.4529e+04, 6.4160e+09, 5.1012e-08, 3.0262e-07, 1.1778e-01])\n",
            "************** t  88\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5497e-06,\n",
            "        -0.0000e+00, 2.2213e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1099e-05, 9.9656e-10, 0.0000e+00, 2.1099e-05, 4.6128e-11,\n",
            "        1.3935e-26, 4.9274e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1099e-05, 2.8520e-05, 2.4730e-10, 1.1541e-07, 1.2445e-38, 1.0370e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1099e-05, 0.0000e+00,\n",
            "        4.9128e-11, 2.5629e-08, 3.0759e-04, 1.1541e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 4.5095e-14, 8.1875e-13, 0.0000e+00, 4.5095e-14, 1.2441e-03,\n",
            "        0.0000e+00, 1.4242e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        4.5095e-14, 9.1241e-01, 1.3339e-19, 7.8562e-14, 0.0000e+00, 5.9498e-08,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 4.5095e-14, 1.0612e-12,\n",
            "        2.7097e-06, 3.3248e+02, 2.2617e-11, 7.8562e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6897e-05, 2.2855e-09, 0.0000e+00, 3.6897e-05, 1.4420e-10,\n",
            "        5.7000e-26, 6.0302e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6897e-05, 3.8296e-05, 5.7554e-10, 2.3755e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6897e-05, 0.0000e+00,\n",
            "        1.1047e-10, 5.1820e-08, 4.4337e-04, 2.3755e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.2222e-09, 3.5823e-04, 0.0000e+00, 1.2222e-09, 8.6280e+06,\n",
            "        0.0000e+00, 2.3617e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.2222e-09, 2.3825e+04, 2.3177e-10, 3.3072e-07, 0.0000e+00, 5.9498e+12,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.2222e-09, 1.0612e+08,\n",
            "        2.4529e+04, 6.4160e+09, 5.1012e-08, 3.3072e-07, 1.1778e-01])\n",
            "************** t  89\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5497e-06,\n",
            "        -0.0000e+00, 2.2213e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1040e-05, 9.9656e-10, 0.0000e+00, 2.1040e-05, 4.6128e-11,\n",
            "        1.3935e-26, 4.9274e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1040e-05, 2.8520e-05, 2.4689e-10, 1.1471e-07, 1.2445e-38, 6.8664e-44,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1040e-05, 0.0000e+00,\n",
            "        4.9046e-11, 2.5629e-08, 3.0674e-04, 1.1471e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 5.5132e-14, 8.1875e-13, 0.0000e+00, 5.5132e-14, 1.2441e-03,\n",
            "        0.0000e+00, 1.4242e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        5.5132e-14, 9.1241e-01, 1.4532e-19, 1.0796e-13, 0.0000e+00, 2.1288e-08,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 5.5132e-14, 1.2506e-12,\n",
            "        2.9089e-06, 3.3248e+02, 2.7848e-11, 1.0796e-13, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6835e-05, 2.2855e-09, 0.0000e+00, 3.6835e-05, 1.4420e-10,\n",
            "        5.7000e-26, 6.0302e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6835e-05, 3.8296e-05, 5.7505e-10, 2.3676e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6835e-05, 0.0000e+00,\n",
            "        1.1038e-10, 5.1820e-08, 4.4273e-04, 2.3676e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.4967e-09, 3.5823e-04, 0.0000e+00, 1.4967e-09, 8.6280e+06,\n",
            "        0.0000e+00, 2.3617e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.4967e-09, 2.3825e+04, 2.5271e-10, 4.5600e-07, 0.0000e+00, 2.1288e+12,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.4967e-09, 1.2506e+08,\n",
            "        2.6354e+04, 6.4160e+09, 6.2901e-08, 4.5600e-07, 1.1778e-01])\n",
            "************** t  90\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.6689e-06,\n",
            "        -0.0000e+00, 2.3793e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1040e-05, 9.9324e-10, 0.0000e+00, 2.1040e-05, 4.5991e-11,\n",
            "        1.3935e-26, 4.9192e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1040e-05, 2.8520e-05, 2.4547e-10, 1.1350e-07, 1.2445e-38, 4.3440e-44,\n",
            "        1.3268e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1040e-05, 0.0000e+00,\n",
            "        4.8964e-11, 2.5629e-08, 3.0674e-04, 1.1350e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 5.5132e-14, 9.5788e-13, 0.0000e+00, 5.5132e-14, 1.3269e-03,\n",
            "        0.0000e+00, 1.5139e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        5.5132e-14, 9.1241e-01, 1.9381e-19, 1.7821e-13, 0.0000e+00, 3.7844e-08,\n",
            "        1.1426e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 5.5132e-14, 6.7272e-13,\n",
            "        3.0879e-06, 3.3248e+02, 2.7848e-11, 1.7821e-13, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6835e-05, 2.2817e-09, 0.0000e+00, 3.6835e-05, 1.4394e-10,\n",
            "        5.7000e-26, 6.0296e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6835e-05, 3.8296e-05, 5.7316e-10, 2.3536e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.6425e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6835e-05, 0.0000e+00,\n",
            "        1.1029e-10, 5.1820e-08, 4.4273e-04, 2.3536e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.4967e-09, 4.1980e-04, 0.0000e+00, 1.4967e-09, 9.2185e+06,\n",
            "        0.0000e+00, 2.5108e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.4967e-09, 2.3825e+04, 3.3814e-10, 7.5717e-07, 0.0000e+00, 3.7844e+12,\n",
            "        3.1369e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.4967e-09, 6.7272e+07,\n",
            "        2.7998e+04, 6.4160e+09, 6.2901e-08, 7.5717e-07, 1.1778e-01])\n",
            "************** t  91\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.6689e-06,\n",
            "        -0.0000e+00, 2.3793e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.0889e-05, 9.9324e-10, 0.0000e+00, 2.0889e-05, 4.5915e-11,\n",
            "        1.3935e-26, 4.9192e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.0889e-05, 2.8520e-05, 2.4438e-10, 1.1312e-07, 1.2445e-38, 3.0829e-44,\n",
            "        1.3202e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.0889e-05, 0.0000e+00,\n",
            "        4.8964e-11, 2.5629e-08, 3.0674e-04, 1.1312e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 7.9119e-14, 9.5788e-13, 0.0000e+00, 7.9119e-14, 1.3458e-03,\n",
            "        0.0000e+00, 1.5139e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        7.9119e-14, 9.1241e-01, 2.3971e-19, 2.0206e-13, 0.0000e+00, 2.1529e-08,\n",
            "        1.3183e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 7.9119e-14, 4.7593e-13,\n",
            "        3.0879e-06, 3.3248e+02, 2.7848e-11, 2.0206e-13, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6690e-05, 2.2817e-09, 0.0000e+00, 3.6690e-05, 1.4379e-10,\n",
            "        5.7000e-26, 6.0296e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6690e-05, 3.8296e-05, 5.7156e-10, 2.3501e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.6319e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6690e-05, 0.0000e+00,\n",
            "        1.1029e-10, 5.1820e-08, 4.4273e-04, 2.3501e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 2.1564e-09, 4.1980e-04, 0.0000e+00, 2.1564e-09, 9.3597e+06,\n",
            "        0.0000e+00, 2.5108e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        2.1564e-09, 2.3825e+04, 4.1940e-10, 8.5980e-07, 0.0000e+00, 2.1529e+12,\n",
            "        3.6298e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 2.1564e-09, 4.7593e+07,\n",
            "        2.7998e+04, 6.4160e+09, 6.2901e-08, 8.5980e-07, 1.1778e-01])\n",
            "************** t  92\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.0266e-06,\n",
            "        -0.0000e+00, 2.3793e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 4.7684e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.0784e-05, 9.9046e-10, 0.0000e+00, 2.0784e-05, 4.5584e-11,\n",
            "        1.3935e-26, 4.9192e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.0784e-05, 2.8520e-05, 2.4398e-10, 1.1275e-07, 1.2445e-38, 2.6625e-44,\n",
            "        1.3061e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.0784e-05, 0.0000e+00,\n",
            "        4.8964e-11, 2.5629e-08, 3.0436e-04, 1.1275e-07, 5.4812e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 9.9385e-14, 1.0489e-12, 0.0000e+00, 9.9385e-14, 1.5875e-03,\n",
            "        0.0000e+00, 1.5139e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        9.9385e-14, 9.1241e-01, 2.5426e-19, 2.2778e-13, 0.0000e+00, 1.1966e-08,\n",
            "        1.7405e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 9.9385e-14, 4.0324e-13,\n",
            "        3.0879e-06, 3.3248e+02, 4.1812e-11, 2.2778e-13, 6.2966e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6626e-05, 2.2791e-09, 0.0000e+00, 3.6626e-05, 1.4302e-10,\n",
            "        5.7000e-26, 6.0296e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6626e-05, 3.8296e-05, 5.7107e-10, 2.3466e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.6079e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6626e-05, 0.0000e+00,\n",
            "        1.1029e-10, 5.1820e-08, 4.4163e-04, 2.3466e-07, 4.4443e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 2.7135e-09, 4.6024e-04, 0.0000e+00, 2.7135e-09, 1.1100e+07,\n",
            "        0.0000e+00, 2.5108e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        2.7135e-09, 2.3825e+04, 4.4523e-10, 9.7069e-07, 0.0000e+00, 1.1966e+12,\n",
            "        4.8240e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 2.7135e-09, 4.0324e+07,\n",
            "        2.7998e+04, 6.4160e+09, 9.4675e-08, 9.7069e-07, 1.4168e-01])\n",
            "************** t  93\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.1458e-06,\n",
            "        -0.0000e+00, 2.3793e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.7277e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 4.7684e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.0543e-05, 9.8343e-10, 0.0000e+00, 2.0543e-05, 4.5357e-11,\n",
            "        1.3935e-26, 4.9192e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.0543e-05, 2.8473e-05, 2.4316e-10, 1.1119e-07, 1.2445e-38, 1.2612e-44,\n",
            "        1.2838e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.0543e-05, 0.0000e+00,\n",
            "        4.8964e-11, 2.5629e-08, 3.0217e-04, 1.1119e-07, 5.4812e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 1.5609e-13, 1.3278e-12, 0.0000e+00, 1.5609e-13, 1.6920e-03,\n",
            "        0.0000e+00, 1.5139e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        1.5609e-13, 9.8517e-01, 2.8340e-19, 3.8897e-13, 0.0000e+00, 1.7106e-08,\n",
            "        2.3371e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 1.5609e-13, 3.4996e-13,\n",
            "        3.0879e-06, 3.3248e+02, 5.9412e-11, 3.8897e-13, 6.2966e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6398e-05, 2.2702e-09, 0.0000e+00, 3.6398e-05, 1.4257e-10,\n",
            "        5.7000e-26, 6.0296e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6398e-05, 3.8287e-05, 5.7010e-10, 2.3249e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.5833e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6398e-05, 0.0000e+00,\n",
            "        1.1029e-10, 5.1820e-08, 4.4019e-04, 2.3249e-07, 4.4443e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 4.2884e-09, 5.8488e-04, 0.0000e+00, 4.2884e-09, 1.1868e+07,\n",
            "        0.0000e+00, 2.5108e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        4.2884e-09, 2.5731e+04, 4.9711e-10, 1.6730e-06, 0.0000e+00, 1.7106e+12,\n",
            "        6.5222e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 4.2884e-09, 3.4996e+07,\n",
            "        2.7998e+04, 6.4160e+09, 1.3497e-07, 1.6730e-06, 1.4168e-01])\n",
            "************** t  94\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.8610e-06,\n",
            "        -0.0000e+00, 2.4862e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.9243e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 4.7684e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.0204e-05, 9.7853e-10, 0.0000e+00, 2.0204e-05, 4.4459e-11,\n",
            "        1.3935e-26, 4.9110e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.0204e-05, 2.8447e-05, 2.4048e-10, 1.0948e-07, 1.2445e-38, 8.4078e-45,\n",
            "        1.2689e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.0204e-05, 0.0000e+00,\n",
            "        4.8964e-11, 2.5629e-08, 3.0067e-04, 1.0948e-07, 5.4812e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.7386e-13, 1.5421e-12, 0.0000e+00, 2.7386e-13, 2.2602e-03,\n",
            "        0.0000e+00, 1.5738e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.7386e-13, 1.0187e+00, 4.2651e-19, 5.9814e-13, 0.0000e+00, 4.1129e-09,\n",
            "        2.9197e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.7386e-13, 3.3061e-13,\n",
            "        3.0879e-06, 3.3248e+02, 7.1959e-11, 5.9814e-13, 6.2966e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6184e-05, 2.2645e-09, 0.0000e+00, 3.6184e-05, 1.4042e-10,\n",
            "        5.7000e-26, 6.0290e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6184e-05, 3.8301e-05, 5.6565e-10, 2.3078e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.5587e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6184e-05, 0.0000e+00,\n",
            "        1.1029e-10, 5.1820e-08, 4.3972e-04, 2.3078e-07, 4.4443e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 7.5687e-09, 6.8099e-04, 0.0000e+00, 7.5687e-09, 1.6096e+07,\n",
            "        0.0000e+00, 2.6104e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        7.5687e-09, 2.6596e+04, 7.5402e-10, 2.5918e-06, 0.0000e+00, 4.1129e+11,\n",
            "        8.2044e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 7.5687e-09, 3.3061e+07,\n",
            "        2.7998e+04, 6.4160e+09, 1.6365e-07, 2.5918e-06, 1.4168e-01])\n",
            "************** t  95\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-06,\n",
            "        -0.0000e+00, 2.7832e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 8.3721e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.1954e-01, -0.0000e+00, -0.0000e+00, 5.4836e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 1.9947e-05, 9.5915e-10, 0.0000e+00, 1.9947e-05, 4.3595e-11,\n",
            "        1.3935e-26, 4.8892e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        1.9947e-05, 2.8392e-05, 2.3598e-10, 1.0642e-07, 1.2445e-38, 5.6052e-45,\n",
            "        1.2442e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 1.9947e-05, 0.0000e+00,\n",
            "        4.8710e-11, 2.5553e-08, 2.9567e-04, 1.0642e-07, 5.4674e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 4.1850e-13, 2.6178e-12, 0.0000e+00, 4.1850e-13, 2.8565e-03,\n",
            "        0.0000e+00, 1.7715e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        4.1850e-13, 1.0673e+00, 6.2750e-19, 1.3455e-12, 0.0000e+00, 7.1434e-09,\n",
            "        4.1078e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 4.1850e-13, 2.7901e-13,\n",
            "        3.5176e-06, 3.4682e+02, 1.3490e-10, 1.3455e-12, 7.1250e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.5915e-05, 2.2381e-09, 0.0000e+00, 3.5915e-05, 1.3853e-10,\n",
            "        5.7000e-26, 6.0210e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.5915e-05, 3.8324e-05, 5.5868e-10, 2.2658e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.5133e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.5915e-05, 0.0000e+00,\n",
            "        1.0990e-10, 5.1763e-08, 4.3636e-04, 2.2658e-07, 4.4537e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.1652e-08, 1.1697e-03, 0.0000e+00, 1.1652e-08, 2.0621e+07,\n",
            "        0.0000e+00, 2.9422e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.1652e-08, 2.7849e+04, 1.1232e-09, 5.9384e-06, 0.0000e+00, 7.1434e+11,\n",
            "        1.1692e-08, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.1652e-08, 2.7901e+07,\n",
            "        3.2007e+04, 6.7003e+09, 3.0915e-07, 5.9384e-06, 1.5998e-01])\n",
            "************** t  96\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.8876e-06,\n",
            "        -0.0000e+00, 3.0219e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 8.9188e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.4070e-01, -0.0000e+00, -0.0000e+00, 5.3644e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 1.9121e-05, 9.3529e-10, 0.0000e+00, 1.9121e-05, 4.2283e-11,\n",
            "        1.3935e-26, 4.8702e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        1.9121e-05, 2.8345e-05, 2.2511e-10, 1.0084e-07, 1.2445e-38, 4.2039e-45,\n",
            "        1.1994e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 1.9121e-05, 0.0000e+00,\n",
            "        4.8414e-11, 2.5468e-08, 2.9030e-04, 1.0084e-07, 5.4522e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 1.2881e-12, 4.5810e-12, 0.0000e+00, 1.2881e-12, 4.0113e-03,\n",
            "        0.0000e+00, 1.8173e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        1.2881e-12, 1.1465e+00, 1.5331e-18, 4.7265e-12, 0.0000e+00, 8.1981e-09,\n",
            "        7.2053e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 1.2881e-12, 1.0414e-13,\n",
            "        4.0162e-06, 3.5115e+02, 2.2382e-10, 4.7265e-12, 7.1324e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.5122e-05, 2.2074e-09, 0.0000e+00, 3.5122e-05, 1.3525e-10,\n",
            "        5.7000e-26, 6.0139e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.5122e-05, 3.8315e-05, 5.4374e-10, 2.1875e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.4282e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.5122e-05, 0.0000e+00,\n",
            "        1.0951e-10, 5.1688e-08, 4.3455e-04, 2.1875e-07, 4.4936e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 3.6675e-08, 2.0753e-03, 0.0000e+00, 3.6675e-08, 2.9659e+07,\n",
            "        0.0000e+00, 3.0218e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        3.6675e-08, 2.9923e+04, 2.8196e-09, 2.1607e-05, 0.0000e+00, 8.1981e+11,\n",
            "        2.1018e-08, 0.0000e+00, 0.0000e+00, 4.5195e+08, 3.6675e-08, 1.0414e+07,\n",
            "        3.6674e+04, 6.7936e+09, 5.1507e-07, 2.1607e-05, 1.5872e-01])\n",
            "************** t  97\n",
            "ce_loss:  tensor([1.5497e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.9870e-06,\n",
            "        -0.0000e+00, 3.4971e-01, -0.0000e+00, -0.0000e+00, 1.5497e-06, 1.3211e+01,\n",
            "        -0.0000e+00, 1.1195e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.6328e-01, -0.0000e+00, -0.0000e+00, 7.7486e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3438e-03, 1.7888e-05, 8.8329e-10, 0.0000e+00, 1.7888e-05, 4.0080e-11,\n",
            "        1.3935e-26, 4.8230e-05, 1.2336e-39, 1.8229e-36, 7.3438e-03, 3.5367e-05,\n",
            "        1.7888e-05, 2.8125e-05, 2.1263e-10, 9.4373e-08, 1.2445e-38, 2.8026e-45,\n",
            "        1.1338e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 1.7888e-05, 0.0000e+00,\n",
            "        4.8172e-11, 2.5342e-08, 2.7614e-04, 9.4373e-08, 5.4134e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.1339e-03, 5.4040e-12, 1.2580e-11, 0.0000e+00, 5.4040e-12, 6.3600e-03,\n",
            "        0.0000e+00, 2.0563e+02, 0.0000e+00, 0.0000e+00, 2.1339e-03, 9.2584e+02,\n",
            "        5.4040e-12, 1.4200e+00, 5.0864e-18, 1.6838e-11, 0.0000e+00, 3.4510e-09,\n",
            "        1.4544e-18, 0.0000e+00, 0.0000e+00, 7.5818e+02, 5.4040e-12, 1.4629e-13,\n",
            "        4.2553e-06, 3.6174e+02, 8.0768e-10, 1.6838e-11, 1.0088e-02])\n",
            "kde_grad  tensor([6.4277e-03, 3.3780e-05, 2.1304e-09, 0.0000e+00, 3.3780e-05, 1.2966e-10,\n",
            "        5.7000e-26, 6.0017e-05, 7.5966e-39, 1.0206e-35, 6.4277e-03, 4.5595e-05,\n",
            "        3.3780e-05, 3.8234e-05, 5.2254e-10, 2.0942e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.2927e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.3780e-05, 0.0000e+00,\n",
            "        1.0924e-10, 5.1577e-08, 4.2500e-04, 2.0942e-07, 4.5011e-02])\n",
            "penalty_factor  tensor([3.3198e-01, 1.5997e-07, 5.9050e-03, 0.0000e+00, 1.5997e-07, 4.9053e+07,\n",
            "        0.0000e+00, 3.4262e+06, 0.0000e+00, 0.0000e+00, 3.3198e-01, 2.0306e+07,\n",
            "        1.5997e-07, 3.7140e+04, 9.7340e-09, 8.0406e-05, 0.0000e+00, 3.4510e+11,\n",
            "        4.4172e-08, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.5997e-07, 1.4629e+07,\n",
            "        3.8953e+04, 7.0135e+09, 1.9004e-06, 8.0406e-05, 2.2412e-01])\n",
            "************** t  98\n",
            "ce_loss:  tensor([1.5497e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7285e-05,\n",
            "        -0.0000e+00, 4.4721e-01, -0.0000e+00, -0.0000e+00, 1.5497e-06, 1.3241e+01,\n",
            "        -0.0000e+00, 1.5107e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6242e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 7.2348e-01, -0.0000e+00, -0.0000e+00, 8.5830e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3438e-03, 1.6265e-05, 8.1999e-10, 0.0000e+00, 1.6265e-05, 3.5905e-11,\n",
            "        1.3935e-26, 4.7223e-05, 1.2336e-39, 1.8229e-36, 7.3438e-03, 3.5288e-05,\n",
            "        1.6265e-05, 2.7753e-05, 1.8955e-10, 8.4881e-08, 1.2445e-38, 1.4013e-45,\n",
            "        1.0168e-11, 2.3542e-38, 7.7047e-27, 1.3043e-06, 1.6265e-05, 0.0000e+00,\n",
            "        4.7080e-11, 2.4979e-08, 2.4968e-04, 8.4881e-08, 5.3954e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.1339e-03, 2.8229e-11, 3.5035e-11, 0.0000e+00, 2.8229e-11, 1.4107e-02,\n",
            "        0.0000e+00, 1.8642e+02, 0.0000e+00, 0.0000e+00, 2.1339e-03, 9.0729e+02,\n",
            "        2.8229e-11, 1.9575e+00, 1.9307e-17, 7.9623e-11, 0.0000e+00, 1.8764e-09,\n",
            "        4.3260e-18, 0.0000e+00, 0.0000e+00, 7.5865e+02, 2.8229e-11, 6.6025e-14,\n",
            "        5.8134e-06, 3.7138e+02, 5.2958e-09, 7.9623e-11, 1.1202e-02])\n",
            "kde_grad  tensor([6.4277e-03, 3.1836e-05, 2.0261e-09, 0.0000e+00, 3.1836e-05, 1.1863e-10,\n",
            "        5.7000e-26, 5.9487e-05, 7.5966e-39, 1.0206e-35, 6.4277e-03, 4.5629e-05,\n",
            "        3.1836e-05, 3.8153e-05, 4.8325e-10, 1.9437e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.0326e-11, 1.4114e-37, 3.1644e-26, 1.6773e-06, 3.1836e-05, 0.0000e+00,\n",
            "        1.0766e-10, 5.1220e-08, 4.0575e-04, 1.9437e-07, 4.5068e-02])\n",
            "penalty_factor  tensor([3.3198e-01, 8.8672e-07, 1.7292e-02, 0.0000e+00, 8.8672e-07, 1.1892e+08,\n",
            "        0.0000e+00, 3.1338e+06, 0.0000e+00, 0.0000e+00, 3.3198e-01, 1.9884e+07,\n",
            "        8.8672e-07, 5.1308e+04, 3.9953e-08, 4.0964e-04, 0.0000e+00, 1.8764e+11,\n",
            "        1.4265e-07, 0.0000e+00, 0.0000e+00, 4.5231e+08, 8.8672e-07, 6.6025e+06,\n",
            "        5.3996e+04, 7.2507e+09, 1.3052e-05, 4.0964e-04, 2.4857e-01])\n",
            "************** t  99\n",
            "ce_loss:  tensor([2.1458e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.7087e-05,\n",
            "        -0.0000e+00, 4.4754e-01, -0.0000e+00, -0.0000e+00, 2.1458e-06, 1.3320e+01,\n",
            "        -0.0000e+00, 2.7434e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6575e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 8.4011e-01, -0.0000e+00, -0.0000e+00, 1.7404e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.2578e-03, 1.2266e-05, 6.6911e-10, 0.0000e+00, 1.2266e-05, 2.9898e-11,\n",
            "        1.3935e-26, 4.6031e-05, 1.2336e-39, 1.8229e-36, 7.2578e-03, 3.5015e-05,\n",
            "        1.2266e-05, 2.6827e-05, 1.4931e-10, 6.4846e-08, 1.2445e-38, 1.4013e-45,\n",
            "        7.7829e-12, 2.3542e-38, 7.7047e-27, 1.2999e-06, 1.2266e-05, 0.0000e+00,\n",
            "        4.4662e-11, 2.4025e-08, 2.0739e-04, 6.4846e-08, 5.2651e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.9067e-03, 5.8433e-10, 2.7148e-10, 0.0000e+00, 5.8433e-10, 3.7705e-02,\n",
            "        0.0000e+00, 2.4103e+02, 0.0000e+00, 0.0000e+00, 2.9067e-03, 9.2584e+02,\n",
            "        5.8433e-10, 3.4353e+00, 1.8361e-16, 1.1033e-09, 0.0000e+00, 1.4530e-09,\n",
            "        3.1562e-17, 0.0000e+00, 0.0000e+00, 7.6962e+02, 5.8433e-10, 9.8756e-14,\n",
            "        9.7430e-06, 4.1465e+02, 5.3841e-08, 1.1033e-09, 2.2469e-02])\n",
            "kde_grad  tensor([6.4358e-03, 2.6570e-05, 1.7454e-09, 0.0000e+00, 2.6570e-05, 1.0181e-10,\n",
            "        5.7000e-26, 5.9750e-05, 7.5966e-39, 1.0206e-35, 6.4358e-03, 4.5543e-05,\n",
            "        2.6570e-05, 3.7699e-05, 4.0898e-10, 1.5991e-07, 7.4443e-38, 0.0000e+00,\n",
            "        2.4614e-11, 1.4114e-37, 3.1644e-26, 1.6767e-06, 2.6570e-05, 0.0000e+00,\n",
            "        1.0401e-10, 5.0178e-08, 3.6442e-04, 1.5991e-07, 4.5190e-02])\n",
            "penalty_factor  tensor([4.5164e-01, 2.1992e-05, 1.5554e-01, 0.0000e+00, 2.1992e-05, 3.7035e+08,\n",
            "        0.0000e+00, 4.0340e+06, 0.0000e+00, 0.0000e+00, 4.5164e-01, 2.0329e+07,\n",
            "        2.1992e-05, 9.1123e+04, 4.4895e-07, 6.8995e-03, 0.0000e+00, 1.4530e+11,\n",
            "        1.2823e-06, 0.0000e+00, 0.0000e+00, 4.5902e+08, 2.1992e-05, 9.8756e+06,\n",
            "        9.3670e+04, 8.2634e+09, 1.4774e-04, 6.8995e-03, 4.9722e-01])\n",
            "PGD linf: Attack effectiveness 55.172%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KDE(top_500_high_confidence_benign_samples[:1], top_500_high_confidence_benign_samples, 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f69a7a-f057-4d64-dfed-ed340f57e640",
        "id": "VOMCDbzGz4WV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0462], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KDE(mals.to(device), top_500_high_confidence_benign_samples, 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou-DsXVKz__Q",
        "outputId": "79272e49-ab94-4c52-8590-778242ebc244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.3344e-05, 2.2177e-10, 1.1448e-15, 0.0000e+00, 2.2177e-10, 3.4137e-18,\n",
              "        1.3935e-26, 6.6971e-06, 1.2336e-39, 1.8229e-36, 1.3344e-05, 5.0246e-11,\n",
              "        2.2177e-10, 5.7728e-08, 3.5037e-15, 4.1197e-12, 1.2445e-38, 3.1590e-40,\n",
              "        8.5677e-20, 2.3542e-38, 7.7047e-27, 2.7702e-10, 2.2177e-10, 4.0407e-39,\n",
              "        1.8901e-17, 1.1674e-15, 2.3014e-07, 4.1197e-12, 7.4781e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KDE(top_500_high_confidence_benign_samples, top_500_high_confidence_benign_samples, 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad26ebd-178b-4472-abeb-5e391dc05ddf",
        "id": "lTIxz5QFz4WW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0995, 1.0995, 1.0995, 1.0995, 1.0995, 1.0244, 1.0995, 1.0244, 1.0244,\n",
              "        1.0244, 0.1000, 1.0995, 0.1000, 0.2199, 0.3237, 0.1000, 1.0995, 1.9344,\n",
              "        0.9219, 0.9219, 0.1569, 0.8929, 0.1552, 1.0244, 1.9344, 0.4189, 1.0244,\n",
              "        0.1063, 1.0244, 0.1000, 0.1000, 0.1300, 0.1000, 1.9344, 0.1038, 1.9344,\n",
              "        1.9344, 0.1000, 1.9344, 0.9219, 0.2001, 0.1000, 0.3195, 0.1081, 1.9344,\n",
              "        0.1250, 0.1143, 0.2000, 0.1026, 0.9219, 1.9344, 1.0995, 0.2000, 0.1000,\n",
              "        1.9344, 0.4189, 0.1251, 1.9344, 0.9219, 0.1559, 0.2273, 0.3001, 0.9219,\n",
              "        0.2273, 0.1065, 0.3001, 0.1307, 0.9219, 0.8929, 0.5761, 1.9344, 1.9344,\n",
              "        1.9344, 0.1021, 0.2199, 0.1000, 0.2000, 1.9344, 0.8929, 1.9344, 0.1307,\n",
              "        0.9219, 1.0244, 0.1584, 0.1157, 0.3195, 0.1388, 0.5761, 0.1076, 0.2000,\n",
              "        0.3001, 1.9344, 0.2199, 0.3237, 0.1569, 1.9344, 0.2001, 0.5823, 0.8929,\n",
              "        0.3164])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zf0SckOmaOJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KDE(top_500_high_confidence_benign_samples, top_500_high_confidence_benign_samples, 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhmvPK-7gnx5",
        "outputId": "881a224d-ac2d-4495-e071-aa4c4be2b620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0995, 1.0995, 1.0995, 1.0995, 1.0995, 1.0244, 1.0995, 1.0244, 1.0244,\n",
              "        1.0244, 0.1000, 1.0995, 0.1000, 0.2199, 0.3237, 0.1000, 1.0995, 1.9344,\n",
              "        0.9219, 0.9219, 0.1569, 0.8929, 0.1552, 1.0244, 1.9344, 0.4189, 1.0244,\n",
              "        0.1063, 1.0244, 0.1000, 0.1000, 0.1300, 0.1000, 1.9344, 0.1038, 1.9344,\n",
              "        1.9344, 0.1000, 1.9344, 0.9219, 0.2001, 0.1000, 0.3195, 0.1081, 1.9344,\n",
              "        0.1250, 0.1143, 0.2000, 0.1026, 0.9219, 1.9344, 1.0995, 0.2000, 0.1000,\n",
              "        1.9344, 0.4189, 0.1251, 1.9344, 0.9219, 0.1559, 0.2273, 0.3001, 0.9219,\n",
              "        0.2273, 0.1065, 0.3001, 0.1307, 0.9219, 0.8929, 0.5761, 1.9344, 1.9344,\n",
              "        1.9344, 0.1021, 0.2199, 0.1000, 0.2000, 1.9344, 0.8929, 1.9344, 0.1307,\n",
              "        0.9219, 1.0244, 0.1584, 0.1157, 0.3195, 0.1388, 0.5761, 0.1076, 0.2000,\n",
              "        0.3001, 1.9344, 0.2199, 0.3237, 0.1569, 1.9344, 0.2001, 0.5823, 0.8929,\n",
              "        0.3164])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KDE(mals, top_500_high_confidence_benign_samples, 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-Cj5yipzIyT",
        "outputId": "a82ba158-1747-4e03-982c-45b8490fa331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.3344e-05, 2.2177e-10, 1.1448e-15, 0.0000e+00, 2.2177e-10, 3.4137e-18,\n",
              "        1.3935e-26, 6.6971e-06, 1.2336e-39, 1.8229e-36, 1.3344e-05, 5.0246e-11,\n",
              "        2.2177e-10, 5.7728e-08, 3.5037e-15, 4.1197e-12, 1.2445e-38, 3.1590e-40,\n",
              "        8.5677e-20, 2.3542e-38, 7.7047e-27, 2.7702e-10, 2.2177e-10, 4.0407e-39,\n",
              "        1.8901e-17, 1.1674e-15, 2.3014e-07, 4.1197e-12, 7.4781e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9dCx4yRxESe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "def get_loss_kde(adv_x,y,model,benigns, bandwidth, penalty_factor):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute CE loss\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y)\n",
        "    #print('ce ',ce)\n",
        "\n",
        "    # Compute KDE loss\n",
        "    kde = KDE(adv_x, benigns, bandwidth)\n",
        "    #print('kde ',kde)\n",
        "\n",
        "    # Combine the losses with the penalty factor\n",
        "    loss = ce - penalty_factor * kde\n",
        "    #print('loss ',loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def gkde(x, y, model,benigns, bandwidth, insertion_array, removal_array, k=25, step_length=0.02, norm='linf',\n",
        "        initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "\n",
        "    :param x: Feature vector\n",
        "    :param y: Ground truth labels\n",
        "    :param model: Neural network model\n",
        "    :param RBFModel: Gaussian model for KDE\n",
        "    :param insertion_array: Array for insertion operations\n",
        "    :param removal_array: Array for removal operations\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf', 'l2', 'l1')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    #target's class\n",
        "    traget_labels = torch.zeros_like(y.view(-1).long())\n",
        "\n",
        "    # Compute natural loss and penalty_factor\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), traget_labels)\n",
        "    kde = KDE(x, benigns, bandwidth)\n",
        "    #penalty_factor = 0.\n",
        "    penalty_factor = 1e6\n",
        "    #print(penalty_factor)\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    for t in range(k):\n",
        "        print('*************** t ',t)\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        outputs = model(x_var)\n",
        "        ce_loss = criterion(outputs, traget_labels)\n",
        "        print('ce_loss: ', ce_loss)\n",
        "        kde_loss = KDE(x_var, benigns, bandwidth)\n",
        "        print('kde_loss: ', kde_loss)\n",
        "        ce_grad = torch.autograd.grad(ce_loss.mean(), x_var, retain_graph=True)[0].data\n",
        "        kde_grad = torch.autograd.grad(kde_loss.mean(), x_var)[0].data\n",
        "        print('ce_grad ',torch.abs(ce_grad).sum(dim=-1).detach())\n",
        "        print('kde_grad ',torch.abs(kde_grad).sum(dim=-1).detach())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde(x_var,traget_labels,model,benigns, bandwidth, penalty_factor)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "        elif norm == 'l1':\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1', 'l2', or 'linf' norm.\")\n",
        "\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    if random:\n",
        "        round_threshold = torch.rand_like(x_next)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    outputs = model(x_next)\n",
        "    loss_adv = criterion(outputs, traget_labels).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size(0) * 100:.3f}%.\")\n",
        "\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "VBdPXckRESw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv = gkde(mals.to(torch.float32).to(device), mals_y.to(device), model_AT_rFGSM, top_500_high_confidence_benign_samples,0.6, insertion_array, removal_array, k=100, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c202679-c741-496f-c09a-6b7993b55a69",
        "id": "ep1TpS0gESw7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************** t  0\n",
            "ce_loss:  tensor([4.1028e+01, 8.6630e+01, 5.8158e+01, 1.8685e+02, 8.6630e+01, 3.6857e+01,\n",
            "        1.5623e+02, 9.2854e+00, 1.8450e+02, 1.0696e+02, 4.1028e+01, 1.2040e-05,\n",
            "        8.6630e+01, 3.8186e+01, 9.1592e+01, 7.5509e+01, 1.2146e+02, 2.8487e+01,\n",
            "        6.3303e+01, 9.9407e+01, 1.4897e+02, 1.9638e+01, 8.6630e+01, 4.9636e+01,\n",
            "        3.4985e+01, 2.0074e+01, 7.7117e+01, 7.5509e+01, 3.8682e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.3344e-05, 2.2177e-10, 1.1448e-15, 0.0000e+00, 2.2177e-10, 3.4137e-18,\n",
            "        1.3935e-26, 6.6971e-06, 1.2336e-39, 1.8229e-36, 1.3344e-05, 5.0246e-11,\n",
            "        2.2177e-10, 5.7728e-08, 3.5037e-15, 4.1197e-12, 1.2445e-38, 3.1590e-40,\n",
            "        8.5677e-20, 2.3542e-38, 7.7047e-27, 2.7702e-10, 2.2177e-10, 4.0407e-39,\n",
            "        1.8901e-17, 1.1674e-15, 2.3014e-07, 4.1197e-12, 7.4781e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  1\n",
            "ce_loss:  tensor([3.8930e+01, 8.1252e+01, 5.4561e+01, 1.8050e+02, 8.1252e+01, 3.0279e+01,\n",
            "        1.4982e+02, 6.5895e+00, 1.7741e+02, 1.0231e+02, 3.8930e+01, 8.4638e-06,\n",
            "        8.1252e+01, 3.4357e+01, 8.6054e+01, 7.0332e+01, 1.1706e+02, 2.4853e+01,\n",
            "        6.0047e+01, 9.3228e+01, 1.4309e+02, 1.7637e+01, 8.1252e+01, 4.6975e+01,\n",
            "        3.3508e+01, 1.7785e+01, 6.9806e+01, 7.0332e+01, 3.6410e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.7481e-05, 3.2444e-10, 1.5661e-15, 0.0000e+00, 3.2444e-10, 5.1213e-18,\n",
            "        2.2153e-26, 6.9365e-06, 3.6995e-39, 7.1828e-36, 1.7481e-05, 9.2380e-11,\n",
            "        3.2444e-10, 7.0069e-08, 4.8779e-15, 5.5370e-12, 5.8609e-38, 1.0459e-39,\n",
            "        1.2037e-19, 4.9772e-38, 1.4259e-26, 3.8639e-10, 3.2444e-10, 1.0220e-38,\n",
            "        2.9978e-17, 1.9726e-15, 2.5856e-07, 5.5370e-12, 7.7116e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  2\n",
            "ce_loss:  tensor([3.7697e+01, 7.5129e+01, 5.0083e+01, 1.7474e+02, 7.5129e+01, 2.5249e+01,\n",
            "        1.4335e+02, 5.0435e+00, 1.7031e+02, 9.7716e+01, 3.7697e+01, 8.2254e-06,\n",
            "        7.5129e+01, 3.0199e+01, 8.0268e+01, 6.5271e+01, 1.1249e+02, 2.1447e+01,\n",
            "        5.6916e+01, 8.7844e+01, 1.3668e+02, 1.5970e+01, 7.5129e+01, 4.4444e+01,\n",
            "        3.1907e+01, 1.5580e+01, 6.2219e+01, 6.5271e+01, 3.6912e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.3883e-05, 3.8361e-10, 2.0089e-15, 0.0000e+00, 3.8361e-10, 5.7168e-18,\n",
            "        2.6224e-26, 7.0469e-06, 8.1165e-39, 2.0340e-35, 2.3883e-05, 1.7992e-10,\n",
            "        3.8361e-10, 6.9996e-08, 5.4021e-15, 5.9843e-12, 2.2279e-37, 2.1919e-39,\n",
            "        1.3901e-19, 8.4430e-38, 2.0278e-26, 4.7897e-10, 3.8361e-10, 2.1283e-38,\n",
            "        4.3941e-17, 2.5209e-15, 2.1052e-07, 5.9843e-12, 9.0025e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  3\n",
            "ce_loss:  tensor([3.6372e+01, 6.8012e+01, 4.6156e+01, 1.6872e+02, 6.8012e+01, 2.1276e+01,\n",
            "        1.3708e+02, 4.0988e+00, 1.6330e+02, 9.2996e+01, 3.6372e+01, 8.1062e-06,\n",
            "        6.8012e+01, 2.5732e+01, 7.4219e+01, 5.9230e+01, 1.0791e+02, 1.8765e+01,\n",
            "        5.3867e+01, 8.3245e+01, 1.3027e+02, 1.3951e+01, 6.8012e+01, 4.1988e+01,\n",
            "        3.0173e+01, 1.3419e+01, 5.5117e+01, 5.9230e+01, 3.4501e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.2474e-05, 3.5571e-10, 2.0376e-15, 0.0000e+00, 3.5571e-10, 7.6090e-18,\n",
            "        2.2872e-26, 6.9976e-06, 1.3032e-38, 4.8171e-35, 3.2474e-05, 3.3850e-10,\n",
            "        3.5571e-10, 5.6601e-08, 4.7640e-15, 5.8324e-12, 6.6298e-37, 5.0324e-39,\n",
            "        1.4140e-19, 1.1735e-37, 2.1743e-26, 5.5507e-10, 3.5571e-10, 5.0171e-38,\n",
            "        5.8801e-17, 2.9546e-15, 1.2714e-07, 5.8324e-12, 9.5536e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  4\n",
            "ce_loss:  tensor([3.5693e+01, 6.0436e+01, 4.2251e+01, 1.6281e+02, 6.0436e+01, 1.7927e+01,\n",
            "        1.3122e+02, 2.9584e+00, 1.5689e+02, 8.8479e+01, 3.5693e+01, 1.3351e-05,\n",
            "        6.0436e+01, 2.1148e+01, 6.9034e+01, 5.2443e+01, 1.0354e+02, 1.6370e+01,\n",
            "        5.0637e+01, 7.8845e+01, 1.2464e+02, 1.2020e+01, 6.0436e+01, 3.9746e+01,\n",
            "        2.8391e+01, 1.1675e+01, 4.8357e+01, 5.2443e+01, 3.4963e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.6947e-05, 2.5335e-10, 1.8091e-15, 0.0000e+00, 2.5335e-10, 1.0246e-17,\n",
            "        1.7469e-26, 8.0763e-06, 1.6811e-38, 1.0668e-34, 4.6947e-05, 6.5464e-10,\n",
            "        2.5335e-10, 3.5302e-08, 3.2485e-15, 4.1125e-12, 1.5588e-36, 8.9929e-39,\n",
            "        1.1511e-19, 1.2433e-37, 2.2122e-26, 5.4535e-10, 2.5335e-10, 1.0030e-37,\n",
            "        7.3290e-17, 2.8866e-15, 5.8597e-08, 4.1125e-12, 1.1083e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  5\n",
            "ce_loss:  tensor([3.4153e+01, 5.4125e+01, 3.8365e+01, 1.5739e+02, 5.4125e+01, 1.6181e+01,\n",
            "        1.2538e+02, 3.1100e+00, 1.5047e+02, 8.4070e+01, 3.4153e+01, 7.6294e-06,\n",
            "        5.4125e+01, 1.8826e+01, 6.3195e+01, 4.6690e+01, 9.9030e+01, 1.4972e+01,\n",
            "        4.7197e+01, 7.4499e+01, 1.1906e+02, 1.0320e+01, 5.4125e+01, 3.8128e+01,\n",
            "        2.6527e+01, 9.6715e+00, 4.3945e+01, 4.6690e+01, 3.2560e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([6.0873e-05, 1.3940e-10, 1.3233e-15, 0.0000e+00, 1.3940e-10, 7.8138e-18,\n",
            "        9.8363e-27, 9.0182e-06, 1.7344e-38, 2.0068e-34, 6.0873e-05, 1.1552e-09,\n",
            "        1.3940e-10, 1.9010e-08, 2.1643e-15, 2.5068e-12, 3.3104e-36, 1.7560e-38,\n",
            "        8.3941e-20, 1.3680e-37, 1.7296e-26, 4.8314e-10, 1.3940e-10, 1.5532e-37,\n",
            "        8.0618e-17, 3.1185e-15, 2.1080e-08, 2.5068e-12, 1.1773e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  6\n",
            "ce_loss:  tensor([3.4010e+01, 4.8596e+01, 3.4752e+01, 1.5182e+02, 4.8596e+01, 1.3492e+01,\n",
            "        1.1958e+02, 2.6949e+00, 1.4409e+02, 7.9969e+01, 3.4010e+01, 1.1921e-05,\n",
            "        4.8596e+01, 1.8937e+01, 5.7903e+01, 4.0158e+01, 9.4331e+01, 1.2748e+01,\n",
            "        4.3888e+01, 7.0171e+01, 1.1359e+02, 8.8632e+00, 4.8596e+01, 3.6127e+01,\n",
            "        2.4837e+01, 9.4074e+00, 3.9406e+01, 4.0158e+01, 3.2815e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([8.6440e-05, 1.0158e-10, 7.7744e-16, 0.0000e+00, 1.0158e-10, 9.3283e-18,\n",
            "        4.1493e-27, 1.0434e-05, 1.3102e-38, 4.6153e-34, 8.6440e-05, 2.1388e-09,\n",
            "        1.0158e-10, 1.7292e-08, 1.0008e-15, 1.0983e-12, 5.7929e-36, 5.1872e-38,\n",
            "        4.8253e-20, 1.1814e-37, 1.0012e-26, 3.9555e-10, 1.0158e-10, 2.8090e-37,\n",
            "        8.0780e-17, 4.2774e-15, 1.1214e-08, 1.0983e-12, 1.3749e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  7\n",
            "ce_loss:  tensor([3.2153e+01, 4.4087e+01, 3.3239e+01, 1.4629e+02, 4.4087e+01, 1.1476e+01,\n",
            "        1.1361e+02, 2.5695e+00, 1.3775e+02, 7.6312e+01, 3.2153e+01, 6.6757e-06,\n",
            "        4.4087e+01, 1.5597e+01, 5.3609e+01, 3.5886e+01, 8.9677e+01, 1.1759e+01,\n",
            "        4.0825e+01, 6.6155e+01, 1.0816e+02, 7.5149e+00, 4.4087e+01, 3.4263e+01,\n",
            "        2.3218e+01, 7.5782e+00, 3.5800e+01, 3.5886e+01, 3.0683e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.1005e-04, 5.1548e-11, 9.3912e-16, 0.0000e+00, 5.1548e-11, 9.0561e-18,\n",
            "        1.4918e-27, 1.1225e-05, 7.4976e-39, 5.6524e-34, 1.1005e-04, 3.6655e-09,\n",
            "        5.1548e-11, 1.9620e-08, 4.9293e-16, 5.9008e-13, 7.7354e-36, 7.2068e-38,\n",
            "        2.2751e-20, 9.2832e-38, 4.3627e-27, 3.7771e-10, 5.1548e-11, 4.9585e-37,\n",
            "        7.2501e-17, 4.0019e-15, 6.0169e-09, 5.9008e-13, 1.4340e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  8\n",
            "ce_loss:  tensor([3.2166e+01, 3.9352e+01, 3.0879e+01, 1.4076e+02, 3.9352e+01, 1.0441e+01,\n",
            "        1.0791e+02, 2.6108e+00, 1.3175e+02, 7.2063e+01, 3.2166e+01, 1.0133e-05,\n",
            "        3.9352e+01, 1.6618e+01, 5.0377e+01, 3.2789e+01, 8.5046e+01, 9.6797e+00,\n",
            "        3.8293e+01, 6.2264e+01, 1.0274e+02, 7.1608e+00, 3.9352e+01, 3.2438e+01,\n",
            "        2.0866e+01, 7.1068e+00, 3.1948e+01, 3.2789e+01, 3.0697e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.5413e-04, 2.4821e-11, 4.7479e-16, 0.0000e+00, 2.4821e-11, 9.8035e-18,\n",
            "        5.4667e-28, 1.3350e-05, 3.1532e-39, 8.7733e-34, 1.5413e-04, 6.6402e-09,\n",
            "        2.4821e-11, 1.4917e-08, 3.1224e-16, 3.4873e-13, 8.1030e-36, 1.9693e-37,\n",
            "        1.0023e-20, 5.7106e-38, 1.4431e-27, 8.3776e-10, 2.4821e-11, 7.4939e-37,\n",
            "        5.7613e-17, 4.1681e-15, 6.1339e-09, 3.4873e-13, 1.6802e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  9\n",
            "ce_loss:  tensor([3.0319e+01, 3.5838e+01, 2.8099e+01, 1.3528e+02, 3.5838e+01, 1.0520e+01,\n",
            "        1.0230e+02, 2.4138e+00, 1.2573e+02, 6.8229e+01, 3.0319e+01, 5.4836e-06,\n",
            "        3.5838e+01, 1.3429e+01, 4.6459e+01, 2.9519e+01, 8.0610e+01, 8.0906e+00,\n",
            "        3.6513e+01, 5.9381e+01, 9.8085e+01, 6.0780e+00, 3.5838e+01, 3.0702e+01,\n",
            "        1.8984e+01, 6.2101e+00, 2.9006e+01, 2.9519e+01, 2.8741e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.9385e-04, 9.2558e-12, 4.6361e-16, 0.0000e+00, 9.2558e-12, 1.9438e-17,\n",
            "        1.3779e-28, 1.4175e-05, 1.5986e-39, 1.3124e-33, 1.9385e-04, 1.1332e-08,\n",
            "        9.2558e-12, 1.4697e-08, 1.9281e-16, 2.9821e-13, 6.5077e-36, 3.3926e-37,\n",
            "        8.0055e-21, 2.8475e-38, 3.6321e-28, 5.7929e-10, 9.2558e-12, 1.0145e-36,\n",
            "        4.4128e-17, 3.3585e-15, 3.5561e-09, 2.9821e-13, 1.7360e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  10\n",
            "ce_loss:  tensor([3.0361e+01, 3.2539e+01, 2.7095e+01, 1.2994e+02, 3.2539e+01, 9.1584e+00,\n",
            "        9.6984e+01, 2.5860e+00, 1.2004e+02, 6.5066e+01, 3.0361e+01, 8.8214e-06,\n",
            "        3.2539e+01, 1.3960e+01, 4.2792e+01, 2.6997e+01, 7.6399e+01, 6.9529e+00,\n",
            "        3.4944e+01, 5.6746e+01, 9.3713e+01, 5.3933e+00, 3.2539e+01, 2.9421e+01,\n",
            "        1.7172e+01, 5.9933e+00, 2.5695e+01, 2.6997e+01, 2.8654e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.6685e-04, 6.5668e-12, 3.0685e-16, 0.0000e+00, 6.5668e-12, 1.8554e-17,\n",
            "        3.1733e-29, 1.6543e-05, 6.2563e-40, 1.3894e-33, 2.6685e-04, 2.1211e-08,\n",
            "        6.5668e-12, 9.2419e-09, 1.3699e-16, 1.6664e-13, 4.6991e-36, 5.2027e-37,\n",
            "        2.2694e-21, 2.6258e-38, 1.5758e-28, 1.2546e-09, 6.5668e-12, 1.2218e-36,\n",
            "        2.9813e-17, 4.2569e-15, 2.2972e-09, 1.6664e-13, 2.0071e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  11\n",
            "ce_loss:  tensor([2.8225e+01, 2.9476e+01, 2.4512e+01, 1.2471e+02, 2.9476e+01, 8.7359e+00,\n",
            "        9.2509e+01, 2.0550e+00, 1.1476e+02, 6.1659e+01, 2.8225e+01, 5.7220e-06,\n",
            "        2.9476e+01, 1.1969e+01, 3.9288e+01, 2.3714e+01, 7.2707e+01, 5.6736e+00,\n",
            "        3.2897e+01, 5.3758e+01, 8.9624e+01, 4.9634e+00, 2.9476e+01, 2.7452e+01,\n",
            "        1.5670e+01, 5.1828e+00, 2.5034e+01, 2.3714e+01, 2.6819e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.3075e-04, 6.5836e-12, 1.9371e-16, 0.0000e+00, 6.5836e-12, 4.0721e-17,\n",
            "        1.3238e-29, 1.7419e-05, 4.4818e-40, 1.4603e-33, 3.3075e-04, 3.8476e-08,\n",
            "        6.5836e-12, 9.2313e-09, 6.0136e-17, 1.3758e-13, 3.0403e-36, 8.8526e-37,\n",
            "        3.0239e-21, 3.7140e-38, 1.1006e-28, 9.7228e-10, 6.5836e-12, 1.2623e-36,\n",
            "        1.8949e-17, 3.2929e-15, 9.9868e-10, 1.3758e-13, 2.0726e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  12\n",
            "ce_loss:  tensor([2.8177e+01, 2.6952e+01, 2.2112e+01, 1.1949e+02, 2.6952e+01, 9.5389e+00,\n",
            "        8.7798e+01, 2.3999e+00, 1.0980e+02, 5.8715e+01, 2.8177e+01, 7.1525e-06,\n",
            "        2.6952e+01, 1.0179e+01, 3.6835e+01, 2.1433e+01, 6.8981e+01, 4.8317e+00,\n",
            "        3.1625e+01, 5.1693e+01, 8.6030e+01, 5.0014e+00, 2.6952e+01, 2.5698e+01,\n",
            "        1.4825e+01, 3.7228e+00, 2.3241e+01, 2.1433e+01, 2.6577e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.4859e-04, 2.3624e-12, 1.4023e-16, 0.0000e+00, 2.3624e-12, 2.7602e-17,\n",
            "        2.2904e-30, 2.0128e-05, 1.4820e-40, 1.1573e-33, 4.4859e-04, 7.4124e-08,\n",
            "        2.3624e-12, 1.0043e-08, 2.9125e-17, 7.7540e-14, 2.4236e-36, 2.1854e-36,\n",
            "        2.0070e-21, 2.1498e-38, 3.3468e-29, 1.4016e-09, 2.3624e-12, 1.1834e-36,\n",
            "        2.5482e-17, 3.9978e-15, 2.5103e-09, 7.7540e-14, 2.3989e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  13\n",
            "ce_loss:  tensor([2.6093e+01, 2.4256e+01, 2.1059e+01, 1.1439e+02, 2.4256e+01, 7.5106e+00,\n",
            "        8.3343e+01, 1.8842e+00, 1.0444e+02, 5.5686e+01, 2.6093e+01, 5.0068e-06,\n",
            "        2.4256e+01, 9.5717e+00, 3.5082e+01, 2.2721e+01, 6.5309e+01, 4.1872e+00,\n",
            "        2.9868e+01, 4.9893e+01, 8.1769e+01, 3.9296e+00, 2.4256e+01, 2.4055e+01,\n",
            "        1.3727e+01, 3.5677e+00, 2.0930e+01, 2.2721e+01, 2.4829e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.4871e-04, 2.4888e-12, 1.5264e-16, 0.0000e+00, 2.4888e-12, 5.9678e-17,\n",
            "        1.0077e-30, 2.0913e-05, 8.2241e-41, 8.9597e-34, 5.4871e-04, 1.3197e-07,\n",
            "        2.4888e-12, 1.2445e-08, 2.4968e-17, 4.2783e-14, 1.6511e-36, 2.6825e-36,\n",
            "        9.2691e-22, 5.4920e-38, 1.5752e-29, 1.0312e-09, 2.4888e-12, 9.6559e-37,\n",
            "        1.4052e-17, 3.0484e-15, 1.7410e-09, 4.2783e-14, 2.4416e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  14\n",
            "ce_loss:  tensor([2.6116e+01, 2.2150e+01, 1.9908e+01, 1.0945e+02, 2.2150e+01, 8.0429e+00,\n",
            "        7.9491e+01, 2.1196e+00, 9.9586e+01, 5.3317e+01, 2.6116e+01, 4.8876e-06,\n",
            "        2.2150e+01, 1.0518e+01, 3.2422e+01, 1.9530e+01, 6.1778e+01, 3.0575e+00,\n",
            "        2.8138e+01, 4.7200e+01, 7.7675e+01, 3.6365e+00, 2.2150e+01, 2.2730e+01,\n",
            "        1.2610e+01, 2.1501e+00, 2.0194e+01, 1.9530e+01, 2.4554e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3267e-04, 1.7433e-12, 5.5143e-17, 0.0000e+00, 1.7433e-12, 4.9468e-17,\n",
            "        3.6823e-31, 2.3968e-05, 3.4237e-41, 2.3987e-33, 7.3267e-04, 2.4630e-07,\n",
            "        1.7433e-12, 1.0458e-08, 1.2749e-17, 3.9192e-14, 8.6367e-37, 8.1521e-36,\n",
            "        8.5610e-22, 4.8823e-38, 6.1725e-30, 1.6726e-09, 1.7433e-12, 7.6385e-37,\n",
            "        1.7523e-17, 3.5995e-15, 1.9369e-09, 3.9192e-14, 2.8324e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  15\n",
            "ce_loss:  tensor([2.3942e+01, 2.1279e+01, 1.9508e+01, 1.0500e+02, 2.1279e+01, 6.2354e+00,\n",
            "        7.5647e+01, 1.7008e+00, 9.5483e+01, 5.1006e+01, 2.3942e+01, 3.3379e-06,\n",
            "        2.1279e+01, 9.0695e+00, 3.0269e+01, 1.8889e+01, 5.8730e+01, 3.1739e+00,\n",
            "        2.7514e+01, 4.5701e+01, 7.4198e+01, 2.9821e+00, 2.1279e+01, 2.2061e+01,\n",
            "        1.4937e+01, 2.3869e+00, 1.8083e+01, 1.8889e+01, 2.2789e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([8.8628e-04, 8.5598e-13, 1.3578e-16, 0.0000e+00, 8.5598e-13, 9.3441e-17,\n",
            "        7.0917e-32, 2.5586e-05, 1.2514e-41, 1.4156e-33, 8.8628e-04, 4.2383e-07,\n",
            "        8.5598e-13, 1.0019e-08, 1.0728e-17, 3.9246e-14, 5.6956e-37, 8.7475e-36,\n",
            "        2.5507e-22, 9.0462e-38, 1.7802e-30, 1.0965e-09, 8.5598e-13, 8.3105e-37,\n",
            "        2.2088e-17, 2.5046e-15, 1.9703e-09, 3.9246e-14, 2.8605e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  16\n",
            "ce_loss:  tensor([2.4206e+01, 1.9655e+01, 1.8462e+01, 1.0049e+02, 1.9655e+01, 7.3036e+00,\n",
            "        7.1824e+01, 2.4572e+00, 9.0619e+01, 4.8324e+01, 2.4206e+01, 3.5763e-06,\n",
            "        1.9655e+01, 6.3381e+00, 2.8099e+01, 1.7080e+01, 5.6461e+01, 1.9973e+00,\n",
            "        2.6865e+01, 4.3609e+01, 7.0732e+01, 1.6490e+00, 1.9655e+01, 2.0710e+01,\n",
            "        1.2041e+01, 1.4656e+00, 1.7988e+01, 1.7080e+01, 2.2704e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.1681e-03, 3.0480e-12, 8.9182e-17, 0.0000e+00, 3.0480e-12, 8.1178e-17,\n",
            "        1.7466e-32, 2.8984e-05, 6.6800e-42, 3.5487e-33, 1.1681e-03, 7.5470e-07,\n",
            "        3.0480e-12, 1.9064e-08, 1.5783e-17, 3.6753e-14, 2.0194e-36, 2.3217e-35,\n",
            "        5.6135e-22, 3.7848e-38, 4.7865e-31, 2.0036e-09, 3.0480e-12, 6.2851e-37,\n",
            "        2.4554e-17, 3.0089e-15, 3.1004e-09, 3.6753e-14, 3.3073e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  17\n",
            "ce_loss:  tensor([2.1977e+01, 1.8071e+01, 1.9891e+01, 9.7608e+01, 1.8071e+01, 5.0471e+00,\n",
            "        6.8821e+01, 1.3486e+00, 8.6058e+01, 4.6682e+01, 2.1977e+01, 2.5034e-06,\n",
            "        1.8071e+01, 5.7812e+00, 2.6443e+01, 1.7092e+01, 5.3780e+01, 1.2412e+00,\n",
            "        2.4849e+01, 4.1451e+01, 6.7235e+01, 1.3668e+00, 1.8071e+01, 1.9215e+01,\n",
            "        1.3407e+01, 8.3890e-01, 1.6676e+01, 1.7092e+01, 2.0778e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.3926e-03, 2.3270e-12, 9.1585e-17, 0.0000e+00, 2.3270e-12, 1.4139e-16,\n",
            "        2.5736e-32, 2.9693e-05, 2.7690e-42, 1.8196e-33, 1.3926e-03, 1.2466e-06,\n",
            "        2.3270e-12, 1.9287e-08, 6.2643e-18, 5.3939e-14, 8.9073e-37, 3.7610e-35,\n",
            "        5.4865e-22, 6.6769e-38, 2.1266e-31, 3.1841e-09, 2.3270e-12, 5.6177e-37,\n",
            "        2.7105e-17, 2.7090e-15, 2.4707e-09, 5.3939e-14, 3.3383e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  18\n",
            "ce_loss:  tensor([2.2013e+01, 1.6773e+01, 1.6994e+01, 9.3360e+01, 1.6773e+01, 5.4467e+00,\n",
            "        6.6223e+01, 1.9694e+00, 8.2025e+01, 4.3745e+01, 2.2013e+01, 2.5034e-06,\n",
            "        1.6773e+01, 7.0613e+00, 2.5578e+01, 1.5793e+01, 5.0919e+01, 1.0557e+00,\n",
            "        2.5027e+01, 3.9658e+01, 6.4660e+01, 1.2907e+00, 1.6773e+01, 1.8592e+01,\n",
            "        9.9354e+00, 5.7871e-01, 1.5751e+01, 1.5793e+01, 2.0639e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.8137e-03, 3.1064e-12, 6.7468e-17, 0.0000e+00, 3.1064e-12, 2.5256e-16,\n",
            "        2.3421e-33, 3.3514e-05, 1.7698e-42, 3.2529e-33, 1.8137e-03, 2.1270e-06,\n",
            "        3.1064e-12, 1.5352e-08, 5.0899e-18, 3.0001e-14, 2.1347e-36, 4.4845e-35,\n",
            "        8.4475e-22, 3.0566e-38, 3.7930e-31, 2.5292e-09, 3.1064e-12, 3.5964e-37,\n",
            "        2.7210e-17, 1.9341e-15, 2.6295e-09, 3.0001e-14, 3.8426e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  19\n",
            "ce_loss:  tensor([1.9672e+01, 1.5918e+01, 1.6716e+01, 9.0029e+01, 1.5918e+01, 5.1999e+00,\n",
            "        6.3207e+01, 1.3196e+00, 7.8904e+01, 4.2088e+01, 1.9672e+01, 2.0266e-06,\n",
            "        1.5918e+01, 6.3989e+00, 2.3160e+01, 1.3436e+01, 4.9448e+01, 5.8847e-01,\n",
            "        2.2852e+01, 3.8102e+01, 6.1725e+01, 6.0001e-01, 1.5918e+01, 1.7096e+01,\n",
            "        9.7839e+00, 2.7849e-01, 1.4825e+01, 1.3436e+01, 1.8822e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.1455e-03, 1.4460e-12, 9.5575e-17, 0.0000e+00, 1.4460e-12, 1.9964e-16,\n",
            "        4.5558e-33, 3.5045e-05, 3.5313e-43, 5.9003e-33, 2.1455e-03, 3.3581e-06,\n",
            "        1.4460e-12, 1.1013e-08, 5.4618e-18, 7.5676e-14, 6.8760e-37, 9.3766e-35,\n",
            "        2.9113e-22, 3.1284e-38, 1.2031e-31, 4.0008e-09, 1.4460e-12, 2.9233e-37,\n",
            "        4.2729e-17, 1.8455e-15, 1.8013e-09, 7.5676e-14, 3.8399e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  20\n",
            "ce_loss:  tensor([1.9643e+01, 1.4240e+01, 1.4835e+01, 8.6439e+01, 1.4240e+01, 4.0998e+00,\n",
            "        6.0304e+01, 2.2266e+00, 7.5410e+01, 3.9924e+01, 1.9643e+01, 1.7881e-06,\n",
            "        1.4240e+01, 5.4978e+00, 2.1828e+01, 1.3610e+01, 4.7861e+01, 4.0302e-01,\n",
            "        2.2212e+01, 3.6581e+01, 5.9404e+01, 6.8242e-01, 1.4240e+01, 1.5850e+01,\n",
            "        8.6865e+00, 2.5945e-01, 1.3430e+01, 1.3610e+01, 1.8572e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.7405e-03, 4.1643e-12, 8.5173e-17, 0.0000e+00, 4.1643e-12, 4.6201e-16,\n",
            "        1.7676e-33, 3.9155e-05, 1.4027e-42, 3.1230e-33, 2.7405e-03, 5.5701e-06,\n",
            "        4.1643e-12, 8.1600e-09, 3.2419e-18, 5.0746e-14, 6.6765e-36, 1.2883e-34,\n",
            "        2.1649e-22, 1.8538e-38, 1.3000e-31, 3.2686e-09, 4.1643e-12, 2.0776e-37,\n",
            "        2.2764e-17, 1.2899e-15, 2.0354e-09, 5.0746e-14, 4.4077e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  21\n",
            "ce_loss:  tensor([1.7375e+01, 1.4344e+01, 1.4716e+01, 8.2429e+01, 1.4344e+01, 5.1285e+00,\n",
            "        5.6939e+01, 9.9396e-01, 7.2113e+01, 3.8089e+01, 1.7375e+01, 1.4305e-06,\n",
            "        1.4344e+01, 3.2504e+00, 2.0137e+01, 1.4417e+01, 4.5695e+01, 2.2342e-01,\n",
            "        2.1870e+01, 3.3889e+01, 5.7013e+01, 3.5319e-01, 1.4344e+01, 1.5034e+01,\n",
            "        8.0217e+00, 1.1194e-01, 1.2279e+01, 1.4417e+01, 1.6923e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.2348e-03, 2.5751e-12, 8.9800e-17, 0.0000e+00, 2.5751e-12, 3.1245e-16,\n",
            "        2.2930e-33, 3.9933e-05, 6.6141e-43, 7.6455e-33, 3.2348e-03, 8.5912e-06,\n",
            "        2.5751e-12, 1.1890e-08, 1.8657e-18, 4.5685e-14, 1.0423e-35, 3.8489e-34,\n",
            "        4.5002e-22, 4.1106e-38, 1.9121e-32, 4.4103e-09, 2.5751e-12, 1.1408e-37,\n",
            "        2.3778e-17, 1.2894e-15, 2.2463e-09, 4.5685e-14, 4.3998e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  22\n",
            "ce_loss:  tensor([1.7484e+01, 1.1966e+01, 1.4026e+01, 7.9643e+01, 1.1966e+01, 2.9334e+00,\n",
            "        5.5441e+01, 1.6438e+00, 6.8557e+01, 3.6389e+01, 1.7484e+01, 1.1921e-06,\n",
            "        1.1966e+01, 2.5427e+00, 1.9675e+01, 1.3335e+01, 4.6571e+01, 1.6399e-01,\n",
            "        2.1412e+01, 3.3621e+01, 5.3585e+01, 2.9682e-01, 1.1966e+01, 1.3578e+01,\n",
            "        7.6932e+00, 8.9878e-02, 1.0006e+01, 1.3335e+01, 1.6574e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.0346e-03, 4.2906e-12, 7.5003e-17, 0.0000e+00, 4.2906e-12, 4.6089e-16,\n",
            "        4.0105e-34, 4.4590e-05, 2.0417e-42, 3.1086e-33, 4.0346e-03, 1.3715e-05,\n",
            "        4.2906e-12, 1.2899e-08, 7.7742e-18, 2.8883e-14, 1.4200e-35, 8.3001e-34,\n",
            "        5.1630e-22, 3.4159e-38, 3.0460e-32, 3.3235e-09, 4.2906e-12, 8.2443e-38,\n",
            "        5.0444e-17, 1.2127e-15, 4.0775e-09, 2.8883e-14, 5.0113e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  23\n",
            "ce_loss:  tensor([1.5023e+01, 1.2180e+01, 1.5112e+01, 7.5552e+01, 1.2180e+01, 4.1376e+00,\n",
            "        5.2433e+01, 6.7151e-01, 6.6211e+01, 3.3920e+01, 1.5023e+01, 8.3446e-07,\n",
            "        1.2180e+01, 4.8253e+00, 1.8579e+01, 1.0727e+01, 4.2628e+01, 1.5910e-01,\n",
            "        2.2487e+01, 3.4581e+01, 5.1841e+01, 1.2301e-01, 1.2180e+01, 1.2759e+01,\n",
            "        7.6965e+00, 5.5545e-02, 1.1458e+01, 1.0727e+01, 1.5041e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.6961e-03, 3.6705e-12, 8.2787e-17, 0.0000e+00, 3.6705e-12, 8.7444e-16,\n",
            "        4.1425e-34, 4.5461e-05, 3.1669e-43, 6.9690e-33, 4.6961e-03, 2.0449e-05,\n",
            "        3.6705e-12, 7.7224e-09, 2.9832e-18, 3.7101e-14, 2.1275e-35, 7.2055e-34,\n",
            "        3.5244e-22, 3.9902e-38, 9.5511e-33, 5.3268e-09, 3.6705e-12, 5.2009e-38,\n",
            "        2.4901e-17, 1.0758e-15, 2.0740e-09, 3.7101e-14, 4.9443e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  24\n",
            "ce_loss:  tensor([1.5234e+01, 1.0495e+01, 1.3032e+01, 7.2154e+01, 1.0495e+01, 4.9132e+00,\n",
            "        5.0659e+01, 1.2283e+00, 6.2259e+01, 3.2701e+01, 1.5234e+01, 7.1526e-07,\n",
            "        1.0495e+01, 3.5272e+00, 2.0377e+01, 1.0060e+01, 3.9933e+01, 9.8363e-02,\n",
            "        1.9971e+01, 3.0088e+01, 4.9081e+01, 7.1411e-02, 1.0495e+01, 1.1994e+01,\n",
            "        6.5052e+00, 3.2479e-02, 9.2378e+00, 1.0060e+01, 1.4734e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.7992e-03, 5.5766e-12, 5.8388e-17, 0.0000e+00, 5.5766e-12, 5.6796e-16,\n",
            "        3.6854e-33, 5.0317e-05, 7.6511e-43, 5.3774e-33, 5.7992e-03, 3.1751e-05,\n",
            "        5.5766e-12, 5.0452e-09, 4.1585e-18, 4.4548e-14, 1.3965e-34, 1.4559e-33,\n",
            "        3.7221e-22, 6.3653e-38, 6.8484e-33, 7.4569e-09, 5.5766e-12, 3.1058e-38,\n",
            "        5.1644e-17, 1.2595e-15, 1.7578e-09, 4.4548e-14, 5.6502e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  25\n",
            "ce_loss:  tensor([1.2680e+01, 1.1741e+01, 1.1590e+01, 6.8885e+01, 1.1741e+01, 2.6597e+00,\n",
            "        4.9908e+01, 5.5398e-01, 6.0348e+01, 3.0033e+01, 1.2680e+01, 5.9605e-07,\n",
            "        1.1741e+01, 1.2849e+00, 1.6576e+01, 9.1883e+00, 4.2379e+01, 7.6340e-02,\n",
            "        2.1253e+01, 2.9492e+01, 4.8575e+01, 3.3864e-02, 1.1741e+01, 1.0874e+01,\n",
            "        6.4180e+00, 1.7458e-02, 7.8190e+00, 9.1883e+00, 1.2948e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([6.6641e-03, 2.7522e-12, 6.9669e-17, 0.0000e+00, 2.7522e-12, 1.1795e-15,\n",
            "        2.6086e-34, 5.1658e-05, 1.4125e-42, 9.7499e-33, 6.6641e-03, 4.5891e-05,\n",
            "        2.7522e-12, 6.1870e-09, 4.0130e-18, 4.9660e-14, 1.3582e-34, 2.3120e-33,\n",
            "        2.2422e-22, 2.9567e-38, 1.0680e-32, 9.0548e-09, 2.7522e-12, 1.7192e-38,\n",
            "        4.8571e-17, 1.0179e-15, 2.8158e-09, 4.9660e-14, 5.5871e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  26\n",
            "ce_loss:  tensor([1.2734e+01, 8.5219e+00, 1.0903e+01, 6.4947e+01, 8.5219e+00, 3.6707e+00,\n",
            "        4.7012e+01, 1.1825e+00, 6.0341e+01, 2.9155e+01, 1.2734e+01, 4.7684e-07,\n",
            "        8.5219e+00, 6.3199e-01, 1.5813e+01, 8.8044e+00, 3.8916e+01, 5.7303e-02,\n",
            "        1.9466e+01, 2.8537e+01, 4.7324e+01, 1.6871e-02, 8.5219e+00, 1.0342e+01,\n",
            "        7.1414e+00, 9.4348e-03, 7.3639e+00, 8.8044e+00, 1.2644e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([8.0890e-03, 4.8379e-12, 6.4162e-17, 0.0000e+00, 4.8379e-12, 7.1827e-16,\n",
            "        2.1448e-33, 5.6586e-05, 1.2261e-42, 1.0148e-32, 8.0890e-03, 6.9034e-05,\n",
            "        4.8379e-12, 5.3085e-09, 7.1603e-18, 4.5760e-14, 2.1109e-34, 2.4000e-33,\n",
            "        3.4968e-22, 1.0380e-37, 8.1513e-33, 1.7499e-08, 4.8379e-12, 8.9247e-39,\n",
            "        5.2879e-17, 8.6078e-16, 2.4480e-09, 4.5760e-14, 6.3072e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  27\n",
            "ce_loss:  tensor([1.0361e+01, 7.8391e+00, 9.8513e+00, 6.2402e+01, 7.8391e+00, 1.6850e+00,\n",
            "        4.6601e+01, 3.5696e-01, 5.6101e+01, 3.0166e+01, 1.0361e+01, 3.5763e-07,\n",
            "        7.8391e+00, 2.6323e-01, 1.5350e+01, 7.3644e+00, 3.6291e+01, 4.3522e-02,\n",
            "        1.7418e+01, 2.7762e+01, 4.4931e+01, 5.5149e-03, 7.8391e+00, 9.8083e+00,\n",
            "        5.2764e+00, 5.0051e-03, 5.4537e+00, 7.3644e+00, 1.0936e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([9.2133e-03, 1.0141e-11, 6.8842e-17, 0.0000e+00, 1.0141e-11, 1.2028e-15,\n",
            "        2.6078e-33, 5.7473e-05, 1.1743e-42, 1.5095e-32, 9.2133e-03, 9.7909e-05,\n",
            "        1.0141e-11, 5.9781e-09, 3.1024e-18, 4.8063e-14, 3.8043e-33, 3.8146e-33,\n",
            "        4.2782e-22, 1.3980e-37, 1.0502e-32, 3.0315e-08, 1.0141e-11, 1.0149e-38,\n",
            "        3.8288e-17, 8.8940e-16, 2.9232e-09, 4.8063e-14, 6.2402e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  28\n",
            "ce_loss:  tensor([9.9003e+00, 7.6373e+00, 1.0169e+01, 5.9955e+01, 7.6373e+00, 4.1579e+00,\n",
            "        4.6417e+01, 8.1046e-01, 5.2726e+01, 2.7399e+01, 9.9003e+00, 3.5763e-07,\n",
            "        7.6373e+00, 2.3391e-01, 1.3012e+01, 8.7862e+00, 3.6519e+01, 3.4934e-02,\n",
            "        1.7323e+01, 2.8053e+01, 4.4478e+01, 3.2658e-03, 7.6373e+00, 8.9009e+00,\n",
            "        6.1685e+00, 1.8613e-03, 6.2471e+00, 8.7862e+00, 1.0504e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.1004e-02, 4.0630e-12, 4.7888e-17, 0.0000e+00, 4.0630e-12, 6.7569e-16,\n",
            "        1.7675e-33, 6.2701e-05, 9.5821e-42, 1.2884e-32, 1.1004e-02, 1.4151e-04,\n",
            "        4.0630e-12, 5.1484e-09, 6.9825e-18, 2.2826e-14, 9.1102e-34, 4.1620e-33,\n",
            "        1.3123e-21, 1.1983e-37, 9.2683e-33, 6.5556e-08, 4.0630e-12, 5.3180e-39,\n",
            "        4.0916e-17, 7.6160e-16, 2.6785e-09, 2.2826e-14, 6.9745e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  29\n",
            "ce_loss:  tensor([7.7026e+00, 5.8384e+00, 8.0544e+00, 5.6255e+01, 5.8384e+00, 1.0837e+00,\n",
            "        4.3904e+01, 2.0897e-01, 5.1982e+01, 2.4985e+01, 7.7026e+00, 2.3842e-07,\n",
            "        5.8384e+00, 7.2960e-02, 1.2820e+01, 5.7877e+00, 3.3960e+01, 2.4265e-02,\n",
            "        1.7137e+01, 2.5890e+01, 4.2889e+01, 2.4790e-03, 5.8384e+00, 9.1140e+00,\n",
            "        4.4160e+00, 7.8480e-04, 5.3155e+00, 5.7877e+00, 8.9914e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.2416e-02, 7.4410e-12, 7.6589e-17, 0.0000e+00, 7.4410e-12, 9.3655e-16,\n",
            "        1.2658e-33, 6.3658e-05, 3.0100e-42, 6.2212e-32, 1.2416e-02, 1.9358e-04,\n",
            "        7.4410e-12, 5.0962e-09, 1.7613e-17, 2.9979e-14, 1.3904e-32, 8.5307e-33,\n",
            "        4.0240e-22, 1.5704e-37, 5.9049e-33, 1.2878e-07, 7.4410e-12, 5.3848e-39,\n",
            "        3.4239e-17, 7.6520e-16, 1.3385e-09, 2.9979e-14, 6.8888e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  30\n",
            "ce_loss:  tensor([7.2468e+00, 5.5440e+00, 8.6821e+00, 5.4924e+01, 5.5440e+00, 2.3846e+00,\n",
            "        4.0669e+01, 5.2440e-01, 4.8612e+01, 2.4741e+01, 7.2468e+00, 2.3842e-07,\n",
            "        5.5440e+00, 3.3122e-02, 1.2455e+01, 5.8726e+00, 3.3815e+01, 2.5131e-02,\n",
            "        1.6011e+01, 2.6187e+01, 3.9562e+01, 1.6334e-03, 5.5440e+00, 7.6882e+00,\n",
            "        4.8530e+00, 4.6314e-04, 3.4738e+00, 5.8726e+00, 8.4373e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.4552e-02, 1.0046e-11, 1.1167e-16, 0.0000e+00, 1.0046e-11, 1.4180e-15,\n",
            "        1.1070e-32, 6.8811e-05, 1.8839e-41, 4.2485e-32, 1.4552e-02, 2.7062e-04,\n",
            "        1.0046e-11, 4.5800e-09, 6.8134e-18, 4.6126e-14, 1.5732e-31, 2.1699e-32,\n",
            "        1.0999e-21, 1.9188e-37, 7.1962e-32, 2.3304e-07, 1.0046e-11, 2.3596e-39,\n",
            "        3.2337e-17, 7.8304e-16, 1.8903e-09, 4.6126e-14, 7.6270e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  31\n",
            "ce_loss:  tensor([4.9094e+00, 6.6864e+00, 7.9782e+00, 5.2993e+01, 6.6864e+00, 2.6414e+00,\n",
            "        4.1439e+01, 1.2373e-01, 4.6590e+01, 2.2250e+01, 4.9094e+00, 1.1921e-07,\n",
            "        6.6864e+00, 1.3552e-02, 1.3447e+01, 5.9865e+00, 3.2641e+01, 1.9355e-02,\n",
            "        1.7466e+01, 2.3921e+01, 4.0607e+01, 1.0901e-03, 6.6864e+00, 7.6519e+00,\n",
            "        3.3442e+00, 1.9143e-04, 4.4208e+00, 5.9865e+00, 7.0630e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.6220e-02, 4.2372e-12, 4.5532e-17, 0.0000e+00, 4.2372e-12, 1.0005e-15,\n",
            "        1.0515e-32, 6.9653e-05, 1.4010e-40, 2.0097e-31, 1.6220e-02, 3.6186e-04,\n",
            "        4.2372e-12, 5.0644e-09, 7.2752e-18, 2.4182e-14, 4.0488e-32, 1.4545e-32,\n",
            "        9.2172e-22, 2.2902e-37, 5.1996e-32, 3.8408e-07, 4.2372e-12, 2.0959e-39,\n",
            "        2.4583e-17, 7.3112e-16, 2.8159e-09, 2.4182e-14, 7.5292e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  32\n",
            "ce_loss:  tensor([4.4832e+00, 4.4800e+00, 6.6056e+00, 5.0166e+01, 4.4800e+00, 7.3508e-01,\n",
            "        3.9132e+01, 3.1141e-01, 4.5876e+01, 2.1857e+01, 4.4832e+00, 1.1921e-07,\n",
            "        4.4800e+00, 4.6255e-03, 1.0730e+01, 3.9075e+00, 3.1078e+01, 1.1133e-02,\n",
            "        1.7139e+01, 2.4838e+01, 3.7923e+01, 7.2382e-04, 4.4800e+00, 6.5686e+00,\n",
            "        2.4609e+00, 1.0943e-04, 4.7691e+00, 3.9075e+00, 6.3101e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.8748e-02, 4.8206e-12, 6.4995e-17, 0.0000e+00, 4.8206e-12, 1.6605e-15,\n",
            "        7.1519e-33, 7.4853e-05, 3.2917e-41, 1.1942e-30, 1.8748e-02, 4.8819e-04,\n",
            "        4.8206e-12, 7.5617e-09, 5.8044e-18, 2.9607e-14, 4.0609e-31, 2.6724e-32,\n",
            "        5.5438e-22, 2.9167e-37, 4.1731e-32, 6.1833e-07, 4.8206e-12, 8.7658e-40,\n",
            "        3.6159e-17, 4.9292e-16, 1.7038e-09, 2.9607e-14, 8.3222e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  33\n",
            "ce_loss:  tensor([2.0850e+00, 4.7386e+00, 6.1405e+00, 4.8316e+01, 4.7386e+00, 1.9009e+00,\n",
            "        3.6119e+01, 6.4286e-02, 4.2708e+01, 2.1072e+01, 2.0850e+00, 1.1921e-07,\n",
            "        4.7386e+00, 1.8949e-03, 9.0711e+00, 5.2002e+00, 3.1163e+01, 8.5015e-03,\n",
            "        1.5955e+01, 2.3311e+01, 3.5171e+01, 4.0702e-04, 4.7386e+00, 6.3566e+00,\n",
            "        2.9377e+00, 5.7219e-05, 2.5661e+00, 5.2002e+00, 5.0255e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.0619e-02, 3.0556e-12, 4.3166e-17, 0.0000e+00, 3.0556e-12, 1.0637e-15,\n",
            "        2.6882e-32, 7.5497e-05, 3.8868e-40, 4.0756e-31, 2.0619e-02, 6.3034e-04,\n",
            "        3.0556e-12, 1.9735e-08, 1.0040e-17, 3.6078e-14, 8.1547e-31, 4.5243e-32,\n",
            "        6.6490e-22, 2.5044e-37, 5.1018e-31, 9.2526e-07, 3.0556e-12, 6.8583e-40,\n",
            "        2.9771e-17, 5.9839e-16, 1.4847e-09, 3.6078e-14, 8.1926e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  34\n",
            "ce_loss:  tensor([1.6846e+00, 3.0193e+00, 8.0543e+00, 4.4719e+01, 3.0193e+00, 3.1581e-01,\n",
            "        3.4001e+01, 1.6410e-01, 4.0802e+01, 1.8953e+01, 1.6846e+00, 1.1921e-07,\n",
            "        3.0193e+00, 1.0344e-03, 7.9534e+00, 6.0331e+00, 3.0757e+01, 6.2645e-03,\n",
            "        1.3999e+01, 2.0465e+01, 3.8563e+01, 2.7152e-04, 3.0193e+00, 5.4370e+00,\n",
            "        1.7562e+00, 1.1741e-04, 4.1742e+00, 6.0331e+00, 4.0988e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.3453e-02, 3.3231e-12, 3.8999e-17, 0.0000e+00, 3.3231e-12, 1.5752e-15,\n",
            "        1.5829e-31, 8.0776e-05, 4.7884e-39, 2.2733e-30, 2.3453e-02, 8.2756e-04,\n",
            "        3.3231e-12, 4.9999e-08, 1.3522e-17, 1.4590e-14, 8.0322e-31, 7.3257e-32,\n",
            "        1.3226e-21, 1.0423e-36, 4.1885e-31, 1.3690e-06, 3.3231e-12, 2.3858e-40,\n",
            "        1.9340e-17, 3.8617e-16, 9.8402e-10, 1.4590e-14, 8.9703e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  35\n",
            "ce_loss:  tensor([2.8706e-01, 2.4744e+00, 5.9123e+00, 4.3348e+01, 2.4744e+00, 8.0279e-02,\n",
            "        3.4305e+01, 3.3500e-02, 4.0827e+01, 1.9541e+01, 2.8706e-01, -0.0000e+00,\n",
            "        2.4744e+00, 7.6253e-04, 7.3373e+00, 2.8939e+00, 2.8659e+01, 4.0092e-03,\n",
            "        1.3534e+01, 2.0156e+01, 3.4904e+01, 1.5663e-04, 2.4744e+00, 5.1107e+00,\n",
            "        1.9217e+00, 2.8371e-05, 1.8174e+00, 2.8939e+00, 3.0214e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.5565e-02, 4.0536e-12, 2.3257e-17, 0.0000e+00, 4.0536e-12, 1.7727e-15,\n",
            "        5.8276e-32, 8.1241e-05, 2.2130e-39, 3.0412e-30, 2.5565e-02, 1.0375e-03,\n",
            "        4.0536e-12, 1.1594e-07, 2.3540e-17, 1.9642e-14, 1.8104e-30, 1.0597e-31,\n",
            "        2.9670e-21, 3.7891e-36, 2.3811e-31, 1.9082e-06, 4.0536e-12, 1.5207e-40,\n",
            "        1.5738e-17, 5.5051e-16, 9.0954e-10, 1.9642e-14, 8.9642e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  36\n",
            "ce_loss:  tensor([4.2514e-01, 4.5562e+00, 3.8287e+00, 4.1690e+01, 4.5562e+00, 4.8421e-02,\n",
            "        3.1226e+01, 8.0305e-02, 3.9974e+01, 1.8591e+01, 4.2514e-01, -0.0000e+00,\n",
            "        4.5562e+00, 3.5244e-04, 6.8710e+00, 6.7786e+00, 2.6148e+01, 2.8925e-03,\n",
            "        1.4468e+01, 2.0000e+01, 3.1663e+01, 1.1038e-04, 4.5562e+00, 4.5093e+00,\n",
            "        1.2376e+00, 2.6583e-05, 1.7388e+00, 6.7786e+00, 1.9307e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.8566e-02, 1.9916e-12, 2.5807e-17, 0.0000e+00, 1.9916e-12, 1.7584e-15,\n",
            "        2.6014e-31, 8.6489e-05, 2.5113e-39, 3.0960e-30, 2.8566e-02, 1.3270e-03,\n",
            "        1.9916e-12, 2.4270e-07, 1.2513e-17, 8.2475e-15, 9.3738e-30, 1.0526e-31,\n",
            "        9.1198e-22, 9.3413e-37, 2.4182e-30, 2.6466e-06, 1.9916e-12, 8.6967e-41,\n",
            "        9.7569e-18, 3.7147e-16, 1.2769e-09, 8.2475e-15, 9.5781e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  37\n",
            "ce_loss:  tensor([2.1956e-02, 1.7368e+00, 3.0148e+00, 4.1442e+01, 1.7368e+00, 1.6917e-02,\n",
            "        3.1129e+01, 1.5805e-02, 3.7407e+01, 1.5842e+01, 2.1956e-02, -0.0000e+00,\n",
            "        1.7368e+00, 3.0513e-04, 5.7654e+00, 3.9140e+00, 2.4994e+01, 2.2121e-03,\n",
            "        1.2597e+01, 1.7842e+01, 3.2357e+01, 6.5325e-05, 1.7368e+00, 3.6060e+00,\n",
            "        6.6441e-01, 1.8477e-05, 3.8078e+00, 3.9140e+00, 1.2252e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.0573e-02, 1.8086e-12, 3.2593e-17, 0.0000e+00, 1.8086e-12, 1.7195e-15,\n",
            "        1.2643e-30, 8.6938e-05, 3.4587e-39, 1.5868e-29, 3.0573e-02, 1.6217e-03,\n",
            "        1.8086e-12, 4.6495e-07, 1.1749e-17, 5.4131e-15, 5.5551e-29, 1.7105e-31,\n",
            "        1.8673e-21, 3.0956e-36, 3.5552e-30, 3.5277e-06, 1.8086e-12, 4.0360e-41,\n",
            "        8.7356e-18, 5.2948e-16, 7.5143e-10, 5.4131e-15, 9.5715e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  38\n",
            "ce_loss:  tensor([4.3266e-02, 2.0502e+00, 2.9672e+00, 3.7828e+01, 2.0502e+00, 8.8074e-03,\n",
            "        3.0251e+01, 4.0702e-02, 3.4553e+01, 1.6038e+01, 4.3266e-02, -0.0000e+00,\n",
            "        2.0502e+00, 1.4280e-04, 6.3330e+00, 3.6932e+00, 2.6418e+01, 1.4466e-03,\n",
            "        1.2655e+01, 1.9750e+01, 3.1337e+01, 4.2557e-05, 2.0502e+00, 3.4506e+00,\n",
            "        8.2110e-01, 7.6294e-06, 7.6769e-01, 3.6932e+00, 5.2498e-01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.3954e-02, 2.5143e-12, 2.1350e-17, 0.0000e+00, 2.5143e-12, 1.4233e-15,\n",
            "        2.1743e-31, 9.1633e-05, 2.9669e-38, 5.9875e-29, 3.3954e-02, 1.9896e-03,\n",
            "        2.5143e-12, 8.5965e-07, 3.9921e-18, 4.2269e-15, 3.6300e-29, 1.3939e-31,\n",
            "        1.9492e-21, 3.0014e-36, 2.7126e-30, 4.6764e-06, 2.5143e-12, 5.3106e-41,\n",
            "        7.4123e-18, 4.8418e-16, 9.1047e-10, 4.2269e-15, 1.0199e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  39\n",
            "ce_loss:  tensor([2.4873e-03, 3.7330e+00, 1.6091e+00, 3.7479e+01, 3.7330e+00, 6.9091e-03,\n",
            "        2.7455e+01, 8.3192e-03, 3.2619e+01, 1.5735e+01, 2.4873e-03, -0.0000e+00,\n",
            "        3.7330e+00, 1.1384e-04, 4.1837e+00, 1.8534e+00, 2.3934e+01, 9.6692e-04,\n",
            "        1.3370e+01, 1.6913e+01, 2.9711e+01, 2.5391e-05, 3.7330e+00, 2.9910e+00,\n",
            "        4.3695e-01, 1.9073e-05, 1.5113e+00, 1.8534e+00, 2.9972e-01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.5778e-02, 1.2825e-12, 2.2566e-17, 0.0000e+00, 1.2825e-12, 1.4712e-15,\n",
            "        1.2216e-30, 9.1803e-05, 1.3790e-37, 2.0975e-29, 3.5778e-02, 2.3387e-03,\n",
            "        1.2825e-12, 1.4717e-06, 5.1473e-18, 5.4417e-15, 7.1946e-29, 1.4501e-31,\n",
            "        1.7000e-21, 2.6163e-36, 2.4897e-30, 5.9785e-06, 1.2825e-12, 1.5811e-41,\n",
            "        4.5711e-18, 4.0201e-16, 1.0007e-09, 5.4417e-15, 9.9954e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  40\n",
            "ce_loss:  tensor([4.5661e-03, 8.7514e-01, 1.0927e+00, 3.4443e+01, 8.7514e-01, 1.0924e-02,\n",
            "        2.7221e+01, 2.1179e-02, 3.3340e+01, 1.3181e+01, 4.5661e-03, -0.0000e+00,\n",
            "        8.7514e-01, 6.4729e-05, 3.0094e+00, 1.1887e+00, 2.1455e+01, 5.9837e-04,\n",
            "        1.3951e+01, 1.5512e+01, 2.7573e+01, 1.6808e-05, 8.7514e-01, 2.3397e+00,\n",
            "        2.5889e-01, 3.3379e-06, 2.4538e+00, 1.1887e+00, 9.4124e-02],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.9034e-02, 1.0613e-12, 1.6150e-17, 0.0000e+00, 1.0613e-12, 9.8479e-16,\n",
            "        3.1632e-30, 9.6064e-05, 1.3218e-37, 6.1097e-29, 3.9034e-02, 2.8031e-03,\n",
            "        1.0613e-12, 2.4502e-06, 6.9174e-18, 6.7700e-15, 3.4055e-28, 1.5288e-31,\n",
            "        8.6679e-22, 8.6900e-36, 1.6199e-29, 7.6001e-06, 1.0613e-12, 2.1586e-41,\n",
            "        3.5350e-18, 5.3906e-16, 5.2902e-10, 6.7700e-15, 1.0710e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  41\n",
            "ce_loss:  tensor([4.2072e-04, 1.0382e+00, 5.8139e-01, 3.1320e+01, 1.0382e+00, 2.9816e-03,\n",
            "        2.6855e+01, 4.5296e-03, 3.0832e+01, 1.3659e+01, 4.2072e-04, -0.0000e+00,\n",
            "        1.0382e+00, 8.2132e-05, 2.9106e+00, 3.3597e+00, 2.3453e+01, 4.8483e-04,\n",
            "        1.3345e+01, 1.6666e+01, 2.8506e+01, 9.8943e-06, 1.0382e+00, 1.9283e+00,\n",
            "        3.2388e-01, 1.3590e-05, 1.6281e-01, 3.3597e+00, 4.5345e-02],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.0972e-02, 1.1842e-12, 1.9053e-17, 1.4013e-45, 1.1842e-12, 1.2045e-15,\n",
            "        8.3008e-31, 9.6028e-05, 2.1516e-37, 6.8499e-29, 4.0972e-02, 3.1869e-03,\n",
            "        1.1842e-12, 3.7852e-06, 6.8752e-18, 2.5143e-15, 2.2092e-28, 1.7783e-31,\n",
            "        6.0428e-22, 5.4445e-36, 9.7868e-30, 9.2728e-06, 1.1842e-12, 1.0845e-41,\n",
            "        1.7345e-18, 3.9470e-16, 6.1286e-10, 2.5143e-15, 1.0479e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  42\n",
            "ce_loss:  tensor([9.3631e-04, 2.2363e+00, 3.4986e-01, 2.9451e+01, 2.2363e+00, 2.1406e-03,\n",
            "        2.3975e+01, 1.0945e-02, 2.8064e+01, 1.3383e+01, 9.3631e-04, -0.0000e+00,\n",
            "        2.2363e+00, 3.6477e-05, 3.1837e+00, 6.6820e-01, 2.0616e+01, 2.5615e-04,\n",
            "        1.0789e+01, 1.4550e+01, 2.6851e+01, 7.0333e-06, 2.2363e+00, 2.1788e+00,\n",
            "        1.7533e-01, 1.5497e-06, 2.8973e-01, 6.6820e-01, 1.7123e-02],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.3889e-02, 5.8873e-13, 1.0363e-17, 1.6816e-44, 5.8873e-13, 9.7665e-16,\n",
            "        3.7618e-30, 9.9985e-05, 1.1453e-36, 5.6611e-29, 4.3889e-02, 3.6924e-03,\n",
            "        5.8873e-13, 5.8632e-06, 2.3669e-18, 2.9319e-15, 1.4521e-28, 1.5882e-31,\n",
            "        9.7847e-22, 5.8528e-36, 1.0411e-29, 1.1419e-05, 5.8873e-13, 1.6203e-41,\n",
            "        1.4696e-18, 5.1345e-16, 7.4587e-10, 2.9319e-15, 1.1165e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  43\n",
            "ce_loss:  tensor([9.4409e-05, 3.0194e-01, 1.1254e-01, 3.0987e+01, 3.0194e-01, 1.3000e-03,\n",
            "        2.3669e+01, 2.4963e-03, 2.7387e+01, 1.1530e+01, 9.4409e-05, -0.0000e+00,\n",
            "        3.0194e-01, 1.2921e-04, 1.7353e+00, 3.9993e-01, 1.8315e+01, 1.4590e-04,\n",
            "        1.0246e+01, 1.5960e+01, 2.4078e+01, 4.0531e-06, 3.0194e-01, 1.5485e+00,\n",
            "        1.7543e-01, 1.3113e-06, 9.4324e-01, 3.9993e-01, 9.4218e-03],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.5787e-02, 7.3709e-13, 1.0722e-17, 2.2421e-44, 7.3709e-13, 7.5565e-16,\n",
            "        1.4779e-29, 9.9615e-05, 3.3349e-36, 1.5076e-28, 4.5787e-02, 4.1058e-03,\n",
            "        7.3709e-13, 8.1182e-06, 2.7665e-18, 2.8583e-15, 6.0011e-28, 2.2247e-31,\n",
            "        1.8151e-21, 4.9457e-36, 5.9817e-29, 1.3512e-05, 7.3709e-13, 7.8192e-42,\n",
            "        9.0827e-19, 3.2678e-16, 4.0356e-10, 2.8583e-15, 1.0918e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  44\n",
            "ce_loss:  tensor([1.9775e-04, 1.1757e+01, 5.0511e-02, 2.6657e+01, 1.1757e+01, 1.3333e-03,\n",
            "        2.3040e+01, 5.5021e-03, 2.7944e+01, 9.6109e+00, 1.9775e-04, -0.0000e+00,\n",
            "        1.1757e+01, 1.9312e-05, 1.2608e+00, 1.3940e+00, 1.7790e+01, 1.1324e-04,\n",
            "        1.0920e+01, 1.4304e+01, 2.6028e+01, 3.4571e-06, 1.1757e+01, 1.1026e+00,\n",
            "        1.5291e-01, 1.0729e-06, 5.0260e-02, 1.3940e+00, 3.9162e-03],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.8183e-02, 6.5128e-13, 8.8440e-18, 4.4842e-44, 6.5128e-13, 4.7590e-16,\n",
            "        5.5016e-30, 1.0349e-04, 2.2411e-36, 4.3764e-28, 4.8183e-02, 4.5908e-03,\n",
            "        6.5128e-13, 1.2083e-05, 3.8247e-18, 1.0507e-15, 1.9968e-27, 2.0831e-31,\n",
            "        8.1431e-22, 2.2512e-36, 3.6632e-29, 1.5983e-05, 6.5128e-13, 3.6434e-42,\n",
            "        4.9500e-19, 3.1180e-16, 4.0437e-10, 1.0507e-15, 1.1480e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  45\n",
            "ce_loss:  tensor([2.2530e-05, 1.3911e+00, 2.6095e-02, 2.3745e+01, 1.3911e+00, 7.3072e-04,\n",
            "        2.0374e+01, 1.3893e-03, 2.5204e+01, 8.9671e+00, 2.2530e-05, -0.0000e+00,\n",
            "        1.3911e+00, 1.9550e-05, 1.7359e+00, 1.5431e-01, 1.8074e+01, 8.9761e-05,\n",
            "        9.3604e+00, 1.2066e+01, 2.3796e+01, 2.1458e-06, 1.3911e+00, 8.5435e-01,\n",
            "        1.1739e-01, 4.7684e-07, 9.8488e-03, 1.5431e-01, 2.3139e-03],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.9518e-02, 3.2267e-13, 6.3700e-18, 2.9147e-43, 3.2267e-13, 5.8684e-16,\n",
            "        1.9878e-29, 1.0236e-04, 3.7643e-36, 8.2332e-28, 4.9518e-02, 5.0315e-03,\n",
            "        3.2267e-13, 1.5852e-05, 1.6977e-18, 1.4029e-15, 7.5630e-28, 2.8392e-31,\n",
            "        1.4907e-21, 5.2866e-36, 3.0824e-29, 1.8405e-05, 3.2267e-13, 3.0352e-42,\n",
            "        3.1951e-19, 2.3566e-16, 5.1688e-10, 1.4029e-15, 1.1276e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  46\n",
            "ce_loss:  tensor([4.8517e-05, 1.3394e-01, 1.4914e-02, 2.5551e+01, 1.3394e-01, 2.0862e-03,\n",
            "        2.0856e+01, 2.9510e-03, 2.2396e+01, 1.0505e+01, 4.8517e-05, -0.0000e+00,\n",
            "        1.3394e-01, 6.6757e-06, 9.5280e-01, 1.1030e+00, 1.5439e+01, 5.1378e-05,\n",
            "        9.3933e+00, 1.1663e+01, 2.0773e+01, 1.4305e-06, 1.3394e-01, 8.9196e-01,\n",
            "        8.1850e-02, 2.3842e-07, 3.7373e-03, 1.1030e+00, 8.9296e-04],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.1362e-02, 3.6044e-13, 7.3234e-18, 2.6485e-43, 3.6044e-13, 3.2312e-16,\n",
            "        2.5778e-29, 1.0581e-04, 2.1903e-35, 1.0198e-27, 5.1362e-02, 5.3605e-03,\n",
            "        3.6044e-13, 2.1922e-05, 1.5293e-18, 1.3612e-15, 2.2862e-27, 2.7455e-31,\n",
            "        1.8307e-21, 1.3558e-35, 1.2478e-28, 2.1220e-05, 3.6044e-13, 1.2163e-42,\n",
            "        1.3610e-19, 1.7939e-16, 5.4456e-10, 1.3612e-15, 1.1738e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  47\n",
            "ce_loss:  tensor([5.8412e-06, 2.6433e+00, 5.8116e-03, 2.1754e+01, 2.6433e+00, 5.7299e-04,\n",
            "        2.0248e+01, 7.6396e-04, 2.1201e+01, 8.0831e+00, 5.8412e-06, -0.0000e+00,\n",
            "        2.6433e+00, 4.8876e-06, 6.1613e-01, 1.4081e+00, 1.8404e+01, 3.9934e-05,\n",
            "        1.0182e+01, 1.1951e+01, 2.0562e+01, 8.3446e-07, 2.6433e+00, 6.0096e-01,\n",
            "        5.1611e-02, 1.3113e-06, 1.8414e-03, 1.4081e+00, 6.4269e-04],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.2143e-02, 4.3594e-13, 4.0898e-18, 4.8065e-43, 4.3594e-13, 3.0884e-16,\n",
            "        1.3302e-29, 1.0432e-04, 9.5519e-35, 1.0215e-27, 5.2143e-02, 5.7237e-03,\n",
            "        4.3594e-13, 2.7039e-05, 1.9551e-18, 6.9022e-16, 1.4852e-27, 3.3198e-31,\n",
            "        7.9618e-22, 4.1905e-36, 3.7715e-28, 2.3846e-05, 4.3594e-13, 1.1435e-42,\n",
            "        8.1654e-20, 1.2175e-16, 1.0516e-09, 6.9022e-16, 1.1466e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  48\n",
            "ce_loss:  tensor([1.5020e-05, 4.3684e-01, 2.5664e-03, 1.8784e+01, 4.3684e-01, 3.7151e-04,\n",
            "        1.9519e+01, 1.5390e-03, 2.2067e+01, 6.4566e+00, 1.5020e-05, -0.0000e+00,\n",
            "        4.3684e-01, 2.1458e-06, 7.9996e-01, 6.3883e-02, 1.6194e+01, 2.4199e-05,\n",
            "        8.2748e+00, 9.6899e+00, 1.9982e+01, 5.9605e-07, 4.3684e-01, 5.8374e-01,\n",
            "        4.2797e-02, 1.1921e-07, 1.3628e-03, 6.3883e-02, 2.1265e-04],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3487e-02, 2.3281e-13, 3.8342e-18, 2.4369e-42, 2.3281e-13, 2.1577e-16,\n",
            "        1.6081e-29, 1.0723e-04, 6.3319e-35, 1.8879e-27, 5.3487e-02, 5.9111e-03,\n",
            "        2.3281e-13, 3.5415e-05, 8.9315e-19, 7.9152e-16, 2.2904e-27, 1.6953e-31,\n",
            "        9.1823e-22, 9.9205e-36, 6.3606e-29, 2.6739e-05, 2.3281e-13, 8.6320e-43,\n",
            "        6.9982e-20, 1.0950e-16, 2.5080e-09, 7.9152e-16, 1.1882e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  49\n",
            "ce_loss:  tensor([1.6689e-06, 7.4897e-02, 9.0963e-04, 1.7914e+01, 7.4897e-02, 3.5542e-04,\n",
            "        1.6537e+01, 4.2561e-04, 1.9473e+01, 9.2550e+00, 1.6689e-06, -0.0000e+00,\n",
            "        7.4897e-02, 1.6689e-06, 2.8926e-01, 5.1430e+00, 1.3285e+01, 1.6212e-05,\n",
            "        9.4689e+00, 1.2538e+01, 1.7569e+01, 3.5763e-07, 7.4897e-02, 4.8088e-01,\n",
            "        3.0286e-02, 1.1921e-07, 8.5675e-04, 5.1430e+00, 1.4721e-04],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3463e-02, 2.1586e-13, 2.8278e-18, 9.5765e-42, 2.1586e-13, 1.8969e-16,\n",
            "        4.7540e-29, 1.0531e-04, 8.5829e-35, 1.5018e-27, 5.3463e-02, 6.0810e-03,\n",
            "        2.1586e-13, 4.1307e-05, 1.1159e-18, 5.4751e-16, 6.7208e-27, 1.2960e-31,\n",
            "        6.5497e-22, 8.2071e-36, 1.7612e-28, 2.9026e-05, 2.1586e-13, 3.1529e-43,\n",
            "        3.0221e-20, 6.6095e-17, 8.3632e-09, 5.4751e-16, 1.1587e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  50\n",
            "ce_loss:  tensor([5.0068e-06, 1.2121e+00, 5.8133e-04, 1.6697e+01, 1.2121e+00, 2.3696e-04,\n",
            "        1.4717e+01, 7.8611e-04, 1.6875e+01, 6.7465e+00, 5.0068e-06, -0.0000e+00,\n",
            "        1.2121e+00, 8.3446e-07, 1.1806e-01, 1.9963e-01, 1.1944e+01, 1.1921e-05,\n",
            "        1.0141e+01, 9.7113e+00, 1.9536e+01, 3.5763e-07, 1.2121e+00, 3.6733e-01,\n",
            "        2.3205e-02, 1.1921e-07, 4.9817e-04, 1.9963e-01, 5.2212e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.4145e-02, 2.0393e-13, 2.5872e-18, 4.5332e-42, 2.0393e-13, 1.1261e-16,\n",
            "        1.4037e-28, 1.0771e-04, 2.9746e-34, 1.1665e-27, 5.4145e-02, 6.0945e-03,\n",
            "        2.0393e-13, 5.2034e-05, 1.4841e-18, 2.9437e-16, 1.2421e-26, 9.5905e-32,\n",
            "        3.5800e-22, 4.5090e-36, 1.1613e-28, 3.1939e-05, 2.0393e-13, 2.1019e-43,\n",
            "        1.7634e-20, 4.9201e-17, 2.6771e-08, 2.9437e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  51\n",
            "ce_loss:  tensor([9.5367e-07, 1.6974e-01, 2.8320e-04, 1.5458e+01, 1.6974e-01, 3.5649e-04,\n",
            "        1.8997e+01, 3.2551e-04, 1.7899e+01, 4.7905e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.6974e-01, 1.1802e-05, 6.8670e-02, 2.7911e-02, 1.2990e+01, 1.0133e-05,\n",
            "        1.0139e+01, 8.5150e+00, 1.8476e+01, 2.3842e-07, 1.6974e-01, 3.5476e-01,\n",
            "        1.8689e-02, 1.7881e-06, 4.0511e-04, 2.7911e-02, 8.4754e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3285e-02, 1.1285e-13, 1.7649e-18, 1.8983e-41, 1.1285e-13, 6.7884e-17,\n",
            "        1.0370e-28, 1.0554e-04, 9.3213e-34, 2.0249e-27, 5.3285e-02, 6.1387e-03,\n",
            "        1.1285e-13, 5.6063e-05, 1.8619e-18, 3.6988e-16, 2.5818e-26, 8.9733e-32,\n",
            "        2.3604e-22, 9.1454e-36, 7.6778e-29, 3.3938e-05, 1.1285e-13, 1.2472e-43,\n",
            "        9.4276e-21, 2.6802e-17, 7.2183e-08, 3.6988e-16, 1.1529e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  52\n",
            "ce_loss:  tensor([5.3644e-06, 6.0739e-02, 3.5816e-04, 1.6893e+01, 6.0739e-02, 5.7967e-04,\n",
            "        1.6219e+01, 7.8742e-04, 1.7682e+01, 4.7661e+00, 5.3644e-06, -0.0000e+00,\n",
            "        6.0739e-02, 4.7684e-07, 3.9877e-02, 3.2007e-02, 1.2870e+01, 1.3709e-05,\n",
            "        7.8504e+00, 8.9780e+00, 1.7868e+01, 2.3842e-07, 6.0739e-02, 4.3767e-01,\n",
            "        1.5577e-02, -0.0000e+00, 3.7413e-04, 3.2007e-02, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3995e-02, 1.1723e-13, 1.6425e-18, 1.8777e-41, 1.1723e-13, 1.0153e-16,\n",
            "        8.1260e-29, 1.0765e-04, 4.0634e-34, 3.1286e-27, 5.3995e-02, 6.0878e-03,\n",
            "        1.1723e-13, 7.1452e-05, 1.1181e-18, 4.3440e-16, 1.3459e-26, 5.2467e-32,\n",
            "        2.7645e-22, 4.2106e-36, 6.9618e-29, 3.6828e-05, 1.1723e-13, 1.1351e-43,\n",
            "        7.6497e-21, 3.5975e-17, 1.8666e-07, 4.3440e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  53\n",
            "ce_loss:  tensor([9.5367e-07, 7.4793e-02, 1.7069e-04, 1.7541e+01, 7.4793e-02, 5.4654e-04,\n",
            "        1.3705e+01, 3.2551e-04, 1.6576e+01, 8.4663e+00, 9.5367e-07, -0.0000e+00,\n",
            "        7.4793e-02, 1.1563e-05, 3.5602e-02, 4.3850e-03, 1.2742e+01, 9.1791e-06,\n",
            "        8.0773e+00, 8.0802e+00, 1.6412e+01, 2.3842e-07, 7.4793e-02, 2.8948e-01,\n",
            "        1.4052e-02, 4.7684e-07, 2.3769e-02, 4.3850e-03, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 6.2017e-13, 1.0557e-18, 1.5902e-41, 6.2017e-13, 8.9811e-17,\n",
            "        1.9146e-28, 1.0554e-04, 8.9980e-34, 2.5041e-27, 5.3552e-02, 6.1387e-03,\n",
            "        6.2017e-13, 7.4491e-05, 1.2078e-18, 3.5535e-16, 2.9536e-26, 5.6050e-32,\n",
            "        4.0493e-22, 4.6772e-36, 2.3093e-28, 3.8648e-05, 6.2017e-13, 4.9045e-44,\n",
            "        4.5019e-21, 1.7207e-17, 3.8497e-07, 3.5535e-16, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  54\n",
            "ce_loss:  tensor([5.3644e-06, 3.7485e-01, 2.5698e-04, 1.5385e+01, 3.7485e-01, 2.6425e-04,\n",
            "        1.5358e+01, 7.8742e-04, 1.7517e+01, 5.2954e+00, 5.3644e-06, -0.0000e+00,\n",
            "        3.7485e-01, 3.5763e-07, 3.4866e-02, 1.6247e-02, 1.2717e+01, 8.5830e-06,\n",
            "        9.0831e+00, 9.2945e+00, 1.9612e+01, 2.3842e-07, 3.7485e-01, 2.1984e-01,\n",
            "        1.3313e-02, -0.0000e+00, 4.4622e-04, 1.6247e-02, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 6.4968e-13, 1.1208e-18, 2.2501e-41, 6.4968e-13, 9.3450e-17,\n",
            "        1.7469e-28, 1.0765e-04, 3.6917e-34, 1.6228e-27, 5.3935e-02, 6.0878e-03,\n",
            "        6.4968e-13, 9.2231e-05, 5.4857e-19, 3.0814e-16, 1.2742e-26, 2.8126e-32,\n",
            "        1.8989e-22, 2.2468e-36, 1.6059e-28, 4.1346e-05, 6.4968e-13, 5.3249e-44,\n",
            "        3.0217e-21, 2.2489e-17, 8.5949e-07, 3.0814e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  55\n",
            "ce_loss:  tensor([9.5367e-07, 5.0354e-01, 1.1396e-04, 1.7242e+01, 5.0354e-01, 1.6128e-04,\n",
            "        1.4670e+01, 3.2551e-04, 1.6131e+01, 3.6132e+00, 9.5367e-07, -0.0000e+00,\n",
            "        5.0354e-01, 8.1062e-06, 2.2581e-02, 2.8277e-03, 1.2332e+01, 6.7949e-06,\n",
            "        7.1968e+00, 9.8078e+00, 1.7413e+01, 2.3842e-07, 5.0354e-01, 3.9468e-01,\n",
            "        1.0452e-02, 3.5763e-07, 3.3594e-01, 2.8277e-03, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 4.2831e-13, 7.9649e-19, 1.7708e-41, 4.2831e-13, 5.2012e-17,\n",
            "        1.1657e-28, 1.0554e-04, 1.3174e-33, 2.5441e-27, 5.3552e-02, 6.1387e-03,\n",
            "        4.2831e-13, 9.3534e-05, 7.0352e-19, 2.1895e-16, 2.5302e-26, 2.0406e-32,\n",
            "        3.2353e-22, 1.6875e-36, 1.1023e-28, 4.2840e-05, 4.2831e-13, 4.7644e-44,\n",
            "        1.6217e-21, 1.1410e-17, 1.5988e-06, 2.1895e-16, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  56\n",
            "ce_loss:  tensor([5.3644e-06, 5.1554e-02, 1.0514e-04, 1.4929e+01, 5.1554e-02, 1.7284e-04,\n",
            "        1.4052e+01, 7.8742e-04, 1.7238e+01, 4.6120e+00, 5.3644e-06, -0.0000e+00,\n",
            "        5.1554e-02, 2.3842e-07, 3.7778e-02, 8.7044e-04, 1.2434e+01, 6.0797e-06,\n",
            "        8.2190e+00, 8.8176e+00, 1.5999e+01, 2.3842e-07, 5.1554e-02, 2.0694e-01,\n",
            "        8.1808e-03, -0.0000e+00, 3.4898e-04, 8.7044e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 3.9285e-13, 9.0518e-19, 2.4497e-41, 3.9285e-13, 3.5686e-17,\n",
            "        2.5228e-28, 1.0765e-04, 5.6285e-34, 3.3791e-27, 5.3935e-02, 6.0878e-03,\n",
            "        3.9285e-13, 1.1301e-04, 3.7559e-19, 1.9204e-16, 1.2250e-26, 1.1544e-32,\n",
            "        2.3263e-22, 8.7429e-37, 3.1239e-28, 4.5485e-05, 3.9285e-13, 2.2421e-44,\n",
            "        9.5246e-22, 1.3236e-17, 2.9926e-06, 1.9204e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  57\n",
            "ce_loss:  tensor([9.5367e-07, 3.6923e+01, 7.4503e-05, 1.7005e+01, 3.6923e+01, 1.3696e-04,\n",
            "        1.5750e+01, 3.2551e-04, 1.6107e+01, 5.1721e+00, 9.5367e-07, -0.0000e+00,\n",
            "        3.6923e+01, 9.6559e-06, 4.5119e-02, 5.1247e-04, 1.2117e+01, 6.1989e-06,\n",
            "        7.3989e+00, 9.5768e+00, 1.6726e+01, 2.3842e-07, 3.6923e+01, 2.0349e-01,\n",
            "        5.5235e-03, 2.3842e-07, 8.8343e-01, 5.1247e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.9529e-13, 5.7092e-19, 1.8852e-41, 1.9529e-13, 4.0018e-17,\n",
            "        2.1586e-28, 1.0554e-04, 1.8723e-33, 1.7329e-27, 5.3552e-02, 6.1387e-03,\n",
            "        1.9529e-13, 1.1110e-04, 5.4657e-19, 2.0081e-16, 2.4413e-26, 6.4976e-33,\n",
            "        3.0729e-22, 5.5000e-37, 7.9739e-29, 4.6817e-05, 1.9529e-13, 2.3822e-44,\n",
            "        5.0654e-22, 7.9504e-18, 5.1630e-06, 2.0081e-16, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  58\n",
            "ce_loss:  tensor([5.3644e-06, 1.1083e-01, 5.1259e-05, 1.5167e+01, 1.1083e-01, 1.0597e-04,\n",
            "        1.6984e+01, 7.8742e-04, 1.6589e+01, 3.6952e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.1083e-01, 1.1921e-07, 2.6315e-02, 4.3109e-04, 1.1853e+01, 5.9604e-06,\n",
            "        9.6222e+00, 7.9456e+00, 1.5822e+01, 2.3842e-07, 1.1083e-01, 2.0364e-01,\n",
            "        5.8833e-03, -0.0000e+00, 2.7998e-04, 4.3109e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 2.2396e-13, 5.4601e-19, 1.7285e-41, 2.2396e-13, 2.4948e-17,\n",
            "        1.2732e-28, 1.0765e-04, 8.4480e-34, 1.8145e-27, 5.3935e-02, 6.0878e-03,\n",
            "        2.2396e-13, 1.3245e-04, 3.3826e-19, 1.3813e-16, 1.1074e-26, 5.1422e-33,\n",
            "        1.7583e-22, 4.2367e-37, 2.1966e-28, 4.9412e-05, 2.2396e-13, 1.1210e-44,\n",
            "        3.7523e-22, 1.0353e-17, 9.3537e-06, 1.3813e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  59\n",
            "ce_loss:  tensor([9.5367e-07, 1.0855e+00, 4.5537e-05, 1.7159e+01, 1.0855e+00, 2.1634e-04,\n",
            "        1.5198e+01, 3.2551e-04, 1.6308e+01, 4.8022e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.0855e+00, 1.3947e-05, 1.1622e-02, 5.5334e-04, 1.2324e+01, 4.8876e-06,\n",
            "        9.3524e+00, 9.6656e+00, 1.7499e+01, 2.3842e-07, 1.0855e+00, 1.7517e-01,\n",
            "        4.7569e-03, -0.0000e+00, 5.0494e-01, 5.5334e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 2.6290e-13, 4.6397e-19, 1.2373e-41, 2.6290e-13, 1.2906e-17,\n",
            "        1.4228e-28, 1.0554e-04, 2.4691e-33, 1.1351e-27, 5.3552e-02, 6.1387e-03,\n",
            "        2.6290e-13, 1.2675e-04, 3.1894e-19, 1.1977e-16, 2.0662e-26, 4.1403e-33,\n",
            "        1.1645e-22, 3.5937e-37, 1.3037e-28, 5.0522e-05, 2.6290e-13, 9.8091e-45,\n",
            "        2.0958e-22, 5.1511e-18, 1.5047e-05, 1.1977e-16, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  60\n",
            "ce_loss:  tensor([5.3644e-06, 1.2950e-01, 3.7193e-05, 1.4372e+01, 1.2950e-01, 1.1062e-04,\n",
            "        1.3153e+01, 7.8742e-04, 1.7487e+01, 3.3862e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.2950e-01, 1.1921e-07, 2.6075e-02, 6.2232e-04, 1.2706e+01, 6.1989e-06,\n",
            "        9.4422e+00, 8.2915e+00, 1.8733e+01, 2.3842e-07, 1.2950e-01, 1.8780e-01,\n",
            "        4.1310e-03, -0.0000e+00, 2.5508e-04, 6.2232e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.4824e-13, 3.5436e-19, 1.7540e-41, 1.4824e-13, 2.2111e-17,\n",
            "        3.2374e-28, 1.0765e-04, 2.2743e-33, 1.6221e-27, 5.3935e-02, 6.0878e-03,\n",
            "        1.4824e-13, 1.4908e-04, 1.7194e-19, 1.2740e-16, 1.0479e-26, 3.9625e-33,\n",
            "        8.1562e-23, 1.7661e-37, 9.4651e-29, 5.2947e-05, 1.4824e-13, 5.6052e-45,\n",
            "        1.3880e-22, 4.6756e-18, 2.6659e-05, 1.2740e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  61\n",
            "ce_loss:  tensor([9.5367e-07, 6.3472e-02, 7.9033e-05, 1.3947e+01, 6.3472e-02, 1.0240e-04,\n",
            "        1.4646e+01, 3.2551e-04, 1.9458e+01, 5.1992e+00, 9.5367e-07, -0.0000e+00,\n",
            "        6.3472e-02, 3.6955e-06, 1.6113e-02, 2.6485e-04, 1.6238e+01, 5.1260e-06,\n",
            "        7.4134e+00, 7.0363e+00, 1.6995e+01, 2.3842e-07, 6.3472e-02, 1.6373e-01,\n",
            "        3.3660e-03, -0.0000e+00, 5.1888e-01, 2.6485e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.6733e-13, 2.9726e-19, 5.3960e-41, 1.6733e-13, 1.2416e-17,\n",
            "        1.7857e-28, 1.0554e-04, 1.8668e-33, 1.3398e-27, 5.3552e-02, 6.1387e-03,\n",
            "        1.6733e-13, 1.4189e-04, 2.1359e-19, 9.0245e-17, 5.9665e-27, 2.8347e-33,\n",
            "        9.2325e-23, 1.8372e-37, 5.5171e-29, 5.3778e-05, 1.6733e-13, 4.2039e-45,\n",
            "        6.8497e-23, 2.1527e-18, 3.9470e-05, 9.0245e-17, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  62\n",
            "ce_loss:  tensor([5.3644e-06, 5.6743e-01, 3.7550e-05, 1.4109e+01, 5.6743e-01, 7.8317e-05,\n",
            "        1.3063e+01, 7.8742e-04, 1.6610e+01, 5.9270e+00, 5.3644e-06, -0.0000e+00,\n",
            "        5.6743e-01, 1.1921e-07, 1.1870e-02, 8.1470e-04, 1.2649e+01, 5.1260e-06,\n",
            "        8.4673e+00, 7.7370e+00, 1.8596e+01, 2.3842e-07, 5.6743e-01, 1.8053e-01,\n",
            "        3.5638e-03, -0.0000e+00, 1.2790e-04, 8.1470e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.1645e-13, 2.9084e-19, 2.3470e-41, 1.1645e-13, 6.8520e-18,\n",
            "        3.9765e-28, 1.0765e-04, 1.2689e-33, 9.0269e-28, 5.3935e-02, 6.0878e-03,\n",
            "        1.1645e-13, 1.6254e-04, 1.2904e-19, 6.4343e-17, 3.5798e-27, 3.4067e-33,\n",
            "        6.9697e-23, 1.0286e-37, 4.1796e-29, 5.6022e-05, 1.1645e-13, 2.8026e-45,\n",
            "        5.7670e-23, 3.2749e-18, 6.6184e-05, 6.4343e-17, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  63\n",
            "ce_loss:  tensor([9.5367e-07, 4.7383e-02, 1.0931e-04, 1.3700e+01, 4.7383e-02, 7.5337e-05,\n",
            "        1.4982e+01, 3.2551e-04, 1.5427e+01, 4.2700e+00, 9.5367e-07, -0.0000e+00,\n",
            "        4.7383e-02, 4.6492e-06, 5.2628e-03, 3.8938e-04, 1.1351e+01, 3.9339e-06,\n",
            "        9.0458e+00, 7.3302e+00, 1.6264e+01, 2.3842e-07, 4.7383e-02, 2.2317e-01,\n",
            "        1.9979e-03, 1.1921e-07, 3.7520e-01, 3.8938e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.3595e-13, 2.0703e-19, 6.6961e-41, 1.3595e-13, 4.4104e-18,\n",
            "        2.7736e-28, 1.0554e-04, 3.7178e-33, 7.2203e-28, 5.3552e-02, 6.1387e-03,\n",
            "        1.3595e-13, 1.5200e-04, 1.5243e-19, 3.8981e-17, 8.1980e-27, 2.0932e-33,\n",
            "        7.5093e-23, 1.9563e-37, 3.0414e-29, 5.6525e-05, 1.3595e-13, 2.8026e-45,\n",
            "        2.4133e-23, 1.6135e-18, 8.9890e-05, 3.8981e-17, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  64\n",
            "ce_loss:  tensor([5.3644e-06, 1.1639e+01, 3.5047e-05, 1.3791e+01, 1.1639e+01, 5.4940e-04,\n",
            "        1.6414e+01, 7.8742e-04, 1.5883e+01, 6.1409e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.1639e+01, 1.1921e-07, 2.1248e-02, 1.1646e-04, 1.1424e+01, 3.9339e-06,\n",
            "        8.3320e+00, 8.0336e+00, 1.4657e+01, 1.1921e-07, 1.1639e+01, 1.5883e-01,\n",
            "        1.8632e-03, -0.0000e+00, 8.7853e-05, 1.1646e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.2414e-13, 2.0127e-19, 2.3889e-41, 1.2414e-13, 5.2080e-18,\n",
            "        2.3934e-28, 1.0765e-04, 1.6833e-33, 5.3597e-28, 5.3935e-02, 6.0878e-03,\n",
            "        1.2414e-13, 1.7298e-04, 6.5434e-20, 2.5793e-17, 7.2841e-27, 1.4488e-33,\n",
            "        6.0869e-23, 8.4246e-38, 6.8791e-29, 5.8500e-05, 1.2414e-13, 1.4013e-45,\n",
            "        1.7345e-23, 2.7156e-18, 1.3779e-04, 2.5793e-17, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  65\n",
            "ce_loss:  tensor([9.5367e-07, 1.0063e-01, 4.2795e-05, 1.3568e+01, 1.0063e-01, 9.3098e-05,\n",
            "        1.5122e+01, 3.2551e-04, 1.5553e+01, 4.7941e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.0063e-01, 3.8147e-06, 3.9831e-03, 9.1310e-05, 1.1027e+01, 3.4571e-06,\n",
            "        7.3548e+00, 9.0522e+00, 1.6665e+01, 1.1921e-07, 1.0063e-01, 1.9146e-01,\n",
            "        1.9670e-03, -0.0000e+00, 5.9769e-01, 9.1310e-05, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 9.3160e-14, 1.4768e-19, 6.3419e-41, 9.3160e-14, 6.0945e-18,\n",
            "        1.0898e-28, 1.0554e-04, 4.7465e-33, 3.7222e-28, 5.3552e-02, 6.1387e-03,\n",
            "        9.3160e-14, 1.6079e-04, 6.8123e-20, 1.8502e-17, 1.2934e-26, 1.0955e-33,\n",
            "        6.7995e-23, 7.9334e-38, 5.4598e-29, 5.8895e-05, 9.3160e-14, 1.4013e-45,\n",
            "        9.5605e-24, 1.2741e-18, 1.7554e-04, 1.8502e-17, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  66\n",
            "ce_loss:  tensor([5.3644e-06, 3.2503e+00, 3.1590e-05, 1.3965e+01, 3.2503e+00, 6.2106e-05,\n",
            "        1.2803e+01, 7.8742e-04, 1.6521e+01, 2.8180e+00, 5.3644e-06, -0.0000e+00,\n",
            "        3.2503e+00, 1.1921e-07, 5.6364e-03, 9.1429e-05, 1.1279e+01, 3.9339e-06,\n",
            "        7.4278e+00, 7.8834e+00, 1.7929e+01, 1.1921e-07, 3.2503e+00, 1.3682e-01,\n",
            "        1.2629e-03, -0.0000e+00, 4.0054e-05, 9.1429e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 8.8935e-14, 1.5535e-19, 2.4133e-41, 8.8935e-14, 3.7249e-18,\n",
            "        2.4890e-28, 1.0765e-04, 2.4570e-33, 5.3730e-28, 5.3935e-02, 6.0878e-03,\n",
            "        8.8935e-14, 1.8291e-04, 5.2340e-20, 1.4290e-17, 6.1089e-27, 7.4292e-34,\n",
            "        1.0927e-22, 4.2705e-38, 3.9193e-29, 6.0320e-05, 8.8935e-14, 0.0000e+00,\n",
            "        5.8360e-24, 1.4942e-18, 2.5278e-04, 1.4290e-17, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  67\n",
            "ce_loss:  tensor([9.5367e-07, 3.7764e-02, 1.0514e-04, 1.3134e+01, 3.7764e-02, 5.9960e-05,\n",
            "        1.3999e+01, 3.2551e-04, 1.9636e+01, 3.7130e+00, 9.5367e-07, -0.0000e+00,\n",
            "        3.7764e-02, 4.4107e-06, 3.3208e-03, 3.5405e-05, 1.0902e+01, 3.0994e-06,\n",
            "        9.8563e+00, 8.7286e+00, 1.6794e+01, 1.1921e-07, 3.7764e-02, 1.6470e-01,\n",
            "        1.3679e-03, -0.0000e+00, 1.1097e-01, 3.5405e-05, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 6.0269e-14, 1.2412e-19, 6.5295e-41, 6.0269e-14, 3.1233e-18,\n",
            "        2.6200e-28, 1.0554e-04, 1.9573e-33, 8.1116e-28, 5.3552e-02, 6.1387e-03,\n",
            "        6.0269e-14, 1.6890e-04, 4.2684e-20, 9.0092e-18, 1.2171e-26, 5.2646e-34,\n",
            "        7.9922e-23, 3.5724e-38, 2.2709e-29, 6.0862e-05, 6.0269e-14, 0.0000e+00,\n",
            "        2.6017e-24, 7.8408e-19, 3.0288e-04, 9.0092e-18, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  68\n",
            "ce_loss:  tensor([5.3644e-06, 3.6230e+01, 2.9563e-05, 1.3415e+01, 3.6230e+01, 7.6053e-05,\n",
            "        1.6009e+01, 7.8742e-04, 1.6014e+01, 3.9892e+00, 5.3644e-06, -0.0000e+00,\n",
            "        3.6230e+01, 1.1921e-07, 3.0251e-03, 8.2244e-04, 1.2243e+01, 3.9339e-06,\n",
            "        7.6887e+00, 6.9208e+00, 1.8094e+01, 1.1921e-07, 3.6230e+01, 1.2910e-01,\n",
            "        9.3226e-04, -0.0000e+00, 2.3961e-05, 8.2244e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 3.6305e-14, 1.2289e-19, 2.3343e-41, 3.6305e-14, 1.7654e-18,\n",
            "        1.4473e-28, 1.0765e-04, 1.4574e-33, 4.4827e-28, 5.3935e-02, 6.0878e-03,\n",
            "        3.6305e-14, 1.9121e-04, 3.6478e-20, 8.2194e-18, 8.7597e-27, 4.5078e-34,\n",
            "        7.1327e-23, 2.5959e-38, 1.7337e-29, 6.1345e-05, 3.6305e-14, 0.0000e+00,\n",
            "        1.7001e-24, 9.5482e-19, 4.1874e-04, 8.2194e-18, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  69\n",
            "ce_loss:  tensor([9.5367e-07, 8.6856e-02, 4.4941e-05, 1.2697e+01, 8.6856e-02, 8.0940e-05,\n",
            "        1.4707e+01, 3.2551e-04, 1.5174e+01, 3.4931e+00, 9.5367e-07, -0.0000e+00,\n",
            "        8.6856e-02, 4.4107e-06, 3.5738e-03, 1.1515e-04, 1.4432e+01, 4.7684e-06,\n",
            "        7.3545e+00, 8.4274e+00, 1.6651e+01, 1.1921e-07, 8.6856e-02, 1.4700e-01,\n",
            "        1.0367e-03, -0.0000e+00, 8.2971e-04, 1.1515e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 3.5904e-14, 8.8349e-20, 5.5535e-41, 3.5904e-14, 1.5017e-18,\n",
            "        1.4047e-28, 1.0554e-04, 4.1698e-33, 6.2519e-28, 5.3552e-02, 6.1387e-03,\n",
            "        3.5904e-14, 1.7589e-04, 2.8219e-20, 5.7412e-18, 4.9354e-27, 3.5268e-34,\n",
            "        8.5874e-23, 1.8341e-38, 7.6802e-30, 6.1657e-05, 3.5904e-14, 0.0000e+00,\n",
            "        9.4391e-25, 8.0712e-19, 4.8722e-04, 5.7412e-18, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  70\n",
            "ce_loss:  tensor([5.3644e-06, 8.1387e-01, 2.1934e-05, 1.4217e+01, 8.1387e-01, 5.6861e-05,\n",
            "        1.2417e+01, 7.8742e-04, 1.5349e+01, 4.0409e+00, 5.3644e-06, -0.0000e+00,\n",
            "        8.1387e-01, 1.1921e-07, 7.0033e-03, 5.1974e-05, 1.1922e+01, 3.9339e-06,\n",
            "        9.1735e+00, 6.7555e+00, 1.4431e+01, 1.1921e-07, 8.1387e-01, 1.1514e-01,\n",
            "        6.8486e-04, -0.0000e+00, 1.9073e-05, 5.1974e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 3.6564e-14, 9.1581e-20, 3.7692e-41, 3.6564e-14, 1.3896e-18,\n",
            "        2.1747e-28, 1.0765e-04, 2.1489e-33, 4.2627e-28, 5.3935e-02, 6.0878e-03,\n",
            "        3.6564e-14, 1.9837e-04, 2.5121e-20, 5.1387e-18, 4.2414e-27, 4.2559e-34,\n",
            "        5.4879e-23, 1.3451e-38, 2.0721e-29, 6.1979e-05, 3.6564e-14, 0.0000e+00,\n",
            "        7.9849e-25, 5.6663e-19, 6.3434e-04, 5.1387e-18, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  71\n",
            "ce_loss:  tensor([9.5367e-07, 2.8086e-02, 2.5510e-05, 1.5082e+01, 2.8086e-02, 6.1987e-05,\n",
            "        1.4114e+01, 3.2551e-04, 1.5056e+01, 6.4101e+00, 9.5367e-07, -0.0000e+00,\n",
            "        2.8086e-02, 4.2915e-06, 1.7644e-03, 2.6092e-04, 1.0723e+01, 3.2186e-06,\n",
            "        8.4984e+00, 6.3412e+00, 1.7901e+01, 1.1921e-07, 2.8086e-02, 9.3886e-02,\n",
            "        7.2953e-04, -0.0000e+00, 7.3311e-05, 2.6092e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 2.1929e-14, 6.3495e-20, 2.3113e-41, 2.1929e-14, 5.6731e-19,\n",
            "        3.8852e-28, 1.0554e-04, 4.9896e-33, 3.4230e-28, 5.3552e-02, 6.1387e-03,\n",
            "        2.1929e-14, 1.8158e-04, 1.9348e-20, 3.6743e-18, 8.5774e-27, 2.6866e-34,\n",
            "        4.0190e-23, 2.5539e-38, 1.0736e-29, 6.2156e-05, 2.1929e-14, 0.0000e+00,\n",
            "        4.0644e-25, 6.1636e-19, 7.2992e-04, 3.6743e-18, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  72\n",
            "ce_loss:  tensor([5.3644e-06, 3.5591e+00, 2.0504e-05, 1.3138e+01, 3.5591e+00, 4.2557e-05,\n",
            "        1.3872e+01, 7.8742e-04, 1.5607e+01, 3.3667e+00, 5.3644e-06, -0.0000e+00,\n",
            "        3.5591e+00, 1.1921e-07, 1.7455e-03, 3.8981e-05, 1.1318e+01, 3.4571e-06,\n",
            "        8.0341e+00, 7.3334e+00, 1.5193e+01, 1.1921e-07, 3.5591e+00, 1.5796e-01,\n",
            "        4.9007e-04, -0.0000e+00, 1.2994e-05, 3.8981e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 2.0188e-14, 5.7094e-20, 2.2810e-41, 2.0188e-14, 6.7104e-19,\n",
            "        1.7619e-28, 1.0765e-04, 2.0508e-33, 2.3990e-28, 5.3935e-02, 6.0878e-03,\n",
            "        2.0188e-14, 2.0389e-04, 1.4035e-20, 4.0952e-18, 4.6024e-27, 2.3073e-34,\n",
            "        3.9280e-23, 9.0870e-39, 8.3919e-30, 6.2348e-05, 2.0188e-14, 0.0000e+00,\n",
            "        3.1474e-25, 4.6868e-19, 9.0839e-04, 4.0952e-18, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  73\n",
            "ce_loss:  tensor([9.5367e-07, 1.9104e-02, 4.2676e-05, 1.5477e+01, 1.9104e-02, 3.7312e-05,\n",
            "        1.4050e+01, 3.2551e-04, 1.5053e+01, 2.8120e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.9104e-02, 2.6226e-06, 3.2597e-03, 1.5603e-03, 1.0484e+01, 2.6226e-06,\n",
            "        6.3145e+00, 6.3619e+00, 1.4149e+01, 1.1921e-07, 1.9104e-02, 9.3440e-02,\n",
            "        6.0087e-04, -0.0000e+00, 5.6265e-05, 1.5603e-03, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.6559e-14, 4.9633e-20, 1.6405e-41, 1.6559e-14, 4.4964e-19,\n",
            "        3.1009e-28, 1.0554e-04, 4.9741e-33, 3.6890e-28, 5.3552e-02, 6.1387e-03,\n",
            "        1.6559e-14, 1.8686e-04, 6.9002e-21, 2.5483e-18, 8.1218e-27, 1.2607e-34,\n",
            "        5.5272e-23, 1.6513e-38, 1.6160e-29, 6.2387e-05, 1.6559e-14, 0.0000e+00,\n",
            "        2.4713e-25, 3.7738e-19, 1.0115e-03, 2.5483e-18, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  74\n",
            "ce_loss:  tensor([5.3644e-06, 6.4994e+00, 1.5497e-05, 1.3533e+01, 6.4994e+00, 2.5749e-05,\n",
            "        1.3586e+01, 7.8742e-04, 1.5710e+01, 4.1855e+00, 5.3644e-06, -0.0000e+00,\n",
            "        6.4994e+00, 1.1921e-07, 1.7571e-03, 3.2901e-05, 1.1232e+01, 3.2186e-06,\n",
            "        6.8870e+00, 7.6255e+00, 1.6670e+01, 1.1921e-07, 6.4994e+00, 8.5987e-02,\n",
            "        3.5089e-04, -0.0000e+00, 1.0610e-05, 3.2901e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.6004e-14, 4.6595e-20, 9.0748e-42, 1.6004e-14, 3.3999e-19,\n",
            "        1.6954e-28, 1.0765e-04, 2.0099e-33, 2.6366e-28, 5.3935e-02, 6.0878e-03,\n",
            "        1.6004e-14, 2.0752e-04, 9.9963e-21, 1.5034e-18, 3.7972e-27, 1.2352e-34,\n",
            "        8.4402e-23, 6.7074e-39, 1.0334e-29, 6.2445e-05, 1.6004e-14, 0.0000e+00,\n",
            "        1.1479e-25, 2.8782e-19, 1.2187e-03, 1.5034e-18, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  75\n",
            "ce_loss:  tensor([9.5367e-07, 1.2405e-02, 1.5020e-05, 1.5331e+01, 1.2405e-02, 2.9802e-05,\n",
            "        1.4096e+01, 3.2551e-04, 1.4970e+01, 5.2091e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.2405e-02, 1.6689e-06, 2.5556e-03, 1.8358e-05, 1.0480e+01, 2.6226e-06,\n",
            "        7.9924e+00, 7.5466e+00, 1.6674e+01, 1.1921e-07, 1.2405e-02, 9.3538e-02,\n",
            "        2.9905e-04, -0.0000e+00, 4.9351e-05, 1.8358e-05, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.0555e-14, 3.3121e-20, 5.7902e-42, 1.0555e-14, 2.5539e-19,\n",
            "        3.0595e-28, 1.0554e-04, 4.3489e-33, 1.9480e-28, 5.3552e-02, 6.1387e-03,\n",
            "        1.0555e-14, 1.9061e-04, 6.4439e-21, 8.7238e-19, 7.3419e-27, 7.9760e-35,\n",
            "        4.6861e-23, 4.7235e-39, 6.3421e-30, 6.2387e-05, 1.0555e-14, 0.0000e+00,\n",
            "        7.1214e-26, 2.4707e-19, 1.3182e-03, 8.7238e-19, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  76\n",
            "ce_loss:  tensor([5.3644e-06, 4.4467e-03, 1.0967e-05, 1.2593e+01, 4.4467e-03, 2.0504e-05,\n",
            "        1.3643e+01, 7.8742e-04, 1.6051e+01, 3.8934e+00, 5.3644e-06, -0.0000e+00,\n",
            "        4.4467e-03, 1.1921e-07, 8.3817e-04, 1.6689e-05, 1.1666e+01, 3.0994e-06,\n",
            "        9.1573e+00, 7.2156e+00, 1.5017e+01, 1.1921e-07, 4.4467e-03, 7.9438e-02,\n",
            "        2.5305e-04, -0.0000e+00, 8.4638e-06, 1.6689e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.1322e-14, 2.9207e-20, 6.1531e-42, 1.1322e-14, 2.4891e-19,\n",
            "        1.3683e-28, 1.0765e-04, 2.9729e-33, 1.6353e-28, 5.3935e-02, 6.0878e-03,\n",
            "        1.1322e-14, 2.1030e-04, 9.1629e-21, 6.8541e-19, 6.2527e-27, 1.0808e-34,\n",
            "        4.4324e-23, 2.6866e-39, 4.4006e-30, 6.2445e-05, 1.1322e-14, 0.0000e+00,\n",
            "        3.6795e-26, 2.7196e-19, 1.5517e-03, 6.8541e-19, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  77\n",
            "ce_loss:  tensor([9.5367e-07, 5.7821e-02, 7.4265e-05, 1.5250e+01, 5.7821e-02, 5.5312e-05,\n",
            "        1.4006e+01, 3.2551e-04, 1.9314e+01, 5.3801e+00, 9.5367e-07, -0.0000e+00,\n",
            "        5.7821e-02, 1.7881e-06, 2.5852e-03, 3.1590e-05, 1.4237e+01, 3.5763e-06,\n",
            "        7.3039e+00, 7.9665e+00, 1.3892e+01, 1.1921e-07, 5.7821e-02, 8.6903e-02,\n",
            "        2.8153e-04, -0.0000e+00, 4.4702e-05, 3.1590e-05, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 9.5674e-15, 2.3322e-20, 3.6364e-42, 9.5674e-15, 1.6608e-19,\n",
            "        2.3057e-28, 1.0554e-04, 2.2476e-33, 1.0470e-28, 5.3552e-02, 6.1387e-03,\n",
            "        9.5674e-15, 1.9253e-04, 4.1455e-21, 4.1363e-19, 3.2839e-27, 7.6122e-35,\n",
            "        2.5610e-23, 2.2213e-39, 1.0384e-29, 6.2387e-05, 9.5674e-15, 0.0000e+00,\n",
            "        2.1330e-26, 1.4936e-19, 1.6399e-03, 4.1363e-19, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  78\n",
            "ce_loss:  tensor([5.3644e-06, 1.8370e-03, 1.3590e-05, 1.2417e+01, 1.8370e-03, 1.6570e-05,\n",
            "        1.4133e+01, 7.8742e-04, 1.5571e+01, 3.4129e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.8370e-03, 1.1921e-07, 6.5353e-04, 1.1444e-05, 1.1601e+01, 3.0994e-06,\n",
            "        6.5266e+00, 6.3244e+00, 1.5458e+01, 1.1921e-07, 1.8370e-03, 7.0812e-02,\n",
            "        2.2897e-04, -0.0000e+00, 6.7949e-06, 1.1444e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.3799e-14, 2.4485e-20, 2.5237e-42, 1.3799e-14, 2.0112e-19,\n",
            "        1.5770e-28, 1.0765e-04, 1.2480e-33, 7.2620e-29, 5.3935e-02, 6.0878e-03,\n",
            "        1.3799e-14, 2.1196e-04, 6.2702e-21, 4.7918e-19, 3.0746e-27, 6.3308e-35,\n",
            "        4.1211e-23, 1.5804e-39, 3.1183e-30, 6.2445e-05, 1.3799e-14, 0.0000e+00,\n",
            "        1.2419e-26, 1.4967e-19, 1.8909e-03, 4.7918e-19, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  79\n",
            "ce_loss:  tensor([9.5367e-07, 6.3823e+00, 1.8835e-05, 1.2286e+01, 6.3823e+00, 1.9908e-05,\n",
            "        1.6520e+01, 3.2551e-04, 1.4702e+01, 2.2485e+00, 9.5367e-07, -0.0000e+00,\n",
            "        6.3823e+00, 1.6689e-06, 1.1030e-03, 8.7022e-06, 1.0157e+01, 2.3842e-06,\n",
            "        8.2546e+00, 7.8352e+00, 1.3815e+01, 1.1921e-07, 6.3823e+00, 8.9538e-02,\n",
            "        2.5019e-04, 2.3842e-07, 2.6941e-05, 8.7022e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.2575e-14, 1.7663e-20, 6.0816e-42, 1.2575e-14, 1.3827e-19,\n",
            "        9.1885e-29, 1.0554e-04, 2.9085e-33, 1.0414e-28, 5.3552e-02, 6.1387e-03,\n",
            "        1.2575e-14, 1.9374e-04, 4.9307e-21, 2.8989e-19, 5.5687e-27, 3.0196e-35,\n",
            "        2.5128e-23, 1.2083e-39, 8.1051e-30, 6.2387e-05, 1.2575e-14, 0.0000e+00,\n",
            "        1.0618e-26, 1.0720e-19, 1.9887e-03, 2.8989e-19, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  80\n",
            "ce_loss:  tensor([5.3644e-06, 1.0889e-03, 1.1444e-05, 1.2573e+01, 1.0889e-03, 1.3828e-05,\n",
            "        1.3735e+01, 7.8742e-04, 1.5645e+01, 4.0973e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.0889e-03, 1.1921e-07, 1.4935e-03, 1.5020e-05, 1.1465e+01, 2.3842e-06,\n",
            "        8.5531e+00, 6.8366e+00, 1.5048e+01, 1.1921e-07, 1.0889e-03, 6.5681e-02,\n",
            "        1.6974e-04, -0.0000e+00, 5.8412e-06, 1.5020e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 9.3976e-15, 1.4538e-20, 2.1636e-42, 9.3976e-15, 9.9349e-20,\n",
            "        8.2302e-29, 1.0765e-04, 3.2612e-33, 1.0820e-28, 5.3935e-02, 6.0878e-03,\n",
            "        9.3976e-15, 2.1271e-04, 2.4404e-21, 2.4934e-19, 3.3971e-27, 2.7867e-35,\n",
            "        1.7419e-23, 8.4926e-40, 3.3705e-30, 6.2445e-05, 9.3976e-15, 0.0000e+00,\n",
            "        6.4073e-27, 1.7341e-19, 2.2300e-03, 2.4934e-19, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  81\n",
            "ce_loss:  tensor([9.5367e-07, 2.0928e+00, 7.8678e-06, 1.2190e+01, 2.0928e+00, 3.5643e-05,\n",
            "        1.2633e+01, 3.2551e-04, 1.8322e+01, 3.7643e+00, 9.5367e-07, -0.0000e+00,\n",
            "        2.0928e+00, 1.7881e-06, 6.0957e-04, 1.6332e-05, 1.4262e+01, 2.3842e-06,\n",
            "        8.6121e+00, 5.4691e+00, 1.7880e+01, 1.1921e-07, 2.0928e+00, 1.0901e-01,\n",
            "        2.3148e-04, -0.0000e+00, 2.5272e-05, 1.6332e-05, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 6.7648e-15, 9.5352e-21, 5.0755e-42, 6.7648e-15, 6.9179e-20,\n",
            "        1.0730e-28, 1.0554e-04, 1.8712e-33, 6.4405e-29, 5.3552e-02, 6.1387e-03,\n",
            "        6.7648e-15, 1.9400e-04, 2.8769e-21, 1.5174e-19, 2.0770e-27, 2.2883e-35,\n",
            "        1.2636e-23, 1.0844e-39, 2.2564e-30, 6.2387e-05, 6.7648e-15, 0.0000e+00,\n",
            "        5.1428e-27, 9.3036e-20, 2.2942e-03, 1.5174e-19, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  82\n",
            "ce_loss:  tensor([5.3644e-06, 5.3475e-04, 1.0490e-05, 1.2669e+01, 5.3475e-04, 1.1086e-05,\n",
            "        1.3138e+01, 7.8742e-04, 1.6046e+01, 3.3031e+00, 5.3644e-06, -0.0000e+00,\n",
            "        5.3475e-04, 1.1921e-07, 2.1689e-03, 1.0490e-05, 1.1322e+01, 2.6226e-06,\n",
            "        6.6583e+00, 6.8347e+00, 1.5369e+01, 1.1921e-07, 5.3475e-04, 6.8252e-02,\n",
            "        1.6688e-04, -0.0000e+00, 4.7684e-06, 1.0490e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 5.2419e-15, 7.8523e-21, 1.8988e-42, 5.2419e-15, 5.3771e-20,\n",
            "        1.1561e-28, 1.0765e-04, 1.2484e-33, 6.8435e-29, 5.3935e-02, 6.0878e-03,\n",
            "        5.2419e-15, 2.1271e-04, 1.5104e-21, 1.0172e-19, 1.1574e-27, 2.5284e-35,\n",
            "        1.4557e-23, 7.6628e-40, 8.5807e-31, 6.2445e-05, 5.2419e-15, 0.0000e+00,\n",
            "        3.3898e-27, 7.8816e-20, 2.5435e-03, 1.0172e-19, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  83\n",
            "ce_loss:  tensor([9.5367e-07, 1.7410e-01, 7.0333e-06, 1.2258e+01, 1.7410e-01, 1.8477e-05,\n",
            "        1.5682e+01, 3.2551e-04, 1.8268e+01, 3.4275e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.7410e-01, 1.7881e-06, 5.5822e-04, 9.2983e-06, 1.0080e+01, 2.1458e-06,\n",
            "        7.4965e+00, 7.0863e+00, 1.3407e+01, 1.1921e-07, 1.7410e-01, 8.8089e-02,\n",
            "        1.1741e-04, -0.0000e+00, 2.3246e-05, 9.2983e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 3.2042e-15, 4.5895e-21, 5.2381e-42, 3.2042e-15, 4.1230e-20,\n",
            "        5.9828e-29, 1.0554e-04, 9.3013e-34, 3.4315e-29, 5.3552e-02, 6.1387e-03,\n",
            "        3.2042e-15, 1.9400e-04, 1.9317e-21, 8.6537e-20, 2.6490e-27, 1.1229e-35,\n",
            "        1.2200e-23, 4.9125e-40, 2.3978e-30, 6.2387e-05, 3.2042e-15, 0.0000e+00,\n",
            "        1.7700e-27, 4.3095e-20, 2.5782e-03, 8.6537e-20, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  84\n",
            "ce_loss:  tensor([5.3644e-06, 2.7474e-04, 8.5830e-06, 1.2889e+01, 2.7474e-04, 9.7751e-06,\n",
            "        1.3561e+01, 7.8742e-04, 1.5930e+01, 3.1959e+00, 5.3644e-06, -0.0000e+00,\n",
            "        2.7474e-04, 1.1921e-07, 1.9001e-03, 5.1260e-06, 1.1415e+01, 2.3842e-06,\n",
            "        5.7497e+00, 6.2826e+00, 1.4882e+01, 1.1921e-07, 2.7474e-04, 6.8215e-02,\n",
            "        1.1241e-04, -0.0000e+00, 4.1723e-06, 5.1260e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 2.7443e-15, 3.3328e-21, 1.6409e-42, 2.7443e-15, 4.9922e-20,\n",
            "        5.1023e-29, 1.0765e-04, 4.5866e-34, 3.7821e-29, 5.3935e-02, 6.0878e-03,\n",
            "        2.7443e-15, 2.1271e-04, 8.5017e-22, 5.3653e-20, 1.9981e-27, 1.1150e-35,\n",
            "        1.6099e-23, 4.3358e-40, 9.3978e-31, 6.2445e-05, 2.7443e-15, 0.0000e+00,\n",
            "        1.2866e-27, 6.8367e-20, 2.8197e-03, 5.3653e-20, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  85\n",
            "ce_loss:  tensor([9.5367e-07, 5.9026e-02, 6.3181e-06, 1.2174e+01, 5.9026e-02, 1.8358e-05,\n",
            "        1.2069e+01, 3.2551e-04, 1.4130e+01, 3.9965e+00, 9.5367e-07, -0.0000e+00,\n",
            "        5.9026e-02, 1.7881e-06, 5.5488e-04, 8.3446e-06, 1.2834e+01, 2.7418e-06,\n",
            "        6.6673e+00, 7.3477e+00, 1.7390e+01, 1.1921e-07, 5.9026e-02, 5.5724e-02,\n",
            "        1.2981e-04, -0.0000e+00, 2.0981e-05, 8.3446e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.6031e-15, 2.7949e-21, 3.7471e-42, 1.6031e-15, 3.1099e-20,\n",
            "        8.6966e-29, 1.0554e-04, 1.0954e-33, 2.0255e-29, 5.3552e-02, 6.1387e-03,\n",
            "        1.6031e-15, 1.9400e-04, 1.2864e-21, 4.4110e-20, 1.4339e-27, 1.0806e-35,\n",
            "        2.9803e-23, 3.4888e-40, 6.4507e-31, 6.2387e-05, 1.6031e-15, 0.0000e+00,\n",
            "        9.2995e-28, 3.8965e-20, 2.8252e-03, 4.4110e-20, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  86\n",
            "ce_loss:  tensor([5.3644e-06, 2.0001e-04, 3.3259e-05, 1.2565e+01, 2.0001e-04, 7.0333e-06,\n",
            "        1.3636e+01, 7.8742e-04, 1.5130e+01, 6.0645e+00, 5.3644e-06, -0.0000e+00,\n",
            "        2.0001e-04, 1.1921e-07, 9.0880e-04, 1.9431e-05, 1.1345e+01, 2.0266e-06,\n",
            "        7.3090e+00, 6.7435e+00, 1.5472e+01, 1.1921e-07, 2.0001e-04, 9.5303e-02,\n",
            "        9.2979e-05, -0.0000e+00, 3.6955e-06, 1.9431e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.7336e-15, 2.2365e-21, 1.4700e-42, 1.7336e-15, 3.6023e-20,\n",
            "        4.5626e-29, 1.0765e-04, 1.2929e-33, 1.3747e-29, 5.3935e-02, 6.0878e-03,\n",
            "        1.7336e-15, 2.1271e-04, 4.9500e-22, 3.6948e-20, 1.4559e-27, 1.0672e-35,\n",
            "        1.4390e-23, 2.2674e-40, 3.4762e-31, 6.2445e-05, 1.7336e-15, 0.0000e+00,\n",
            "        6.6080e-28, 3.9351e-20, 3.0428e-03, 3.6948e-20, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  87\n",
            "ce_loss:  tensor([9.5367e-07, 4.8951e-03, 5.6028e-06, 1.2292e+01, 4.8951e-03, 1.0610e-05,\n",
            "        1.2138e+01, 3.2551e-04, 1.7255e+01, 2.8779e+00, 9.5367e-07, -0.0000e+00,\n",
            "        4.8951e-03, 1.7881e-06, 4.3538e-04, 4.8876e-06, 1.0068e+01, 1.7881e-06,\n",
            "        8.7673e+00, 7.2396e+00, 1.3455e+01, 1.1921e-07, 4.8951e-03, 5.8598e-02,\n",
            "        1.0967e-04, -0.0000e+00, 1.8596e-05, 4.8876e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.2106e-15, 1.6969e-21, 3.8354e-42, 1.2106e-15, 2.2420e-20,\n",
            "        8.1510e-29, 1.0554e-04, 1.0010e-33, 8.3117e-30, 5.3552e-02, 6.1387e-03,\n",
            "        1.2106e-15, 1.9400e-04, 7.7199e-22, 3.6844e-20, 1.6837e-27, 5.7367e-36,\n",
            "        1.2784e-23, 1.8427e-40, 8.7468e-31, 6.2387e-05, 1.2106e-15, 0.0000e+00,\n",
            "        4.6043e-28, 3.9198e-20, 2.9969e-03, 3.6844e-20, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  88\n",
            "ce_loss:  tensor([5.3644e-06, 1.1110e-04, 5.4836e-06, 1.3267e+01, 1.1110e-04, 6.9141e-06,\n",
            "        1.3134e+01, 7.8742e-04, 1.5752e+01, 2.6991e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.1110e-04, 1.1921e-07, 1.3854e-03, 6.9141e-06, 1.1518e+01, 1.9073e-06,\n",
            "        7.2033e+00, 5.9564e+00, 1.4550e+01, 1.1921e-07, 1.1110e-04, 5.4306e-02,\n",
            "        7.4741e-05, -0.0000e+00, 3.4571e-06, 6.9141e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.2142e-15, 1.1751e-21, 2.2281e-42, 1.2142e-15, 1.8693e-20,\n",
            "        6.5559e-29, 1.0765e-04, 6.9819e-34, 1.0335e-29, 5.3935e-02, 6.0878e-03,\n",
            "        1.2142e-15, 2.1271e-04, 4.6808e-22, 2.4852e-20, 1.5836e-27, 5.6984e-36,\n",
            "        6.7291e-24, 1.2988e-40, 1.6039e-30, 6.2445e-05, 1.2142e-15, 0.0000e+00,\n",
            "        3.0624e-28, 2.9638e-20, 3.2035e-03, 2.4852e-20, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  89\n",
            "ce_loss:  tensor([9.5367e-07, 9.0236e-04, 4.2915e-06, 1.5135e+01, 9.0236e-04, 8.4638e-06,\n",
            "        1.5778e+01, 3.2551e-04, 1.3832e+01, 3.8973e+00, 9.5367e-07, -0.0000e+00,\n",
            "        9.0236e-04, 1.7881e-06, 4.6969e-04, 3.4571e-06, 1.2214e+01, 2.0266e-06,\n",
            "        5.7628e+00, 7.7688e+00, 1.4934e+01, 1.1921e-07, 9.0236e-04, 6.2189e-02,\n",
            "        8.8807e-05, -0.0000e+00, 2.1100e-05, 3.4571e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 8.3636e-16, 9.5114e-22, 1.8637e-42, 8.3636e-16, 1.2311e-20,\n",
            "        3.5998e-29, 1.0554e-04, 1.6713e-33, 6.6919e-30, 5.3552e-02, 6.1387e-03,\n",
            "        8.3636e-16, 1.9400e-04, 8.0055e-22, 2.0803e-20, 8.9639e-28, 4.9004e-36,\n",
            "        1.2075e-23, 1.0950e-40, 4.5855e-31, 6.2387e-05, 8.3636e-16, 0.0000e+00,\n",
            "        2.1060e-28, 3.8439e-20, 3.1101e-03, 2.0803e-20, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  90\n",
            "ce_loss:  tensor([5.3644e-06, 6.1987e-05, 7.9870e-06, 1.1919e+01, 6.1987e-05, 9.7751e-06,\n",
            "        1.3473e+01, 7.8742e-04, 1.4915e+01, 5.1603e+00, 5.3644e-06, -0.0000e+00,\n",
            "        6.1987e-05, 1.1921e-07, 3.1196e-03, 2.5034e-06, 1.1496e+01, 1.6689e-06,\n",
            "        6.4270e+00, 6.6778e+00, 1.4013e+01, 1.1921e-07, 6.1987e-05, 5.1317e-02,\n",
            "        5.8172e-05, -0.0000e+00, 3.3379e-06, 2.5034e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 9.2335e-16, 6.5385e-22, 1.3060e-42, 9.2335e-16, 8.4650e-21,\n",
            "        2.0856e-29, 1.0765e-04, 1.4710e-33, 5.1300e-30, 5.3935e-02, 6.0878e-03,\n",
            "        9.2335e-16, 2.1271e-04, 3.8228e-22, 9.7326e-21, 7.2084e-28, 3.9337e-36,\n",
            "        8.0252e-24, 4.1904e-41, 1.4843e-30, 6.2445e-05, 9.2335e-16, 0.0000e+00,\n",
            "        9.7133e-29, 1.9840e-20, 3.3306e-03, 9.7326e-21, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  91\n",
            "ce_loss:  tensor([9.5367e-07, 5.3404e-05, 4.0531e-06, 1.5541e+01, 5.3404e-05, 6.6757e-06,\n",
            "        1.1931e+01, 3.2551e-04, 1.8265e+01, 3.6106e+00, 9.5367e-07, -0.0000e+00,\n",
            "        5.3404e-05, 1.7881e-06, 2.9917e-04, 3.3379e-06, 9.3006e+00, 1.6689e-06,\n",
            "        6.1918e+00, 5.1693e+00, 1.4372e+01, 1.1921e-07, 5.3404e-05, 5.6408e-02,\n",
            "        7.2238e-05, -0.0000e+00, 2.7537e-05, 3.3379e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 5.8558e-16, 5.3724e-22, 8.5479e-43, 5.8558e-16, 6.1356e-21,\n",
            "        3.6542e-29, 1.0554e-04, 9.8855e-34, 4.2036e-30, 5.3552e-02, 6.1387e-03,\n",
            "        5.8558e-16, 1.9400e-04, 6.0263e-22, 5.9822e-21, 1.2919e-27, 2.5435e-36,\n",
            "        9.7027e-24, 8.4165e-41, 4.7641e-31, 6.2387e-05, 5.8558e-16, 0.0000e+00,\n",
            "        7.6778e-29, 3.4839e-20, 3.1434e-03, 5.9822e-21, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  92\n",
            "ce_loss:  tensor([5.3644e-06, 2.3722e-05, 1.1086e-05, 1.2587e+01, 2.3722e-05, 3.5763e-06,\n",
            "        1.3511e+01, 7.8742e-04, 1.5686e+01, 2.0657e+00, 5.3644e-06, -0.0000e+00,\n",
            "        2.3722e-05, 1.1921e-07, 2.4113e-04, 7.6294e-06, 1.0238e+01, 1.6689e-06,\n",
            "        7.8049e+00, 6.5966e+00, 1.4063e+01, 1.1921e-07, 2.3722e-05, 6.4857e-02,\n",
            "        4.8636e-05, -0.0000e+00, 3.4571e-06, 7.6294e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 4.4531e-16, 4.9940e-22, 6.1097e-43, 4.4531e-16, 2.8149e-21,\n",
            "        1.8359e-29, 1.0765e-04, 4.0448e-34, 3.2290e-30, 5.3935e-02, 6.0878e-03,\n",
            "        4.4531e-16, 2.1271e-04, 2.8401e-22, 5.2188e-21, 1.5907e-27, 1.9329e-36,\n",
            "        6.0127e-24, 5.8396e-41, 1.0643e-30, 6.2445e-05, 4.4531e-16, 0.0000e+00,\n",
            "        4.3768e-29, 1.7862e-20, 3.3744e-03, 5.2188e-21, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  93\n",
            "ce_loss:  tensor([9.5367e-07, 8.6423e-05, 3.6955e-06, 1.2227e+01, 8.6423e-05, 4.1723e-06,\n",
            "        1.1952e+01, 3.2551e-04, 1.3753e+01, 3.8522e+00, 9.5367e-07, -0.0000e+00,\n",
            "        8.6423e-05, 1.7881e-06, 2.4852e-04, 3.5763e-06, 1.0972e+01, 2.1458e-06,\n",
            "        7.5636e+00, 6.9719e+00, 1.5602e+01, 1.1921e-07, 8.6423e-05, 5.6963e-02,\n",
            "        5.0663e-05, -0.0000e+00, 1.7881e-05, 3.5763e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 2.3306e-16, 4.5065e-22, 1.3144e-42, 2.3306e-16, 1.9813e-21,\n",
            "        3.1831e-29, 1.0554e-04, 9.5937e-34, 2.6144e-30, 5.3552e-02, 6.1387e-03,\n",
            "        2.3306e-16, 1.9400e-04, 3.1884e-22, 6.4684e-21, 1.3153e-27, 1.5958e-36,\n",
            "        5.4240e-24, 5.3656e-41, 4.9564e-31, 6.2387e-05, 2.3306e-16, 0.0000e+00,\n",
            "        3.3095e-29, 2.4742e-20, 3.1859e-03, 6.4684e-21, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  94\n",
            "ce_loss:  tensor([5.3644e-06, 2.1219e-05, 1.0014e-05, 1.2595e+01, 2.1219e-05, 2.7418e-06,\n",
            "        1.4078e+01, 7.8742e-04, 1.5912e+01, 5.0770e+00, 5.3644e-06, -0.0000e+00,\n",
            "        2.1219e-05, 1.1921e-07, 6.1040e-04, 1.9073e-06, 1.2254e+01, 1.5497e-06,\n",
            "        7.1712e+00, 6.6193e+00, 1.6466e+01, 1.1921e-07, 2.1219e-05, 6.1111e-02,\n",
            "        3.0040e-05, -0.0000e+00, 3.4571e-06, 1.9073e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 2.1663e-16, 3.8955e-22, 1.5428e-42, 2.1663e-16, 8.9086e-22,\n",
            "        1.4729e-29, 1.0765e-04, 8.5335e-34, 1.4217e-30, 5.3935e-02, 6.0878e-03,\n",
            "        2.1663e-16, 2.1271e-04, 1.7356e-22, 2.7650e-21, 6.6919e-28, 2.6010e-36,\n",
            "        3.7923e-24, 3.3710e-41, 3.6224e-31, 6.2445e-05, 2.1663e-16, 0.0000e+00,\n",
            "        1.4546e-29, 1.3865e-20, 3.3955e-03, 2.7650e-21, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  95\n",
            "ce_loss:  tensor([9.5367e-07, 1.0133e-05, 3.4571e-06, 1.4684e+01, 1.0133e-05, 2.3842e-06,\n",
            "        1.5613e+01, 3.2551e-04, 1.6303e+01, 2.9628e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.0133e-05, 1.7881e-06, 1.5532e-04, 2.1458e-06, 1.1371e+01, 1.4305e-06,\n",
            "        5.6466e+00, 7.1336e+00, 1.4845e+01, 1.1921e-07, 1.0133e-05, 5.3017e-02,\n",
            "        3.7550e-05, -0.0000e+00, 1.9669e-05, 2.1458e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.2091e-16, 3.6796e-22, 8.0995e-43, 1.2091e-16, 1.0220e-21,\n",
            "        1.1746e-29, 1.0554e-04, 3.8656e-34, 1.2493e-30, 5.3552e-02, 6.1387e-03,\n",
            "        1.2091e-16, 1.9400e-04, 2.6047e-22, 2.4493e-21, 5.3127e-28, 2.2048e-36,\n",
            "        4.1893e-24, 3.1811e-41, 2.4494e-31, 6.2387e-05, 1.2091e-16, 0.0000e+00,\n",
            "        1.5871e-29, 9.7931e-21, 3.1953e-03, 2.4493e-21, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  96\n",
            "ce_loss:  tensor([5.3644e-06, 2.9206e-05, 1.0371e-05, 1.1848e+01, 2.9206e-05, 6.4373e-06,\n",
            "        1.3778e+01, 7.8742e-04, 1.5805e+01, 1.8100e+00, 5.3644e-06, -0.0000e+00,\n",
            "        2.9206e-05, 1.1921e-07, 2.6735e-04, 7.7486e-06, 1.1211e+01, 1.7881e-06,\n",
            "        6.5510e+00, 5.8741e+00, 1.3427e+01, 1.1921e-07, 2.9206e-05, 5.9885e-02,\n",
            "        2.3007e-05, -0.0000e+00, 3.4571e-06, 7.7486e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 8.0215e-17, 2.8088e-22, 8.2957e-43, 8.0215e-17, 5.4988e-22,\n",
            "        7.2997e-30, 1.0765e-04, 3.2205e-34, 1.9050e-30, 5.3935e-02, 6.0878e-03,\n",
            "        8.0215e-17, 2.1271e-04, 1.8522e-22, 2.1443e-21, 3.1422e-28, 2.0255e-36,\n",
            "        4.5851e-24, 2.3309e-41, 5.6629e-31, 6.2445e-05, 8.0215e-17, 0.0000e+00,\n",
            "        8.4145e-30, 1.0255e-20, 3.4001e-03, 2.1443e-21, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  97\n",
            "ce_loss:  tensor([9.5367e-07, 1.1206e-05, 3.3379e-06, 1.5128e+01, 1.1206e-05, 2.1458e-06,\n",
            "        1.1594e+01, 3.2551e-04, 1.3687e+01, 3.7241e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.1206e-05, 1.7881e-06, 1.0299e-04, 4.7684e-06, 1.0500e+01, 1.6689e-06,\n",
            "        7.6681e+00, 7.5492e+00, 1.4612e+01, 1.1921e-07, 1.1206e-05, 5.0008e-02,\n",
            "        4.2438e-05, -0.0000e+00, 1.8239e-05, 4.7684e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 6.4616e-17, 2.8570e-22, 5.4090e-43, 6.4616e-17, 7.3684e-22,\n",
            "        1.1447e-29, 1.0554e-04, 5.2284e-34, 1.6461e-30, 5.3552e-02, 6.1387e-03,\n",
            "        6.4616e-17, 1.9400e-04, 2.2339e-22, 2.8907e-21, 2.6747e-28, 2.1793e-36,\n",
            "        3.3064e-24, 1.8871e-41, 2.6576e-31, 6.2387e-05, 6.4616e-17, 0.0000e+00,\n",
            "        7.6742e-30, 6.8688e-21, 3.1989e-03, 2.8907e-21, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  98\n",
            "ce_loss:  tensor([5.3644e-06, 5.3644e-06, 6.3181e-06, 1.2534e+01, 5.3644e-06, 1.1921e-06,\n",
            "        1.2746e+01, 7.8742e-04, 1.4803e+01, 4.5225e+00, 5.3644e-06, -0.0000e+00,\n",
            "        5.3644e-06, 1.1921e-07, 2.0621e-04, 6.5565e-06, 8.9167e+00, 1.4305e-06,\n",
            "        6.7271e+00, 6.3392e+00, 1.6927e+01, 1.1921e-07, 5.3644e-06, 4.1950e-02,\n",
            "        2.1219e-05, -0.0000e+00, 3.4571e-06, 6.5565e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 3.3747e-17, 2.0422e-22, 3.5173e-43, 3.3747e-17, 3.8772e-22,\n",
            "        1.0189e-29, 1.0765e-04, 1.1395e-33, 1.0298e-30, 5.3935e-02, 6.0878e-03,\n",
            "        3.3747e-17, 2.1271e-04, 1.3335e-22, 2.5123e-21, 4.8971e-28, 1.6127e-36,\n",
            "        2.4535e-24, 9.8904e-42, 2.1490e-31, 6.2445e-05, 3.3747e-17, 0.0000e+00,\n",
            "        5.3578e-30, 1.0805e-20, 3.4001e-03, 2.5123e-21, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  99\n",
            "ce_loss:  tensor([9.5367e-07, 2.5391e-05, 2.8610e-06, 1.4799e+01, 2.5391e-05, 1.3113e-06,\n",
            "        1.5295e+01, 3.2551e-04, 1.4968e+01, 2.7837e+00, 9.5367e-07, -0.0000e+00,\n",
            "        2.5391e-05, 1.7881e-06, 8.1297e-05, 3.2186e-06, 9.3950e+00, 1.7881e-06,\n",
            "        5.4443e+00, 5.0777e+00, 1.5164e+01, 1.1921e-07, 2.5391e-05, 6.4237e-02,\n",
            "        3.7073e-05, -0.0000e+00, 1.8239e-05, 3.2186e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 2.6728e-17, 1.6308e-22, 2.6625e-43, 2.6728e-17, 1.3729e-22,\n",
            "        7.6119e-30, 1.0554e-04, 5.4178e-34, 7.4133e-31, 5.3552e-02, 6.1387e-03,\n",
            "        2.6728e-17, 1.9400e-04, 7.9936e-23, 2.2176e-21, 8.2115e-28, 8.3784e-37,\n",
            "        2.4493e-24, 2.0777e-41, 1.0287e-31, 6.2387e-05, 2.6728e-17, 0.0000e+00,\n",
            "        3.5654e-30, 5.5248e-21, 3.1989e-03, 2.2176e-21, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "PGD linf: Attack effectiveness 72.414%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different kinds of GKDE"
      ],
      "metadata": {
        "id": "ghFEgIubQVGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-select benign samples\n",
        "benign_samples = []\n",
        "for x_batch, y_batch in test_loader:\n",
        "  benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "# Forward pass to get logits for benign samples\n",
        "with torch.no_grad():  # No need for gradients\n",
        "    outputs = model_AT_rFGSM(ben_x.to(torch.float32))\n",
        "\n",
        "# Calculate softmax probabilities\n",
        "probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "# Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "# Select the top 1000 high confidence benign samples\n",
        "top_1000_high_confidence_benign_samples = ben_x[sorted_indices[:1000]]\n",
        "\n",
        "# Set to track indices of samples to be removed\n",
        "removed_set = set()\n",
        "\n",
        "# Step 1: Identify similar samples\n",
        "for i in range(len(top_1000_high_confidence_benign_samples)):\n",
        "    if i in removed_set:\n",
        "        continue\n",
        "    sample1 = top_1000_high_confidence_benign_samples[i]\n",
        "    for j in range(i + 1, len(top_1000_high_confidence_benign_samples)):\n",
        "        if j in removed_set:\n",
        "            continue\n",
        "        sample2 = top_1000_high_confidence_benign_samples[j]\n",
        "        if torch.abs(sample1 - sample2).sum().item() < 5:\n",
        "            removed_set.add(j)\n",
        "\n",
        "# Step 2: Select samples that are not in removed_set\n",
        "selected_samples = [sample for idx, sample in enumerate(top_1000_high_confidence_benign_samples) if idx not in removed_set]\n",
        "\n",
        "# Step 3: Stack tensors along a new dimension\n",
        "selected_benigns = torch.stack(selected_samples)\n",
        "\n",
        "del benign_samples, outputs, probabilities, top_1000_high_confidence_benign_samples, selected_samples   # Free up memory"
      ],
      "metadata": {
        "id": "YLO4kDBORXNj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done"
      ],
      "metadata": {
        "id": "sbCNYQPAQvLJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multidimensional KDE implementation\n",
        "def KDE(x, data, bandwidth,kernel):\n",
        "    \"\"\"\n",
        "    Compute the kernel density estimate (KDE) for given data points.\n",
        "\n",
        "    Parameters:\n",
        "        x (torch.Tensor): Points at which to evaluate the KDE (shape: [num_samples, num_dimensions]).\n",
        "        data (torch.Tensor): Data points used to estimate the density (shape: [num_data_points, num_dimensions]).\n",
        "        bandwidth (float): Bandwidth parameter for the KDE.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Density estimate at each point in x (shape: [num_samples,]).\n",
        "    \"\"\"\n",
        "    n = data.shape[0]  # Number of data points\n",
        "    d = x.shape[1]  # Dimensionality of the data\n",
        "\n",
        "    # Convert bandwidth to tensor\n",
        "    bandwidth_tensor = torch.tensor(bandwidth)\n",
        "\n",
        "    # Calculate standardized distances for all data points\n",
        "    u = torch.abs(x[:, None, :] - data)\n",
        "\n",
        "    # Compute kernel contributions for all data points\n",
        "    if kernel == 'gaussian':\n",
        "        kernel_contributions = 10. * torch.exp(-0.5 * torch.sum(u**2, dim=-1) / bandwidth**2)\n",
        "    else:\n",
        "        kernel_contributions = (1./d) * torch.exp(-1. * torch.sum(u, dim=-1) / bandwidth_tensor)\n",
        "\n",
        "    # Sum contributions across all data points\n",
        "    estimate = torch.mean(kernel_contributions, dim=1)\n",
        "\n",
        "    # Normalize the density estimate by the number of points and the bandwidth raised to the dimensionality\n",
        "    #estimate /= (bandwidth_tensor ** d)\n",
        "\n",
        "    return estimate\n"
      ],
      "metadata": {
        "id": "izCn5s8Y7tdK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde_old(adv_x,y,model,benigns, bandwidth, penalty_factor,kernel):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y.view(-1).long())\n",
        "    #print('ce: ', ce)\n",
        "    kde = KDE(adv_x, benigns, bandwidth,kernel)\n",
        "    #print('kde : ', kde)\n",
        "    loss_no_reduction = ce + penalty_factor * kde\n",
        "    return loss_no_reduction\n",
        "\n",
        "\n",
        "def gkde_old(x, y, model,bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde_old(x_var,y,model,bens, bandwidth, penalty_factor, kernel)\n",
        "        #loss,_ = get_loss_kde(x_var,y,model,bens, bandwidth, penalty_factor)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "        pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        #pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x_next - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "\n",
        "            val, _ = torch.topk(torch.abs(gradients), 1)\n",
        "            perturbation = (torch.abs(gradients) >= val.expand_as(gradients)).float() * torch.sign(gradients).float()\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(x_next)\n",
        "    loss_adv = criterion(outputs, y.view(-1).long()).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "y8be3sudY43I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde_old(adv_x,y,model,benigns, bandwidth, penalty_factor,kernel):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y.view(-1).long())\n",
        "    #print('ce: ', ce)\n",
        "    kde = KDE(adv_x, benigns, bandwidth,kernel)\n",
        "    #print('kde : ', kde)\n",
        "    loss_no_reduction = ce + penalty_factor * kde\n",
        "    return loss_no_reduction\n",
        "\n",
        "\n",
        "def gkde_old2(x, y, model,bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde_old(x_var,y,model,bens, bandwidth, penalty_factor, kernel)\n",
        "        #loss,_ = get_loss_kde(x_var,y,model,bens, bandwidth, penalty_factor)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x_next - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "\n",
        "            val, _ = torch.topk(torch.abs(gradients), 1)\n",
        "            perturbation = (torch.abs(gradients) >= val.expand_as(gradients)).float() * torch.sign(gradients).float()\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(x_next)\n",
        "    loss_adv = criterion(outputs, y.view(-1).long()).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "co0DuPkBMf2Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    if (attack ==  gkde) or (attack ==  mimicry) or (attack ==  gkde_old) or (attack ==  gkde_old2):\n",
        "      # Pre-select benign samples\n",
        "      benign_samples = []\n",
        "      for x_batch, y_batch in test_loader:\n",
        "        benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "      ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "      # Forward pass to get logits for benign samples\n",
        "      with torch.no_grad():  # No need for gradients\n",
        "          outputs = model_AT_rFGSM(ben_x.to(torch.float32))\n",
        "\n",
        "      # Calculate softmax probabilities\n",
        "      probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "      # Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "      sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "      # Select the top 500 high confidence benign samples\n",
        "      top_1000_high_confidence_benign_samples = ben_x[sorted_indices[:1000]]\n",
        "\n",
        "      # Set to track indices of samples to be removed\n",
        "      removed_set = set()\n",
        "\n",
        "      # Step 1: Identify similar samples\n",
        "      for i in range(len(top_1000_high_confidence_benign_samples)):\n",
        "          if i in removed_set:\n",
        "              continue\n",
        "          sample1 = top_1000_high_confidence_benign_samples[i]\n",
        "          for j in range(i + 1, len(top_1000_high_confidence_benign_samples)):\n",
        "              if j in removed_set:\n",
        "                  continue\n",
        "              sample2 = top_1000_high_confidence_benign_samples[j]\n",
        "              if torch.abs(sample1 - sample2).sum().item() < 5:\n",
        "                  removed_set.add(j)\n",
        "\n",
        "      # Step 2: Select samples that are not in removed_set\n",
        "      selected_samples = [sample for idx, sample in enumerate(top_1000_high_confidence_benign_samples) if idx not in removed_set]\n",
        "\n",
        "      # Step 3: Stack tensors along a new dimension\n",
        "      selected_benigns = torch.stack(selected_samples)\n",
        "\n",
        "      del benign_samples, outputs, probabilities, ben_x, top_1000_high_confidence_benign_samples, selected_samples   # Free up memory\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            if attack == mimicry:\n",
        "                pertb_mal_x = mimicry(top_500_high_confidence_benign_samples, mal_x_batch, model, **kwargs)\n",
        "            elif (attack == gkde):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old2):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old2(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            else :\n",
        "                with torch.enable_grad():\n",
        "                    pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "p5bJtpokI_uu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adv_predict0(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    if (attack ==  gkde) or (attack ==  mimicry) or (attack ==  gkde_old) or (attack ==  gkde_old2):\n",
        "      # Pre-select benign samples\n",
        "      benign_samples = []\n",
        "      for x_batch, y_batch in test_loader:\n",
        "        benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "      ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "      # Forward pass to get logits for benign samples\n",
        "      with torch.no_grad():  # No need for gradients\n",
        "          outputs = model_AT_rFGSM(ben_x.to(torch.float32))\n",
        "\n",
        "      # Calculate softmax probabilities\n",
        "      probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "      # Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "      sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "      # Select the top 500 high confidence benign samples\n",
        "      selected_benigns = ben_x[sorted_indices[:500]]\n",
        "\n",
        "\n",
        "      del benign_samples, outputs, probabilities, ben_x   # Free up memory\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            if attack == mimicry:\n",
        "                pertb_mal_x = mimicry(selected_benigns, mal_x_batch, model, **kwargs)\n",
        "            elif (attack == gkde):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old2):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old2(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            else :\n",
        "                with torch.enable_grad():\n",
        "                    pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Gv05ZocqXdFt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adv_predict2(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    if (attack ==  gkde) or (attack ==  mimicry) or (attack ==  gkde_old) or (attack ==  gkde_old2):\n",
        "      # Pre-select benign samples\n",
        "      benign_samples = []\n",
        "      for x_batch, y_batch in test_loader:\n",
        "        benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "      ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "      selected_samples = ben_x[100]\n",
        "      del benign_samples\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            if attack == mimicry:\n",
        "                pertb_mal_x = mimicry(top_500_high_confidence_benign_samples, mal_x_batch, model, **kwargs)\n",
        "            elif (attack == gkde):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old2):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old2(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            else :\n",
        "                with torch.enable_grad():\n",
        "                    pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n"
      ],
      "metadata": {
        "id": "mX2CUeHZ0j3m"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old(maximize loss & default removal insertion arrays), adv_predict(top benigns), laplacian\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e5]\n",
        "bandwidths = [1.,2.,3.,5.,10.,15.,20.,30.,40.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbc498a-4692-4dee-eb5b-b960fd31245a",
        "id": "N_lecXmXcIEa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 76.37%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 83.45%.\n",
            "********* penalty_factor: 1 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 70.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 67.52%.\n",
            "********* penalty_factor: 1 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 70.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 62.83%.\n",
            "********* penalty_factor: 1 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 64.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 58.05%.\n",
            "********* penalty_factor: 1 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.46%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.77%.\n",
            "********* penalty_factor: 1 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 54.6%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.42%.\n",
            "********* penalty_factor: 1 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.24%.\n",
            "********* penalty_factor: 1 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.15%.\n",
            "********* penalty_factor: 1 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.06%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 75.4%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 79.38%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 68.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 65.04%.\n",
            "********* penalty_factor: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.43%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 66.11%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 60.8%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 64.51%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.11%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.42%.\n",
            "********* penalty_factor: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.08%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.24%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.62%.\n",
            "********* penalty_factor: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.53%.\n",
            "********* penalty_factor: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 75.22%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 77.35%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 68.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 63.19%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 65.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 59.73%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 61.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.27%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.11%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "********* penalty_factor: 100 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.8%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 100 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "********* penalty_factor: 100 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 73.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 75.66%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 64.69%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 61.86%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 64.6%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 57.26%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 57.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.65%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "********* penalty_factor: 1000 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.53%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 1000 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "********* penalty_factor: 1000 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 73.45%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 72.3%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 63.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 59.47%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 63.98%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 55.4%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 58.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.42%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.15%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 71.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 69.91%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 63.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 57.7%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 59.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.98%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.8%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.45%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.8%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old(maximize loss & default removal insertion arrays), adv_predict2(random benigns), laplacian\n",
        "\n",
        "penalty_factors = [10,100,1000,1e4,1e5]\n",
        "bandwidths = [5.,10.,15.,20.,30.,40.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e71da573-eba6-4745-f572-d34968d0ac89",
        "id": "NOaZSky91tDS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 61.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.48%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.81%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.33%.\n",
            "********* penalty_factor: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.1%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.8%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "********* penalty_factor: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.62%.\n",
            "********* penalty_factor: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.74%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 59.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.68%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.06%.\n",
            "********* penalty_factor: 100 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.53%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 100 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.46%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.15%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "********* penalty_factor: 1000 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 1000 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.55%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 30.0 ******************\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3e10c3c015c9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'laplac'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0madv_predict2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_AT_rFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattack_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'laplac'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-d49f3b3ba210>\u001b[0m in \u001b[0;36madv_predict2\u001b[0;34m(test_loader, model, attack, device, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                   \u001b[0mpertb_mal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmal_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmal_y_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_benigns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-87032243fc66>\u001b[0m in \u001b[0;36mgkde_old\u001b[0;34m(x, y, model, bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k, step_length, norm, initial_rounding_threshold, round_threshold, random, is_report_loss_diff, is_sample)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mperturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;31m# stop perturbing the examples that are successful to evade the victim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old(maximize loss & default removal insertion arrays), adv_predict0(top 500 confidence benigns), laplacian\n",
        "\n",
        "penalty_factors = [10,100,1000]\n",
        "bandwidths = [10.,20.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict0(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict0(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict0(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8MXwjFUYd2j",
        "outputId": "6e298225-9a15-40b9-b58e-c90082c8aa4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.43%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.19%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 54.69%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.04%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 55.22%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.57%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.74%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.59%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.77%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.5%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tEuv7Nay1Xky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old2(maximize loss & updated removal insertion arrays), adv_predict(top benigns), laplacian\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e5]\n",
        "bandwidths = [1.,2.,3.,5.,10.,15.,20.,30.,40.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1rExFdCclMZ",
        "outputId": "8be64a63-679e-416c-edde-42812c239161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.2%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "********* penalty_factor: 1 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.61%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 1 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.43%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "********* penalty_factor: 1 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.28%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.54%.\n",
            "********* penalty_factor: 1 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.63%.\n",
            "********* penalty_factor: 1 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.72%.\n",
            "********* penalty_factor: 1 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.16%.\n",
            "********* penalty_factor: 1 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.07%.\n",
            "********* penalty_factor: 1 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.45%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.03%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.26%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.61%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.48%.\n",
            "********* penalty_factor: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.98%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.04%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.67%.\n",
            "********* penalty_factor: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.17%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.31%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.96%.\n",
            "********* penalty_factor: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.42%.\n",
            "********* penalty_factor: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.51%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.38%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.61%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.92%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.47%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.81%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.87%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.69%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.03%.\n",
            "********* penalty_factor: 100 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.29%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.37%.\n",
            "********* penalty_factor: 100 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.6%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.29%.\n",
            "********* penalty_factor: 100 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.2%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.19%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.16%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "********* penalty_factor: 1000 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.0%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.24%.\n",
            "********* penalty_factor: 1000 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.42%.\n",
            "********* penalty_factor: 1000 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.95%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.78%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.38%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.62%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.74%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.07%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.07%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.89%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.3%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.64%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.58%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.48%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.19%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.45%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.07%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.49%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.34%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.84%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.34%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.02%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.66%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.25%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.66%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old2(maximize loss & updated removal insertion arrays), adv_predict2(random benigns), laplacian\n",
        "\n",
        "penalty_factors = [1,10,100]\n",
        "bandwidths = [5.,10.,20.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "93dw0vya6Khr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "54698735-a6e3-49ff-8d16-021f269f5f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1 bandwidth: 5.0 ******************\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ee9ea09d1222>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'laplac'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0madv_predict2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_AT_rFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattack_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'laplac'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d49f3b3ba210>\u001b[0m in \u001b[0;36madv_predict2\u001b[0;34m(test_loader, model, attack, device, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                   \u001b[0mpertb_mal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmal_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmal_y_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_benigns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-406d89ad1df1>\u001b[0m in \u001b[0;36mgkde_old2\u001b[0;34m(x, y, model, bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k, step_length, norm, initial_rounding_threshold, round_threshold, random, is_report_loss_diff, is_sample)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# stop perturbing the examples that are successful to evade the victim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mperturbation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Obl7KqKq7ZYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70iWYt9Z7ZPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old(maximize loss & default removal insertion arrays), adv_predict(top benigns), gaussian\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e5]\n",
        "bandwidths = [0.1,0.2,0.4,0.6,1.,1.5,2.,5.,10.,20.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "-6go4nY4ck6Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "116f5138-3e86-4176-d361-42ac032e494a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1 bandwidth: 0.1 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 1 bandwidth: 0.2 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 1 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 84.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 83.01%.\n",
            "********* penalty_factor: 1 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 79.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 66.11%.\n",
            "********* penalty_factor: 1 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 54.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.5%.\n",
            "********* penalty_factor: 1 bandwidth: 1.5 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.38%.\n",
            "********* penalty_factor: 1 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 1 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "********* penalty_factor: 1 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "********* penalty_factor: 10 bandwidth: 0.1 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 10 bandwidth: 0.2 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 10 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 85.13%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 81.15%.\n",
            "********* penalty_factor: 10 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.47%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 75.22%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 63.89%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.72%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.5%.\n",
            "********* penalty_factor: 10 bandwidth: 1.5 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.47%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.25%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 0.1 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 100 bandwidth: 0.2 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 100 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 85.13%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 80.62%.\n",
            "********* penalty_factor: 100 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.47%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 75.93%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 62.83%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.88%.\n",
            "********* penalty_factor: 100 bandwidth: 1.5 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.64%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.1 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.2 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 84.16%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 77.35%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 70.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 60.8%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.88%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.5 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.1 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.2 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 82.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 76.19%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 68.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 59.47%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.62%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.5 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-61c3583ca183>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0madv_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_AT_rFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattack_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d2a3f468a2b5>\u001b[0m in \u001b[0;36madv_predict\u001b[0;34m(test_loader, model, attack, device, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                   \u001b[0mpertb_mal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmal_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmal_y_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_benigns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-87032243fc66>\u001b[0m in \u001b[0;36mgkde_old\u001b[0;34m(x, y, model, bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k, step_length, norm, initial_rounding_threshold, round_threshold, random, is_report_loss_diff, is_sample)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss_kde_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m#loss,_ = get_loss_kde(x_var,y,model,bens, bandwidth, penalty_factor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Compute gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-406d89ad1df1>\u001b[0m in \u001b[0;36mget_loss_kde_old\u001b[0;34m(adv_x, y, model, benigns, bandwidth, penalty_factor, kernel)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print('ce: ', ce)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mkde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKDE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenigns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print('kde : ', kde)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss_no_reduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpenalty_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d0c6fc92c747>\u001b[0m in \u001b[0;36mKDE\u001b[0;34m(x, data, bandwidth, kernel)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Calculate standardized distances for all data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Compute kernel contributions for all data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old(maximize loss & default removal insertion arrays), adv_predict2(random benigns), gaussian\n",
        "\n",
        "penalty_factors = [10,100,1000]\n",
        "bandwidths = [10.,20.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "85LtK4b11ycV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74f9c55-e39b-4983-cbcd-14a0f9ed07e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.34%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old2(maximize loss & updated removal insertion arrays), adv_predict(top benigns), gaussian\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e5]\n",
        "bandwidths = [0.6,0.8,1.,2.,3.,4.,5.,10.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "50ny1UUT78eY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "715849d3-4d2d-48a6-bf90-00669aeeb985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.59%.\n",
            "********* penalty_factor: 1 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.89%.\n",
            "********* penalty_factor: 1 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "********* penalty_factor: 1 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.4%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.11%.\n",
            "********* penalty_factor: 1 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.98%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.43%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "********* penalty_factor: 1 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "********* penalty_factor: 1 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.03%.\n",
            "********* penalty_factor: 1 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.67%.\n",
            "********* penalty_factor: 10 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* penalty_factor: 10 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.03%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.4%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.81%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.3%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.2%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "********* penalty_factor: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.17%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.47%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.38%.\n",
            "********* penalty_factor: 10 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.38%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.25%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.03%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.26%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.07%.\n",
            "********* penalty_factor: 100 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.1%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.38%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 100 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.81%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.45%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.93%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ab094ca2a76e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'linf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0madv_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_AT_rFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattack_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-d2a3f468a2b5>\u001b[0m in \u001b[0;36madv_predict\u001b[0;34m(test_loader, model, attack, device, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                   \u001b[0mpertb_mal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmal_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmal_y_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_benigns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-406d89ad1df1>\u001b[0m in \u001b[0;36mgkde_old2\u001b[0;34m(x, y, model, bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k, step_length, norm, initial_rounding_threshold, round_threshold, random, is_report_loss_diff, is_sample)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#loss,_ = get_loss_kde(x_var,y,model,bens, bandwidth, penalty_factor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Compute gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mgrad_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old2(maximize loss & updated removal insertion arrays), adv_predict2(random benigns), gaussian\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e5]\n",
        "bandwidths = [0.6,0.8,1.,2.,3.,4.,5.,10.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "J677NN_j78eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAkAN5JU778i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde(adv_x,y,model,benigns, bandwidth, penalty_factor,kernel):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute CE loss\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y)\n",
        "    #print('ce ',ce)\n",
        "\n",
        "    # Compute KDE loss\n",
        "    kde = KDE(adv_x, benigns, bandwidth,kernel)\n",
        "    #print('kde ',kde)\n",
        "\n",
        "    # Combine the losses with the penalty factor\n",
        "    loss = ce - penalty_factor * kde\n",
        "    #print('loss ',loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def gkde(x, y, model,bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k=25, step_length=0.02, norm='linf',\n",
        "        initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "\n",
        "    :param x: Feature vector\n",
        "    :param y: Ground truth labels\n",
        "    :param model: Neural network model\n",
        "    :param RBFModel: Gaussian model for KDE\n",
        "    :param insertion_array: Array for insertion operations\n",
        "    :param removal_array: Array for removal operations\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf', 'l2', 'l1')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    #target's class\n",
        "    traget_labels = torch.zeros_like(y.view(-1).long())\n",
        "\n",
        "    # Compute natural loss and penalty_factor\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), traget_labels)\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    for t in range(k):\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde(x_var,traget_labels,model,bens, bandwidth, penalty_factor, kernel)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "        elif norm == 'l1':\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1', 'l2', or 'linf' norm.\")\n",
        "\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    if random:\n",
        "        round_threshold = torch.rand_like(x_next)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_adv = criterion(model(x_next), traget_labels).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size(0) * 100:.3f}%.\")\n",
        "\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "xZkNUBSk7zzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fixed penalty factor\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e6,1e8,1e10]\n",
        "bandwidths = [1.,2.,3.,5.,10.,15.,20.,30.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "t9vYJDKg7zzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a5d89d-389d-4dc7-cd66-5ec1d90d966b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 1.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "********* penalty_factor: 100 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.44%.\n",
            "********* penalty_factor: 100 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.73%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.57%.\n",
            "********* penalty_factor: 1000 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.27%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.68%.\n",
            "********* penalty_factor: 1000 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.31%.\n",
            "********* penalty_factor: 1000 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.59%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.39%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.83%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.49%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.65%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.56%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.74%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.11%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.75%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.73%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.15%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.26%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.19%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.61%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.78%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.11%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.55%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.72%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.74%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.16%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.61%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.19%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.02%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.76%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.45%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.4%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.29%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.13%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.12%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.72%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.14%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.99%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.07%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.2%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fixed penalty factor\n",
        "\n",
        "penalty_factors = [100,1000,1e4,1e6,1e8]\n",
        "bandwidths = [0.6,0.8,1.,2.,3.,4.,5.,10.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)"
      ],
      "metadata": {
        "id": "LS1hW_l_JQW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2ccae5-abe1-48cf-fe41-c60013663fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 100 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.47%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.93%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "********* penalty_factor: 100 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.38%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.08%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.9%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.8%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.47%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 26.64%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.36%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.62%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.28%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.98%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.81%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.22%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.87%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.55%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.37%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.33%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.69%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.37%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.55%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.37%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde(adv_x,y,model,benigns, bandwidth, penalty_factor,kernel,alpha):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute CE loss\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y)\n",
        "    #print('ce ',ce)\n",
        "\n",
        "    # Compute KDE loss\n",
        "    kde = KDE(adv_x, benigns, bandwidth,kernel)\n",
        "    #print('kde ',kde)\n",
        "\n",
        "    # Combine the losses with the penalty factor\n",
        "    loss = alpha * ce - penalty_factor * kde\n",
        "    #print('loss ',loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def gkde(x, y, model,bens, bandwidth, penalty_factor, kernel,t_threshold, insertion_array, removal_array, k=25, step_length=0.02, norm='linf',\n",
        "        initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "\n",
        "    :param x: Feature vector\n",
        "    :param y: Ground truth labels\n",
        "    :param model: Neural network model\n",
        "    :param RBFModel: Gaussian model for KDE\n",
        "    :param insertion_array: Array for insertion operations\n",
        "    :param removal_array: Array for removal operations\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf', 'l2', 'l1')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    #target's class\n",
        "    traget_labels = torch.zeros_like(y.view(-1).long())\n",
        "\n",
        "    # Compute natural loss and penalty_factor\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), traget_labels)\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    for t in range(k):\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "\n",
        "        if t<t_threshold:\n",
        "          alpha = 0.\n",
        "        else:\n",
        "          alpha = 1.\n",
        "          penalty_factor = 0.\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde(x_var,traget_labels,model,bens, bandwidth, penalty_factor, kernel,alpha)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "        elif norm == 'l1':\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1', 'l2', or 'linf' norm.\")\n",
        "\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    if random:\n",
        "        round_threshold = torch.rand_like(x_next)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_adv = criterion(model(x_next), traget_labels).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size(0) * 100:.3f}%.\")\n",
        "\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "QRoDHAXrjpgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first t_threshold steps, kde , then ce(penalty factor is meaningless here)\n",
        "\n",
        "t_thresholds = [3,5,10,15,20,25,30]\n",
        "bandwidths = [1.,2.,3.,5.,10.,15.,20.,30.,40.]\n",
        "\n",
        "for t_threshold in t_thresholds:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','t_threshold:',t_threshold,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'laplac','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'laplac','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'laplac','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwm37B4apzFE",
        "outputId": "3fc548cd-4207-4702-f3b2-5aa8f80ad13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* t_threshold: 3 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "********* t_threshold: 3 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 3 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.8%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 3 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 3 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.07%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.35%.\n",
            "********* t_threshold: 3 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.62%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.07%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.5%.\n",
            "********* t_threshold: 3 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 3 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.16%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.61%.\n",
            "********* t_threshold: 3 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.61%.\n",
            "********* t_threshold: 5 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.58%.\n",
            "********* t_threshold: 5 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.96%.\n",
            "********* t_threshold: 5 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.96%.\n",
            "********* t_threshold: 5 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.23%.\n",
            "********* t_threshold: 5 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.66%.\n",
            "********* t_threshold: 5 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.0%.\n",
            "********* t_threshold: 5 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.18%.\n",
            "********* t_threshold: 5 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.18%.\n",
            "********* t_threshold: 5 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.18%.\n",
            "********* t_threshold: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.64%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.24%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.5%.\n",
            "********* t_threshold: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.02%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 20.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.41%.\n",
            "********* t_threshold: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.66%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.76%.\n",
            "********* t_threshold: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.67%.\n",
            "********* t_threshold: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.16%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.23%.\n",
            "********* t_threshold: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.67%.\n",
            "********* t_threshold: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "********* t_threshold: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* t_threshold: 15 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 15 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.14%.\n",
            "********* t_threshold: 15 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.41%.\n",
            "********* t_threshold: 15 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.47%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.5%.\n",
            "********* t_threshold: 15 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.85%.\n",
            "********* t_threshold: 15 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.03%.\n",
            "********* t_threshold: 15 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.18%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.5%.\n",
            "********* t_threshold: 15 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.23%.\n",
            "********* t_threshold: 15 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.23%.\n",
            "********* t_threshold: 20 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.29%.\n",
            "********* t_threshold: 20 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 20 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.99%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.14%.\n",
            "********* t_threshold: 20 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.11%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.24%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.76%.\n",
            "********* t_threshold: 20 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.5%.\n",
            "********* t_threshold: 20 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.75%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.32%.\n",
            "********* t_threshold: 20 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.75%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.85%.\n",
            "********* t_threshold: 20 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.66%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 20 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.03%.\n",
            "********* t_threshold: 25 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.79%.\n",
            "********* t_threshold: 25 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.61%.\n",
            "********* t_threshold: 25 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.7%.\n",
            "********* t_threshold: 25 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.37%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 25 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 25 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.14%.\n",
            "********* t_threshold: 25 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.62%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 25 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.57%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n",
            "********* t_threshold: 25 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.41%.\n",
            "********* t_threshold: 30 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.14%.\n",
            "********* t_threshold: 30 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.17%.\n",
            "********* t_threshold: 30 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.35%.\n",
            "********* t_threshold: 30 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.17%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.52%.\n",
            "********* t_threshold: 30 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 30 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.79%.\n",
            "********* t_threshold: 30 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.41%.\n",
            "********* t_threshold: 30 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n",
            "********* t_threshold: 30 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.57%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ysfPisrTn8n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first t_threshold steps, kde , then ce(penalty factor is meaningless here)\n",
        "\n",
        "t_thresholds = [3,5,10,15,20,25,30]\n",
        "bandwidths = [0.4,0.6,0.8,1.,2.,3.,4.,5.,10.]\n",
        "\n",
        "for t_threshold in t_thresholds:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','t_threshold:',t_threshold,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'gaussian','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'gaussian','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'gaussian','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a04f247-bc40-4e12-c0ee-dbbb1e7967dc",
        "id": "D3fLArOwsQRr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* t_threshold: 3 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n",
            "********* t_threshold: 3 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.6%.\n",
            "********* t_threshold: 3 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.5%.\n",
            "********* t_threshold: 3 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.97%.\n",
            "********* t_threshold: 3 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.18%.\n",
            "********* t_threshold: 3 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.62%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.77%.\n",
            "********* t_threshold: 3 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.34%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "********* t_threshold: 3 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* t_threshold: 3 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.89%.\n",
            "********* t_threshold: 5 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n",
            "********* t_threshold: 5 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.24%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.76%.\n",
            "********* t_threshold: 5 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.43%.\n",
            "********* t_threshold: 5 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 20.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "********* t_threshold: 5 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.0%.\n",
            "********* t_threshold: 5 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.89%.\n",
            "********* t_threshold: 5 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.16%.\n",
            "********* t_threshold: 5 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.57%.\n",
            "********* t_threshold: 5 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.51%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* t_threshold: 10 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.2%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.0%.\n",
            "********* t_threshold: 10 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.23%.\n",
            "********* t_threshold: 10 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.81%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.15%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.29%.\n",
            "********* t_threshold: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.76%.\n",
            "********* t_threshold: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.77%.\n",
            "********* t_threshold: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.86%.\n",
            "********* t_threshold: 10 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.24%.\n",
            "********* t_threshold: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "********* t_threshold: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.74%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.84%.\n",
            "********* t_threshold: 15 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 15 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.32%.\n",
            "********* t_threshold: 15 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.08%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.32%.\n",
            "********* t_threshold: 15 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.14%.\n",
            "********* t_threshold: 15 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.77%.\n",
            "********* t_threshold: 15 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n",
            "********* t_threshold: 15 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* t_threshold: 15 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.48%.\n",
            "********* t_threshold: 15 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* t_threshold: 20 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KYqW-o_jo56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde(adv_x,y,model,benigns, bandwidth, penalty_factor,kernel):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute CE loss\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y)\n",
        "    #print('ce ',ce)\n",
        "\n",
        "    # Compute KDE loss\n",
        "    kde = KDE(adv_x, benigns, bandwidth,kernel)\n",
        "    #print('kde ',kde)\n",
        "\n",
        "    # Combine the losses with the penalty factor\n",
        "    loss = ce - penalty_factor * kde\n",
        "    #print('loss ',loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def gkde(x, y, model,bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k=25, step_length=0.02, norm='linf',\n",
        "        initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "\n",
        "    :param x: Feature vector\n",
        "    :param y: Ground truth labels\n",
        "    :param model: Neural network model\n",
        "    :param RBFModel: Gaussian model for KDE\n",
        "    :param insertion_array: Array for insertion operations\n",
        "    :param removal_array: Array for removal operations\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf', 'l2', 'l1')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    #target's class\n",
        "    traget_labels = torch.zeros_like(y.view(-1).long())\n",
        "\n",
        "    # Compute natural loss and penalty_factor\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), traget_labels)\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    for t in range(k):\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        decayed_penalty_factor = penalty_factor * (1 - t / k)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde(x_var,traget_labels,model,bens, bandwidth, decayed_penalty_factor, kernel)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "        elif norm == 'l1':\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1', 'l2', or 'linf' norm.\")\n",
        "\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    if random:\n",
        "        round_threshold = torch.rand_like(x_next)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_adv = criterion(model(x_next), traget_labels).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size(0) * 100:.3f}%.\")\n",
        "\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "JACMzQSN0xDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "penalty_factors = [1.,10,100,1000,1e4,1e5,1e6,1e7,1e8]\n",
        "bandwidths = [1.,2.,3.,5.,10.,15.,20.,30.,40.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669663f6-ccc8-4d93-b0b9-07519f4bba98",
        "id": "zFKmvfGc0xDn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 100 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 100 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 100 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.56%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 1000 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.47%.\n",
            "********* penalty_factor: 1000 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 1000 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.77%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.5%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.19%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.67%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.59%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.95%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.5%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.65%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.86%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.57%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.01%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.63%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.1%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.86%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.69%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.31%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.4%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.01%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.51%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.93%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.0%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.9%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.29%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.46%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.26%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.8%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.86%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.83%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.17%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.72%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.31%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.37%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.26%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.46%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vf0vRQQJ7zQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**study the centers **"
      ],
      "metadata": {
        "id": "XoeGnFXncAWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-select benign samples\n",
        "benign_samples = []\n",
        "for x_batch, y_batch in test_loader:\n",
        "  benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "# Forward pass to get logits for benign samples\n",
        "with torch.no_grad():  # No need for gradients\n",
        "    outputs = model_AT_rFGSM(ben_x.to(torch.float32))\n",
        "\n",
        "# Calculate softmax probabilities\n",
        "probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "# Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "# Select the top 1000 high confidence benign samples\n",
        "top_1000_high_confidence_benign_samples = ben_x[sorted_indices[:5000]]\n",
        "\n",
        "# Set to track indices of samples to be removed\n",
        "removed_set = set()\n",
        "\n",
        "# Step 1: Identify similar samples\n",
        "for i in range(len(top_1000_high_confidence_benign_samples)):\n",
        "    if i in removed_set:\n",
        "        continue\n",
        "    sample1 = top_1000_high_confidence_benign_samples[i]\n",
        "    for j in range(i + 1, len(top_1000_high_confidence_benign_samples)):\n",
        "        if j in removed_set:\n",
        "            continue\n",
        "        sample2 = top_1000_high_confidence_benign_samples[j]\n",
        "        if torch.abs(sample1 - sample2).sum().item() < 10.1:\n",
        "            removed_set.add(j)\n",
        "\n",
        "# Step 2: Select samples that are not in removed_set\n",
        "selected_samples = [sample for idx, sample in enumerate(top_1000_high_confidence_benign_samples) if idx not in removed_set]\n",
        "\n",
        "# Step 3: Stack tensors along a new dimension\n",
        "selected_benigns = torch.stack(selected_samples)\n",
        "\n",
        "print(selected_benigns.shape)\n",
        "del benign_samples, outputs, probabilities, top_1000_high_confidence_benign_samples, selected_samples   # Free up memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkhPb4YakW_7",
        "outputId": "ff9aa6e1-ffdd-47df-ad9f-3634e6575205"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([376, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_447_high_confidence = ben_x[sorted_indices[:479]]\n",
        "bens = ben_x[:479]"
      ],
      "metadata": {
        "id": "PHHOwisFl7Te"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_50_high_confidence = ben_x[sorted_indices[:50]]"
      ],
      "metadata": {
        "id": "Sj7QGiLIztDy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_AT_rFGSM(ben_x[sorted_indices[:5]].to(torch.float32))"
      ],
      "metadata": {
        "id": "FAuqOEgCb0u-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16cdd3ac-2bd9-4a1e-db95-be882f300cc9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 20.1652, -18.1917],\n",
              "        [ 16.0224, -12.1752],\n",
              "        [ 16.3716, -12.1128],\n",
              "        [ 19.6965, -16.6035],\n",
              "        [ 23.4934, -12.6843]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-select benign samples\n",
        "mal_samples = []\n",
        "for x_batch, y_batch in test_loader:\n",
        "  mal_samples.append(x_batch[y_batch.squeeze() == 1])\n",
        "\n",
        "mal_x = torch.cat(mal_samples, dim=0).to(device)\n",
        "\n",
        "print(mal_x.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlr6kEUmoaUp",
        "outputId": "5be94fde-4b5e-4071-f726-cdadaebbe7da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1130, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "centers = [bens,top_50_high_confidence,top_447_high_confidence,selected_benigns]\n",
        "titles = ['random benigns','top_50_high_confidence', 'Top 447 High Confidence', 'Selected Benigns']\n",
        "\n",
        "# Prepare for subplots\n",
        "fig, axes = plt.subplots(len(centers), 1, figsize=(10, 15))\n",
        "\n",
        "for i, center in enumerate(centers):\n",
        "    mins = []\n",
        "    for mal in mal_x:\n",
        "        min_diff = 1000\n",
        "        for sample in center:\n",
        "            dif = (torch.abs(sample - mal).sum()).item()\n",
        "            if dif < min_diff:\n",
        "                min_diff = dif\n",
        "        mins.append(min_diff)\n",
        "\n",
        "    mins = np.array(mins)\n",
        "\n",
        "    print(f'{titles[i]} - Min: {np.min(mins)}')\n",
        "    print(f'{titles[i]} - Max: {np.max(mins)}')\n",
        "    print(f'{titles[i]} - Mean: {np.mean(mins)}')\n",
        "\n",
        "    # Plot histogram in the corresponding subplot\n",
        "    axes[i].hist(mins, bins=120, edgecolor='black')\n",
        "    axes[i].set_title(f'Distribution of Values - {titles[i]}')\n",
        "    axes[i].set_xlabel('Value')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show all plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HJRNzLqqtc9o",
        "outputId": "c3f24a3f-3418-44eb-8ed3-0c7f92b9288e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random benigns - Min: 0\n",
            "random benigns - Max: 129\n",
            "random benigns - Mean: 26.45840707964602\n",
            "top_50_high_confidence - Min: 0\n",
            "top_50_high_confidence - Max: 140\n",
            "top_50_high_confidence - Mean: 32.190265486725664\n",
            "Top 447 High Confidence - Min: 0\n",
            "Top 447 High Confidence - Max: 135\n",
            "Top 447 High Confidence - Mean: 28.926548672566373\n",
            "Selected Benigns - Min: 2\n",
            "Selected Benigns - Max: 126\n",
            "Selected Benigns - Mean: 27.593805309734513\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PGD_min VS RBF"
      ],
      "metadata": {
        "id": "No0B0LWHxde5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_AT_rFGSM(ben_x[sorted_indices[-5:]].to(torch.float32))"
      ],
      "metadata": {
        "id": "hVCkiKb7xiqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbca427-c418-4661-c7f9-e43e9321669f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-12.9775,  17.0473],\n",
              "        [-39.4226,  49.3918],\n",
              "        [ -5.6200,   6.6027],\n",
              "        [-48.5828,  53.0153],\n",
              "        [-86.9643,  90.6685]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(torch.abs(x_next - torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "frsaxe7U8t7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "def get_loss_rbf(adv_x: torch.Tensor, y: torch.Tensor, model: nn.Module, RBFModel: nn.Module, penalty_factor: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute CE loss\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y)\n",
        "    #print('ce ',ce)\n",
        "\n",
        "    # Compute KDE loss\n",
        "    outputs_rbf = RBFModel(adv_x)\n",
        "    kde = criterion(outputs_rbf, y)\n",
        "    #print('kde ',kde)\n",
        "\n",
        "    # Combine the losses with the penalty factor\n",
        "    loss = ce + penalty_factor * kde\n",
        "    #print('loss ',loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "def rbf(x, y, model, RBFModel, insertion_array, removal_array, k=25, step_length=0.02, norm='linf',\n",
        "        initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "\n",
        "    :param x: Feature vector\n",
        "    :param y: Ground truth labels\n",
        "    :param model: Neural network model\n",
        "    :param RBFModel: Gaussian model for KDE\n",
        "    :param insertion_array: Array for insertion operations\n",
        "    :param removal_array: Array for removal operations\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf', 'l2', 'l1')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    #target's class\n",
        "    traget_labels = torch.zeros_like(y.view(-1).long())\n",
        "\n",
        "    # Compute natural loss and penalty_factor\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), traget_labels)\n",
        "    kde = criterion(RBFModel(x), traget_labels)\n",
        "    #penalty_factor = 0.\n",
        "    penalty_factor = (loss_natural / kde).detach()\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    for t in range(k):\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        decayed_penalty_factor = penalty_factor * (1 - t / k)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_rbf(x_var, traget_labels, model, RBFModel, decayed_penalty_factor)\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "        elif norm == 'l1':\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1', 'l2', or 'linf' norm.\")\n",
        "\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    if random:\n",
        "        round_threshold = torch.rand_like(x_next)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    outputs = model(x_next)\n",
        "    loss_adv = criterion(outputs, traget_labels).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size(0) * 100:.3f}%.\")\n",
        "\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next\n"
      ],
      "metadata": {
        "id": "A33nGhZDzOF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = 0\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUH6bf-X4im3",
        "outputId": "181db644-6de1-4a17-b701-8a43a2cd0fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 46.154%.\n",
            "PGD l1: Attack effectiveness 77.143%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 70.588%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 85.000%.\n",
            "PGD l1: Attack effectiveness 61.765%.\n",
            "PGD l1: Attack effectiveness 55.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 64.516%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 76.667%.\n",
            "PGD l1: Attack effectiveness 45.455%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 63.636%.\n",
            "PGD l1: Attack effectiveness 61.538%.\n",
            "PGD l1: Attack effectiveness 82.143%.\n",
            "PGD l1: Attack effectiveness 70.270%.\n",
            "PGD l1: Attack effectiveness 56.250%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 72.222%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 78.947%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl4c1lyu0h3H",
        "outputId": "fac3d9f9-32f5-4ce9-bf26-8f0e72791e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 46.154%.\n",
            "PGD l1: Attack effectiveness 77.143%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 70.588%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 85.000%.\n",
            "PGD l1: Attack effectiveness 61.765%.\n",
            "PGD l1: Attack effectiveness 55.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 64.516%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 76.667%.\n",
            "PGD l1: Attack effectiveness 45.455%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 63.636%.\n",
            "PGD l1: Attack effectiveness 61.538%.\n",
            "PGD l1: Attack effectiveness 82.143%.\n",
            "PGD l1: Attack effectiveness 70.270%.\n",
            "PGD l1: Attack effectiveness 56.250%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 72.222%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 78.947%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzZWr19C-SYp",
        "outputId": "ce8faf41-ec53-4bd3-b02e-188a19f01316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 53.846%.\n",
            "PGD l1: Attack effectiveness 82.857%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 70.588%.\n",
            "PGD l1: Attack effectiveness 63.636%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 75.862%.\n",
            "PGD l1: Attack effectiveness 90.000%.\n",
            "PGD l1: Attack effectiveness 64.706%.\n",
            "PGD l1: Attack effectiveness 75.000%.\n",
            "PGD l1: Attack effectiveness 77.778%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 70.968%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 83.333%.\n",
            "PGD l1: Attack effectiveness 50.000%.\n",
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 75.000%.\n",
            "PGD l1: Attack effectiveness 65.385%.\n",
            "PGD l1: Attack effectiveness 85.714%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 78.125%.\n",
            "PGD l1: Attack effectiveness 83.333%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 81.579%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.84%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSMxrhlx13T2",
        "outputId": "247a1367-9cb9-4e3c-ed76-86f6385db1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l2: Attack effectiveness 72.414%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 53.846%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 68.000%.\n",
            "PGD l2: Attack effectiveness 76.471%.\n",
            "PGD l2: Attack effectiveness 72.727%.\n",
            "PGD l2: Attack effectiveness 71.429%.\n",
            "PGD l2: Attack effectiveness 68.966%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 64.706%.\n",
            "PGD l2: Attack effectiveness 65.000%.\n",
            "PGD l2: Attack effectiveness 81.481%.\n",
            "PGD l2: Attack effectiveness 68.000%.\n",
            "PGD l2: Attack effectiveness 83.871%.\n",
            "PGD l2: Attack effectiveness 70.000%.\n",
            "PGD l2: Attack effectiveness 92.000%.\n",
            "PGD l2: Attack effectiveness 77.419%.\n",
            "PGD l2: Attack effectiveness 77.778%.\n",
            "PGD l2: Attack effectiveness 80.000%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 50.000%.\n",
            "PGD l2: Attack effectiveness 68.966%.\n",
            "PGD l2: Attack effectiveness 72.727%.\n",
            "PGD l2: Attack effectiveness 65.385%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 75.676%.\n",
            "PGD l2: Attack effectiveness 62.500%.\n",
            "PGD l2: Attack effectiveness 75.000%.\n",
            "PGD l2: Attack effectiveness 80.556%.\n",
            "PGD l2: Attack effectiveness 72.973%.\n",
            "PGD l2: Attack effectiveness 81.579%.\n",
            "PGD l2: Attack effectiveness 74.194%.\n",
            "PGD l2: Attack effectiveness 78.571%.\n",
            "PGD l2: Attack effectiveness 82.143%.\n",
            "PGD l2: Attack effectiveness 82.353%.\n",
            "PGD l2: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.4%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czJU9XFM-2mv",
        "outputId": "2cc001dc-f491-414e-d263-6b5cae5d1cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l2: Attack effectiveness 72.414%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 53.846%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 68.000%.\n",
            "PGD l2: Attack effectiveness 79.412%.\n",
            "PGD l2: Attack effectiveness 66.667%.\n",
            "PGD l2: Attack effectiveness 76.190%.\n",
            "PGD l2: Attack effectiveness 68.966%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 64.706%.\n",
            "PGD l2: Attack effectiveness 70.000%.\n",
            "PGD l2: Attack effectiveness 85.185%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 83.871%.\n",
            "PGD l2: Attack effectiveness 73.333%.\n",
            "PGD l2: Attack effectiveness 92.000%.\n",
            "PGD l2: Attack effectiveness 77.419%.\n",
            "PGD l2: Attack effectiveness 77.778%.\n",
            "PGD l2: Attack effectiveness 83.333%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 50.000%.\n",
            "PGD l2: Attack effectiveness 72.414%.\n",
            "PGD l2: Attack effectiveness 75.000%.\n",
            "PGD l2: Attack effectiveness 65.385%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 72.973%.\n",
            "PGD l2: Attack effectiveness 62.500%.\n",
            "PGD l2: Attack effectiveness 71.875%.\n",
            "PGD l2: Attack effectiveness 83.333%.\n",
            "PGD l2: Attack effectiveness 72.973%.\n",
            "PGD l2: Attack effectiveness 81.579%.\n",
            "PGD l2: Attack effectiveness 77.419%.\n",
            "PGD l2: Attack effectiveness 78.571%.\n",
            "PGD l2: Attack effectiveness 82.143%.\n",
            "PGD l2: Attack effectiveness 82.353%.\n",
            "PGD l2: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.69%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCx0_k-sDjcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8N1KkEy19rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hLhT6VX1_V1",
        "outputId": "2c79f354-895a-4e6b-88fd-0a6250b80b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 38.462%.\n",
            "PGD linf: Attack effectiveness 74.286%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 70.588%.\n",
            "PGD linf: Attack effectiveness 60.606%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 62.069%.\n",
            "PGD linf: Attack effectiveness 85.000%.\n",
            "PGD linf: Attack effectiveness 55.882%.\n",
            "PGD linf: Attack effectiveness 45.000%.\n",
            "PGD linf: Attack effectiveness 74.074%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 61.290%.\n",
            "PGD linf: Attack effectiveness 63.333%.\n",
            "PGD linf: Attack effectiveness 76.000%.\n",
            "PGD linf: Attack effectiveness 67.742%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 40.909%.\n",
            "PGD linf: Attack effectiveness 58.621%.\n",
            "PGD linf: Attack effectiveness 61.364%.\n",
            "PGD linf: Attack effectiveness 61.538%.\n",
            "PGD linf: Attack effectiveness 71.429%.\n",
            "PGD linf: Attack effectiveness 67.568%.\n",
            "PGD linf: Attack effectiveness 43.750%.\n",
            "PGD linf: Attack effectiveness 59.375%.\n",
            "PGD linf: Attack effectiveness 69.444%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 65.789%.\n",
            "PGD linf: Attack effectiveness 64.516%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 60.714%.\n",
            "PGD linf: Attack effectiveness 79.412%.\n",
            "PGD linf: Attack effectiveness 40.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.58%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4916de19-0231-48bf-a16b-77420f16b486",
        "id": "d5BJy7BG1_WE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 68.966%.\n",
            "PGD linf: Attack effectiveness 52.000%.\n",
            "PGD linf: Attack effectiveness 38.462%.\n",
            "PGD linf: Attack effectiveness 65.714%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 48.000%.\n",
            "PGD linf: Attack effectiveness 61.765%.\n",
            "PGD linf: Attack effectiveness 57.576%.\n",
            "PGD linf: Attack effectiveness 54.762%.\n",
            "PGD linf: Attack effectiveness 62.069%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 55.882%.\n",
            "PGD linf: Attack effectiveness 35.000%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 48.000%.\n",
            "PGD linf: Attack effectiveness 58.065%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 54.839%.\n",
            "PGD linf: Attack effectiveness 62.963%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 40.909%.\n",
            "PGD linf: Attack effectiveness 51.724%.\n",
            "PGD linf: Attack effectiveness 54.545%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 71.429%.\n",
            "PGD linf: Attack effectiveness 59.459%.\n",
            "PGD linf: Attack effectiveness 37.500%.\n",
            "PGD linf: Attack effectiveness 53.125%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 54.054%.\n",
            "PGD linf: Attack effectiveness 63.158%.\n",
            "PGD linf: Attack effectiveness 61.290%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 57.143%.\n",
            "PGD linf: Attack effectiveness 79.412%.\n",
            "PGD linf: Attack effectiveness 33.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.59%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 200, 'step_length': 0.01, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBDplVFa2Xzi",
        "outputId": "396cfc92-d78a-4f8f-c754-f6fe45a60494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 46.154%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 70.588%.\n",
            "PGD linf: Attack effectiveness 60.606%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 62.069%.\n",
            "PGD linf: Attack effectiveness 85.000%.\n",
            "PGD linf: Attack effectiveness 58.824%.\n",
            "PGD linf: Attack effectiveness 40.000%.\n",
            "PGD linf: Attack effectiveness 81.481%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 67.742%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 84.000%.\n",
            "PGD linf: Attack effectiveness 64.516%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 76.667%.\n",
            "PGD linf: Attack effectiveness 45.455%.\n",
            "PGD linf: Attack effectiveness 62.069%.\n",
            "PGD linf: Attack effectiveness 68.182%.\n",
            "PGD linf: Attack effectiveness 57.692%.\n",
            "PGD linf: Attack effectiveness 71.429%.\n",
            "PGD linf: Attack effectiveness 67.568%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 62.500%.\n",
            "PGD linf: Attack effectiveness 72.222%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 68.421%.\n",
            "PGD linf: Attack effectiveness 67.742%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 67.857%.\n",
            "PGD linf: Attack effectiveness 79.412%.\n",
            "PGD linf: Attack effectiveness 40.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.92%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()/2.\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkn64rNd--PM",
        "outputId": "243e5f6f-7f90-42b3-dc60-84516f1e8a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 46.154%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 90.000%.\n",
            "PGD l1: Attack effectiveness 61.765%.\n",
            "PGD l1: Attack effectiveness 55.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 64.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 70.968%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 73.333%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 50.000%.\n",
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 68.182%.\n",
            "PGD l1: Attack effectiveness 65.385%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 56.250%.\n",
            "PGD l1: Attack effectiveness 68.750%.\n",
            "PGD l1: Attack effectiveness 75.000%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 73.684%.\n",
            "PGD l1: Attack effectiveness 70.968%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.03%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()  fixed\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybfmw4np_24q",
        "outputId": "451f0adc-3c4f-456f-ef60-f660a24ed5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 42.308%.\n",
            "PGD l1: Attack effectiveness 82.857%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 76.471%.\n",
            "PGD l1: Attack effectiveness 54.545%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 90.000%.\n",
            "PGD l1: Attack effectiveness 58.824%.\n",
            "PGD l1: Attack effectiveness 65.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 64.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 67.742%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 45.455%.\n",
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 68.182%.\n",
            "PGD l1: Attack effectiveness 65.385%.\n",
            "PGD l1: Attack effectiveness 85.714%.\n",
            "PGD l1: Attack effectiveness 67.568%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 59.375%.\n",
            "PGD l1: Attack effectiveness 77.778%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 73.684%.\n",
            "PGD l1: Attack effectiveness 67.742%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 64.286%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()  decrease by /2\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN_8pD9wALwe",
        "outputId": "8bb71b0f-2eb4-4033-8df1-ce871ba9b326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 50.000%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 76.471%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 75.862%.\n",
            "PGD l1: Attack effectiveness 90.000%.\n",
            "PGD l1: Attack effectiveness 64.706%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 80.645%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 67.742%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 73.333%.\n",
            "PGD l1: Attack effectiveness 73.333%.\n",
            "PGD l1: Attack effectiveness 50.000%.\n",
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 65.909%.\n",
            "PGD l1: Attack effectiveness 57.692%.\n",
            "PGD l1: Attack effectiveness 82.143%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 71.875%.\n",
            "PGD l1: Attack effectiveness 75.000%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 78.947%.\n",
            "PGD l1: Attack effectiveness 70.968%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.32%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()  penalty_factor /(2**(t-1))\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1037d2-c97f-4414-accc-c0f0f20fb11f",
        "id": "EciUXJgpBhG5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 53.846%.\n",
            "PGD l1: Attack effectiveness 74.286%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 76.471%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 75.862%.\n",
            "PGD l1: Attack effectiveness 85.000%.\n",
            "PGD l1: Attack effectiveness 64.706%.\n",
            "PGD l1: Attack effectiveness 55.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 63.333%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 67.742%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 73.333%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 50.000%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 65.909%.\n",
            "PGD l1: Attack effectiveness 61.538%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 59.375%.\n",
            "PGD l1: Attack effectiveness 71.875%.\n",
            "PGD l1: Attack effectiveness 75.000%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 76.316%.\n",
            "PGD l1: Attack effectiveness 70.968%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.29%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhvM7ueGCpqR",
        "outputId": "d9edecc2-810d-4eed-eac6-65859a74c32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 46.154%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 67.647%.\n",
            "PGD l1: Attack effectiveness 63.636%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 75.862%.\n",
            "PGD l1: Attack effectiveness 95.000%.\n",
            "PGD l1: Attack effectiveness 64.706%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "PGD l1: Attack effectiveness 85.185%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 83.333%.\n",
            "PGD l1: Attack effectiveness 90.000%.\n",
            "PGD l1: Attack effectiveness 54.545%.\n",
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 70.455%.\n",
            "PGD l1: Attack effectiveness 65.385%.\n",
            "PGD l1: Attack effectiveness 85.714%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 59.375%.\n",
            "PGD l1: Attack effectiveness 78.125%.\n",
            "PGD l1: Attack effectiveness 77.778%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 84.211%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 26.37%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vRkMRmST4QDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}