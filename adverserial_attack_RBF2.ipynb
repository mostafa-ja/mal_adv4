{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/mal_adv4/blob/main/adverserial_attack_RBF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RBF_models\n",
        "\n",
        "download_links = [\n",
        "                  'https://drive.google.com/uc?id=1-8lJXLdAl_4NdDwzw9kFML0aiOCTrI9f',\n",
        "                  'https://drive.google.com/uc?id=1-OHACrNCt0yKBbdqQPVfNZcjKt5_jxKD',\n",
        "                  'https://drive.google.com/uc?id=1-KeXJXtU1_6m9JOhormeVwigy0myX3HL',\n",
        "                  'https://drive.google.com/uc?id=1-13RDdZqnrNkdHg3D8PC5KI0CZREwlsz',\n",
        "                  'https://drive.google.com/uc?id=1-8LjsCdzKh6asxCFsYLiQZbSEXXKSQBP',\n",
        "\n",
        "]\n",
        "\n",
        "import gdown\n",
        "output_filepath = '/content/'\n",
        "for link in download_links:\n",
        "  gdown.download(link, output_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM_3KjraHnkn",
        "outputId": "d677dafd-567b-4fb6-ad7f-a0c0f75fecc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-8lJXLdAl_4NdDwzw9kFML0aiOCTrI9f\n",
            "From (redirected): https://drive.google.com/uc?id=1-8lJXLdAl_4NdDwzw9kFML0aiOCTrI9f&confirm=t&uuid=9a2544fa-52e9-4364-874e-a1f0433996a8\n",
            "To: /content/best_model_gaussian_400.pth\n",
            "100%|██████████| 32.0M/32.0M [00:01<00:00, 30.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-OHACrNCt0yKBbdqQPVfNZcjKt5_jxKD\n",
            "To: /content/best_model_gaussian_600_nonremoval.pth\n",
            "100%|██████████| 5.50M/5.50M [00:00<00:00, 65.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-KeXJXtU1_6m9JOhormeVwigy0myX3HL\n",
            "To: /content/best_model_gaussian_600.pth\n",
            "100%|██████████| 24.0M/24.0M [00:00<00:00, 49.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-13RDdZqnrNkdHg3D8PC5KI0CZREwlsz\n",
            "To: /content/best_model_gaussian_1000_nonremoval.pth\n",
            "100%|██████████| 9.16M/9.16M [00:00<00:00, 72.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-8LjsCdzKh6asxCFsYLiQZbSEXXKSQBP\n",
            "From (redirected): https://drive.google.com/uc?id=1-8LjsCdzKh6asxCFsYLiQZbSEXXKSQBP&confirm=t&uuid=f7c0e119-93e8-48b3-8fb7-d594b2b66059\n",
            "To: /content/best_model_gaussian_1000.pth\n",
            "100%|██████████| 40.0M/40.0M [00:00<00:00, 49.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "download_links = ['https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_0.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_1.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_2.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y0.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y1.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y2.npz',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_DNN_drebin_best.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM_weightedLoss.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM.pth',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/insertion_array.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/removal_array.pkl',\n",
        "                  'https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/adverserial_attacks_functions.py'\n",
        "]"
      ],
      "metadata": {
        "id": "1IW4pHac9VLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "output_filepath = '/content/'\n",
        "for link in download_links:\n",
        "  gdown.download(link, output_filepath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kzSbjaXGVeG",
        "outputId": "2120ca2b-b2b0-4fee-b318-242c641f808f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_0.npz\n",
            "To: /content/sparse_matrix_0.npz\n",
            "100%|██████████| 461k/461k [00:00<00:00, 5.77MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_1.npz\n",
            "To: /content/sparse_matrix_1.npz\n",
            "100%|██████████| 148k/148k [00:00<00:00, 2.81MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_2.npz\n",
            "To: /content/sparse_matrix_2.npz\n",
            "100%|██████████| 150k/150k [00:00<00:00, 7.97MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y0.npz\n",
            "To: /content/sparse_matrix_y0.npz\n",
            "100%|██████████| 5.79k/5.79k [00:00<00:00, 12.4MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y1.npz\n",
            "To: /content/sparse_matrix_y1.npz\n",
            "100%|██████████| 2.64k/2.64k [00:00<00:00, 3.18MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/sparse_matrix_y2.npz\n",
            "To: /content/sparse_matrix_y2.npz\n",
            "100%|██████████| 2.71k/2.71k [00:00<00:00, 6.12MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_DNN_drebin_best.pth\n",
            "To: /content/model_DNN_drebin_best.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 20.5MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM_weightedLoss.pth\n",
            "To: /content/model_AT_rFGSM_weightedLoss.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 21.3MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv3/raw/main/drebin/model_AT_rFGSM.pth\n",
            "To: /content/model_AT_rFGSM.pth\n",
            "100%|██████████| 8.17M/8.17M [00:00<00:00, 22.7MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/insertion_array.pkl\n",
            "To: /content/insertion_array.pkl\n",
            "100%|██████████| 80.2k/80.2k [00:00<00:00, 1.87MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/removal_array.pkl\n",
            "To: /content/removal_array.pkl\n",
            "100%|██████████| 80.2k/80.2k [00:00<00:00, 1.78MB/s]\n",
            "Downloading...\n",
            "From: https://github.com/mostafa-ja/mal_adv4/raw/main/dataset/adverserial_attacks_functions.py\n",
            "To: /content/adverserial_attacks_functions.py\n",
            "67.1kB [00:00, 65.9MB/s]                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,balanced_accuracy_score\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "from adverserial_attacks_functions import *\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "JKDdI3K9LrlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328d69d3-785e-4955-f3b9-7e020d3a3be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ce3da750a90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    print(f\"Random seed set as {seed}\")\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX4ncRLLFDnN",
        "outputId": "6056d283-191f-4fda-e737-7eb940f11b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set as 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the .pkl file\n",
        "with open('/content/insertion_array.pkl', 'rb') as f:\n",
        "    # Load the object\n",
        "    insertion_array = pickle.load(f)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n",
        "\n",
        "insertion_array = torch.tensor(insertion_array, dtype=torch.uint8).to(device)\n",
        "print(len(insertion_array))\n",
        "\n",
        "# Open the .pkl file\n",
        "with open('/content/removal_array.pkl', 'rb') as f:\n",
        "    # Load the object\n",
        "    removal_array = pickle.load(f)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n",
        "\n",
        "removal_array = torch.tensor(removal_array, dtype=torch.uint8).to(device)\n",
        "print(len(removal_array))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXV0WIjsJG_F",
        "outputId": "6fbd4130-8ed6-4f56-b182-db058ee3a463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset\n",
        "X_train = sparse.load_npz(\"/content/sparse_matrix_0.npz\").toarray()\n",
        "X_val = sparse.load_npz(\"/content/sparse_matrix_1.npz\").toarray()\n",
        "X_test = sparse.load_npz(\"/content/sparse_matrix_2.npz\").toarray()\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.int8)\n",
        "X_val = torch.tensor(X_val, dtype=torch.int8)\n",
        "X_test = torch.tensor(X_test, dtype=torch.int8)\n",
        "\n",
        "\n",
        "y_train = sparse.load_npz(\"/content/sparse_matrix_y0.npz\").toarray().reshape((-1, 1))\n",
        "y_val = sparse.load_npz(\"/content/sparse_matrix_y1.npz\").toarray().reshape((-1, 1))\n",
        "y_test = sparse.load_npz(\"/content/sparse_matrix_y2.npz\").toarray().reshape((-1, 1))\n",
        "\n",
        "y_train = torch.tensor(y_train, dtype=torch.int8)\n",
        "y_val = torch.tensor(y_val, dtype=torch.int8)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int8)\n",
        "\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"x_train:\", X_train.shape)\n",
        "print(\"x_val:\", X_val.shape)\n",
        "print(\"x_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_val:\", y_val.shape)\n",
        "print(\"y_test:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5blmEg4h-GKy",
        "outputId": "5f9a68bc-726e-4910-a257-74128302db1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "x_train: torch.Size([28683, 10000])\n",
            "x_val: torch.Size([9562, 10000])\n",
            "x_test: torch.Size([9562, 10000])\n",
            "y_train: torch.Size([28683, 1])\n",
            "y_val: torch.Size([9562, 1])\n",
            "y_test: torch.Size([9562, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of benigns and malicious sample in training dataset\n",
        "n_ben = (y_train.squeeze()== 0).sum().item()\n",
        "n_mal = (y_train.squeeze()== 1).sum().item()\n",
        "print('the proportion of malwares : ', n_mal/(n_mal+n_ben))\n",
        "\n",
        "# Combine features and labels into datasets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Define the DataLoader for training, validation, and test sets\n",
        "batch_size = 1024\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del train_dataset, val_dataset, test_dataset, y_train, y_val, y_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81AZSXOV-HoW",
        "outputId": "bdc722c6-d810-4abf-a82b-4a85df907f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the proportion of malwares :  0.11386535578565701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your model\n",
        "model_AT_rFGSM = MalwareDetectionModel().to(device)\n",
        "\n",
        "# Load model parameters\n",
        "model_AT_rFGSM.load_state_dict(torch.load('model_AT_rFGSM.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE8WMAUgSCms",
        "outputId": "7155966f-ba6f-4ca5-adf9-7d687965c1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RBFModel(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, init_centers, init_sigmas, kernel):\n",
        "        super(RBFModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.centers = nn.Parameter(torch.Tensor(init_centers))\n",
        "        self.sigmas = nn.Parameter(torch.Tensor(init_sigmas))\n",
        "        self.kernel = kernel\n",
        "        # Linear layer for output\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def gaussian(self, x, c, sigma):\n",
        "        return torch.exp(-torch.sum((x[:, None, :] - c) ** 2, dim=-1) / (2 * sigma ** 2))\n",
        "\n",
        "    def laplacian(self, x, c, sigma):\n",
        "        return torch.exp(-torch.sum(torch.abs(x[:, None, :] - c) , dim=-1) / sigma)\n",
        "\n",
        "    def forward(self, x):\n",
        "      if self.kernel == 'gaussian':\n",
        "        radial_out = self.gaussian(x, self.centers, self.sigmas)\n",
        "      elif self.kernel == 'laplacian':\n",
        "        radial_out = self.laplacian(x, self.centers, self.sigmas)\n",
        "      else:\n",
        "        raise ValueError(\"Invalid kernel type. Choose 'gaussian' or 'laplacian'.\")\n",
        "\n",
        "      output = self.linear(radial_out.to(torch.float32))\n",
        "      return output\n"
      ],
      "metadata": {
        "id": "WHAI-VGJSGa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_removal_features = False\n",
        "non_removal_mask = torch.logical_not(removal_array).to('cpu')\n",
        "sigma = 4.15\n",
        "kernel = 'gaussian'\n",
        "all_centers = torch.rand((1000, 10000))\n",
        "model_gaussian_1000 = RBFModel(1000, 2, all_centers, [sigma], kernel)\n",
        "model_gaussian_1000 = model_gaussian_1000.to(device)\n",
        "\n",
        "# Load the model state dictionary\n",
        "model_gaussian_1000.load_state_dict(torch.load('/content/best_model_gaussian_1000.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "id": "W9qJYeK0bbnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24a4edf-5a79-45d5-9b8b-1a6f1ac12ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_removal_features = True\n",
        "non_removal_mask = torch.logical_not(removal_array).to('cpu')\n",
        "sigma = 4.15\n",
        "kernel = 'gaussian'\n",
        "all_centers = torch.rand((1000, 1144))\n",
        "model_gaussian_1000_nonremoval = RBFModel(1000, 2, all_centers, [sigma], kernel)\n",
        "model_gaussian_1000_nonremoval = model_gaussian_1000_nonremoval.to(device)\n",
        "\n",
        "# Load the model state dictionary\n",
        "model_gaussian_1000_nonremoval.load_state_dict(torch.load('/content/best_model_gaussian_1000_nonremoval.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMA7rzpTPhv7",
        "outputId": "2bb436c6-a375-4a90-c434-30c6d98e46fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QcyXy3kSd6QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in test_loader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "bens = x[y.squeeze()==0]\n",
        "bens_y = y[y.squeeze()==0]\n",
        "print(bens.shape)\n",
        "\n",
        "mals = x[y.squeeze()==1]\n",
        "mals_y = y[y.squeeze()==1]\n",
        "print(mals.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StL135L1JUiE",
        "outputId": "b0b6b8a1-09a4-4a6d-c942-10db04c9d5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 10000])\n",
            "torch.Size([1024, 1])\n",
            "torch.Size([909, 10000])\n",
            "torch.Size([115, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`analysis`**"
      ],
      "metadata": {
        "id": "lR06hA6gn_by"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PGD VS PGD2"
      ],
      "metadata": {
        "id": "lBVu5_O4xF4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = mals.to(device)\n",
        "\n",
        "# Expand insertion_array and removal_array to match the batch size\n",
        "expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "# Update insertion and removal arrays based on input x\n",
        "insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))"
      ],
      "metadata": {
        "id": "5Ybdj5iAOo_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(insertion_array.sum())\n",
        "print(insertion_array_updated.sum(dim=-1))\n",
        "\n",
        "print(removal_array.sum())\n",
        "print(removal_array_updated.sum(dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk3PeXfviDL7",
        "outputId": "197ac01e-11d2-4654-8570-7f1d13aa1736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9943, device='cuda:0')\n",
            "tensor([9943, 9944, 9943, 9949, 9944, 9944, 9944, 9945, 9946, 9947, 9943, 9945,\n",
            "        9944, 9943, 9943, 9944, 9946, 9945, 9947, 9945, 9945, 9946, 9944, 9945,\n",
            "        9945, 9945, 9943, 9944, 9944], device='cuda:0')\n",
            "tensor(8856, device='cuda:0')\n",
            "tensor([9996, 9993, 9986, 9962, 9993, 9986, 9973, 9989, 9967, 9980, 9996, 9989,\n",
            "        9993, 9992, 9985, 9989, 9977, 9968, 9984, 9968, 9976, 9989, 9993, 9965,\n",
            "        9984, 9989, 9994, 9989, 9997], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((x.to(torch.uint8)[0]).sum())\n",
        "print((removal_array_updated[0]*x.to(torch.uint8)[0]).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGD5H7qPqL3s",
        "outputId": "e223f1fc-6408-4e8e-c810-0d2cbb0c60aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10, device='cuda:0')\n",
            "tensor(6, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((x.to(torch.uint8)[1]).sum())\n",
        "print((removal_array_updated[1]*x.to(torch.uint8)[1]).sum())"
      ],
      "metadata": {
        "id": "2uCejvFCpYLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c42626-3ec1-44f2-e0c4-1bc4d2638ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(18, device='cuda:0')\n",
            "tensor(11, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        #pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            #perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            #perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            #perturbation[torch.isnan(perturbation)] = 0.\n",
        "            #perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "gCxNx4sfOr-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd2(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    #insertion_array_updated = torch.bitwise_or(insertion_array.to(torch.uint8), x.squeeze().to(torch.uint8) )\n",
        "    #removal_array_updated = torch.bitwise_or(removal_array.to(torch.uint8), (1 - x.squeeze().to(torch.uint8)) )\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        #pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "            #perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            #perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            #perturbation[torch.isnan(perturbation)] = 0.\n",
        "            #perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "ljfazeC6MOcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=13\n",
        "adv3 = pgd(mals[i:i+1].to(torch.float32).to(device), mals_y[i:i+1].to(device), model_AT_rFGSM, insertion_array, removal_array, k=200, step_length=.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfb42f2-39b7-4d36-d6f3-851143bc691e",
        "id": "wno-FScyQc7r"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********  0\n",
            "loss_mal :  tensor([38.1857], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  1\n",
            "loss_mal :  tensor([34.2699], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  2\n",
            "loss_mal :  tensor([29.9703], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  3\n",
            "loss_mal :  tensor([25.4033], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  4\n",
            "loss_mal :  tensor([21.0827], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  5\n",
            "loss_mal :  tensor([18.6158], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  6\n",
            "loss_mal :  tensor([19.1730], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  7\n",
            "loss_mal :  tensor([18.5722], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  8\n",
            "loss_mal :  tensor([17.9622], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  9\n",
            "loss_mal :  tensor([18.2092], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  10\n",
            "loss_mal :  tensor([17.6264], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  11\n",
            "loss_mal :  tensor([17.2231], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  12\n",
            "loss_mal :  tensor([17.6461], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  13\n",
            "loss_mal :  tensor([17.0438], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  14\n",
            "loss_mal :  tensor([16.5450], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  15\n",
            "loss_mal :  tensor([16.6668], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  16\n",
            "loss_mal :  tensor([16.7365], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  17\n",
            "loss_mal :  tensor([16.2144], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  18\n",
            "loss_mal :  tensor([15.8460], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  19\n",
            "loss_mal :  tensor([16.2541], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  20\n",
            "loss_mal :  tensor([15.6365], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  21\n",
            "loss_mal :  tensor([15.1847], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  22\n",
            "loss_mal :  tensor([15.2408], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  23\n",
            "loss_mal :  tensor([15.3970], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  24\n",
            "loss_mal :  tensor([14.9240], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  25\n",
            "loss_mal :  tensor([14.4805], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([4.7684e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  26\n",
            "loss_mal :  tensor([15.1770], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  27\n",
            "loss_mal :  tensor([15.3370], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  28\n",
            "loss_mal :  tensor([18.0399], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  29\n",
            "loss_mal :  tensor([17.5291], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  30\n",
            "loss_mal :  tensor([18.5759], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  31\n",
            "loss_mal :  tensor([18.5073], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  32\n",
            "loss_mal :  tensor([19.3361], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  33\n",
            "loss_mal :  tensor([19.5283], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  34\n",
            "loss_mal :  tensor([19.8090], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  35\n",
            "loss_mal :  tensor([20.6668], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  36\n",
            "loss_mal :  tensor([20.5820], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  37\n",
            "loss_mal :  tensor([21.0349], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  38\n",
            "loss_mal :  tensor([22.0950], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  39\n",
            "loss_mal :  tensor([25.7003], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  40\n",
            "loss_mal :  tensor([25.0341], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  41\n",
            "loss_mal :  tensor([25.9631], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  42\n",
            "loss_mal :  tensor([26.0358], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  43\n",
            "loss_mal :  tensor([26.2545], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  44\n",
            "loss_mal :  tensor([27.0234], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  45\n",
            "loss_mal :  tensor([27.2249], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  46\n",
            "loss_mal :  tensor([27.8303], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  47\n",
            "loss_mal :  tensor([28.4456], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  48\n",
            "loss_mal :  tensor([28.5112], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  49\n",
            "loss_mal :  tensor([28.7157], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  50\n",
            "loss_mal :  tensor([29.7865], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  51\n",
            "loss_mal :  tensor([29.8639], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  52\n",
            "loss_mal :  tensor([30.0937], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  53\n",
            "loss_mal :  tensor([30.4724], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  54\n",
            "loss_mal :  tensor([31.2638], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  55\n",
            "loss_mal :  tensor([31.3466], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  56\n",
            "loss_mal :  tensor([31.4016], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  57\n",
            "loss_mal :  tensor([32.3835], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  58\n",
            "loss_mal :  tensor([32.7490], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  59\n",
            "loss_mal :  tensor([32.6407], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  60\n",
            "loss_mal :  tensor([32.9704], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  61\n",
            "loss_mal :  tensor([34.3624], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  62\n",
            "loss_mal :  tensor([36.8631], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  63\n",
            "loss_mal :  tensor([37.2833], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  64\n",
            "loss_mal :  tensor([37.5473], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  65\n",
            "loss_mal :  tensor([38.0170], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  66\n",
            "loss_mal :  tensor([38.4765], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  67\n",
            "loss_mal :  tensor([38.8761], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  68\n",
            "loss_mal :  tensor([39.2470], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  69\n",
            "loss_mal :  tensor([39.7409], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  70\n",
            "loss_mal :  tensor([39.2557], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  71\n",
            "loss_mal :  tensor([39.6956], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  72\n",
            "loss_mal :  tensor([40.5667], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  73\n",
            "loss_mal :  tensor([40.7395], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  74\n",
            "loss_mal :  tensor([41.2648], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  75\n",
            "loss_mal :  tensor([40.6510], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  76\n",
            "loss_mal :  tensor([41.3032], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  77\n",
            "loss_mal :  tensor([42.1344], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  78\n",
            "loss_mal :  tensor([42.0639], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  79\n",
            "loss_mal :  tensor([42.6217], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  80\n",
            "loss_mal :  tensor([42.4742], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  81\n",
            "loss_mal :  tensor([43.2104], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  82\n",
            "loss_mal :  tensor([42.8373], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  83\n",
            "loss_mal :  tensor([43.8001], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  84\n",
            "loss_mal :  tensor([43.6212], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  85\n",
            "loss_mal :  tensor([44.2113], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  86\n",
            "loss_mal :  tensor([44.4069], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  87\n",
            "loss_mal :  tensor([44.8563], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  88\n",
            "loss_mal :  tensor([44.4067], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  89\n",
            "loss_mal :  tensor([45.6805], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  90\n",
            "loss_mal :  tensor([45.2139], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  91\n",
            "loss_mal :  tensor([46.1938], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  92\n",
            "loss_mal :  tensor([46.2202], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  93\n",
            "loss_mal :  tensor([46.6961], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  94\n",
            "loss_mal :  tensor([46.2663], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  95\n",
            "loss_mal :  tensor([47.0882], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  96\n",
            "loss_mal :  tensor([47.0736], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  97\n",
            "loss_mal :  tensor([47.5637], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  98\n",
            "loss_mal :  tensor([46.9779], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  99\n",
            "loss_mal :  tensor([47.8359], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  100\n",
            "loss_mal :  tensor([47.7958], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  101\n",
            "loss_mal :  tensor([48.2153], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  102\n",
            "loss_mal :  tensor([48.4137], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  103\n",
            "loss_mal :  tensor([48.6403], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  104\n",
            "loss_mal :  tensor([48.9003], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  105\n",
            "loss_mal :  tensor([48.8218], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  106\n",
            "loss_mal :  tensor([49.4160], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  107\n",
            "loss_mal :  tensor([49.1052], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  108\n",
            "loss_mal :  tensor([49.3387], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  109\n",
            "loss_mal :  tensor([49.3371], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  110\n",
            "loss_mal :  tensor([50.3853], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  111\n",
            "loss_mal :  tensor([49.5553], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  112\n",
            "loss_mal :  tensor([50.2185], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  113\n",
            "loss_mal :  tensor([49.6733], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  114\n",
            "loss_mal :  tensor([50.8431], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  115\n",
            "loss_mal :  tensor([50.0224], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  116\n",
            "loss_mal :  tensor([50.2238], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  117\n",
            "loss_mal :  tensor([51.2280], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  118\n",
            "loss_mal :  tensor([50.3908], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  119\n",
            "loss_mal :  tensor([51.5084], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  120\n",
            "loss_mal :  tensor([50.5488], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  121\n",
            "loss_mal :  tensor([50.7055], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  122\n",
            "loss_mal :  tensor([51.7984], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  123\n",
            "loss_mal :  tensor([50.8001], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  124\n",
            "loss_mal :  tensor([52.6957], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  125\n",
            "loss_mal :  tensor([51.2729], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  126\n",
            "loss_mal :  tensor([52.1815], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  127\n",
            "loss_mal :  tensor([51.9248], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  128\n",
            "loss_mal :  tensor([51.9660], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  129\n",
            "loss_mal :  tensor([53.9478], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  130\n",
            "loss_mal :  tensor([52.9872], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  131\n",
            "loss_mal :  tensor([52.9911], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  132\n",
            "loss_mal :  tensor([53.0953], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  133\n",
            "loss_mal :  tensor([53.1235], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  134\n",
            "loss_mal :  tensor([53.1636], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  135\n",
            "loss_mal :  tensor([53.1505], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  136\n",
            "loss_mal :  tensor([53.2048], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  137\n",
            "loss_mal :  tensor([53.1772], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  138\n",
            "loss_mal :  tensor([53.2403], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  139\n",
            "loss_mal :  tensor([53.2744], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  140\n",
            "loss_mal :  tensor([53.2884], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  141\n",
            "loss_mal :  tensor([53.2570], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  142\n",
            "loss_mal :  tensor([53.2793], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  143\n",
            "loss_mal :  tensor([53.2307], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  144\n",
            "loss_mal :  tensor([53.3362], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  145\n",
            "loss_mal :  tensor([54.3056], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  146\n",
            "loss_mal :  tensor([53.5258], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  147\n",
            "loss_mal :  tensor([54.6378], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  148\n",
            "loss_mal :  tensor([53.6328], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  149\n",
            "loss_mal :  tensor([53.7278], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  150\n",
            "loss_mal :  tensor([54.0571], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  151\n",
            "loss_mal :  tensor([54.0202], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  152\n",
            "loss_mal :  tensor([55.1657], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  153\n",
            "loss_mal :  tensor([54.2403], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  154\n",
            "loss_mal :  tensor([55.3521], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  155\n",
            "loss_mal :  tensor([54.3920], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  156\n",
            "loss_mal :  tensor([54.3931], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  157\n",
            "loss_mal :  tensor([54.5385], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  158\n",
            "loss_mal :  tensor([54.5983], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  159\n",
            "loss_mal :  tensor([54.6775], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  160\n",
            "loss_mal :  tensor([55.6373], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  161\n",
            "loss_mal :  tensor([54.7709], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  162\n",
            "loss_mal :  tensor([55.8183], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  163\n",
            "loss_mal :  tensor([54.8311], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  164\n",
            "loss_mal :  tensor([56.0166], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  165\n",
            "loss_mal :  tensor([54.8923], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  166\n",
            "loss_mal :  tensor([55.0505], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  167\n",
            "loss_mal :  tensor([54.9815], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  168\n",
            "loss_mal :  tensor([56.0288], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  169\n",
            "loss_mal :  tensor([55.0874], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  170\n",
            "loss_mal :  tensor([56.0652], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  171\n",
            "loss_mal :  tensor([55.1147], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  172\n",
            "loss_mal :  tensor([55.2418], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  173\n",
            "loss_mal :  tensor([56.3768], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  174\n",
            "loss_mal :  tensor([55.3652], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  175\n",
            "loss_mal :  tensor([55.3756], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  176\n",
            "loss_mal :  tensor([55.4923], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  177\n",
            "loss_mal :  tensor([55.5233], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  178\n",
            "loss_mal :  tensor([56.4636], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  179\n",
            "loss_mal :  tensor([55.5444], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  180\n",
            "loss_mal :  tensor([56.5000], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  181\n",
            "loss_mal :  tensor([55.5681], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  182\n",
            "loss_mal :  tensor([57.3156], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  183\n",
            "loss_mal :  tensor([55.7465], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  184\n",
            "loss_mal :  tensor([55.8201], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  185\n",
            "loss_mal :  tensor([56.1730], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  186\n",
            "loss_mal :  tensor([55.9438], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  187\n",
            "loss_mal :  tensor([59.8039], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  188\n",
            "loss_mal :  tensor([58.7559], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  189\n",
            "loss_mal :  tensor([58.7960], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  190\n",
            "loss_mal :  tensor([58.7328], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  191\n",
            "loss_mal :  tensor([58.7429], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  192\n",
            "loss_mal :  tensor([58.8203], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  193\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  194\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  195\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  196\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  197\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  198\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  199\n",
            "loss_mal :  tensor([58.7509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "PGD linf: Attack effectiveness 0.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=13\n",
        "adv3 = pgd2(mals[i:i+1].to(torch.float32).to(device), mals_y[i:i+1].to(device), model_AT_rFGSM, insertion_array, removal_array, k=200, step_length=.01, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6ffcbd-df80-4a08-c94c-d2ee834981fb",
        "id": "VA7I64WAQlzh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_mal :  tensor([38.1857], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([36.2778], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([34.1459], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([32.0459], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([29.7434], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([27.4553], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([25.1175], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([22.7606], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([20.7420], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([18.7855], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([17.7333], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([17.0531], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([16.1223], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([15.3524], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([14.7656], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([13.8007], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.0729e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([13.0112], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.2650e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([12.5444], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.5763e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([11.6671], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([8.5830e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([11.3863], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1325e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([10.2367], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.5881e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.2161], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([9.9416e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.4727], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.6887e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([8.7340], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.7916], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0004], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.4897], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0006], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.3333], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([8.8449e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.3538], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0006], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([5.9726], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0026], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([6.3044], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0018], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.2304], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0007], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([6.0834], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0023], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.7234], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0089], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.3221], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0134], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([5.8951], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0028], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.3293], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0133], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.2786], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0384], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.0669], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0173], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.9127], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0202], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.9162], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0557], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.9034], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0204], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.6429], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0265], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.9277], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0550], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.9581], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1521], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.9436], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1545], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.6260], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0751], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.8590], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0590], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.4482], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0904], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.2558], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.3352], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.9197], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.5085], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.7572], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.6330], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.9694], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.4769], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.3177], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.3116], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.5444], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.8679], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.5643], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.8410], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3931], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0958], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.8096], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.5888], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.3027], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.3425], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.7227], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.6645], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.3755], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1613], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0892], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.4609], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0557], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.9151], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0451], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.1222], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0605], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.8352], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0731], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.6521], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0147], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([4.2298], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.1553], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.9392], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0211], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.8668], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0074], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([4.9123], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0599], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.8451], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0096], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([4.6477], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0040], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.5347], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0038], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.5780], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0036], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.6367], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0025], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([6.0047], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0015], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([6.4953], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0008], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.1494], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0004], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.7841], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0004], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.8195], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([8.4549], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([8.6143], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.7866e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([9.2318], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([0.0005], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.5747], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.3669e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([9.5154], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.3736e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.2967], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.5868e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.5637], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.7789e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.1824], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.7404e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.9576], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.6451e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.0120], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1086e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.4133], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.0371e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.4801], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([5.9604e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.0398], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([6.0797e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.0062], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.2915e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.3569], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.6955e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.4986], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.2650e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.9914], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.1458e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.0478], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.4305e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.4899], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.5497e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.3926], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.5367e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.9123], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([9.5367e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.8642], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([5.9605e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.2797], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([8.3446e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.9929], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([4.7684e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.4852], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([7.1526e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.1059], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.7666], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.8205], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.2218], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.1785], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.5289], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.0717], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.6906], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.8573], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.8415], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.0516], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.9979], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.1779], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.2113], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.1807], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.5920], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.8915], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.7072], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.9245], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.1821], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.9369], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.8058], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.1525], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.2827], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.3518], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.3100], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.4926], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.9303], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.5889], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.6794], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.7684], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.8297], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.5572], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.3395], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.0459], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.0215], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.0777], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2277], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2953], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2866], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.4161], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.6764], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.5151], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.5406], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.6368], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.4993], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.6868], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.8509], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.6713], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.8222], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.3812], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.8683], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.5569], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.9306], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.8948], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.1698], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.0208], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.5820], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.1567], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.9064], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.2704], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.6482], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.3006], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.3837], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.7682], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.7323], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.7673], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.4611], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.9545], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.9501], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.1487], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.1800], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.3351], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.5411], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.6605], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.2835], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.6128], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([20.8182], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.0505], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.3736], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.3705], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.3748], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.5194], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.6121], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([21.6836], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.0233], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.2959], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.2906], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.0599], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.3548], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.6085], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.4280], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.2590], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.6373], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.5472], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([22.7872], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zc-0yhEwDPT",
        "outputId": "b372804d-2ebb-43d6-aa4f-74e7dbeaa5ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 6.897%.\n",
            "PGD linf: Attack effectiveness 8.000%.\n",
            "PGD linf: Attack effectiveness 7.692%.\n",
            "PGD linf: Attack effectiveness 2.857%.\n",
            "PGD linf: Attack effectiveness 8.000%.\n",
            "PGD linf: Attack effectiveness 8.000%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 6.061%.\n",
            "PGD linf: Attack effectiveness 9.524%.\n",
            "PGD linf: Attack effectiveness 10.345%.\n",
            "PGD linf: Attack effectiveness 10.000%.\n",
            "PGD linf: Attack effectiveness 11.765%.\n",
            "PGD linf: Attack effectiveness 10.000%.\n",
            "PGD linf: Attack effectiveness 14.815%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 3.226%.\n",
            "PGD linf: Attack effectiveness 3.333%.\n",
            "PGD linf: Attack effectiveness 8.000%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 14.815%.\n",
            "PGD linf: Attack effectiveness 16.667%.\n",
            "PGD linf: Attack effectiveness 10.000%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 17.241%.\n",
            "PGD linf: Attack effectiveness 18.182%.\n",
            "PGD linf: Attack effectiveness 3.846%.\n",
            "PGD linf: Attack effectiveness 10.714%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 3.125%.\n",
            "PGD linf: Attack effectiveness 6.250%.\n",
            "PGD linf: Attack effectiveness 8.333%.\n",
            "PGD linf: Attack effectiveness 8.108%.\n",
            "PGD linf: Attack effectiveness 0.000%.\n",
            "PGD linf: Attack effectiveness 3.226%.\n",
            "PGD linf: Attack effectiveness 7.143%.\n",
            "PGD linf: Attack effectiveness 3.571%.\n",
            "PGD linf: Attack effectiveness 2.941%.\n",
            "PGD linf: Attack effectiveness 6.667%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7TiMkblwKWx",
        "outputId": "af65e2b9-876f-4810-eb13-f219664addf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 52.000%.\n",
            "PGD linf: Attack effectiveness 52.941%.\n",
            "PGD linf: Attack effectiveness 48.485%.\n",
            "PGD linf: Attack effectiveness 73.810%.\n",
            "PGD linf: Attack effectiveness 55.172%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 55.882%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 56.000%.\n",
            "PGD linf: Attack effectiveness 64.516%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 58.065%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 63.333%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 45.455%.\n",
            "PGD linf: Attack effectiveness 65.517%.\n",
            "PGD linf: Attack effectiveness 61.364%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 70.270%.\n",
            "PGD linf: Attack effectiveness 43.750%.\n",
            "PGD linf: Attack effectiveness 46.875%.\n",
            "PGD linf: Attack effectiveness 61.111%.\n",
            "PGD linf: Attack effectiveness 64.865%.\n",
            "PGD linf: Attack effectiveness 68.421%.\n",
            "PGD linf: Attack effectiveness 61.290%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 64.286%.\n",
            "PGD linf: Attack effectiveness 76.471%.\n",
            "PGD linf: Attack effectiveness 46.667%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.26%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PG2 VS PGD_min"
      ],
      "metadata": {
        "id": "ZPY-dkCCw36O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "\n",
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        #pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(torch.abs(x_next - torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "bbCZIZiQlS30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mkFFSOXLuzWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=13\n",
        "adv3 = pgd2(mals[i:i+1].to(torch.float32).to(device), mals_y[i:i+1].to(device), model_AT_rFGSM, insertion_array, removal_array, k=100, step_length=.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55e6e04-edac-48fb-c943-d3704baebed4",
        "id": "p4HK-RBquzoT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********  0\n",
            "loss_mal :  tensor([38.1857], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  1\n",
            "loss_mal :  tensor([34.2699], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  2\n",
            "loss_mal :  tensor([29.9657], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  3\n",
            "loss_mal :  tensor([25.3872], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  4\n",
            "loss_mal :  tensor([20.8934], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  5\n",
            "loss_mal :  tensor([17.8123], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  6\n",
            "loss_mal :  tensor([18.1594], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  7\n",
            "loss_mal :  tensor([15.2948], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  8\n",
            "loss_mal :  tensor([15.7294], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  9\n",
            "loss_mal :  tensor([12.9326], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  10\n",
            "loss_mal :  tensor([12.9426], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  11\n",
            "loss_mal :  tensor([10.3893], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.0756e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  12\n",
            "loss_mal :  tensor([8.0923], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0003], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  13\n",
            "loss_mal :  tensor([9.6567], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([6.4013e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  14\n",
            "loss_mal :  tensor([7.6484], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0005], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  15\n",
            "loss_mal :  tensor([6.4177], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0016], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  16\n",
            "loss_mal :  tensor([7.7147], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0004], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  17\n",
            "loss_mal :  tensor([6.7666], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0012], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  18\n",
            "loss_mal :  tensor([5.8183], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0030], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  19\n",
            "loss_mal :  tensor([3.7913], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0228], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  20\n",
            "loss_mal :  tensor([3.6471], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0264], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  21\n",
            "loss_mal :  tensor([6.5451], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0014], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  22\n",
            "loss_mal :  tensor([3.4312], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0329], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  23\n",
            "loss_mal :  tensor([1.7684], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1871], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  24\n",
            "loss_mal :  tensor([2.7752], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0644], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  25\n",
            "loss_mal :  tensor([3.6827], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0255], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  26\n",
            "loss_mal :  tensor([0.7988], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.5976], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  27\n",
            "loss_mal :  tensor([4.7537], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0087], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  28\n",
            "loss_mal :  tensor([3.3431], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0360], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  29\n",
            "loss_mal :  tensor([1.8469], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1717], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  30\n",
            "loss_mal :  tensor([0.3787], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1545], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  31\n",
            "loss_mal :  tensor([1.0695], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.4204], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  32\n",
            "loss_mal :  tensor([1.9833], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1481], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  33\n",
            "loss_mal :  tensor([1.8189], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1770], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  34\n",
            "loss_mal :  tensor([1.7494], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1910], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  35\n",
            "loss_mal :  tensor([0.1565], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.9320], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  36\n",
            "loss_mal :  tensor([0.2397], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.5459], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  37\n",
            "loss_mal :  tensor([0.9653], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.4794], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  38\n",
            "loss_mal :  tensor([0.0746], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.6333], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  39\n",
            "loss_mal :  tensor([0.3412], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.2410], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  40\n",
            "loss_mal :  tensor([0.3077], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.3284], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  41\n",
            "loss_mal :  tensor([0.0294], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.5420], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  42\n",
            "loss_mal :  tensor([0.3282], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.2736], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  43\n",
            "loss_mal :  tensor([0.0365], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.3295], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  44\n",
            "loss_mal :  tensor([0.8298], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.5729], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  45\n",
            "loss_mal :  tensor([0.0195], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.9491], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  46\n",
            "loss_mal :  tensor([0.0035], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.6541], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  47\n",
            "loss_mal :  tensor([0.0026], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.9476], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  48\n",
            "loss_mal :  tensor([0.0009], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([6.9745], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  49\n",
            "loss_mal :  tensor([0.0007], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.2607], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  50\n",
            "loss_mal :  tensor([0.0006], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.3728], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  51\n",
            "loss_mal :  tensor([0.0005], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.5356], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  52\n",
            "loss_mal :  tensor([5.0543e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([9.8933], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  53\n",
            "loss_mal :  tensor([2.6703e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.5321], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  54\n",
            "loss_mal :  tensor([3.8742e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.1599], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  55\n",
            "loss_mal :  tensor([1.2278e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.3117], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  56\n",
            "loss_mal :  tensor([2.7179e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.5147], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  57\n",
            "loss_mal :  tensor([9.2983e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.5798], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  58\n",
            "loss_mal :  tensor([2.0027e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.8165], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  59\n",
            "loss_mal :  tensor([5.0068e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.2088], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  60\n",
            "loss_mal :  tensor([3.5047e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.2593], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  61\n",
            "loss_mal :  tensor([3.2186e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.6607], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  62\n",
            "loss_mal :  tensor([1.3113e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.5381], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  63\n",
            "loss_mal :  tensor([9.5367e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.8148], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  64\n",
            "loss_mal :  tensor([1.3113e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.5161], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  65\n",
            "loss_mal :  tensor([1.6689e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.3080], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  66\n",
            "loss_mal :  tensor([1.3113e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.5208], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  67\n",
            "loss_mal :  tensor([7.1526e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.2034], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  68\n",
            "loss_mal :  tensor([2.7418e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.8268], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  69\n",
            "loss_mal :  tensor([4.7684e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.6124], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  70\n",
            "loss_mal :  tensor([1.7881e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.2622], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  71\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.0045], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  72\n",
            "loss_mal :  tensor([8.3446e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.9858], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  73\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.3753], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  74\n",
            "loss_mal :  tensor([7.1526e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.2143], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  75\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.7451], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  76\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.0896], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  77\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.0098], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  78\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.4239], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  79\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.0536], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  80\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.5323], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  81\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.1801], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  82\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.4030], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  83\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.2882], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  84\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.0552], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  85\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.1186], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  86\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.5041], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  87\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.9173], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  88\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.2590], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  89\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.7290], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  90\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.7429], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  91\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.4080], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  92\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2161], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  93\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2731], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  94\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.4290], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  95\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.0085], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  96\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.4040], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  97\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.8233], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  98\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.7385], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  99\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.1252], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=13\n",
        "adv3 = pgd_min(mals[i:i+1].to(torch.float32).to(device), mals_y[i:i+1].to(device), model_AT_rFGSM, insertion_array, removal_array, k=100, step_length=.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ad855a-1a8e-416f-d870-1889cb1de5d1",
        "id": "LVWC9nF7aZia"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********  0\n",
            "loss_mal :  tensor([38.1857], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  1\n",
            "loss_mal :  tensor([34.2345], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  2\n",
            "loss_mal :  tensor([29.9309], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  3\n",
            "loss_mal :  tensor([25.2920], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  4\n",
            "loss_mal :  tensor([20.6400], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  5\n",
            "loss_mal :  tensor([17.5564], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  6\n",
            "loss_mal :  tensor([17.9629], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  7\n",
            "loss_mal :  tensor([15.0592], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  8\n",
            "loss_mal :  tensor([15.8962], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  9\n",
            "loss_mal :  tensor([12.6979], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([3.0994e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  10\n",
            "loss_mal :  tensor([12.9943], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.2650e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  11\n",
            "loss_mal :  tensor([11.0808], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.5378e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  12\n",
            "loss_mal :  tensor([8.7707], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  13\n",
            "loss_mal :  tensor([9.1304], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0001], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  14\n",
            "loss_mal :  tensor([8.6260], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  15\n",
            "loss_mal :  tensor([8.2149], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0003], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  16\n",
            "loss_mal :  tensor([6.0652], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0023], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  17\n",
            "loss_mal :  tensor([4.6464], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0096], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  18\n",
            "loss_mal :  tensor([7.6485], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0005], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  19\n",
            "loss_mal :  tensor([5.5824], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0038], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  20\n",
            "loss_mal :  tensor([3.1646], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0432], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  21\n",
            "loss_mal :  tensor([3.1386], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0443], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  22\n",
            "loss_mal :  tensor([5.6555], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0035], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  23\n",
            "loss_mal :  tensor([3.6233], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0271], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  24\n",
            "loss_mal :  tensor([3.2767], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0385], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  25\n",
            "loss_mal :  tensor([1.4218], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.2761], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  26\n",
            "loss_mal :  tensor([0.9565], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.4849], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  27\n",
            "loss_mal :  tensor([4.6454], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.0097], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  28\n",
            "loss_mal :  tensor([1.7336], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1944], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  29\n",
            "loss_mal :  tensor([2.0485], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.1380], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  30\n",
            "loss_mal :  tensor([0.4938], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.9424], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  31\n",
            "loss_mal :  tensor([0.2037], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.6913], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  32\n",
            "loss_mal :  tensor([0.1557], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([1.9365], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  33\n",
            "loss_mal :  tensor([0.0924], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.4275], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  34\n",
            "loss_mal :  tensor([1.2486], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([0.3382], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  35\n",
            "loss_mal :  tensor([0.1078], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([2.2809], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  36\n",
            "loss_mal :  tensor([0.0104], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([4.5724], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  37\n",
            "loss_mal :  tensor([0.0039], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([5.5394], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  38\n",
            "loss_mal :  tensor([0.0020], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([6.1924], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  39\n",
            "loss_mal :  tensor([0.0006], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.4991], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  40\n",
            "loss_mal :  tensor([0.0007], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.2758], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  41\n",
            "loss_mal :  tensor([0.0002], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([8.4886], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  42\n",
            "loss_mal :  tensor([0.0001], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([9.0697], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  43\n",
            "loss_mal :  tensor([4.1365e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.0920], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  44\n",
            "loss_mal :  tensor([2.1457e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.7516], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  45\n",
            "loss_mal :  tensor([1.5378e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.0796], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  46\n",
            "loss_mal :  tensor([6.6757e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.9121], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  47\n",
            "loss_mal :  tensor([4.1723e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.3956], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  48\n",
            "loss_mal :  tensor([2.9802e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.7136], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  49\n",
            "loss_mal :  tensor([3.2186e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.6321], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  50\n",
            "loss_mal :  tensor([1.3113e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.5583], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  51\n",
            "loss_mal :  tensor([2.7895e-05], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([10.4868], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  52\n",
            "loss_mal :  tensor([9.5367e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.8692], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  53\n",
            "loss_mal :  tensor([0.0009], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([7.0662], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  54\n",
            "loss_mal :  tensor([8.3446e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.0070], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  55\n",
            "loss_mal :  tensor([3.4571e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([12.5824], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  56\n",
            "loss_mal :  tensor([5.9605e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.3068], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  57\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.7389], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  58\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.0653], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  59\n",
            "loss_mal :  tensor([7.3909e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.8143], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  60\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.1435], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  61\n",
            "loss_mal :  tensor([3.5763e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([14.9123], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  62\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.5133], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  63\n",
            "loss_mal :  tensor([9.5367e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([11.5583], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  64\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.7569], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  65\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.4895], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  66\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.2097], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  67\n",
            "loss_mal :  tensor([1.1921e-06], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([13.6698], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  68\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.2498], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  69\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.6732], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  70\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.7017], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  71\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.6343], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  72\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.6715], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  73\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.2091], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  74\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.8232], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  75\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.5095], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  76\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.5486], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  77\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.6970], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  78\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.7999], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  79\n",
            "loss_mal :  tensor([1.1921e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([16.0975], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  80\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.7299], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  81\n",
            "loss_mal :  tensor([2.3842e-07], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([15.4905], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  82\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.8879], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  83\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2015], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  84\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.0201], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  85\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2666], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  86\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.9117], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  87\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.2953], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  88\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.0091], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  89\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.7422], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  90\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([17.6773], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  91\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.6948], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  92\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.1372], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  93\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.4145], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  94\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.9832], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  95\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.4932], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  96\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.3465], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  97\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.7350], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  98\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([18.3766], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "***********  99\n",
            "loss_mal :  tensor([-0.], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "loss :  tensor([19.5599], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K6yI8cjxbARk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYqfSitVY0z6",
        "outputId": "52fb3a24-204b-4ea4-be27-267a8085f9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 52.000%.\n",
            "PGD linf: Attack effectiveness 52.941%.\n",
            "PGD linf: Attack effectiveness 48.485%.\n",
            "PGD linf: Attack effectiveness 73.810%.\n",
            "PGD linf: Attack effectiveness 55.172%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 55.882%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 56.000%.\n",
            "PGD linf: Attack effectiveness 64.516%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 58.065%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 63.333%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 45.455%.\n",
            "PGD linf: Attack effectiveness 65.517%.\n",
            "PGD linf: Attack effectiveness 61.364%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 70.270%.\n",
            "PGD linf: Attack effectiveness 43.750%.\n",
            "PGD linf: Attack effectiveness 46.875%.\n",
            "PGD linf: Attack effectiveness 61.111%.\n",
            "PGD linf: Attack effectiveness 64.865%.\n",
            "PGD linf: Attack effectiveness 68.421%.\n",
            "PGD linf: Attack effectiveness 61.290%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 64.286%.\n",
            "PGD linf: Attack effectiveness 76.471%.\n",
            "PGD linf: Attack effectiveness 46.667%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.26%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weBpa3AzZt0B",
        "outputId": "01b53b9a-24c5-4539-da24-13ffeff109ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 46.154%.\n",
            "PGD linf: Attack effectiveness 82.857%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 70.588%.\n",
            "PGD linf: Attack effectiveness 63.636%.\n",
            "PGD linf: Attack effectiveness 71.429%.\n",
            "PGD linf: Attack effectiveness 62.069%.\n",
            "PGD linf: Attack effectiveness 85.000%.\n",
            "PGD linf: Attack effectiveness 61.765%.\n",
            "PGD linf: Attack effectiveness 55.000%.\n",
            "PGD linf: Attack effectiveness 74.074%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 70.968%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 67.742%.\n",
            "PGD linf: Attack effectiveness 77.778%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 40.909%.\n",
            "PGD linf: Attack effectiveness 65.517%.\n",
            "PGD linf: Attack effectiveness 65.909%.\n",
            "PGD linf: Attack effectiveness 61.538%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 59.375%.\n",
            "PGD linf: Attack effectiveness 62.500%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 73.684%.\n",
            "PGD linf: Attack effectiveness 70.968%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 67.857%.\n",
            "PGD linf: Attack effectiveness 79.412%.\n",
            "PGD linf: Attack effectiveness 46.667%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.33%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 1000, 'step_length': 0.01, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "id": "tqEgfkUMbAJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f02e812-de50-4bf6-89f6-928f75c9eb1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 53.846%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 58.824%.\n",
            "PGD linf: Attack effectiveness 54.545%.\n",
            "PGD linf: Attack effectiveness 73.810%.\n",
            "PGD linf: Attack effectiveness 58.621%.\n",
            "PGD linf: Attack effectiveness 85.000%.\n",
            "PGD linf: Attack effectiveness 64.706%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 77.778%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 74.194%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 70.968%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 83.333%.\n",
            "PGD linf: Attack effectiveness 54.545%.\n",
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 53.846%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 75.676%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 62.500%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 78.947%.\n",
            "PGD linf: Attack effectiveness 70.968%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 76.471%.\n",
            "PGD linf: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 1000, 'step_length': 0.01, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GABElx1ezzPg",
        "outputId": "cf6dc2d9-d88c-435f-f2ae-147faaf0c425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 76.000%.\n",
            "PGD linf: Attack effectiveness 57.692%.\n",
            "PGD linf: Attack effectiveness 82.857%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 76.000%.\n",
            "PGD linf: Attack effectiveness 82.353%.\n",
            "PGD linf: Attack effectiveness 69.697%.\n",
            "PGD linf: Attack effectiveness 73.810%.\n",
            "PGD linf: Attack effectiveness 75.862%.\n",
            "PGD linf: Attack effectiveness 95.000%.\n",
            "PGD linf: Attack effectiveness 67.647%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 85.185%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 77.419%.\n",
            "PGD linf: Attack effectiveness 76.667%.\n",
            "PGD linf: Attack effectiveness 84.000%.\n",
            "PGD linf: Attack effectiveness 80.645%.\n",
            "PGD linf: Attack effectiveness 77.778%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 93.333%.\n",
            "PGD linf: Attack effectiveness 59.091%.\n",
            "PGD linf: Attack effectiveness 79.310%.\n",
            "PGD linf: Attack effectiveness 81.818%.\n",
            "PGD linf: Attack effectiveness 65.385%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 81.081%.\n",
            "PGD linf: Attack effectiveness 65.625%.\n",
            "PGD linf: Attack effectiveness 81.250%.\n",
            "PGD linf: Attack effectiveness 80.556%.\n",
            "PGD linf: Attack effectiveness 75.676%.\n",
            "PGD linf: Attack effectiveness 84.211%.\n",
            "PGD linf: Attack effectiveness 77.419%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 82.143%.\n",
            "PGD linf: Attack effectiveness 82.353%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.83%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGjwR_talp00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJQQUoXLmZ0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bmCcaJuelpx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PGD vs PGD2 vs PGD_min"
      ],
      "metadata": {
        "id": "vixkX5NhbFXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd2(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    #insertion_array_updated = torch.bitwise_or(insertion_array.to(torch.uint8), x.squeeze().to(torch.uint8) )\n",
        "    #removal_array_updated = torch.bitwise_or(removal_array.to(torch.uint8), (1 - x.squeeze().to(torch.uint8)) )\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    loss_steps_i = []\n",
        "    loss_steps_d = []\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        loss_steps_i.append(loss.detach().item())\n",
        "        loss_steps_d.append(criterion(y_model, torch.zeros_like(y.view(-1).long())).detach().item())\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        #pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "            #perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            #perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            #perturbation[torch.isnan(perturbation)] = 0.\n",
        "            #perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next,loss_steps_i,loss_steps_d"
      ],
      "metadata": {
        "id": "ObTFtZ2kbDo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "\n",
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    loss_steps_i = []\n",
        "    loss_steps_d = []\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print(loss)\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "\n",
        "        loss_steps_i.append(criterion(y_model, y.view(-1).long()).detach().item())\n",
        "        loss_steps_d.append(loss.detach().item())\n",
        "\n",
        "\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var < (0.999)) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var < 1.) * 1\n",
        "        grad4insertion = (gradients > 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #print(torch.abs(gradients).sum())\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(torch.abs(x_next - torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        #a = (x_next + perturbation * step_length)\n",
        "        #b = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "        c = ((x_next + perturbation * step_length)- torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum()\n",
        "        if c!=0.:\n",
        "          print(c)\n",
        "        #print(((x_next + perturbation * step_length)- torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "        #loss_steps_i.append(criterion(y_model, y.view(-1).long()).detach().item())\n",
        "        #loss_steps_d.append(loss.detach().item())\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next,loss_steps_i,loss_steps_d"
      ],
      "metadata": {
        "id": "J5E7iDDWbDo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    loss_steps_i = []\n",
        "    loss_steps_d = []\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "        loss_steps_i.append(loss.detach().item())\n",
        "        loss_steps_d.append(criterion(y_model, torch.zeros_like(y.view(-1).long())).detach().item())\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "        #print(torch.abs(gradients).sum())\n",
        "\n",
        "        pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        #pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "        #print('nonzero(grad4insertion)',torch.count_nonzero(grad4insertion))\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 0 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "        #print('nonzero(grad4removal)',torch.count_nonzero(grad4removal))\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  gradients * tensor\n",
        "        #print(torch.abs(gradients).sum())\n",
        "        #print('nonzero(gradients)',torch.count_nonzero(gradients))\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next,loss_steps_i,loss_steps_d"
      ],
      "metadata": {
        "id": "Jm3RO3UvbDo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_next,loss_steps_i,loss_steps_d = pgd(mals[14:15].to(torch.float32).to(device), mals_y[13:14].to(device), model_AT_rFGSM, insertion_array, removal_array, k=100, step_length=.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641abe1c-4d5e-444d-a475-bca8ce02ece3",
        "id": "4xsfKgxv_nsS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 0.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Convert the loss data to DataFrames for seaborn\n",
        "df_i = pd.DataFrame({\n",
        "    'Step': range(len(loss_steps_i)),\n",
        "    'Loss': loss_steps_i\n",
        "})\n",
        "\n",
        "df_d = pd.DataFrame({\n",
        "    'Step': range(len(loss_steps_d)),\n",
        "    'Loss': loss_steps_d\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 1, figsize=(20, 25))\n",
        "\n",
        "# Plot loss_steps_i\n",
        "sns.lineplot(data=df_i, x='Step', y='Loss', ax=axes[0], marker='o', color='blue')\n",
        "axes[0].set_title('Loss Steps I')\n",
        "axes[0].set_xlabel('Step')\n",
        "axes[0].set_ylabel('Loss')\n",
        "\n",
        "# Plot loss_steps_d\n",
        "sns.lineplot(data=df_d, x='Step', y='Loss', ax=axes[1], marker='o', color='red')\n",
        "axes[1].set_title('Loss Steps D')\n",
        "axes[1].set_xlabel('Step')\n",
        "axes[1].set_ylabel('Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g2PX1EOD_2Ys",
        "outputId": "958deed4-690a-4d79-898e-3d27eb4623e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x2500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB8AAAAmzCAYAAABjyUM4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzde7SVdZ348c9BQBAEwhjUwFFgICxIUEFFGO+Kl8xb3hVBIkMsTJuVNYpaZmqmHFJRIQlL8NY4WWLmjUTHMkRNsRGBSSVRQg4XEY6c/fvDH2c6w51ztjx8fL3WYrl4zvN8z+e7a32Xa73de1eUSqVSAAAAAAAAAMBWrtGWHgAAAAAAAAAAGoIADgAAAAAAAEAKAjgAAAAAAAAAKQjgAAAAAAAAAKQggAMAAAAAAACQggAOAAAAAAAAQAoCOAAAAAAAAAApCOAAAAAAAAAApCCAAwAAAAAAAJCCAA4AAAAAAABACgI4AAAANJD7778/unXrFi+99NKWHmWDnnvuuTj33HOjf//+0aNHjzjggAPiq1/9avzqV7+qvWf58uVRWVkZzz777BacdMPOPPPMOProo7f0GAAAABSAAA4AAACfMA899FCcccYZ8fe//z3OOuus+Pd///f44he/GFVVVXH33XfX3rd8+fIYM2ZM/OEPf9iC0wIAAMDGa7ylBwAAAAA+XmPGjIkuXbrE5MmTo2nTpnV+9ve//30LTQUAAAD15x3gAAAA8DF75ZVX4txzz43evXtHr1694uyzz44ZM2bUuae6ujrGjBkThx12WPTo0SP69u0bp556akybNq32nnfffTe+/e1vx4ABA+Lzn/987L///nHeeefFm2++ud7f/9e//jV69OixRvyOiNhhhx0iIuLNN9+MfffdNyI+CubdunWLbt26RWVlZe29r7/+elxwwQXRp0+f6NGjRxx//PHx6KOP1llv9cfC//GPf4xLL700+vbtG717945vfetbUVVVVefel156KYYMGRJ9+/aNnj17xkEHHRTf/va3N/yCAgAAwP/nHeAAAADwMXrttdfi9NNPjxYtWsS5554bjRs3jsmTJ8eZZ54Zd955Z3zhC1+IiI+i89ixY+Okk06Knj17xtKlS+PPf/5zvPzyy9GvX7+IiBgxYkTMmjUrzjjjjPjMZz4TCxcujGnTpsXf/va36NChwzpn2HnnneOZZ56Jt99+O3bccce13tO2bdsYNWpUjBo1Kg499NA49NBDIyKiW7dutfs49dRTo3379jF06NDYbrvt4qGHHorhw4dHZWVl7f2rXXHFFdGqVas4//zzY86cOXHXXXfFvHnzYuLEiVFRURF///vfY8iQIfGpT30qvvKVr0SrVq3izTffjEceeaTerzkAAACfHAI4AAAAfIxuuOGGqK6ujrvuuis6duwYERFf+tKX4ogjjohrr7027rzzzoiIeOKJJ+Jf//Vf48orr1zrOosXL47nn38+vvWtb8WQIUNqrw8bNmyDMwwdOjS+853vxCGHHBK9e/eOPffcM/r16xe9e/eORo0++rC47bbbLg4//PAYNWpUdOvWLY499tg6a3z/+9+PnXbaKe67777ad5Kfdtppceqpp8Z11123RgBv0qRJ3HHHHdGkSZOI+CjCX3vttfHYY4/FwQcfHM8//3xUVVXFuHHjokePHrXPjRw5coP7AQAAgNV8BDoAAAB8TFatWhXTpk2LQw45pDZ+R0T80z/9Uxx99NHxpz/9KZYuXRoREa1atYrXXnst5s6du9a1mjVrFk2aNIk//OEPa3yU+IaceOKJcfvtt0ffvn1j+vTpcdNNN8Xpp58ehx12WEyfPn2Dzy9atCj+67/+KwYOHBhLly6NhQsXxsKFC+O9996L/fffP+bOnRvz58+v88zJJ59cG78jIk499dRo3LhxPPnkkxERsf3220fER+G/urp6k/YDAAAAqwngAAAA8DFZuHBhLF++PHbbbbc1fta5c+eoqamJv/3tbxERccEFF8SSJUvi8MMPj2OOOSZ++MMfxquvvlp7f9OmTeOiiy6KqVOnRr9+/eL000+P2267Ld59992NmqV///4xbty4+OMf/xg///nP4/TTT4958+bFV7/61fj73/++3mf/+te/RqlUihtvvDH23XffOn9Wf0f4/13jn//5n+v8vUWLFtGuXbt46623IiKiT58+cfjhh8eYMWNin332ifPOOy/uu+++WLly5UbtBwAAACJ8BDoAAAAU0t577x2PPPJIPProozFt2rS49957Y8KECXH55ZfHSSedFBERgwYNioMOOih+97vfxVNPPRU33nhj3HrrrTFhwoTYfffdN+r3NG/ePPbaa6/Ya6+94lOf+lSMGTMmpk6dGscdd9w6n6mpqYmIiMGDB0f//v3Xes8uu+yySfutqKiI0aNHx4wZM+Lxxx+P3//+93HJJZfET3/605g8eXK0aNFik9YDAADgk8k7wAEAAOBj0rZt22jevHnMmTNnjZ/Nnj07GjVqFDvttFPttTZt2sQJJ5wQ119/fTzxxBPRrVu32ndYr7bLLrvE4MGDY/z48fHggw9GdXV1jB8/frPm+/znPx8RUfsu8oqKirXet/rj25s0aRL77bffWv+0bNmyzjP/8z//U+fvy5Yti3fffTc+85nP1Lm+xx57xMiRI+P++++P6667Ll577bX4zW9+s1n7AQAA4JNHAAcAAICPyTbbbBP9+vWLRx99NN58883a6wsWLIgHH3ww9txzz9pw/N5779V5tkWLFrHLLrvUfiT48uXLY8WKFXXu2WWXXaJFixYb/NjwZ555Zq3XV38f9+qPaG/evHlERCxevLjOfTvssEP06dMnJk+eHO+8884a6yxcuHCNa5MnT67z3d533XVXfPjhhzFgwICIiKiqqopSqVTnme7du0dE+Bh0AAAANpqPQAcAAIAGdt9998Xvf//7Na6fddZZ8Y1vfCOefvrpOO200+K0006LbbbZJiZPnhwrV66Miy++uPbeo446Kvr06ROf+9znok2bNvHSSy/Fww8/HGeccUZERMydOzcGDRoURxxxRHTp0iW22Wab+N3vfhcLFiyIo446ar3zfe1rX4sOHTrEgQceGB07dozly5fH008/HY8//nj06NEjDjzwwIiIaNasWXTp0iUeeuih2HXXXaNNmzbxL//yL9G1a9e47LLL4rTTTotjjjkmvvzlL0fHjh1jwYIFMWPGjHj77bfjP//zP+v8zurq6hg0aFAMHDgw5syZE7/4xS9izz33jIMPPjgiIn75y1/GXXfdFYccckjssssusWzZsrj77rujZcuWtZEcAAAANqSi9H//82oAAABgs9x///3x7W9/e50/f/LJJ2PHHXeMV155JX70ox/F9OnTo1QqRc+ePWPkyJHRq1ev2ntvvvnmeOyxx2Lu3LmxcuXK2HnnnePYY4+NIUOGRJMmTeK9996LysrKeOaZZ+Ltt9+ObbbZJjp16hTnnHNODBw4cL1z/vrXv45HH300XnrppXjnnXeiVCpFx44d45BDDomhQ4fW+fjy559/Pq688sr47//+76iuro7zzz8/RowYERERb7zxRowZMyamTZsWixYtirZt28buu+8exx13XBx++OF1XpM777wzfvWrX8WUKVOiuro6Dj744Pjud78bbdq0iYiIV155JcaNGxfTp0+PBQsWxPbbbx89e/aM888/v/aj2dflzDPPjPfeey8efPDB9d4HAABAfgI4AAAAUDarA/i9994bPXr02NLjAAAAkJzvAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABS8B3gAAAAAAAAAKTgHeAAAAAAAAAApNB4Sw/wSfP8889HqVSKJk2abOlRAAAAAAAAAAqvuro6KioqolevXhu81zvAP2alUil86nzDK5VKsXLlSq8tQBk4YwHKyzkLUD7OWIDyccYClJdztq5NaazeAf4xW/3O7x49emzhSXJ5//33Y+bMmdGlS5fYbrvttvQ4AKk4YwHKyzkLUD7OWIDyccYClJdztq6XXnppo+/1DnAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASGGrD+Cvv/56nHPOObHHHntEv3794pprromVK1du8LlSqRS33nprHHDAAdGzZ884+eSTY8aMGeu8v6amJo4//vjo1q1bTJkypQF3AAAAAAAAAEBD2KoDeFVVVZx99tlRXV0dlZWVMXLkyLj77rvj6quv3uCzt912W4wePToGDRoUY8eOjXbt2sXgwYPjjTfeWOv9kyZNivnz5zf0FgAAAAAAAABoIFt1AJ80aVIsW7YsxowZE/37948TTzwxLr744g3G6hUrVsTYsWNj8ODBMWjQoNh3333j+uuvjzZt2sS4cePWuH/hwoVx4403xoUXXljO7QAAAAAAAABQD1t1AJ86dWrsu+++0aZNm9prAwcOjJqampg2bdo6n5s+fXosXbo0Bg4cWHutadOmceihh8bUqVPXuP/666+Pvn37Rt++fRt0fgAAAAAAAAAaTuMtPUB9zJ49O0444YQ611q1ahXt2rWL2bNnr/e5iIhOnTrVud65c+eYMGFCfPDBB9GsWbOIiHjxxRfjwQcfjAcffLDB5i6VSvH+++832HpELF++vM4/AWg4zliA8nLOApSPMxagfJyxAOXlnK2rVCpFRUXFRt27VQfwxYsXR6tWrda43rp166iqqlrvc02bNo1tt922zvVWrVpFqVSKqqqqaNasWdTU1MTll18e55xzTnTo0CHefPPNBpm7uro6Zs6c2SBrUdfcuXO39AgAaTljAcrLOQtQPs5YgPJxxgKUl3P2fzVt2nSj7tuqA3i53XPPPbFgwYL4yle+0qDrNmnSJLp06dKga37SLV++PObOnRu77rprNG/efEuPA5CKMxagvJyzAOXjjAUoH2csQHk5Z+uaNWvWRt+7VQfwVq1axZIlS9a4XlVVFa1bt17vcytXrowVK1bUeRf44sWLo6KiIlq3bh3Lli2L66+/PkaOHBnV1dVRXV0dS5cujYiIDz74IJYuXRotW7bcrLkrKipiu+2226xnWb/mzZt7bQHKxBkLUF7OWYDyccYClI8zFqC8nLMf2diPP4+IaFTGOcquU6dOa3zX95IlS+Ldd99d4/u9/+9zERFz5sypc3327Nmx8847R7NmzeK9996LRYsWxWWXXRZ777137L333nHsscdGRMS//du/xeGHH97AuwEAAAAAAACgPrbqd4APGDAgbrnlljrfBT5lypRo1KhR9OvXb53P9e7dO1q2bBkPPfRQfPazn42Ij76X+7e//W0MGDAgIiLatWsXP/vZz+o8t2DBgrjwwgtjxIgRsd9++5VpVwAAAAAAAABsjq06gJ9yyikxceLEGD58eAwbNizmz58f11xzTZxyyinRvn372vvOPvvsmDdvXjzyyCMREbHtttvGsGHDorKyMtq2bRtdu3aNu+66KxYtWhRDhgypvadv3751ft+bb74ZERFdunSJ3r17f0y7BAAAAAAAAGBjbNUBvHXr1jFhwoS48sorY/jw4dGiRYs48cQTY+TIkXXuq6mpiVWrVtW5NnTo0CiVSjF+/PhYuHBhdO/ePcaNGxcdO3b8OLcAAAAAAAAAQAPZqgN4RETnzp3jjjvuWO89EydOXONaRUVFDBs2LIYNG7bRv6tDhw7xl7/8ZVNHBAAAAAAAAOBj0GhLDwAAAAAAAAAADUEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFLY6gP466+/Huecc07sscce0a9fv7jmmmti5cqVG3yuVCrFrbfeGgcccED07NkzTj755JgxY0ade55++ukYOXJkHHTQQfGFL3whjjzyyLj99tujurq6TLsBAAAAAAAAYHNt1QG8qqoqzj777Kiuro7KysoYOXJk3H333XH11Vdv8NnbbrstRo8eHYMGDYqxY8dGu3btYvDgwfHGG2/U3jNp0qRYtmxZXHDBBXHrrbfGl770paisrIxLL720nNsCAAAAAAAAYDM03tID1MfqQD1mzJho06ZNRESsWrUqLr/88hg2bFi0b99+rc+tWLEixo4dG4MHD45BgwZFRMSee+4ZRxxxRIwbNy5GjRoVERGjRo2Ktm3b1j7Xt2/fqKmpiRtuuCEuvvjiOj8DAAAAAAAAYMvaqt8BPnXq1Nh3331r43dExMCBA6OmpiamTZu2zuemT58eS5cujYEDB9Zea9q0aRx66KExderU2mtrC9zdu3ePUqkU7777bsNsAgAAAAAAAIAGsVW/A3z27Nlxwgkn1LnWqlWraNeuXcyePXu9z0VEdOrUqc71zp07x4QJE+KDDz6IZs2arfXZ6dOnR9OmTaNDhw6bPXepVIr3339/s59nTcuXL6/zTwAajjMWoLycswDl44wFKB9nLEB5OWfrKpVKUVFRsVH3btUBfPHixdGqVas1rrdu3TqqqqrW+1zTpk1j2223rXO9VatWUSqVoqqqaq0BfO7cufGzn/0sTjnllGjRosVmz11dXR0zZ87c7OdZt7lz527pEQDScsYClJdzFqB8nLEA5eOMBSgv5+z/atq06Ubdt1UH8I/T0qVLY8SIEdGhQ4cYOXJkvdZq0qRJdOnSpYEmI+Kj//pl7ty5seuuu0bz5s239DgAqThjAcrLOQtQPs5YgPJxxgKUl3O2rlmzZm30vVt1AG/VqlUsWbJkjetVVVXRunXr9T63cuXKWLFiRZ13gS9evDgqKirWeHblypUxfPjwqKqqismTJ8d2221Xr7krKirqvQZr17x5c68tQJk4YwHKyzkLUD7OWIDyccYClJdz9iMb+/HnERGNyjhH2XXq1GmN7/pesmRJvPvuu2t8v/f/fS4iYs6cOXWuz549O3beeec6H39eU1MTF110Ubz88stx2223xU477dSAOwAAAAAAAACgoWzVAXzAgAHx9NNPx+LFi2uvTZkyJRo1ahT9+vVb53O9e/eOli1bxkMPPVR7rbq6On7729/GgAED6tx7+eWXx+OPPx433XRTdOvWreE3AQAAAAAAAECD2Ko/Av2UU06JiRMnxvDhw2PYsGExf/78uOaaa+KUU06J9u3b19539tlnx7x58+KRRx6JiIhtt902hg0bFpWVldG2bdvo2rVr3HXXXbFo0aIYMmRI7XO33HJLTJo0KYYMGRJNmzaNGTNm1P6sS5cu0bJly49trwAAAAAAAACs31YdwFu3bh0TJkyIK6+8MoYPHx4tWrSIE088MUaOHFnnvpqamli1alWda0OHDo1SqRTjx4+PhQsXRvfu3WPcuHHRsWPH2numTZsWERHjxo2LcePG1Xn+Zz/7WfTt27dMOwMAAAAAAABgU23VATwionPnznHHHXes956JEyeuca2ioiKGDRsWw4YN26TnAAAAAAAAACimrfo7wAEAAAAAAABgNQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFJoXJ+H582bF/PmzYu99tqr9tqrr74a48ePj5UrV8bRRx8dhxxySL2HBAAAAAAAAIANqVcA/973vhfvv/9+3HHHHRERsWDBgjjrrLOiuro6WrRoEQ8//HDceOONcdhhhzXErAAAAAAAAACwTvX6CPQXX3wx9ttvv9q//8d//Ed88MEH8cADD8TUqVNj3333jfHjx9d7SAAAAAAAAADYkHoF8Kqqqthhhx1q//7EE0/E3nvvHbvssks0atQoDj300Jg9e3a9hwQAAAAAAACADalXAG/btm3MmzcvIiIWL14cM2bMiP79+9f+fNWqVfHhhx/Wb0IAAAAAAAAA2Aj1+g7w/fbbLyZOnBgtW7aMZ599NkqlUhx88MG1P581a1bstNNO9R4SAAAAAAAAADakXgH8m9/8ZsyZMyd++MMfRpMmTeJb3/pWdOzYMSIiVq5cGQ899FAcc8wxDTIoAAAAAAAAAKxPvQL4pz/96Zg0aVIsWbIktt1222jatGntz2pqamLChAmx44471ntIAAAAAAAAANiQegXw1bbffvs1rjVr1iw++9nPNsTyAAAAAAAAALBBjerz8DPPPBO33357nWv33ntvHHDAAbHffvvFVVddFatWrarXgBvy+uuvxznnnBN77LFH9OvXL6655ppYuXLlBp8rlUpx6623xgEHHBA9e/aMk08+OWbMmLHGffPnz48RI0ZEr169ok+fPvGd73wnli5dWoadUF/NmjXb0iMApOWMBSgv5yxA+ThjAcrHGQtAEdUrgFdWVsarr75a+/e//OUvcdlll0Xbtm2jT58+MXHixBg3bly9h1yXqqqqOPvss6O6ujoqKytj5MiRcffdd8fVV1+9wWdvu+22GD16dAwaNCjGjh0b7dq1i8GDB8cbb7xRe091dXWce+65MXfu3PjRj34Uo0aNiqeeeiq++c1vlm1PbLplyyIaN24eO+zw2WjcuHksW7Z5a6xcGfHOOx/9c0utUaRZirJGkWYpyhpFmsV+yrNGkWYpyhnbUOtkWqNIs9hPedYo0ixFWaNIszTkfopwzhZljSLNUpQ1ijSL/ZRnjSLNUpQ1GnKWIpyxDbVOpjWKNEtR1ijSLEVZo0iz2M/a16jvGduQs2RZo0iz2E951ijSLEVZo0izFGk/1E+9Avjrr78en//852v//sADD0TLli3j5z//edxwww1x0kknxQMPPFDvIddl0qRJsWzZshgzZkz0798/TjzxxLj44otj0qRJMX/+/HU+t2LFihg7dmwMHjw4Bg0aFPvuu29cf/310aZNmzrB/uGHH47XXnstbrzxxjjooIPiyCOPjO9///vxxBNPxIsvvli2fbHxPvgg4pprItq3r4iddtom2reviGuu+ej6pq/xv3+2xBpFmqUoaxRplqKsUaRZ7Kc8axRplqKcsQ2/n61/jSLNYj/lWaNIsxRljSLN0vD72fr/Xdb/xuVZo0iz2E951ijSLEVZo+Fn8e+yRVujSLMUZY0izVKUNYo0i/2sb43NP2OLuR//P7Gf8q5RpFmKskaRZinSfmgApXro0aNH6Z577qn9+9FHH1268MILa/9+9913l/bYY4/6/Ir1Ou2000rnnXdenWtVVVWlbt26le677751Pvf000+XunbtWnrllVfqXL/qqqtKBx54YO3fL7744tIXv/jFOvfU1NSU+vTpUxo9evRmzfziiy+WXnzxxc16lrqWLi2VLr20VIpY88+ll5ZKVVUf3bO+P1VVxVijSLMUZY0izVKUNYo0i/14TbwmXhP78Zp4TezHa+I1+STvx2viNfkk7sdr4jXxmtiP18Rr8kndj9fEa9JQ+1m6dNM62LJly0rPPfdcadmyZeUJbVuZTWmsUZ9fdNhhh5UuvfTSUqlUKs2dO7fUrVu30v3331/789tuu63Up0+f+vyK9dpnn31K11577RrX999//7VeX+3OO+8sde3atfTBBx/UuT558uRSt27dSsuXLy+VSqXSCSecUPrGN76xxvMnn3zyWq9vjBdffLH0wgsvlJYtW+ZPPf+sWFFTatNm7QdJmzYfHSSf/vTafx7x0c+WLi1t8TWKNEtR1ijSLEVZo0iz2I/XxGviNbEfr4nXxH68Jl6TT/J+vCZek0/ifrwmXhOvif14Tbwmn9T9eE28Jg25nxUrajapgy1YsKD03HPPlRYsWLDFm1wR/rzwwgsbHcAb1+fd48ccc0z85Cc/ifnz58esWbOidevWcfDBB9f+/OWXX45dd921vm9SX6fFixdHq1at1rjeunXrqKqqWu9zTZs2jW233bbO9VatWkWpVIqqqqpo1qxZLF68OLbffvtNXn9DqqurY+bMmZv9PBHNmjWLHXb4bCxatM1af75oUcS770bsuGPEggVrX2PHHT/6/oVFi9b+849rjSLNUpQ1ijRLUdYo0iz2U541ijRLUdYo0ixFWaNIs9hPedYo0ixFWaNIs9hPedYo0ixFWaNIs9hPedYo0ixFWaNIs9hPedYo0ixFWaNIsxRljSLNYj/lWaNIsxRljSLNYj/lWaNIsxRljSLN8nHu5733amLBgjnxwSZ+HvrcuXM36f7MmjZtulH3VZRKpdLm/pIPP/wwKisr48knn4ztt98+vv71r8dee+0VERGLFi2Ko446Ks4666wYNmzY5v6K9frc5z4XX//61+MrX/lKnetHH3109OrVK6688sq1PnfzzTfHTTfdFC+99FKd61OmTImvf/3rMXXq1Gjfvn0cdthhsc8++8QVV1xR575hw4ZFdXV1jB8/fpNnfumll6JUKkWXLl02+Vnqaty4ebRvX7HWw6RNm4i33y7FkiXLY13/D6+oiNh+++ax445bdo0izVKUNYo0S1HWKNIs9lOeNYo0S1HWKNIsRVmjSLPYT3nWKNIsRVmjSLPYT3nWKNIsRVmjSLPYT3nWKNIsRVmjSLPYT3nWKNIsRVmjSLMUZY0izWI/5VmjSLMUZY0izWI/5VmjSLMUZY0izfJx7mf+/FJ8+OHydS/yfyxfvjzmzp0bu+66azRv3nyjn8tq1qxZUVFRET169NjwzZv1Od4Fsc8++5Suu+66Na435Eegjxw5co3n6/sR6L4DvGEsXVr/71IoyhpFmqUoaxRplqKsUaRZ7Kc8axRplqKsUaRZirJGkWaxn/KsUaRZirJGkWaxn/KsUaRZirJGkWaxn/KsUaRZirJGkWaxn/KsUaRZirJGkWYpyhpFmsV+yrNGkWYpyhpFmsV+yrNGkWYpyhpFmqVI+/lHy5b5DvB/9LF9B/g/Wrp0aWnWrFmlWbNmlZZu6v+Cm+m0004rfe1rX6tzbfHixaVu3bqV7rvvvnU+9/TTT5e6du1amjlzZp3rP/jBD0oHHnhg7d8vvvji0rHHHlvnnpqamlKfPn1Ko0eP3qyZBfCGtXz5R4fG6u9UaNPmo7////+GYatao0izFGWNIs1SlDWKNIv9lGeNIs1SlDWKNEtR1ijSLPZTnjWKNEtR1ijSLPZTnjWKNEtR1ijSLPZTnjWKNEtR1ijSLPZTnjWKNEtR1ijSLEVZo0iz2E951ijSLEVZo0iz2E951ijSLEVZo0izFGk/qwngdW1KY63XR6BHRLz44otx7bXXxvTp06OmpiYiIho1ahR77rlnXHzxxRv3NvTNNHbs2LjlllviySefrP0u8HvuuScuu+yyePzxx6N9+/ZrfW7FihWx3377xRlnnBEjR46MiI++l/vwww+PAQMGxKhRoyIi4sEHH4yLLroopkyZUvtd5k8//XScc845cc8990TPnj03eebVH7teztflk2bZsogmTUrx3ns18alPNYrq6opo0WJz1oioqopo3Tqiujq2yBpFmqUoaxRplqKsUaRZ7Kc8axRplqKcsQ21TqY1ijSL/ZRnjSLNUpQ1ijRLw+5ny5+zRVmjSLMUZY0izWI/5VmjSLMUZY2GnWXLn7ENtU6mNYo0S1HWKNIsRVmjSLPYz7rWqN8Z27Cz5FijSLPYT3nWKNIsRVmjSLMUaT8REe+//37MnDkzunfvHtttt92mL5DMpjTWegXwF154Ic4888xo0qRJHH300dG5c+eIiHj99dfj17/+dVRXV8fEiRM3KxRvjKqqqjjqqKNit912i2HDhsX8+fPj6quvjmOOOSYuvfTS2vvOPvvsmDdvXjzyyCO112699daorKyMiy66KLp27Rp33XVXPPXUU/HAAw9Ex44dI+KjKH788cdHRMSFF14Yy5cvj2uuuSa6desWY8eO3ayZBfDyeP/992POnDmx2267OQQAGpgzFqC8nLMA5eOMBSgfZyxAeQngdW1KY21cn1/04x//ONq3bx+/+MUvol27dnV+NmLEiDj11FPjxz/+cfz0pz+tz69Zp9atW8eECRPiyiuvjOHDh0eLFi3ixBNPrH1X92o1NTWxatWqOteGDh0apVIpxo8fHwsXLozu3bvHuHHjauN3RESTJk3i9ttvj+9973tx4YUXRuPGjePQQw+NSy65pCz7oX4++OCDLT0CQFrOWIDycs4ClI8zFqB8nLEAFFG9AvgLL7wQw4cPXyN+R0R8+tOfji9/+ctx00031edXbFDnzp3jjjvuWO89EydOXONaRUVFDBs2LIYNG7beZ9u3bx+VlZX1GREAAAAAAACAj0Gjej3cqNEa76z+RzU1NdGoUb1+BQAAAAAAAABslHrV6V69esXPf/7zeOutt9b42bx58+IXv/hF9O7duz6/AgAAAAAAAAA2Sr0+Av3CCy+M008/PQYOHBiHHnpo7LrrrhERMWfOnHj00UejUaNG8c1vfrMh5gQAAAAAAACA9apXAN99993jnnvuiR//+Mfx2GOPxfLlyyMionnz5tG/f/84//zz41Of+lSDDAoAAAAAAAAA61OvAB4R0aVLl/jJT34SNTU1sXDhwoiIaNu2bTRq1ChuvvnmGD16dMycObPegwIAAAAAAADA+tQ7gK/WqFGj+PSnP91QywEAAAAAAADAJmm0pQcAAAAAAAAAgIYggAMAAAAAAACQggAOAAAAAAAAQAqb/B3gL7/88kbf+84772zq8gAAAAAAAACwWTY5gJ9wwglRUVGxUfeWSqWNvhcAAAAAAAAA6mOTA/gPfvCDcswBAAAAAAAAAPWyyQH8uOOOK8ccAAAAAAAAAFAvjbb0AAAAAAAAAADQEARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIQQAHAAAAAAAAIAUBHAAAAAAAAIAUBHAAAAAAAAAAUhDAAQAAAAAAAEhBAAcAAAAAAAAgBQEcAAAAAAAAgBQEcAAAAAAAAABSEMABAAAAAAAASEEABwAAAAAAACAFARwAAAAAAACAFARwAAAAAAAAAFIQwAEAAAAAAABIYasP4I899lh88YtfjB49esThhx8e991330Y9t2TJkrjkkkuiT58+0atXr7jgggvinXfeqXPPpEmTYvDgwdGvX7/o3bt3fPnLX47f/e535dgGAAAAAAAAAPW0VQfw5557Ls4///zYY4894rbbbouBAwfGd77znZgyZcoGn/3GN74R06ZNi1GjRsV1110Xc+bMiaFDh8aHH35Ye88tt9wSO++8c4waNSoqKyujW7duMXz48PjlL39Zzm0BAAAAAAAAsBkab+kB6uPmm2+Onj17xhVXXBEREfvss0+88cYbMXr06DjiiCPW+dzzzz8fTz31VIwbNy7233//iIjYbbfd4sgjj4zf/va3ceSRR0ZExP333x9t27atfa5fv37x1ltvxfjx4+O4444r484AAAAAAAAA2FRb7TvAV65cGc8+++waofvII4+M119/Pd588811Pjt16tRo1apV9OvXr/Zap06donv37jF16tTaa/8Yv1fr3r37Gh+VDgAAAAAAAMCWt9W+A/yvf/1rVFdXR6dOnepc79y5c0REzJ49Ozp06LDWZ2fPnh277bZbVFRU1LneqVOnmD179np/75/+9Kc1fuemKpVK8f7779drDepavnx5nX8C0HCcsQDl5ZwFKB9nLED5OGMByss5W1epVFqj7a7LVhvAq6qqIiKiVatWda6v/vvqn6/N4sWLY/vtt1/jeuvWrePPf/7zOp/71a9+Fc8//3z85Cc/2ZyRa1VXV8fMmTPrtQZrN3fu3C09AkBazliA8nLOApSPMxagfJyxAOXlnP1fTZs23aj7ChXAlyxZslEfL96xY8ePYZq6Xn311bjsssvi+OOPj0MOOaReazVp0iS6dOnSQJMR8dF//TJ37tzYddddo3nz5lt6HIBUnLEA5eWcBSgfZyxA+ThjAcrLOVvXrFmzNvreQgXwKVOmxHe/+90N3veb3/wmWrduHREfRfN/tHjx4oiI2p+vTatWreLtt99e43pVVdVan3vrrbdi6NCh0bNnz7jiiis2ON+GVFRUxHbbbVfvdVhT8+bNvbYAZeKMBSgv5yxA+ThjAcrHGQtQXs7Zj2zsx59HFCyAn3TSSXHSSSdt1L0rV66MJk2axOzZs6N///6111d/h/f6vqe7U6dO8cwzz6zxWfFz5syJrl271rl34cKFMWTIkNhhhx1izJgx0aRJk03ZEgAAAAAAAAAfk0ZbeoDN1bRp0+jbt288/PDDda7/5je/ic6dO0eHDh3W+eyAAQOiqqoqnnnmmdprc+bMiVdeeSUGDBhQe23ZsmUxdOjQqK6ujltvvTVatmzZ8BsBAAAAAAAAoEFstQE8IuK8886LGTNmxKhRo+LZZ5+N0aNHx4MPPhgjRoyoc9/uu+8el1xySe3fe/XqFfvvv39ccskl8dBDD8Vjjz0WF1xwQXTr1i0OO+yw2vtGjBgRr776aowYMSLmzZsXM2bMqP0DAAAAAAAAQLEU6iPQN9Vee+0VlZWVccMNN8S9994bO++8c3zve9+LgQMH1rlv1apVUVNTU+faDTfcED/4wQ/i0ksvjQ8//DD233//+O53vxuNG//vSzJt2rSIiPi3f/u3NX73X/7ylzLsCAAAAAAAAIDNtVUH8IiIgw8+OA4++OD13rO2WL399tvHVVddFVddddUmPQcAAAAAAABAMW3VH4EOAAAAAAAAAKsJ4AAAAAAAAAD/j717j9O6rvP//xw5yEEQNQ8hIgKJHAM10SAqtRAsT6WrtpJpVm5E0bYmamXqz1orD3igDW2zMrOwcjPETE3LDDO1EPHEQUQzFeSMMuj8/pjvTI6Acpq5mPfc77fb3K5rPtfnmnld7O77xvrg/flQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUodkH8DvuuCNHHHFEBg4cmJEjR+bGG2/coPctW7YsZ511Vg444IAMGTIk48aNy/PPP7/e85977rkMGTIkffr0yaJFi7bU+AAAAAAAAABsIc06gN9///0ZO3ZsBg8enMmTJ2fUqFE5++yzM23atLd87xe+8IXcc889Offcc/Ptb387c+fOzWmnnZY1a9as8/xvfvOb6dChw5b+CAAAAAAAAABsIa0rPcDmmDRpUgYNGpTzzjsvSXLggQfm6aefzsSJE3PYYYet930PPvhg/vjHP+aaa67J8OHDkyR77bVXRo8end/+9rcZPXp0g/Pvvffe3Hvvvfn0pz+d//7v/268DwQAAAAAAADAJmu2O8BXr16d6dOnrxW6R48endmzZ2fBggXrfe/dd9+dzp07Z9iwYfXHevbsmb59++buu+9ucG51dXXOP//8fO5zn0uXLl226GcAAAAAAAAAYMtptjvA58+fn+rq6vTs2bPB8V69eiVJ5syZk27duq3zvXPmzMlee+2VqqqqBsd79uyZOXPmNDj2wx/+MK1atcoJJ5yQm266aYvMXlNTk5UrV26Rn0WtVatWNXgEYMuxxgI0LussQOOxxgI0HmssQOOyzjZUU1OzVttdn2YbwJcsWZIk6dy5c4Pjdd/Xvb4uS5cuTadOndY6vv322+fhhx+u//6f//xnrrzyylx55ZVp1arVlhg7Se2u8lmzZm2xn8e/zJs3r9IjABTLGgvQuKyzAI3HGgvQeKyxAI3LOvsvbdu23aDztqoAvmzZsjz//PNved4ee+zRBNMkF110UYYNG5aDDjpoi/7cNm3apHfv3lv0Z7Z0q1atyrx589KjR4+0b9++0uMAFMUaC9C4rLMAjccaC9B4rLEAjcs629CTTz65weduVQF82rRpOeecc97yvKlTp2b77bdPUhvNX2/p0qVJUv/6unTu3DnPPffcWseXLFlS/74HH3wwt956a372s5/V/8y6SwysWLEi7du33+T/ZauqqkqHDh026b28ufbt2/uzBWgk1liAxmWdBWg81liAxmONBWhc1tlaG3r582QrC+DHHntsjj322A06d/Xq1WnTpk3mzJmT97znPfXH6+7h/cZ7g79ez549c++99651rfi5c+dm7733rn9eXV2do48+eq33H3rooRk9enQuueSSDZoVAAAAAAAAgMa3VQXwjdG2bdsMHTo0t956az7+8Y/XH586dWp69eqVbt26rfe9I0aMyFVXXZV777037373u5PUBu9HHnkkn/zkJ5Mk73nPe/LDH/6wwfv+8Ic/ZPLkybnyyivTo0ePLf+hAAAAAAAAANhkzTaAJ8npp5+eMWPG5Nxzz82oUaMyffr03HzzzWvtzO7Xr1+OOuqoXHjhhUmSIUOGZPjw4TnrrLPy5S9/Odtuu20uueSS9OnTJx/84AeTJDvvvHN23nnnBj/nmWeeSZLsu+++2XHHHZvgEwIAAAAAAACwoZp1AN9///1z+eWX59JLL82UKVPStWvXXHDBBRk1alSD81599dW89tprDY5deuml+cY3vpGvfvWrWbNmTYYPH55zzjknrVs36z8SAAAAAAAAgBar2dfeQw45JIcccsibnvPYY4+tdaxTp0658MIL63eFb4hjjjkmxxxzzEbPCAAAAAAAAEDj26bSAwAAAAAAAADAliCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIlTV1NTUVHqIluSBBx5ITU1N2rZtW+lRilJTU5Pq6uq0adMmVVVVlR4HoCjWWIDGZZ0FaDzWWIDGY40FaFzW2YZWr16dqqqq7Lvvvm95busmmIfX8b+gjaOqqso/KgBoJNZYgMZlnQVoPNZYgMZjjQVoXNbZhqqqqja4s9oBDgAAAAAAAEAR3AMcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwGnWZs+enU984hMZPHhwhg0blosuuiirV6+u9FgAzc4tt9yS008/PSNGjMjgwYNz5JFHZsqUKampqWlw3s9//vOMHDkyAwcOzBFHHJE777yzQhMDNF8rVqzIiBEj0qdPn8yYMaPBa9ZZgE3zy1/+MkcddVQGDhyYoUOH5pOf/GRefvnl+tfvuOOOHHHEERk4cGBGjhyZG2+8sYLTAjQvt99+e4499tgMGTIkw4cPz+c///k8/fTTa53n77IAb+6pp57KV7/61Rx55JHp169fPvShD63zvA1ZT5ctW5azzjorBxxwQIYMGZJx48bl+eefb+yP0GwI4DRbS5Ysycc//vFUV1fn8ssvz/jx4/Ozn/0s3/zmNys9GkCz84Mf/CDt27fPmWeemUmTJmXEiBH5yle+kiuvvLL+nN/85jf5yle+klGjRmXy5MkZPHhwxo4dm4ceeqhygwM0Q1dddVVeffXVtY5bZwE2zaRJk3L++edn9OjRueaaa3LeeeelW7du9Wvt/fffn7Fjx2bw4MGZPHlyRo0albPPPjvTpk2r8OQAW7/p06dn7Nix6d27d6688sqcddZZefTRR3PKKac0+IdG/i4L8NaeeOKJ3HXXXdlzzz3Tq1evdZ6zoevpF77whdxzzz0599xz8+1vfztz587NaaedljVr1jTBJ9n6VdW8cWsXNBP/8z//k+9+97u5884706VLlyTJDTfckK9//eu58847s+uuu1Z2QIBmZNGiRdlxxx0bHPvKV76SqVOn5i9/+Uu22WabjBw5MgMGDMh3vvOd+nOOP/74dOrUKZMnT27qkQGapdmzZ+ejH/1ovvzlL+drX/tapkyZkoEDByaJdRZgE8yZMycf/vCHc9VVV+W9733vOs859dRTs2LFivz0pz+tP/af//mfmTVrVqZOndpUowI0S1/96ldzzz335He/+12qqqqSJH/+85/z8Y9/PNddd13233//JP4uC7AhXnvttWyzTe3e5DPPPDMPP/xwbr755gbnbMh6+uCDD+b444/PNddck+HDhyep/Xvx6NGjc/HFF2f06NFN9Im2XnaA02zdfffdOeigg+rjd5KMGjUqr732Wu65557KDQbQDL0xfidJ3759s3z58qxcuTJPP/105s2bl1GjRjU4Z/To0bn33nvdfgJgA11wwQU5/vjjs9deezU4bp0F2DS/+MUv0q1bt/XG79WrV2f69Ok57LDDGhwfPXp0Zs+enQULFjTFmADN1po1a9KxY8f6+J0knTp1SpL626b5uyzAhqmL3+uzoevp3Xffnc6dO2fYsGH15/Ts2TN9+/bN3XffveUHb4YEcJqtOXPmpGfPng2Ode7cOTvvvHPmzJlToakAyvHXv/41u+66a7bbbrv6dfWNwaZXr16prq5e572/AGho2rRpefzxx/PZz352rdesswCb5m9/+1v23nvvXHXVVTnooIMyYMCAHH/88fnb3/6WJJk/f36qq6vX+u8HdZec9N8PAN7cMccck9mzZ+e6667LsmXL8vTTT+fiiy9Ov379su+++ybxd1mALWVD19M5c+Zkr732avCPk5LaCO7vt7UEcJqtpUuXpnPnzmsd33777bNkyZIKTARQjvvvvz9Tp07NKaeckiT16+ob19267627AG9u1apV+eY3v5nx48dnu+22W+t16yzApnnhhRfyxz/+MTfddFO+9rWv5corr0xVVVVOOeWULFy40PoKsJn233//XHHFFfnOd76T/fffP4ceemgWLlyYyZMnp1WrVkn8XRZgS9nQ9XTp0qX1V+N4PX3sXwRwAKCB5557LuPHj8/QoUMzZsyYSo8DUIRJkyZlp512ykc+8pFKjwJQlJqamqxcuTKXXXZZDjvssLz3ve/NpEmTUlNTkx//+MeVHg+g2XvggQdyxhln5Ljjjsu1116byy67LK+99lo+9alP5eWXX670eACwTgI4zVbnzp2zbNmytY4vWbIk22+/fQUmAmj+li5dmtNOOy1dunTJ5ZdfXn9fmrp19Y3r7tKlSxu8DsDannnmmXz/+9/PuHHjsmzZsixdujQrV65MkqxcuTIrVqywzgJsos6dO6dLly7ZZ5996o916dIl/fr1y5NPPml9BdhMF1xwQQ488MCceeaZOfDAA3PYYYfle9/7Xh555JHcdNNNSfw3A4AtZUPX086dO2f58uVrvV8f+xcBnGZrXfcyWLZsWV544YW17u0FwFt7+eWX8+lPfzrLli3L1Vdf3eAyOnXr6hvX3Tlz5qRNmzbZY489mnRWgOZkwYIFqa6uzqc+9am8613vyrve9a585jOfSZKMGTMmn/jEJ6yzAJuod+/e633tlVdeSffu3dOmTZt1rq9J/PcDgLcwe/bsBv/IKEl222237LDDDpk/f34S/80AYEvZ0PW0Z8+emTt3bmpqahqcN3fuXH+//X8EcJqtESNG5E9/+lP9v3xJkmnTpmWbbbbJsGHDKjgZQPOzZs2afOELX8icOXNy9dVXZ9ddd23w+h577JEePXpk2rRpDY5PnTo1Bx10UNq2bduU4wI0K3379s0Pf/jDBl8TJkxIknz961/P1772NesswCZ6//vfn8WLF2fWrFn1x1566aXMnDkz/fv3T9u2bTN06NDceuutDd43derU9OrVK926dWvqkQGala5du+aRRx5pcOyZZ57JSy+9lN133z2J/2YAsKVs6Ho6YsSILFmyJPfee2/9OXPnzs0jjzySESNGNOnMW6vWlR4ANtXxxx+fH/3oR/nsZz+bT3/60/nnP/+Ziy66KMcff/xa4QaAN/f1r389d955Z84888wsX748Dz30UP1r/fr1S9u2bfO5z30uX/rSl9K9e/cMHTo0U6dOzd///nf3VgR4C507d87QoUPX+Vr//v3Tv3//JLHOAmyCQw89NAMHDsy4ceMyfvz4bLvttvne976Xtm3b5sQTT0ySnH766RkzZkzOPffcjBo1KtOnT8/NN9+cSy65pMLTA2z9jj/++Fx44YW54IILcvDBB2fx4sWZNGlSdtppp4waNar+PH+XBXhrq1atyl133ZWk9h8TLV++vD52H3DAAdlxxx03aD0dMmRIhg8fnrPOOitf/vKXs+222+aSSy5Jnz598sEPfrAin21rU1Xzxv3x0IzMnj07559/fh588MF07NgxRx55ZMaPH+9fFQJspIMPPjjPPPPMOl+7/fbb63fG/PznP8/kyZPz7LPPZq+99soXv/jFvP/972/KUQGKMH369IwZMyZTpkzJwIED649bZwE23qJFi/KNb3wjd955Z6qrq7P//vtnwoQJDS6Pfvvtt+fSSy/N3Llz07Vr13zqU5/KRz/60QpODdA81NTU5Kc//Wmuv/76PP300+nYsWMGDx6c8ePHp1evXg3O9XdZgDe3YMGCHHLIIet87Yc//GH9P57fkPV02bJl+cY3vpHbbrsta9asyfDhw3POOefYIPr/COAAAAAAAAAAFME9wAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAAChC60oPAAAAAGwZjz32WK688srMmDEjL774Yrp06ZLevXvn4IMPzkknnZQk+e53v5vevXvn0EMPrfC0AAAAsOVV1dTU1FR6CAAAAGDzPPDAAxkzZky6du2ao446KjvvvHP+8Y9/5G9/+1vmz5+f2267LUkyZMiQjBw5Mt/85jcrPDEAAABseXaAAwAAQAG++93vplOnTpkyZUo6d+7c4LWFCxdWaCoAAABoWu4BDgAAAAWYP39+evfuvVb8TpKddtopSdKnT5+sXLkyv/zlL9OnT5/06dMnZ555Zv15//znPzNhwoS8+93vzoABA3L44YdnypQpDX7W9OnT06dPn0ydOjUXX3xxhg0blsGDB+czn/lM/vGPfzTuhwQAAIC3YAc4AAAAFGD33XfPgw8+mMcffzx77733Os+56KKLcs4552TQoEE57rjjkiTdu3dPkrz44os57rjjUlVVlY997GPZcccdc/fdd+fss8/O8uXLc/LJJzf4WZMmTUpVVVVOO+20LFy4MNdee21OPvnk3HTTTWnXrl2jflYAAABYH/cABwAAgALcc889Oe2005IkgwYNyn777ZeDDjooQ4cOTZs2berPW989wM8+++zcdddd+fWvf50ddtih/vgXv/jF3H333fnjH/+Ydu3aZfr06RkzZkx23XXXTJ06Ndttt12S5JZbbskXvvCFnH322RkzZkwTfGIAAABYm0ugAwAAQAGGDRuWn/70pzn44IPz6KOP5uqrr86pp56aESNG5Pbbb3/T99bU1OS3v/1tDj744NTU1GTRokX1X8OHD8+yZcsyc+bMBu856qij6uN3khx22GHZeeedc9dddzXK5wMAAIAN4RLoAAAAUIhBgwbliiuuyOrVq/Poo4/md7/7XX7wgx/k85//fH71q1+ld+/e63zfokWLsnTp0txwww254YYb1nvO6+25554Nvq+qqsqee+6ZZ555Zst8GAAAANgEAjgAAAAUpm3bthk0aFAGDRqUHj16ZMKECZk2bVrGjh27zvNfe+21JMkRRxyRo48+ep3n9OnTp9HmBQAAgC1FAAcAAICCDRgwIEny/PPPr/ecHXfcMR07dsxrr72Wd7/73Rv0c5966qkG39fU1OSpp54SygEAAKgo9wAHAACAAvz5z39OTU3NWsfr7snds2fPJEmHDh2ydOnSBue0atUqI0eOzK233prHH398rZ/xxsufJ8mvfvWrLF++vP77adOm5YUXXsiIESM263MAAADA5qiqWdf/dwwAAAA0Kx/60IeyatWqfOADH0jPnj1TXV2dBx54ILfcckt22223/OpXv0rnzp3zqU99Kn/5y18ybty47LLLLunWrVve+c535sUXX8xxxx2XRYsW5dhjj03v3r2zZMmSzJw5M/fee2/uu+++JMn06dMzZsyY7L333qmqqsoxxxyThQsX5tprr81uu+2Wm266Ke3bt6/wnwYAAAAtlQAOAAAABbj77rszbdq0PPjgg3nuuedSXV2drl27ZsSIETn99NOz0047JUnmzJmTr371q5kxY0ZefvnlHH300fnmN7+ZJFm4cGGuvPLK3HHHHXnxxRfTpUuX9O7dO6NHj85xxx2X5F8B/OKLL85jjz2WKVOmZMWKFTnwwAPzta99LV27dq3YnwEAAAAI4AAAAMAGqwvgl112WQ477LBKjwMAAAANuAc4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBHcAxwAAAAAAACAItgBDgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIrQutIDAAAAQHP1i1/8IhMmTMiUKVMycODASo/zpu6///5897vfzWOPPZbFixdnp512yj777JPDDz88H/7wh5Mkq1atytVXX50DDjggQ4cOrfDE63fSSSflvvvuS5JUVVWlQ4cO2XnnnTNo0KAcddRRGTZsWIUnBAAAoFIEcAAAACjcLbfckvHjx6dv374ZM2ZMtt9++yxYsCB/+ctf8rOf/axBAL/iiisyduzYrTqAJ8luu+2WL37xi0lq537qqady22235f/+7/8yatSofOtb30qbNm0qPCUAAABNTQAHAACAwl1xxRXp3bt3brjhhrRt27bBawsXLqzQVJunU6dOOfLIIxsc+9KXvpQLLrggP/nJT7L77rvnv/7rvyo0HQAAAJXiHuAAAADQyB555JF88pOfzL777pshQ4bk4x//eB566KEG51RXV+eKK67IBz/4wQwcODBDhw7NCSeckHvuuaf+nBdeeCETJkzIiBEjMmDAgAwfPjynn356FixY8Ka/f/78+Rk4cOBa8TtJdtpppyTJggULctBBByWpDeZ9+vRJnz59cvnll9efO3v27IwbNy4HHHBABg4cmGOOOSa33357g5/3i1/8In369Mlf/vKXfPWrX83QoUOz77775owzzsiSJUsanDtjxoyceuqpGTp0aAYNGpSDDz44EyZMeOs/0PVo1apVzjnnnPTu3TvXXXddli1btsk/CwAAgObJDnAAAABoRE888UQ+9rGPpWPHjvnkJz+Z1q1b54YbbshJJ52UH//4x3nnO9+ZpDY6/8///E+OPfbYDBo0KMuXL8/DDz+cmTNn1t/T+nOf+1yefPLJ/Pu//3t23333LFq0KPfcc0/+8Y9/pFu3buudoWvXrrn33nvz3HPPZbfddlvnOTvuuGPOPffcnHvuufnABz6QD3zgA0mSPn361H+OE044IbvuumtOO+20dOjQIbfccks++9nP5vLLL68/v855552Xzp07Z+zYsZk7d26uv/76PPvss/nRj36UqqqqLFy4MKeeemp22GGHfOpTn0rnzp2zYMGC3HbbbZv1592qVascfvjhueyyy/LXv/4173vf+zbr5wEAANC8COAAAADQiC699NJUV1fn+uuvzx577JEkOeqoo3LYYYflW9/6Vn784x8nSX7/+9/nve99b84///x1/pylS5fmwQcfzBlnnJFTTz21/vinP/3pt5zhtNNOy9lnn51DDz00++67b/bbb78MGzYs++67b7bZpvbicB06dMjIkSNz7rnnpk+fPmtdXvz/+//+v7z97W/PjTfeWL+T/MQTT8wJJ5yQb3/722sF8DZt2uQHP/hB/X24u3btmm9961u54447csghh+TBBx/MkiVLcs0112TgwIH17xs/fvxbfp63svfeeyep3fkOAABAy+IS6AAAANBIXn311dxzzz059NBD6+N3kuyyyy750Ic+lL/+9a9Zvnx5kqRz58554oknMm/evHX+rHbt2qVNmza577771rqU+Fv56Ec/mquvvjpDhw7NAw88kKuuuiof+9jH8sEPfjAPPPDAW75/8eLF+fOf/5xRo0Zl+fLlWbRoURYtWpSXXnopw4cPz7x58/LPf/6zwXv+7d/+rT5+J8kJJ5yQ1q1b56677kpSew/vpDb8V1dXb9TneSsdOnRIkqxYsWKL/lwAAAC2fgI4AAAANJJFixZl1apV2WuvvdZ6rVevXnnttdfyj3/8I0kybty4LFu2LCNHjsyHP/zh/Pd//3ceffTR+vPbtm2bL33pS7n77rszbNiwfOxjH8vkyZPzwgsvbNAs73nPe3LNNdfkL3/5S6677rp87GMfy7PPPpvPfOYzWbhw4Zu+d/78+ampqclll12Wgw46qMFX3T3C3/gz9txzzwbfd+zYMTvvvHOeeeaZJMkBBxyQkSNH5oorrsiBBx6Y008/PTfeeGNWr169QZ/nzaxcubL+dwIAANCyuAQ6AAAAbAXe9a535bbbbsvtt9+ee+65J1OmTMm1116br3/96zn22GOTJCeffHIOPvjg/O53v8sf//jHXHbZZfne976Xa6+9Nv369dug39O+ffvsv//+2X///bPDDjvkiiuuyN13352jjz56ve957bXXkiSnnHJK3vOe96zznO7du2/U562qqsrEiRPz0EMP5c4778wf/vCHnHXWWfnf//3f3HDDDZsVrx9//PEka0d4AAAAymcHOAAAADSSHXfcMe3bt8/cuXPXem3OnDnZZptt8va3v73+WJcuXfKRj3wkF198cX7/+9+nT58+9Tus63Tv3j2nnHJKvv/97+fmm29OdXV1vv/972/SfAMGDEiS+l3kVVVV6zyv7vLtbdq0ybvf/e51fm233XYN3vPUU081+H7FihV54YUXsvvuuzc4Pnjw4IwfPz6/+MUv8u1vfztPPPFEpk6dukmfJ6m97PzNN9+c9u3bZ7/99tvknwMAAEDzJIADAABAI2nVqlWGDRuW22+/PQsWLKg//uKLL+bmm2/OfvvtVx+OX3rppQbv7dixY7p3715/SfBVq1bllVdeaXBO9+7d07Fjx7e8bPi99967zuN19+Ouu0R7+/btkyRLly5tcN5OO+2UAw44IDfccEOef/75tX7OokWL1jp2ww03NLi39/XXX581a9ZkxIgRSZIlS5akpqamwXv69u2bJJt8GfRXX301F1xwQWbPnp2TTjpprSgPAABA+VwCHQAAADbTjTfemD/84Q9rHR8zZky+8IUv5E9/+lNOPPHEnHjiiWnVqlVuuOGGrF69Ov/1X/9Vf+7hhx+eAw44IP3790+XLl0yY8aM3Hrrrfn3f//3JMm8efNy8skn57DDDkvv3r3TqlWr/O53v8uLL76Yww8//E3n+4//+I9069Yt73//+7PHHntk1apV+dOf/pQ777wzAwcOzPvf//4kSbt27dK7d+/ccsst6dGjR7p06ZJ3vOMd2XvvvfO1r30tJ554Yj784Q/nuOOOyx577JEXX3wxDz30UJ577rn83//9X4PfWV1dnZNPPjmjRo3K3Llz85Of/CT77bdfDjnkkCTJL3/5y1x//fU59NBD071796xYsSI/+9nPst1229VH8jezbNmy3HTTTUmSl19+OU899VRuu+22zJ8/P4cffng+//nPv+XPAAAAoDxVNW/859YAAADABvnFL36RCRMmrPf1u+66K7vttlseeeSRfOc738kDDzyQmpqaDBo0KOPHj8+QIUPqz500aVLuuOOOzJs3L6tXr07Xrl1z5JFH5tRTT02bNm3y0ksv5fLLL8+9996b5557Lq1atUrPnj3ziU98IqNGjXrTOX/zm9/k9ttvz4wZM/L888+npqYme+yxRw499NCcdtppDXZKP/jggzn//PPz+OOPp7q6OmPHjs3nPve5JMnTTz+dK664Ivfcc08WL16cHXfcMf369cvRRx+dkSNHNvgz+fGPf5xf//rXmTZtWqqrq3PIIYfknHPOSZcuXZIkjzzySK655po88MADefHFF9OpU6cMGjQoY8eOrb80+/qcdNJJue++++q/79ChQ3bZZZcMGjQoRx11VIYNG/am7wcAAKBcAjgAAACwxdQF8ClTpmTgwIGVHgcAAIAWxj3AAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIrgHuAAAAAAAAAAFMEOcAAAAAAAAACK0LrSA7Q0Dz74YGpqatKmTZtKjwIAAAAAAACw1auurk5VVVWGDBnylufaAd7Eampq4qrzW15NTU1Wr17tzxagEVhjARqXdRag8VhjARqPNRagcVlnG9qYxmoHeBOr2/k9cODACk9SlpUrV2bWrFnp3bt3OnToUOlxAIpijQVoXNZZgMZjjQVoPNZYgMZlnW1oxowZG3yuHeAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIADAAAAAAAAUAQBHAAAAAAAAIAiCOAAAAAAAAAAFEEABwAAAAAAAKAIAjgAAAAAAAAARRDAAQAAAAAAACiCAA4AAAAAAABAEQRwAAAAAAAAAIoggAMAAAAAAABQBAEcAAAAAAAAgCII4AAAAAAAAAAUQQAHAAAAAAAAoAgCOAAAAAAAAABFEMABAAAAAAAAKIIADgAAAAAAAEARBHAAAAAAAAAAiiCAAwAAAAAAAFAEARwAAAAAAACAIgjgAAAAAAAAABRBAAcAAAAAAACgCAI4AAAAAAAAAEUQwAEAAAAAAAAoggAOAAAAAAAAQBEEcAAAAAAAAACKIIBTjHbt2lV6BAAAAAAAAKCCBHCavxUr0r516+yz005p37p1smJFpScCAAAAAAAAKkAAp3l7+eXkootSteuuafX2t6dq112Tiy6qPQ4AAAAAAAC0KK0rPQBsshUramP3eef969jixf/6/owzko4dKzIaAAAAAAAA0PTsAKf5atMmmThx3a9NnFj7OgAAAAAAANBiCOA0X4sX136t77UlS5pwGAAAAAAAAKDSBHCary5dar/W99r22zfhMAAAAAAAAEClCeA0X9XVybhx635t3Lja1wEAAAAAAIAWo3WlB4BN1rFjMmFC7fOJE2sve96lS238njAhadeuktMBAAAAAAAATUwAp3lr1y4544zUnHlmqv75z9TsskuqamrEbwAAAAAAAGiBXAKd5q9jx7xyxx3Jhz+cmkMPrd0ZDgAAAAAAALQ4doBThJq9904efjhVrVolr7ySbLttpUcCAAAAAAAAmpgd4BShpmvXrOnUKVWvvprMmlXpcQAAAAAAAIAKEMApQ1VVVvXuXft8xozKzgIAAAAAAABUhABOMQRwAAAAAAAAaNkEcIqxqlev2icCOAAAAAAAALRIAjjFqN8B/vDDlR0EAAAAAAAAqAgBnGK8XLcDfMGC5KWXKjsMAAAAAAAA0OQEcIrxaqdOea1bt9pv7AIHAAAAAACAFkcApyg1/fvXPnEfcAAAAAAAAGhxBHCK8poADgAAAAAAAC2WAE5RBHAAAAAAAABouQRwilIfwB9+OKmpqewwAAAAAAAAQJMSwClKzd57J61aJUuWJAsWVHocAAAAAAAAoAkJ4JRl222TPn1qn7sMOgAAAAAAALQoAjjlGTiw9lEABwAAAAAAgBZFAKc8AjgAAAAAAAC0SAI45RHAAQAAAAAAoEUSwClPXQB/9NGkurqyswAAAAAAAABNRgCnPHvumXTsmKxenTzxRKWnAQAAAAAAAJqIAE55ttkmGTCg9rnLoAMAAAAAAECLIYBTJvcBBwAAAAAAgBZHAKdMAjgAAAAAAAC0OAI4ZRLAAQAAAAAAoMURwClT3T3A585Nli+v7CwAAAAAAABAkxDAKdPOOye77lr7fObMys4CAAAAAAAANAkBnHK5DDoAAAAAAAC0KAI45RLAAQAAAAAAoEURwCmXAA4AAAAAAAAtigBOuV4fwGtqKjsLAAAAAAAA0OhadAC/8847c/TRR2fAgAF573vfm4kTJ+bVV19d67w77rgjRxxxRAYOHJiRI0fmxhtvrMC0bLR+/ZKqquTFF5Pnn6/0NAAAAAAAAEAja7EB/KGHHsp//Md/pFevXpk0aVJOPvnkXHPNNfn2t7/d4Lz7778/Y8eOzeDBgzN58uSMGjUqZ599dqZNm1ahydlgHTokvXrVPncZdAAAAAAAAChe60oPUCmXX355+vbtWx+83/Oe96SmpiYXX3xxTj311LztbW9LkkyaNCmDBg3KeeedlyQ58MAD8/TTT2fixIk57LDDKjY/G2jgwOTJJ2sD+KGHVnoaAAAAAAAAoBG12B3gs2bNyrBhwxocGz58eKqrq/PHP/4xSbJ69epMnz59rdA9evTozJ49OwsWLGiyedlEr78POAAAAAAAAFC0FrsD/JVXXknbtm0bHKv7fvbs2UmS+fPnp7q6Oj179mxwXq//d1ntOXPmpFu3bhv9u2tqarJy5cpNGZv1WLVqVYPHOq323jvbJnn1b3/LK/7MATbJ+tZYALYM6yxA47HGAjQeayxA47LONlRTU5OqqqoNOrfFBvA999wzf//73xsce+ihh5IkS5YsafDYuXPnBufVfV/3+saqrq7OrFmzNum9vLl58+Y1+H7bdu0yIEnVzJmZ9fDDSatWFZkLoARvXGMB2LKsswCNxxoL0HissQCNyzr7L2/c3Lw+LTaAn3jiiTn77LNz7bXX5sgjj8yTTz6ZSy+9NK2aIJC2adMmvXv3bvTf05KsWrUq8+bNS48ePdK+fft/vbD33qlp1y7bvPxy+rVrlxp/7gAbbb1rLABbhHUWoPFYYwEajzUWoHFZZxt68sknN/jcFhvAjznmmDz++OO56KKLcuGFF6ZNmzYZO3Zsrr322uyyyy5Jku233z5JsmzZsgbvXbp0aYPXN1ZVVVU6dOiwGdOzPu3bt1/7z7Zfv+SBB9J+9uxk0KDKDAZQgHWusQBsMdZZgMZjjQVoPNZYgMZlna21oZc/T5JtGnGOrdo222yTs846K3/+859z00035U9/+lOOO+64LFq0KO985zuTJN27d0+bNm0yZ86cBu+t+/6N9wZnKzVgQO3jjBmVnQMAAAAAAABoVC02gNfp1KlT9tlnn3Tu3Dk/+tGP0q1bt7z73e9OUnsd+aFDh+bWW29t8J6pU6emV69e6datWyVGZmMNHFj7KIADAAAAAABA0VrsJdD//ve/57777kvfvn3z8ssv54477shNN92UyZMnN7gP+Omnn54xY8bk3HPPzahRozJ9+vTcfPPNueSSSyo4PRtFAAcAAAAAAIAWocUG8DZt2uS3v/1trrzyyiTJO9/5zvzoRz/KkCFDGpy3//775/LLL8+ll16aKVOmpGvXrrngggsyatSoSozNpqgL4E88kaxalbRvX9l5AAAAAAAAgEbRYgN4375987Of/WyDzj3kkENyyCGHNPJENJq3vz3Zccdk0aJk1qxk330rPREAAAAAAADQCFr8PcBpAaqq/rUL/OGHKzsLAAAAAAAA0GgEcFqGAQNqH90HHAAAAAAAAIolgNMy1O0AF8ABAAAAAACgWAI4LYMADgAAAAAAAMUTwGkZ6i6B/uyzyaJFlZ0FAAAAAAAAaBQCOC1D587JnnvWPrcLHAAAAAAAAIokgNNy1F0G/eGHKzsHAAAAAAAA0CgEcFqOusug2wEOAAAAAAAARRLAaTnqdoAL4AAAAAAAAFAkAZyW4/WXQK+pqewsAAAAAAAAwBYngNNy9OmTtG6dLF2azJ9f6WkAAAAAAACALUwAp+Vo2zbZZ5/a5y6DDgAAAAAAAMURwGlZXn8ZdAAAAAAAAKAoAjgtS10AtwMcAAAAAAAAiiOA07IMGFD7KIADAAAAAABAcQRwWpa6HeCPPppUV1d2FgAAAAAAAGCLEsBpWfbcM+nUqTZ+P/ZYpacBAAAAAAAAtiABnJalqspl0AEAAAAAAKBQAjgtT91l0B9+uLJzAAAAAAAAAFuUAE7LUxfA7QAHAAAAAACAogjgtDwugQ4AAAAAAABFEsBpeep2gM+blyxbVtFRAAAAAAAAgC1HAKfl2Wmn5O1vr33uPuAAAAAAAABQDAGclsl9wAEAAAAAAKA4AjgtU10AtwMcAAAAAAAAiiGA0zLZAQ4AAAAAAADFEcBpmQYMqH2cMSOpqansLAAAAAAAAMAWIYDTMvXrl2yzTbJwYfLcc5WeBgAAAAAAANgCBHBapvbtk969a5+7DDoAAAAAAAAUQQCn5XIfcAAAAAAAACiKAE7LVRfAH364snMAAAAAAAAAW4QATstlBzgAAAAAAAAURQCn5RowoPZx5szk1VcrOwsAAAAAAACw2QRwWq5evZL27ZOXX05mz670NAAAAAAAAMBmEsBpuVq1Svr1q33uMugAAAAAAADQ7AngtGzuAw4AAAAAAADFEMBp2eoC+MMPV3YOAAAAAAAAYLMJ4LRsdoADAAAAAABAMQRwWra6AP7kk8mqVZWdBQAAAAAAANgsAjgt2667JjvtlLz2WvLII5WeBgAAAAAAANgMAjgtW1WVy6ADAAAAAABAIQRwEMABAAAAAACgCAI4COAAAAAAAABQBAEc6gL4ww9Xdg4AAAAAAABgswjg0L9/7eM//pEsXFjZWQAAAAAAAIBNJoBDp05Jjx7J296WzJlT6WkAAAAAAACATSSAQ5Jcd10yb17StWuyenWyYkWlJwIAAAAAAAA2kgAOL7+cTJuWdOtW+7XrrslFF9UeBwAAAAAAAJqN1pUeACpqxYra2H3++f86tnhxct55tc/POCPp2LEiowEAAAAAAAAbxw5wWrY2bZKJE9f92sSJta8DAAAAAAAAzYIATsu2eHHt1/peW7KkCYcBAAAAAAAANocATsvWpUvt1/pe2377JhwGAAAAAAAA2BwCOC1bdXUybty6Xxs3rvZ1AAAAAAAAoFloXekBoKI6dkwmTKh9PnFi7WXPu3Spjd8TJiTt2lVyOgAAAAAAAGAjCODQrl1yxhm1wfu555JddklqasRvAAAAAAAAaGZcAh2S2p3g8+YlH/5w0r9/0qFDpScCAAAAAAAANpId4FCnZ89k1qzk1VeTZ55JunWr9EQAAAAAAADARrADHOq0bZu84x21zx95pLKzAAAAAAAAABtNAIfX69ev9lEABwAAAAAAgGZHAIfX69+/9nHmzMrOAQAAAAAAAGw0ARxezw5wAAAAAAAAaLYEcHi91wfwmprKzgIAAAAAAABsFAEcXm/vvZNttkkWL06ee67S0wAAAAAAAAAbQQCH12vXLundu/a5+4ADAAAAAABAsyKAwxu5DzgAAAAAAAA0SwI4vJEADgAAAAAAAM2SAA5vJIADAAAAAABAsySAwxv171/7OHNmUlNT2VkAAAAAAACADSaAwxv16ZNUVSWLFiXPP1/paQAAAAAAAIANJIDDG7Vvn/TsWfvcZdABAAAAAACg2RDAYV3cBxwAAAAAAACaHQEc1qUugM+cWdk5AAAAAAAAgA0mgMO69O9f+2gHOAAAAAAAADQbAjisi0ugAwAAAAAAQLMjgMO67LNP7eMLL9R+AQAAAAAAAFs9ARzWpWPHpEeP2ud2gQMAAAAAAECzIIDD+rgPOAAAAAAAADQrAjisj/uAAwAAAAAAQLMigMP6COAAAAAAAADQrAjgsD51AXzmzMrOAQAAAAAAAGwQARzWp2/f2sd//jNZuLCyswAAAAAAAABvSQCH9enUKenevfb5rFmVnQUAAAAAAAB4SwI4vBn3AQcAAAAAAIBmQwCHNyOAAwAAAAAAQLMhgMOb6d+/9nHmzMrOAQAAAAAAALwlARzejB3gAAAAAAAA0GwI4PBm+vatfXz22WTx4oqOAgAAAAAAALw5ARzezPbbJ7vvXvt81qzKzgIAAAAAAAC8KQEc3or7gAMAAAAAAECzIIDDW3EfcAAAAAAAAGgWBHB4KwI4AAAAAAAANAsCOLwVARwAAAAAAACaBQEc3kpdAH/66WTp0srOAgAAAAAAAKyXAA5vZYcdkre/vfb5rFmVnQUAAAAAAABYLwEcNoTLoAMAAAAAAMBWTwCHDSGAAwAAAAAAwFZPAIcNURfAZ86s7BwAAAAAAADAegngsCH69699tAMcAAAAAAAAtloCOGyIuh3gTz2VLF9e2VkAAAAAAACAdRLAYUPstFOyyy61zx99tLKzAAAAAAAAAOskgMOGch9wAAAAAAAA2KoJ4LCh3AccAAAAAAAAtmoCOGyouh3gAjgAAAAAAABslQRw2FACOAAAAAAAAGzVBHDYUHUBfO7cZOXKys4CAAAAAAAArEUAhw21yy7J296W1NQkjz5a6WkAAAAAAACANxDAYWO4DDoAAAAAAABstQRw2BgCOAAAAAAAAGy1BHDYGAI4AAAAAAAAbLUEcNgY/fvXPs6cWdk5AAAAAAAAgLUI4LAx6naAz5mTrFpV2VkAAAAAAACABgRw2Bi77prssEPy2mvJ449XehoAAAAAAADgdQRw2BhVVe4DDgAAAAAAAFspARw2Vl0Adx9wAAAAAAAA2KoI4LCx+vevfbQDHAAAAAAAALYqAjhsLJdABwAAAAAAgK1S60oPUEm33357vvvd7+bJJ59Mx44ds99+++VLX/pS9thjj/pzTjrppNx3331rvXfq1Knp1atXU47L1qIugD/5ZPLKK8m221Z2HgAAAAAAACBJCw7g06dPz9ixY3PUUUdl/PjxWbx4cS677LKccsop+fWvf5127drVn7vvvvvmy1/+coP3d+vWralHZmvRtWvSuXOydGny+OPJwIGVnggAAAAAAABICw7gv/nNb9K1a9dceOGFqaqqSpLsuOOO+fjHP56HH344+++/f/25nTt3zuDBgys0KVudqqra+4Dfe2/tZdAFcAAAAAAAANgqtNh7gK9ZsyYdO3asj99J0qlTpyRJTU1NpcaiuXAfcAAAAAAAANjqtNgd4Mccc0xuuummXHfddTniiCOyePHiXHzxxenXr1/23XffBufed999GTx4cF599dW8853vzOc///m8613v2uTfXVNTk5UrV27uR+B1Vq1a1eCxsbXu3Tttk6yZMSOr/c8SKFxTr7EALY11FqDxWGMBGo81FqBxWWcbqqmpabCx+c1U1bTg7c533nln/vM//zMrVqxIkvTt2zdXX3113va2t9WfM3HixHTt2jU9evTI888/n2uuuSaPPfZYfvSjH2XIkCEb/TtnzJiR1atXb7HPQGV0/tOf8o5x47Jqr73yyM9/XulxAAAAAAAAoGht27bNwA24NXGLDeAPPPBAPv3pT+cjH/lI3ve+92Xx4sW56qqr0rp16/zkJz9Ju3bt1vm+lStX5kMf+lB69eqVyZMnb/TvnTFjRmpqatK7d+/N/Qi8zqpVqzJv3rz06NEj7du3b/TfV7VgQdr36ZOa1q2z6oUXkrZtG/13AlRKU6+xAC2NdRag8VhjARqPNRagcVlnG3ryySdTVVW1QQG8xV4C/YILLsiBBx6YM888s/7Y4MGD8773vS833XRT/u3f/m2d7+vQoUPe+9735tZbb93k311VVZUOHTps8vtZv/bt2zfNn+073pFst12qli9Ph2ef/dc9wQEK1mRrLEALZZ0FaDzWWIDGY40FaFzW2VobevnzJNmmEefYqs2ePTv77LNPg2O77bZbdthhh8yfP79CU9FsVFX9K3o/8khlZwEAAAAAAACStOAA3rVr1zzyhnD5zDPP5KWXXsruu+++3vetXLkyv//97zdoez2FqwvgM2dWdg4AAAAAAAAgSQu+BPrxxx+fCy+8MBdccEEOPvjgLF68OJMmTcpOO+2UUaNGJUnuv//+XH311fnABz6Q3XffPc8//3z+93//Ny+88EIuu+yyCn8CKq5//9pHO8ABAAAAAABgq9BiA/iYMWPStm3bXH/99bnxxhvTsWPHDB48OJdeeml22GGHJMnOO++c6urqXHLJJVm8eHHat2+fIUOG5Otf/3oGDRpU4U9AxbkEOgAAAAAAAGxVWmwAr6qqygknnJATTjhhvefsueeeueaaa5pwKpqVugD+2GPJmjVJ6xb7f04AAAAAAACwVWix9wCHzda9e9KhQ1JdnTz5ZKWnAQAAAAAAgBZPAIdNtc02LoMOAAAAAAAAWxEBHDaHAA4AAAAAAABbDQEcNocADgAAAAAAAFsNARw2hwAOAAAAAAAAWw0BHDZHXQB/9NHk1VcrOwsAAAAAAAC0cAI4bI4ePZL27ZNXXknmzKn0NAAAAAAAANCiCeCwOVq1SvbZp/a5y6ADAAAAAABARQngsLncBxwAAAAAAAC2CgI4bK66AD5zZmXnAAAAAAAAgBZOAIfN1b9/7aMd4AAAAAAAAFBRAjhsrrod4LNmJa++WtlZAAAAAAAAoAUTwGFz7bVXsu22ycsvJ089VelpAAAAAAAAoMUSwGFztW6d9OlT+9x9wAEAAAAAAKBiBHDYEtwHHAAAAAAAACpOAIctoe4+4AI4AAAAAAAAVIwADluCAA4AAAAAAAAVJ4DDlvD6AP7aa5WdBQAAAAAAAFooARy2hN69kzZtkpUrk/nzKz0NAAAAAAAAtEgCOGwJrVsnffrUPncZdAAAAAAAAKgIARy2FPcBBwAAAAAAgIoSwGFL6dcvedvbkiVLKj0JAAAAAAAAtEgCOGwpY8Yk8+Yln/xksnp1smJFpScCAAAAAACAFkUAhy3h5ZeTH/wg6dYt6dEj2XXX5KKLao8DAAAAAAAATaJ1pQeAZm/FitrYfd55/zq2ePG/vj/jjKRjx4qMBgAAAAAAAC2JHeCwudq0SSZOXPdrEyfWvg4AAAAAAAA0OgEcNtfixbVf63ttyZImHAYAAAAAAABaLgEcNleXLrVf63tt++2bcBgAAAAAAABouQRw2FzV1cm4cet+bdy42tcBAAAAAACARte60gNAs9exYzJhQu3ziRNrL3vepUtt/J4wIWnXrpLTAQAAAAAAQIshgMOW0K5dcsYZydlnJ88+m+y8c7JmjfgNAAAAAAAATcgl0GFL6dgxads2+cQnkh49ksceq/REAAAAAAAA0KII4NAYXnxRAAcAAAAAAIAmJoDDltanT+3jo49Wdg4AAAAAAABoYQRw2NL22af20Q5wAAAAAAAAaFICOGxpdoADAAAAAABARQjgsKXV7QB/4onk1VcrOwsAAAAAAAC0IAI4bGnduyfbbpusXp3Mm1fpaQAAAAAAAKDFEMBhS2vVKtl779rnLoMOAAAAAAAATUYAh8ZQdxn0xx6r7BwAAAAAAADQggjg0Bj69Kl9tAMcAAAAAAAAmowADo3BDnAAAAAAAABocgI4NAY7wAEAAAAAAKDJCeDQGOoC+PPPJy+9VNlZAAAAAAAAoIUQwKExdOqUdO1a+9xl0AEAAAAAAKBJCODQWOruA+4y6AAAAAAAANAkBHBoLHUB3A5wAAAAAAAAaBICODSWuvuA2wEOAAAAAAAATUIAh8ZiBzgAAAAAAAA0KQEcGkvdDvAnn0zWrKnsLAAAAAAAANACCODQWPbYI2nfPqmuTubOrfQ0AAAAAAAAUDwBHBrLNtu4DzgAAAAAAAA0IQEcGpMADgAAAAAAAE1GAIfGtM8+tY+PPVbZOQAAAAAAAKAFEMChMdkBDgAAAAAAAE1GAIfGZAc4AAAAAAAANBkBHBrT3nvXPr74YrJwYWVnAQAAAAAAgMIJ4NCYOnZM9tij9rld4AAAAAAAANCoBHBobHWXQXcfcAAAAAAAAGhUAjg0tj59ah8FcAAAAAAAAGhUAjg0trod4C6BDgAAAAAAAI1KAIfGZgc4AAAAAAAANAkBHBpb3Q7wOXOS6urKzgIAAAAAAAAFE8Chse2+e9KxY7JmTTJ7dqWnAQAAAAAAgGIJ4NDYqqpcBh0AAAAAAACagAAOTaHuMuiPPVbZOQAAAAAAAKBgAjg0BTvAAQAAAAAAoNEJ4NAU7AAHAAAAAACARieAQ1N4/Q7wmprKzgIAAAAAAACFEsChKbzjHUlVVfLSS8mLL1Z6GgAAAAAAACiSAA5NoUOHZM89a5+7DzjA/8/enYfnVZV7A/4lbUpKKAnQ0oGpzC2DgsNxngfkIE6I41FUHFAhimihIvVYkGJxTEE5ICgHED2OqCgiznhwws8BbEGGIoO0FEkpaUPTNt8f64S0kM5Jdt7kvq9rX2u/e79ZeVJ1Xya/91kLAAAAAAAGhAAcBsvay6ADAAAAAAAA/U4ADoNl2rQy3nRTtXUAAAAAAADAMCUAh8GiAxwAAAAAAAAGlAAcBosOcAAAAAAAABhQAnAYLD0d4Lfdljz8cLW1AAAAAAAAwDAkAIfBMnlyMm5csmZNcuutVVcDAAAAAAAAw44AHAZLXV3vMuj2AQcAAAAAAIB+JwCHwdSzDLoAHAAAAAAAAPqdABwGU08H+E03VVsHAAAAAAAADEMCcBhMOsABAAAAAABgwAjAYTCt3QHe3V1tLQAAAAAAADDMCMBhMO2zT1JXlyxdmixaVHU1AAAAAAAAMKwIwGEwNTYme+5Zzu0DDgAAAAAAAP1KAA6DrWcZdPuAAwAAAAAAQL8SgMNg23//MgrAAQAAAAAAoF8JwGGw9XSAWwIdAAAAAAAA+pUAHAabDnAAAAAAAAAYEAJwGGw9HeALFyadnZWWAgAAAAAAAMOJABwG2847J83NSXd38ve/V10NAAAAAAAADBsCcBhsdXX2AQcAAAAAAIABIACHKvQE4PYBBwAAAAAAgH4jAIcq7L9/GQXgAAAAAAAA0G8E4FAFS6ADAAAAAABAvxOAQxXW7gDv7q62FgAAAAAAABgmBOBQhb33TkaNSh56KPnnP6uuBgAAAAAAAIYFAThUYZttkr32Kuf2AQcAAAAAAIB+IQCHqvQsg24fcAAAAAAAAOgXAnCoyrRpZdQBDgAAAAAAAP1CAA5V6ekAF4ADAAAAAABAvxCAQ1V6OsAtgQ4AAAAAAAD9QgAOVenpAL/jjmT58mprAQAAAAAAgGFAAA5VGT8+2XHHcv73v1dbCwAAAAAAAAwDAnCoSl1d7zLo9gEHAAAAAACArSYAhyr1LIMuAAcAAAAAAICtJgCHKvV0gN90U7V1AAAAAAAAwDAgAIcq6QAHAAAAAACAfiMAhyqt3QG+Zk21tQAAAAAAAECNE4BDlfbaKxk9Olm+PLn77qqrAQAAAAAAgJomAIcqNTQke+9dzu0DDgAAAAAAAFtFAA5V61kG3T7gAAAAAAAAsFUE4FC1/fcvowAcAAAAAAAAtooAHKrW0wFuCXQAAAAAAADYKgJwqJoOcAAAAAAAAOgXAnCoWk8AftddyUMPVVsLAAAAAAAA1LARHYD/5Cc/ydFHH51DDz00z3zmM/O+970vd95552Pe9/Wvfz2HHXZYDj744LzsZS/Lz372swqqZdjaaadkwoRyfvPN1dYCAAAAAAAANWzEBuC//e1vc/zxx2efffbJueeemw9/+MNZsGBB3va2t6Wzs/OR91155ZU57bTTcvjhh+eCCy7IIYcckuOPPz5/+tOfqiue4aenC9w+4AAAAAAAALDFRlddQFWuvPLKTJkyJWeeeWbq6uqSJDvuuGOOOeaY3HDDDXnSk56UJGlra8sRRxyR97///UmSpz71qbn55ptz7rnn5oILLqiqfIabadOSa6+1DzgAAAAAAABshRHbAb5q1ao0NTU9En4nybhx45Ik3d3dSZI777wzCxcuzOGHH77O1/77v/97rrvuuqxcuXLwCmZ46+kAF4ADAAAAAADAFhuxHeCvetWrcsUVV+Syyy7Ly172srS3t+fTn/50DjjggDzhCU9Iktx2221Jkj333HOdr917773T1dWVO++8M3vvvfdmf+/u7u4sX758638IHrFixYp1xlpTv+eeaUyyZv78dPrvBjDE1PozFmCo85wFGDiesQADxzMWYGB5zq6ru7t7ncbmDRmxAfiTnvSknHPOOTnppJMye/bsJMn06dPzxS9+MaNGjUqSLF26NEmy/fbbr/O1Pa977m+urq6uzJ8/f0tLZwMWLlxYdQlbZJv6+hyUJDffnPk33pjUj9jFGYAhrFafsQC1wnMWYOB4xgIMHM9YgIHlOdtrzJgxm/S+ERuA//GPf8yMGTPymte8Js997nPT3t6ez3/+83nnO9+Zr3zlK2lsbByw793Q0JB99tlnwOYfiVasWJGFCxdm6tSpGTt2bNXlbL599013Q0PqH344B4wbl+7dd6+6IoBH1PwzFmCI85wFGDiesQADxzMWYGB5zq7rlltu2eT3jtgA/IwzzshTn/rUnHLKKY9cO+SQQ/Lc5z43V1xxRV772temubk5SbJs2bJMmDDhkfc9+OCDSfLI/c1VV1eXbbfddiuqZ33Gjh1bu/+2++6b/O1vGXvHHcm0aVVXA/AYNf2MBagBnrMAA8czFmDgeMYCDCzP2WJTlz9PkhG7zvKtt96aaY8KGSdNmpQddtgh//jHP5Ike+21V5LevcB73HbbbWloaMhuu+02OMUyMuy/fxlvuqnaOgAAAAAAAKBGjdgAfMqUKfnb3/62zrW77747DzzwQHbZZZckyW677ZapU6fmqquuWud9P/jBD/K0pz1tk9eZh03S84GMBQuqrQMAAAAAAABq1IhdAv11r3tdzjzzzJxxxhl5/vOfn/b29nzhC1/ITjvtlMMPP/yR951wwgn54Ac/mN133z1PecpT8oMf/CB/+ctfcumll1ZYPcNSTwe4ABwAAAAAAAC2yIgNwN/85jdnzJgxufzyy/PNb34zTU1NOeSQQ/LZz342O+ywwyPve+lLX5oVK1bkggsuyPnnn58999wz55xzTg499NAKq2dY6ukAtwQ6AAAAAAAAbJERG4DX1dXl9a9/fV7/+tdv9L1HH310jj766EGoihGtpwP8nnuSBx9Mtt++2noAAAAAAACgxozYPcBhyGlpSSZOLOc331xpKQAAAAAAAFCLBOAwlPQsg24fcAAAAAAAANhsAnAYSnqWQbcPOAAAAAAAAGw2ATgMJTrAAQAAAAAAYIsJwGEo6ekAF4ADAAAAAADAZhOAw1DS0wH+wAPJ6tXV1gIAAAAAAAA1RgAOQ8keeyTf/W7ZA3zJkmTlyqSjo+qqAAAAAAAAoCYIwGEo6epKfve7ZNddk0mTkokTk7lzk87OqisDAAAAAACAIW901QUA/6ejo4TdZ5zRe629PZk9u5zPmJE0NVVSGgAAAAAAANQCHeAwVDQ0JG1tfd9rayv3AQAAAAAAgPUSgMNQ0d5ejvXdW7p0EIsBAAAAAACA2iMAh6GipaUc67vX3DyIxQAAAAAAAEDtEYDDUNHVlbS29n2vtbXcBwAAAAAAANZrdNUFAP+nqSmZObOct7WVZc9bWkr4PXNm0thYZXUAAAAAAAAw5AnAYShpbExmzEhOPjlZvDiZNClZvVr4DQAAAAAAAJvAEugw1DQ1JWeemRx5ZPLxj5fXAAAAAAAAwEbpAIehaPfdkxtuSHbdtepKAAAAAAAAoGboAIehaPr0Ms6fX20dAAAAAAAAUEME4DAU9QTgd9yRdHRUWwsAAAAAAADUCAE4DEXjxyc77VTOb7652loAAAAAAACgRgjAYaiyDDoAAAAAAABsFgE4DFUCcAAAAAAAANgsAnAYqgTgAAAAAAAAsFkE4DBUCcABAAAAAABgswjAYajqCcD//vdk1apqawEAAAAAAIAaIACHoWq33ZJtt026upJbb626GgAAAAAAABjyBOAwVNXXJ/vvX84XLKi2FgAAAAAAAKgBAnAYyuwDDgAAAAAAAJtMAA5DmQAcAAAAAAAANpkAHIYyATgAAAAAAABsMgE4DGU9AfiCBUl3d7W1AAAAAAAAwBAnAIehbJ99klGjkmXLkrvvrroaAAAAAAAAGNIE4DCUjRlTQvDEMugAAAAAAACwEQJwGOqmTSvjggXV1gEAAAAAAABDnAAchrqefcB1gAMAAAAAAMAGCcBhqBOAAwAAAAAAwCYRgMNQJwAHAAAAAACATSIAh6GuZw/wRYuSBx6othYAAAAAAAAYwgTgMNSNG5fsums51wUOAAAAAAAA6yUAh1rQswz6ggXV1gEAAAAAAABDmAAcakHPMug6wAEAAAAAAGC9BOBQC3o6wAXgAAAAAAAAsF4CcKgFAnAAAAAAAADYKAE41IKeAPz225MVK6qtBQAAAAAAAIYoATjUgp13TnbYIenuTm6+uepqAAAAAAAAYEgSgEMtqKuzDDoAAAAAAABshAAcakVPAL5gQbV1AAAAAAAAwBAlAIdaMW1aGXWAAwAAAAAAQJ8E4FArLIEOAAAAAAAAGyQAh1rRE4DffHOyenW1tQAAAAAAAMAQJACHWrHHHkljY/Lww8ntt1ddDQAAAAAAAAw5AnCoFaNGJfvvX84tgw4AAAAAAACPIQCHWmIfcAAAAAAAAFgvATjUkp4AfMGCausAAAAAAACAIUgADrVk2rQy6gAHAAAAAACAxxCAQy1Zewn07u5qawEAAAAAAIAhRgAOtWS//ZL6+mTp0uTee6uuBgAAAAAAAIYUATjUkm22Sfbaq5xbBh0AAAAAAADWIQCHWrP2MugAAAAAAADAIwTgUGsE4AAAAAAAANAnATjUmp4AfMGCausAAAAAAACAIUYADrVGBzgAAAAAAAD0SQAOtWbatDLec0+ydGm1tQAAAAAAAMAQIgCHWtPcnEyeXM4tgw4AAAAAAACPEIBDLbIMOgAAAAAAADyGABxqkQAcAAAAAAAAHkMADrVIAA4AAAAAAACPIQCHWtQTgNsDHAAAAAAAAB4hAIda1BOA33pr8vDD1dYCAAAAAAAAQ4QAHGrRpEnJ9tsna9Ykf/971dUAAAAAAADAkCAAh1pUV2cfcAAAAAAAAHgUATjUKgE4AAAAAAAArEMADrVKAA4AAAAAAADrEIBDrRKAAwAAAAAAwDoE4FCregLwm25K1qypthYAAAAAAAAYAgTgUKv23DMZMybp7EzuuKPqagAAAAAAAKByAnCoVaNGJfvtV84tgw4AAAAAAAACcKhp9gEHAAAAAACARwjAoZYJwAEAAAAAAOARAnCoZQJwAAAAAAAAeIQAHGrZ2gF4d3e1tQAAAAAAAEDFBOBQy/bbL6mrSx54ILnvvqqrAQAAAAAAgEoJwKGWjR2b7LlnObcMOgAAAAAAACOcABxq3bRpZRSAAwAAAAAAMMIJwKHWrb0POAAAAAAAAIxgAnCodQJwAAAAAAAASCIAh9onAAcAAAAAAIAkAnCofT0B+F13JcuWVVsLAAAAAAAAVEgADrVuhx2SiRPL+U03VVsLAAAAAADAYOvoSFauTBYvLmNHRzVz9Oc8bDEBOAwHlkEHAAAAAABGos7OZO7c0izYc8ydW64P5hz9OQ9bZXTVBQD9YNq05Oc/F4ADAAAAAAAjR0dHCZhnz+691t7e+3rGjKSpaeDn6M952GoCcBgOdIADAAAAAAAjTUND0tbW9722tuSUU5L3vS9Ztqxc6+5e9z3jxiWf+MTG5zj++N451tYz37hxydlnb3ieU0/d+M9DvxCAw3AgAAcAAAAAAEaaBx4oXdZ9aW9PFi1KfvrT5IYb+n7PQQcl99678Tl+8Yv1z9Ezz6JFG55n6dJkwoT1z0G/EYDDcNATgN9yS7JyZTJmTLX1AAAAAAAADJTbbks+//nkYx9LWlr6Dp5bWsoe3O94R7JiRe/1urre87Fjk0mTNj7Hu961/n286+o2bZ7m5k360dh6AnAYDnbZpSyvsWxZcuutvYE4AAAAAADAcHHjjclZZyWXX56sXp0861nJCSckp5/+2Pe2tiZr1pRxQzo6ynvW3rv70XMcf/zGa9vYPF1dGhgHiQAchoO6umTatOT3vy/LoAvAAQAAAACA4eL3v0/OPDP5znd6r73kJaVB8LDDSk7S1la6r1taSuA8c2bS2LjxuZuaynuTLZ+jP+dhqwnAYbiYPr03AAcAAAAAAKhl3d1l7+0zz0x+/ONyra4uedWrkg9/OHnCE3rfO2NGcuqpZZ/t5ubSbb05gXNj49bP0Z/zsFUE4DBcTJtWRgE4AAAAAABQKzo6koaG3o7prq7S8PfhDyfXXVfeM2pU8h//kZx8ct+r4DY1lXHChDJuyVLj/TFHf87DFquvugCgn/Q88AXgAAAAAABALejsTObOTSZO7D3OOis58MDkgQeSbbZJ3vOe5JZbki9/2RawbBId4DBc9Dz0FyxI1qxJ6n2+BQAAAAAAGKI6Okr4PXt277X29uSMM8r55ZcnkyaVAzaDhAyGi733LkuELF+e3Hln1dUAAAAAAACsX0ND0tbW971zzkkOOED4zRYRgMNwMXp0su++5XzBgmprAQAAAAAA2JD29nKs797SpYNYDMOJAByGE/uAAwAAAAAAtaClpRzru9fcPIjFMJwIwGE4EYADAAAAAAC14KabkuOP7/tea2vS1TW49TBsjK66AKAfTZtWRgE4AAAAAAAwVF1/ffKWtyQ//WlSX1/2Am9vL53fra3JzJlJY2PFRVKrBOAwnOgABwAAAAAAhrLOzuSYY5Ibb0w+/vFynHpq2fO7ubl0fgu/2QqWQIfhZP/9y7hkSTkAAAAAAACGklmzSvi9887JRz6SNDUlY8YkEyaUsamp6gqpcQJwGE6ampI99ijnusABAAAAAICh5Nprk09+spxfcEEyfny19TAsCcBhuOlZBn3BgmrrAAAAAAAA6PHQQ2Xp8+7usv/3y15WdUUMUwJwGG7sAw4AAAAAAAw1M2Ykt92W7L578tnPVl0Nw5gAHIYbATgAAAAAADCUXH118oUvlPMvfSlpbq62HoY1ATgMNwJwAAAAAABgqHjggeRtbyvnJ5yQPP/51dbDsCcAh+Fm2rQy3nFH0tFRbS0AAAAAAMDI1tqa3H13su++yVlnVV0NI4AAHIab8eN7jzvuqLoaAAAAAABgpPrWt5JLL03q65P//u9k222rrogRQAAOw9E3vpEsXJi0tCQrV+oEBwAAAAAABtfixclxx5Xzk09OnvrUauthxBhddQFVedOb3pTf/e53fd779Kc/nSOOOGK97/nBD36Qvffee6BLhC3T2Zn85CfJK16RtLeXELy1NZk5M2lsrLg4AAAAAABg2OvuTt71ruS++5LHPS756EerrogRZMQG4B/96Efz0EMPrXPt4osvztVXX52nPe1pj1x7whOekJNPPnmd9+26666DUiNsto6OZO7c5PTTe6+1tyezZ5fzGTOSpqZKSgMAAAAAAEaISy5JvvOdpKGhLH2+zTZVV8QIMmID8H322ecx10466aQ84xnPyI477vjIte233z6HHHLIIFYGW6GhIWlr6/teW1ty6qmDWw8AAAAAADCy3HlnWZk2Sf7zP5PHP77Schh57AH+f/74xz/mrrvuypFHHll1KbDl2tvLsb57S5cOYjEAAAAAAMCI0t2dHHtsySOe+tSyMi0MshHbAf5o3//+97PtttvmBS94wTrXf/e73+WQQw7J6tWr8/jHPz7ve9/78uQnP3mrvld3d3eWL1++VXOwrhUrVqwzjlRjW1pS19LSdwje0pLu5uas8N89YDN5xgIMLM9ZgIHjGQswcDxjgb6MPv/8jPnxj9M9dmw6zzsv3StXJitXVl1WTfKcXVd3d3fq6uo26b113d3d3QNcz5C3atWqPOtZz8rTn/70fOpTn3rkeltbW6ZMmZKpU6dm8eLFufDCC3PTTTflkksuyaGHHrpF3+uvf/1rVvofOgNk3ylTMu6881LXs+f3WrpnzcqDxx2XW+65p4LKAAAAAACA4WybO+/M9Ne/PqM6O/OPD34w973udVWXxDAzZsyYHHzwwRt9nwA8yS9+8Yu8853vzHnnnZfnPe95633f8uXL89KXvjR77713Lrjggi36Xn/961/T3d3d5x7kbLkVK1Zk4cKFmTp1asaOHVt1OZWpq6tLY5KcdVbq2tpKJ3hLS7pbW5NTTklnyidkADaHZyzAwPKcBRg4nrEAA8czFuhRV1eX7tWrs82LXpRRv/lNVj/72Xn4yiuTejsxbw3P2XXdcsstqaur26QA3BLoKcuft7S05JnPfOYG37ftttvmOc95Tn70ox9t1ferq6vLtttuu1Vz0LexY8f6t03KnhqnnJIsWpTsvHPquruTsWPj8QhsDc9YgIHlOQswcDxjAQaOZyyMYB0dSUNDacYbNy45+eTkjDMy6r//O9tut13V1Q0bnrPFpi5/niQj/qMXnZ2dueaaa/KSl7wkDQ0NVZcD/aOpKfn1r5Mjj0wOP7y8BgAAAAAA6A+dncncucnEieWYMiW5/vrkZz9L9tij6uoY4UZ8B/hPf/rTLF++PEceeeRG37t8+fL8/Oc/36TWeqjcPvskN9yQjBmTrFqVjB7x/3MHAAAAAIBqrd013dKSdHVtfhNb1XN0dJTwe/bs3mvt7ckZZ5Rlz2fM0JhHpUZ8B/j3vve9TJkyJU984hPXuf6HP/whxx13XL75zW/mN7/5Tb773e/mjW98Y+677768973vraha2Ay7755su22ycmVy661VVwMAAAAAALWro6P8vX3x4jJ2dGz+HI/ump44sbzu7By6c6xalfzjH8mvfpVcemnymc+UkLutre+529pKsA4VGtEtoUuXLs2vfvWrHHPMMY9ZN37ChAnp6urKZz7zmbS3t2fs2LE59NBD87GPfSyPe9zjKqoYNkN9fXLAAckf/pD87W/J/vtXXREAAAAAANSensC4ra23Y7q1NZk5M2ls3LQ51tc13fN6U7qmB2OOV70q+dSnkjvuKMdddyWrV/e+96CDkle8onxNX9rbk6VLkwkTNlwHDKARHYA3Nzfnhhtu6PPeHnvskQsvvHCQK4J+tnYA/spXVl0NAAAAAAADrerlsYdiLQOx3HfP6w99KFm+PLn//vUfXV3J5z+/4a7pGTOSXXct70+SnsbNurpyjB+f3HjjxueYOjW5775yrbu7d+zuLnPcdNPG5/jhD5MlS3qvNzQku+1W9vY+6KBk0qTy79hXCN7SkjQ39z0/DJIRvwQ6DGsHHFDGG2+stg4AAAAAADZua5fZHipLbA+lWjZ3jhUrypLff/hDcs01yahRGw6M6+qSAw8sf49/1rNKd/Sxx5Yg+ROfSL74xeT665NFizbcNX3ffckOO5S6OjtLHStWlHC9oyMZN67892Jjc4wbV75m+fLeOTo7k4cfLvNvbI4HHkjOOiv5yleSX/+6dICvWFG2Wv3pT8vPvGZN6YDvS2trCfyhQiO6AxyGvQMPLOPf/lZtHQAAAAAAbNjWLrO9sU7l448vXb2rVvUeq1ev+3q//Upge/rpfc/xlrckf/5z2YKzpzO5ru6xr6dPTy64YP3zvOMdycKF635Nz5GUcbfdkvPOW//Pc9xxJZxN1u10Xnvcfffkv/6r7zm6u5Mjjyw13ndfCYYXL04eeqj3vQcdlHz3uxsPnSdNKv9+O+302GP8+GSXXZLJkzfcNT15cum8XrPmsZ3bPf8mmzLH975Xvmbtf8uecdSoUs+G5th55xLgb0hTU/nvZbJ1y8LDAKnr7u75Xw6D4a9//WuS5OCDD664kuFl+fLlmT9/fqZPn55tt9226nKGjttuS/beO9lmm/J/fkaNqroioAZ5xgIMLM9ZgIHjGQswcDxj+1lf4XWP005LXvOa5NJLSzi7bFkZe45ly5IxY5Jf/KIsob2+YPOuu8ry2Gsvbb228eNLKL01c/TXPFXPMWZM2cN6v/2S73+/BNjrm+Pee8vf3kdvpOd0Q/8Zz5q15ft3VzHH2nM1NJQ9v5ubt3ypfPrkObuuzclYdYDDcLbHHsnYsWV5kttvT/bZp+qKAAAAAAB4tIaG9S+zPW9ecvLJyYUXrj/sPeigTVsee//9S7f26NG9oW3PMW1amX9Dc9x/f3LYYcktt/R2KvccPa/33nvj8yxZkvzbv5X9qJN15+nuLnXed9/G5zj00DJHX93OmzJHe3vZm3v06NL53HNsv33vXB0dpbO5r8C4tbV0fm+zTd/fY2390TU9VOZYe66kfFggKR8cgCFAAA7D2ahR5f+0/L//V/YBF4ADAAAAAAw9PWHs+u498EDyoQ+Vbu9x45Lttus9xo0rAeaUKRte2nrKlOTaazdcx8qVG55j0qTSib4xG5tn8uTkyiu3fo6rr966OSZMSI4+esNz9Gdg3NhYOqxPPXXdrulanAOGsPqqCwAGmH3AAQAAAACGrgcfLEF2S0vf93v2ZZ4xo+xXPWNG8p73JG9+c/KqVyUvelHy5CeXTuTW1r7naG0tAefGdHVt/Rz9Nc9QmSPpDYwXLSqd9osWlddbEhg3NfUusT5mzJYtGT5U5oAhSgAOw90BB5RRAA4AAAAAMLTceWfyjGckP/5xcvzxfb9nU4Pank7lWbN6w/SWlvJ65sxNCzj7Y46hVEt//Tw9cwmMoSZYAh2Gu54A/MYbq60DAAAAAIBef/lL8u//ntx9d/KpTyU//GHZn3trltkeSstjD5VaLPcNI44AHIa7ngB8/vxk9eqyLzgAAAAAANW55pqyfPmyZcn06ckll5SO4v4Ians6kydMKOOYMZtfX3/MMZRq6a+fB6gJlkCH4W6vvZJttkk6O5M77qi6GgAAAACAke3ii5PDDy/h93Oek/z618kee5R7ltkG2GoCcBjuRo1Kpk0r5/YBBwAAAACoRnd3cvrpyVvekqxalbz+9cmPfpTssEPVlQEMKwJwGAnsAw4AAAAAUJ2uruSd70xmzSqvTz45ufTSsnonAP3KHuAwEvQE4DrAAQAAAAAG10MPJUcfnVx1VVJfn8ybl7znPVVXBTBsCcBhJDjwwDIKwAEAAAAABs+99yZHHJH88Y/J2LHJV7+avOxlVVcFMKwJwGEkWLsDfM2a8ilDAAAAAAAGzvz5yeGHJ3fckUyYkHzve8lTnlJ1VQDDnhQMRoK9904aGpLly5N//KPqagAAAAAAhpeOjmTlymTx4jLedVfy1reW8HuffZLrrhN+AwwSATiMBKNHJ/vvX84tgw4AAAAADBePDp47OgZ/js7OZO7cZOLE3uO//qt0fL/2tcn//m9pUgJgUAjAYaSwDzgAAAAAMJz0FTzPnVuuD9YcHR3JnDnJ7NlJe3u51t6enHFGMm9ecv75ZflzAAaNPcBhpOjZB/zGG6utAwAAAABga3V0lKB69uzea+3tva8/8IGku7sca9aU49HnjY1JW9v653jTm5KrrkoefLAcy5b1nj/4YDJqVHLFFWWOvsybl3zkIwPx0wOwAQJwGCl6AnAd4AAAAABArWtoWH/w3NaWzJiRTJ2aLFnS93vGj08WLtz4HB/72PrnOOigsmx6T+f3o7W3J0uX6gAHGGQCcBgp1l4Cvbs7qaurth4AAAAAgM3V3p786EfJU5+64eD5vvuSSZMeG17X1ZVj1103Hl7/61/Jm99czrffft1j3LgSok+ZkrS09D1PS0vS3LxFPyYAW04ADiPFPvsko0cnDz2U3HVXsttuVVcEAAAAANSwxsbGwflGa9YkP/tZctFFybe+lWy3Xene3lDwPGVKcv31Jeyury/Ho5uCVq7c8BwTJyaf+tSGa+voSFpb111GvUdra9LVlYwZswk/JAD9pb7qAoBB0tCQ7LdfObcPOAAAAACwpTo6Mnb06EzbaaeMHT26hMAD4Y47yhLke+2VvPCFyVe+knR2JpMnJ//4RwmY+7J28NzQUPbq7mtFzK6ujc+xMU1NycyZyaxZJTRPyjhrVrne1LQpPykA/UgADiOJfcABAAAAgK3R2ZnMnZu6iRMzavLk1E2cmMydW65vro6O0oW9eHEZOzrKPF/5SvKiFyV77pn853+WILy5OXn3u5Pf/z7585+T6dO3Pnjur/C6sbHsF75oUflZFi0qrwerQx6AdQjAYSRZex9wAAAAAIDN0dGRzJlTlvvuWTa8vb28njNn8zrB/y9Iz8SJvccnPpEsW5acfnpyzTVJd3dv5/c//5l8/vPJk57U283dH8Fzf4XXTU2l43zChDLq/AaojD3AYSTp6QC3BDoAAAAAsLkaGpK2tr7vtbUlp5ySnHpqCbdHjVr/8bKXJV/9agm6e7S3l9fd3clnPpP89rfJMcckU6duuKaeoHnChDJuyX7b/TEHAEOGABxGkrWXQO/u7nvfGwAAAACAvrS393Z+93Vv0aLku99Nbrhh/XOMH5+ccEIyb17f9885JznttOQlL9nKYgEYqQTgMJLsu2/5hOWDDyb33JPsskvVFQEAAAAAtaKlpRx9heAtLWUZ89e+Njn88GT16r6PyZOTf/1rw0H60qW93dgAsJkE4DCSbLNNCcEXLChd4AJwAAAAAGBTPfxw6d5ee+nyHq2tyZo1yUc+svF5Vq7ccJDe3LyVhQIwktVXXQAwyOwDDgAAAABsru7u5MwzSwD+kY+UoDop46xZycyZvXtpb0xXVwnM+9LaWu4DwBYSgMNIs/Y+4AAAAAAAm2LOnOSss5LnPS95xzvSvWhRVt97b7oXLUpmzEgaGzd9rqamEpjPmrV1QToA9EEADiONABwAAAAA2Bz/8z/JqaeW8/e+N9l996xYtSoLlizJilWrtiywbmwswfmiRcnixWXc3CAdAPpgD3AYaQ48sIx/+1tZtqiurtp6AAAAAICh6ze/Sd785nJ+4onJu9/9yK3Ozs6tm7snOJ8woYxjxmzdfAAQHeAw8uy3X1JfnzzwQHLvvVVXAwAAAAAMVbffnrzsZcnDDydHHpmcfXbVFQHARgnAYaRpbEz23rucWwYdAAAAAOhLe3vy0pcm992XHHpo8pWvJKNGVV0VAGyUABxGIvuAAwAAAADr09WVvOY15e+HU6Yk3/test12VVcFAJtEAA4jUc8+4DfeWG0dAAAAAMDQ0t2dnHBC8uMflz26v//9ZJddqq4KADaZABxGIh3gAAAAAEBfPv3p5L/+K6mrSy6/vCx/DgA1RAAOI1FPAH7jjeUTnQAAAAAA3/lO8qEPlfNPfzo58shKywGALSEAh5Fo2rTyCc5//Su5776qqwEAAAAAqnb99ckb31gaZt7znuR976u6IgDYIgJwGInGjk322quc2wccAAAAAEa2O+8s3d7LlycveUnyuc+VBhoAqEECcBip7AMOAAAAACxbVsLvf/4zOeig5GtfS0aPrroqANhiAnAYqQTgAAAAADCyrVqVvP71yZ//nEycmHz/+8n221ddFQBsFQE4jFQHHlhGATgAAAAAjBwdHcnKlcnixUlXV/L2tyeHHJJ897vJHntUXR0AbDUBOIxUPR3g9gAHAAAAgNqwdni9cmV5vTk6O5O5c0u398SJyZQpyfXXJ7/8ZfJv/zYwNQPAIBOAw0g1bVoZ77uvHAAAAADAwOnv8HrixPK6s3PjX7tmTdLensyZk8yeXc6TMp5xRvLJT25+PQAwRI2uugCgIk1NydSpycKFyfz5yYQJVVcEAAAAAL06OpKGhhLStrSU5bqbmmpzjp7wuq2td57W1mTmzKSxcdNqmDu3hNc92tt7Xx9zTHL11SVcv+++x45Jcttt5fv3pa0tOfXUzfuZAGCIEoDDSHbggSUAv/HG5NnPrroaAAAAAIaDoRAYD6U5NhZev//9yaJFyb/+lTzwwGPHrq7Sob2h8HrGjOS005IlS/p+z0EHlTC8p/P70drbk6VLNckAMCwIwGEkO+CA5Mork7/9repKAAAAABgOBiMwPvHE5OGHk1Wreo/Vq9d9PXlyct5565/jHe9I7rgjqatL6uv7HqdMST7/+fXP8da3Jr/9bbJiRTk6O3vPe16PGlWWHd9YeP2sZ204vF60aMPh9f33J294Q/m+O+9cguy1x4kTkx12KP959DVPS0vS3Nz3/ABQYwTgMJIdcEAZBeAAAAAAbK2NBddvf3vyxz+WTuMHHyzj2ucPPliC5699beOB8b77rj8wHj++rHq4sTkOPXTr5zj++PXPkWxaeH3ffcl++yXbb19C6h12SHbcsXecMqUE+hsKrydNSj73ufXXkZT/fFpb1/3Pp0dra+k0HzNmw3MAQA0QgMNIJgAHAAAAoL80NGw8MH772zceGG9sqe777iuBb3t7Mnp07zFqVBkPOqi8Z0NzLFmS/Nu/JTfdlKxZk3R3l6PnfL/9Nj7H/fcnRx6Z3HtvMnZs6XAfO3bd8x13LLVuKLyeMiX59a/X/2+S9E943dRUOvGTrevQB4AhTgAOI9n06WW8996yp9COO1ZbDwAAAAC1q71946HzYYeVYLm5uXQ8P3qcMGHj3c5TpiR/+UvpFl+flSs3PMfkyWVrwA3Z2ByTJiUXXbThOZKhFV43NpYPIpx6aum6b24u31/4DcAwIgCHkWzcuGT33ZN//KN0gT/zmVVXBAAAAECtamnZeOh86aUbn6c/AuOurqExRzL0wuumpjJOmFBGy54DMMzUV10AUDHLoAMAAADQH269teyJ3ZeewHhT9ATGs2aVoDgp46xZ5XpPgFsLc/ToCa8XLSpLvC9aVF5vSXg9ZkwJr8eM2bwaAGCE0AEOI92BByZXXSUABwAAAGDLffazyX/9V/LLX5alyefN27o9pvuj23mozNFD5zUADAoBOIx0PR3gN95YbR0AAAAA1KZ585ITTyznl1+enHxy8pGPDI3AeKjMAQAMGgE4jHSWQAcAAABgS517bunwTkqX9wknlA7wRGAMAFTCHuAw0k2fXsZ77inLUgEAAADApjjvvN49v08+Ofn4x3vDbwCAigjAYaRrbk523bWc6wIHAAAAYFOcf37y7neX8w9+MJkzR/gNAAwJAnDAMugAAAAAbLoLL0ze9a5y/oEPJHPnCr8BgCFDAA4IwAEAAADYNF/6UvKOd5Tz970v+eQnhd8AwJAiAAcE4AAAAABs3MUXJ8cem3R3JyeckHzmM8JvAGDIEYADyYEHlvHGG6utAwAAAICh6dJLk7e+tYTf73lP8rnPCb8BgCFJAA4k06eX8a67kgcfrLYWAAAAAIaWr3wlOeaYEn4fd1xyzjnCbwBgyBKAA8kOOySTJ5fz+fOrrQUAAACA6nR0JCtXJosXl/HWW5OPfzxZsyZ55zuTc88VfgMAQ5oAHCjsAw4AAAAwsnV2JnPnJhMn9h5f/nLy858nH/lI8oUvJPX+pAwADG2jqy4AGCIOPDD5yU/sAw4AAAAwEnV0lPB79uzea+3tyRlnlI7vU04RfgMANcH/YwEKHeAAAAAAI1dDQ9LW1ve9efPKfQCAGiAABwoBOAAAAMDI1d5ejvXdW7p0EIsBANhyAnCg6AnA77gjeeihamsBAAAAYHBtv33S0tL3vZaWpLl5MKsBANhiAnCg2GmnZOLEcj5/frW1AAAAADA4li5N3va25Ec/So4/vu/3tLYmXV2DWxcAwBYaXXUBwBBywAHJokVlGfQnP7nqagAAAAAYSFdfnRx7bHLXXclvfpP87/8m9fVlL/D29tL53dqazJyZNDZWXS0AwCbRAQ70sg84AAAAwPC3bFly3HHJYYeV8HvvvZPzzy+B94wZpUFi8eIyzpgh/AYAaooOcKDXgQeW8cYbq60DAAAAgIHx05+WJc/vuKO8PuGEZM6cpKmpvO4ZJ0wo45gxg18jAMBWEIADvXSAAwAAAAxPHR3JyScn555bXk+dmnzpS8lzn1tlVQAA/a5mAvB77rkn99xzT570pCc9cm3BggW56KKLsnLlyrz0pS/NC1/4wgorhGGgJwBfuLD8UtTziV8AAAAAatevfpW85S3JbbeV18cdl8ydm4wbV2lZAAADoWb2AD/jjDNyzjnnPPJ6yZIlefOb35wf//jH+cMf/pATTjghV199dYUVwjAwYUIyfnzS3Z3cdFPV1QAAAAAMfR0dycqVZc/slSvL66rmefQcy5YlZ52VPOc5Jfzebbfk6quTL3xB+A0ADFs1E4D/5S9/ydOf/vRHXn/nO99JZ2dnrrjiivzyl7/M0572tFx00UUVVgjDhH3AAQAAgJFia0Pnzs7SST1xYu8xd265PtjzrG+OY49N9t+/7Pv9178mL3rR5tUGAFBjaiYAX7p0aXbaaadHXv/85z/Pk5/85Oy+++6pr6/Pi170otzWs4QPsOXsAw4AAACMBFsbOnd0JHPmJLNnJ+3t5Vp7e3k9Z86mh+n9Mc/65jjjjKStLbnqquTCC5Pm5k2rCQCghtXMHuA77rhj7rnnniTJgw8+mD/96U/54Ac/+Mj91atXZ9WqVVWVB8OHABwAAAAY7jo6Stg9e3bvtZ7QOUne/e7k5pvLEuLLliUPPbTueXd38rGPlXC5L21tycknJy9/efleo0f3feywQ6ljY/O8+c3JkiWlS72ra92xqSm55pr1z3HOOclpp23xPxUAQK2pmQD86U9/ei655JJst912+e1vf5vu7u684AUveOT+LbfcksmTJ1dYIQwTPQG4JdABAACA4aqhYcOh84wZyVFHldC5LwcdlLznPb3d1o/W3l6WVb/ttuSGG9Zfx0EHJffeu/F5/t//W/88Bx1U3rOhOZYuTSZMWH8dAADDSM0E4CeddFJuv/32fOITn0hDQ0NmzJiR3XbbLUmycuXK/PCHP8yRRx5ZcZUwDPTsAX7bbcmKFcnYsdXWAwAAANDf2ts3HBgvWZI87WnJPfck48b1HtttV8aJE5NJk5KWlr7naWkp9884I1m+PFm1qu+joWHT5pkxI1m9OhkzpnzN2mNTUzJlyobnsPQ5ADCC1EwAPn78+Hz1q1/NsmXLss0222TMmDGP3FuzZk0uvvjiTJo0qcIKYZjYeedkxx2Tf/0ruemm5JBDqq4IAAAAoH+1tGw4MJ48Ofnudzc8R0dH0tq67jLqPVpbS2D98pdvvJZNmedNb9q6Obq6SlgOADAC1FddwOYaN27cOuF3kjQ2NmbatGlpaWmppigYTurq7AMOAAAADG9LlybHH9/3vZ7AeGOampKZM5NZs0ponpRx1qxyvalp02rpj3n6qxYAgGGgZgLw6667Ll/84hfXufaNb3wjz33uc/P0pz89Z555ZlavXl1RdTDM2AccAAAAGK46OpK3v70E3aedtnWBcWNjWZ580aKyD/eiReV1Y+Pm1dQf8/RXLQAANa5mlkCfN29epkyZ8sjrm266KR/96Eez//77Z/fdd88ll1yS8ePH553vfGeFVcIw0bMPuA5wAAAAYDjp7k6OO64sb/6qVyVXXJF85COlI7y5uXR+b25g3BOWT5hQxi1darw/5umvWgAAaljNdIDfeuutOeiggx55fcUVV2S77bbLZZddls9+9rM5+uijc8UVV1RYIQwjlkAHAAAAhqMvfCG59NJk1Kjk4x9PdtyxhMQTJpTRUuEAADWvZgLwFStWZLvttnvk9a9+9as885nPzNixY5MkBx98cO65556qyoPhpScAv+WW5OGHq60FAAAAoD/85jfJ+99fzs86K3n2systBwCAgVEzAfjkyZPz17/+NUlyxx135O9//3ue+cxnPnJ/6dKlGWNJH+gfkyeXZb/WrEluuqnqagAAAAC2zn33JUcfXZY4P+qo5KSTqq4IAIABUjN7gB955JE599xzs2jRotxyyy1pbm7OC17wgkfu33jjjZk6dWp1BcJwUldX9gG/+ebk7ruTxz2u6ooAAAAAtszq1ckb3pDcdVey//7JRReVv30AADAs1UwAftxxx6Wrqyu/+MUvMnny5Jx11lnZfvvtkyTt7e353e9+lze/+c0VVwnDyOc+l0yfnjzwQLJyZfmEtH2wAAAAgFoza1ZyzTXJttsm3/xm8n9/UwQAYHiqmQB89OjROfHEE3PiiSc+5l5LS0t+/etfV1AVDFOdncl3v5u86EVJe3vS0pK0tiYzZyaNjVVXBwAAALBpvvvd5Mwzy/mFF5YV7wAAGNZqJgBfW0dHR+69994kyaRJk9KkKxX6T0dHMnducvrpvdfa25PZs8v5jBk6wQEAAICh75Zbkp4VI1tbk9e9rtp6AAAYFDUVgP/lL3/J2WefnT/+8Y9Zs2ZNkqS+vj5PfOIT86EPfSgHH3xwxRXCMNDQkLS19X2vrS059dTBrQcAAABgcy1fnhx1VLJ0afK0pyVnn111RQAADJKaCcD//Oc/501velMaGhry6le/OnvvvXeS5NZbb82VV16Z//iP/8gll1ySxz3ucRVXCjWuvb0c67u3dGkyYcIgFgQAAACwGbq7k/e8J/nLX5Kdd06+/vVkzJiqqwIAYJDUTAD+mc98JhMnTsxXvvKVTHhU+HbCCSfk9a9/fT7zmc/kS1/6UkUVwjDR0lKOvkLwlpakuXlw6wEAAADYHOefn1x8cVJfn3z1q8kuu1RdEQAAg6i+6gI21Z///Oe89rWvfUz4nSTjx4/Pa17zmvzpT38a/MJguOnqKvti9aW1tdwHAAAAGIp+//vev2vMmZM873nV1gMAwKCrmQ7w+vr6rF69er3316xZk/r6msnzYehqakpmziznbW2lE7ylpfzyOHNm0thYZXUAAAAAfVuypOz7vXJl8opXJB/6UNUVAQBQgZpJjA899NBcdtllufvuux9z75577slXvvKVPOEJT6igMhiGGhuTGTOSf/4zuf325O67y2vhNwAAADAUrV6dvPGNyZ13Jvvum3z5y0ldXdVVAQBQgZrpAP/ABz6QN77xjTn88MPzohe9KFOnTk2S3H777fnJT36S+vr6nHTSSdUWCcNJU1Ny883lk9MPPZTcdlvVFQEAAAD06uhIGhrK6nXjxiXvfneyeHHy3/+dNDdXXR0AABWpmQD8gAMOyNe//vV85jOfyU9/+tOsWLEiSTJ27Ng861nPyvHHH58ddtih4iphmJk6NZk/v3yK+u67k113rboiAAAAoEprh84tLUlXV/kQ/WDr7Ezmzl13+7bjj09++csShgMAMGLVzBLoSbLPPvvk3HPPzfXXX59rr7021157ba6//vqcc845+dnPfpbnPve5VZcIw8uYMck++5TzBQuqrQUAAACoVk/oPHFi7zF3brm+uTo6yl7dixeXsaNj8752zpxk9uwSfidlPOOM5JOf3Ly5AAAYdmoqAO9RX1+f8ePHZ/z48amvr8kfAWrHtGllFIADAADAyLW+0Hn27HJ9c0LnTQ3Su7vL9/j735P//d/ku99NLrssqa8vnd99aWsrHeoAAIxYNbMEOlCRadOSK64QgAMAAMBI1tCw4dD5lFNKiL1yZTJqVDJ6dO+49vmzn51cckly+um9X98TpHd3Jy98YXLCCcl99yVLlpQl1td20EHJ05/eG8I/Wnt7snRpMmFCP/zQAADUIgE4sGE9HeDz51dbBwAAAFCd9vYNh86LFpVg+4Yb1j/H+PHJwoXJvHl93583Lzn55OSee0r43aOpqQTaEyYke+2VTJpU9vzuq56WlqS5eRN+IAAAhisBOLBhlkAHAAAAWlo2HDpPnJgcfnjpzl61qhyrV697vssuyf33bzhIf/DB5DvfSRobe0PvsWPXfV9HR9LaWrrGH621tXSNjxmzFT8sAAC1bEgH4DfeeOMmv3fx4sUDWAmMYD0B+D33lF9Ct9++2noAAACAwdfVVZYmX3vp8h6trcmaNWUJ9I1ZuXLDQfpOOyWTJ294jqamZObMct7WVuZqaSl1zJxZwnMAAEasIR2AH3XUUamrq9uk93Z3d2/ye4HN0NJSlha7997kppuSJz+56ooAAACAwfbtb5cAvLs7OeecLQ+du7r6p3u7sTGZMSM59dSy53dzc/la4TcAwIg3pAPwOXPmVF0CkJQu8HvvLcugC8ABAABgZPnFL5K3vCXZd9/kW99KTjtty0Pn/uzebmoq44QJZbTsOQAAGeIB+Ctf+cqqSwCSEoD//OfJ/PlVVwIAAAAMpjvvTI4+uuzh/cQnlr8R1NVtXeisexsAgAE0pANwYIjo2Qd8wYJq6wAAAAAGT2dnctRRyX33JYcckpx/fgm/+4PubQAABkh91QUANWD69DIKwAEAAGBk6O5O3vOe5Pe/T3bcsewBvu22VVcFAAAbJQAHNq6nA/yWW8qSZAAAAMDwdt55yZe+lNTXJ1/7WjJ1atUVAQDAJhGAAxu3667lU95dXcntt1ddDQAAADCQrr02aW0t55/4RPLCF1ZbDwAAbAYBOLBx9fXJ/vuX8/nzq60FAAAAGDh33528+tXJqlXJa1+bnHRS1RUBAMBmGbEB+Jve9Kbsv//+fR5XXnnlI+/7+te/nsMOOywHH3xwXvayl+VnP/tZhVVDhXqWQbcPOAAAAAxPDz9cwu9Fi5KDD04uvDCpq6u6KgAA2Cyjqy6gKh/96Efz0EMPrXPt4osvztVXX52nPe1pSZIrr7wyp512Wo477rg89alPzQ9+8IMcf/zxueyyy3LIIYdUUDVUaPr0MgrAAQAAYHhqbU1+85tkhx2Sb387aWqquiIAANhsIzYA32effR5z7aSTTsoznvGM7LjjjkmStra2HHHEEXn/+9+fJHnqU5+am2++Oeeee24uuOCCwSwXqqcDHAAAAIav888vR11dcvnlyd57V10RAABskRG7BPqj/fGPf8xdd92VI488Mkly5513ZuHChTn88MPXed+///u/57rrrsvKlSurKBOqs3YA3t1dbS0AAABA/7nuuuT448v5mWcmhx1WbT0AALAVRmwH+KN9//vfz7bbbpsXvOAFSZLbbrstSbLnnnuu87699947XV1dufPOO7P3Fn4Stru7O8uXL9+6glnHihUr1hkZALvskrF1dalrb8/y229PJk2quiJgkHjGAgwsz1mAgeMZuwn++c80vupVqe/qyqpXvjIrTzgh8XcrYBN4xgIMLM/ZdXV3d6eurm6T3isAT7Jq1ar88Ic/zPOf//xsu+22SZKlS5cmSbbffvt13tvzuuf+lujq6sr8+fO3+OtZv4ULF1ZdwrB20JQp2ebuu3Pnj3+ch570pKrLAQaZZyzAwPKcBRg4nrF9q+vqyn7HHZf6e+/Nir32yoITT8waW58Bm8kzFmBgec72GjNmzCa9TwCe5Ne//nX+9a9/5aUvfemgfL+GhoY+9yBny61YsSILFy7M1KlTM3bs2KrLGbZGH3xwcvfd2fPhh7Nq+vSqywEGiWcswMDynAUYOEPtGVtXV5fuIbCtWE8dDSeemIY//zndzc3Jt7+d/f29CtgMQ+0ZCzDceM6u65Zbbtnk9wrAU5Y/b2lpyTOf+cxHrjU3NydJli1blgkTJjxy/cEHH1zn/paoq6t7pNOc/jV27Fj/tgPpwAOTq67KmNtuyxj/zjDieMYCDCzPWYCBU/kztqMjaWhI2tuTlpakqytpaqq2ju23Tw4/PPnVr1J39tkZ+7jHDX49wLBQ+TMWYJjznC02dfnzJKkfwDpqQmdnZ6655pq85CUvSUNDwyPX99prryS9e4H3uO2229LQ0JDddtttUOuEIWHatDJaDg0AAAA2TWdnMnduMnFi7zF3brm+OTo6kpUrk8WLy9jRsXV1TJ6cXH99ct11yRFHbN5cAAAwhI34APynP/1pli9fniOPPHKd67vttlumTp2aq666ap3rP/jBD/K0pz1tk9eYh2GlJwC3hz0AAABsXEdHMmdOMnt26bpOyjh7drm+qSH21obo66vjjDOST39688N0AAAYwkb8Eujf+973MmXKlDzxiU98zL0TTjghH/zgB7P77rvnKU95Sn7wgx/kL3/5Sy699NIKKoUhoGff73/8o/xyXMVybQAAAFArGhqStra+77W1Jaeckpx5Znm97bblaGrqPd9222TffZMvfKGE1z16QvQkedvbkt/8JnnooWTZsjKufV5fn5x77obrOPXUfvuRAQCgaiM6AF+6dGl+9atf5Zhjjulz3fiXvvSlWbFiRS644IKcf/752XPPPXPOOefk0EMPraBaGAJ22ikZPz5ZsiS5+ebE/xYAAABg/drbezuu+7q3aFFy+eXJDTf0/Z7x45OFCzccXs+YkRx/fPldvS8HHVS+z4bqWLo0mTBhfT8FAADUlBEdgDc3N+eG9f2C8X+OPvroHH300YNUEdSAadOSa68t+4ALwAEAAGD9tt8+aWnpO3xuaSlLmR95ZPKUpyTLl5fV1pYv7z3fddfkvvs2HF7ff3+ZY8mSZLvtyjFuXO/5hAllv+8N1dHc3D8/LwAADAEjOgAHtsDaATgAAADQt/POK8Hz8ceXvbYfrbU1WbOmdwn09Vm5csPh9aRJyUUXbXiOjo7y/dZeRn3tOrq6kjFjNjwHAADUCAE4sHmmTSvj/PnV1gEAAABD1VlnJTNnlt+hr7uu7MPd1lZC7JaWEjrPnJk0Nm58rq6urQ+vm5rK90u2vA4AAKgRAnBg80yfXkYd4AAAALCu7u4SKH/iE+X1q15VlhefMSM59dSy13ZzcwmtNzV07q/wurFx6+oAAIAaIQAHNk9PB/jNNyerVyejRlVbDwAAAAwFq1cn731v8l//VV6ffXbywQ+W86amMk6YUMbNXW68v8Lrra0DAABqgAAc2Dx77JFss03y8MPJHXcke+1VdUUAAABQrZUrkze/Ofna15K6uuT885O3v71/v4fwGgAANkl91QUANWbUqGS//cq5ZdABAAAY6ZYvT175yhJ+NzQkX/1q/4ffAADAJhOAA5uvZxn0+fOrrQMAAACqtHRp8pKXJD/4QTJ2bPLd7yaveU3VVQEAwIhmCXRg802fXkYd4AAAAIxU991Xwu8//jHZfvvkyiuTZz6z6qoAAGDEE4ADm6+nA1wADgAAwEh0553Ji19cfi+eMCH50Y+SQw+tuioAACACcGBLCMABAAAYqf7+9+SFL0z+8Y9kt92SH/842X//qqsCAAD+jz3Agc23335lXLKkHAAAADAcdXQkK1cmixeX8d57k7e+tYTf++2XXHut8BsAAIYYATiw+Zqakt13L+e6wAEAABiOOjuTuXOTiRN7j3PPTb797eSVr0x+9ave340BAIAhwxLowJaZPr184n3BguSZz6y6GgAAAOg/HR0l/J49u/dae3tyxhlJXV3ypS8lzc2VlQcAAKyfDnBgy9gHHAAAgOGqoSFpa+v73rx5ydixg1sPAACwyQTgwJYRgAMAADAcrVyZLFlSOr770t6eLF06mBUBAACbQQAObBkBOAAAAMNFd3fy618nxx2XHHBAWd68paXv97a0WP4cAACGMHuAA1umJwC//fakszNpbKy2HgAAAOhD44Z+X73lluSSS5JLL01uu633+q9+lZxwQnL66Y/9mtbWpKsrGTOm/4sFAAC2mgAc2DITJ5ZPvbe3J3//e3LwwVVXBAAAAL06OjK2oSHTdtop9aNHJx0dSVNT8q9/JV/7Wgm+r7uu9/1NTcmrX5286U3Jc59bjrq6shd4e3v5Hbi1NZk504fAAQBgCLMEOrBl6uosgw4AAMDA6Ogoe3EvXlzGjo7N+/rOzmTu3NRNnJhRkyenbuLEZO7c5IEHSrD9nveU8Lu+PjnssNIBvmhR8uUvJy94QTJqVAm5Z8wo1xcvLuOMGcJvAAAY4gTgwJYTgAMAANDf/i+8zsSJvcfcueX6pli2LJkzJ5k9u3RuJ2WcPTv59KfL+PjHJ5/8ZHLXXclVVyVvfGPpAH+0pqay1PmECWXs6z0AAMCQYgl0YMsJwAEAAOhPHR0l7J49u/daT3idJO98Z/Kb3/R2ZD96XL06+ctfyrLlfTnnnOTee5NXvGKgfxIAAKAiAnBgy/UE4PPnV1sHAAAAw0NDw/rD67a2sgT5ccclS5b0/Z6DDipheE/n96O1tycPPlg6ugEAgGFJAA5suenTy3jTTcmaNWXvNAAAANhS//rXhsPrJUuSl7wkeeihsjT6zjv3jjvvnEyZUo6Wlr7naWlJmpsHrHwAAKB6AnBgy+25Z/l0/vLlZd+03XevuiIAAABq0eLFZenzj31sw+H15MnJJZdseK6OjqS1dd1l1Hu0tiZdXWU/bwAAYFjSrglsuYaGZJ99yrl9wAEAANhcnZ3JJz5Rfrf81KeSa65JTjih7/f2hNcb09SUzJyZzJpVQvOkjLNmletNTf1VPQAAMATpAAe2zrRpZQ/wBQuSF7+46moAAACoBd3dyde/npx8crJwYbn2xCcmu+ySHHZYUldX9vxuby/hdWtrCa8bGzdt/sbGZMaMdJ96atY88EDqd9ghdV1dm/71AABAzRKAA1tn2rQy6gAHAABgU/zud8mJJyb/+7/l9S67JHPmJG98Y1L/f4sVzpiRnHpqsnRp2bN7S8LrpqasWL48ty9Zkj3Hjcu2Or8BAGBEEIADW2f69DLOn19tHQAAAAxt//hH8uEPJ5ddVl5vu23pAD/ppMcuS97zesKEMm7Fnt2dnZ1b/LUAAEDtEYADW0cHOAAAAD06OpKGht6ly3v27D7rrOSTnyx7fifJMcckH/946f4GAADoR/VVFwDUuP33L+O995Y/cAAAADAydXYmc+cmEyf2HnPnJitWJN/4Rrn/7Gcnf/hD8uUvC78BAIABoQMc2Drbb59MmZLcc09y003JU55SdUUAAAAMto6OEnbPnt17rb29vF6zJvnsZ5Ply5NXvCKpq6uoSAAAYCTQAQ5sPcugAwAAjGwNDUlbW9/3zjkned7zkle+UvgNAAAMOAE4sPWmTy/j/PnV1gEAAEA12tvXvy1We3uydOkgFgMAAIxkAnBg6+kABwAAGNlaWsqxvnvNzYNYDAAAMJIJwIGtJwAHAAAY2e6+Ozn++L7vtbYmXV2DWw8AADBija66AGAY6AnAb721/FGjoaHaegAAABg8P/lJ8oEPJNdcU/b4njevLHve0lLC75kzk8bGqqsEAABGCAE4sPV22SXZbrvkoYdKCN4TiAMAADC8/eEPySteUX4fPP305Mwzk498pOz53dxcPiQt/AYAAAaRJdCBrVdX1xt6z59fbS0AAAAMjgULksMPL+H3C16QnH12+XD0mDHJhAllbGqqukoAAGCEEYAD/cM+4AAAACPHnXcmL35xsmRJ8qQnJd/+drLNNlVXBQAAIAAH+okAHAAAYGRYsqSE33femey/f/LDHybjxlVdFQAAQBIBONBfBOAAAADD37Jlyb//e/ndb7fdkh//OBk/vuqqAAAAHiEAB/rH9OllXLAg6e6uthYAAAD638MPJ698ZfL73yc77ZRcfXUJwQEAAIYQATjQP/beOxk1KnnwweSf/6y6GgAAAPrT6tXJG9+Y/OQnSVNTWfa8ZyUwAACAIUQADvSPbbZJ9tqrnFsGHQAAYPjo7k7e/e7km99MxoxJvvOd5MlPrroqAACAPgnAgf5jH3AAAIDh59RTkwsuSOrrk698JXnhC6uuCAAAYL0E4ED/EYADAAAML5/+dDJnTjk/77zkqKOqrQcAAGAjBOBA/5k+vYwCcAAAgMHV0ZGsXJksXlzGjo6tn/Pii5OTTirnc+Yk73jH1s8JAAAwwATgQP/p6QCfP7/aOgAAAEaSzs5k7txk4sTeY+7ccn1zrB2id3YmLS3l97yTTkpOPnlASgcAAOhvAnCg/+y/fxnvuitZtqzaWgAAAEaCjo7SnT17dtLeXq61t5fXc+YkS5cm3d0bn+fRIfrkyckf/pBcd11y9tlJXd1A/hQAAAD9ZnTVBQDDyI47JjvvXLoFbr45eeITq64IAABgeGtoSNra+r7X1pbMmJFMmlQ6u5uby7H99r3nzc3Ju96VfPObyemn935te3tyxhlJfX2Zo6lpUH4cAACArSUAB/rXtGklAF+wQAAOAAAw0Nrbezu/+7p3333lg8o33ND3+8aPLx3e8+b1PUdbW3Lqqf1TKwAAwCAQgAP9a/r05Je/LAE4AAAAA6ulpRx9hdstLcmUKcmPf1zuL1362GObbZIHHthwiL50aTJhwgD9AAAAAP1LAA70r2nTyjh/frV1AAAAjARdXckJJ6y7fHmP1tZyf9KkcqzPypUbDtGbm/upWAAAgIFXX3UBwDDTE4DrAAcAABh4111XAvCPfKSE1UkZZ81KZs7ctL27u7pKWN6XnhAdAACgRugAB/pXTwD+978nq1Yloz1mAAAABsSqVSX8TpKvfjU57bSyXHlzcwmtGxs3bZ6mphKWJ2XP7/b2EqK3tpbrmzoPAADAECCZAvrX7rsnY8cmK1YkCxcm++xTdUUAAADD08UXl9W3dtopmTo1GTOmd6/uMWM2b67GxmTGjOTUU7csRAcAABgiLIEO9K/6+mT//cu5ZdABAAAGxooVyUc/Ws4//OH+2ae7qak3RB8zZtOWTwcAABhiBOBA/+tZBn3+/GrrAAAAGK7OPTe5++5kt92S97yn6moAAACGDAE40P96AnAd4AAAAP2vvT0588xyPnu2ZcoBAADWIgAH+p8AHAAAYODMnZs88EBywAHJm95UdTUAAABDigAc6H9rL4He3V1tLQAAAMPJPfckn/1sOT/zzGTUqErLAQAAGGoE4ED/22+/pK6udCQsWVJ1NQAAAMPH7NnJihXJ05+evOxlVVcDAAAw5AjAgf43dmwydWo5nz+/0lIAAACGjZtvTr74xXJ+1lnlg8cAAACsQwAODAz7gAMAAPSv005LVq9Ojjgiedazqq4GAABgSBKAAwNDAA4AANB/rr8++Z//KV3fZ55ZdTUAAABDlgAcGBgCcAAAgP5zyillfOMbk8c9rtpaAAAAhjABODAwpk8vowAcAABg61xzTTkaGpLZs6uuBgAAYEgTgAMDo6cDfOHCZMWKSksBAACoWWvW9HZ/v/vdyZ57VlsPAADAECcABwbG+PHJjjsm3d3JzTdXXQ0AAEBt+sY3yv7f222XnHpq1dUAAAAMeQJwYGDU1dkHHAAAYGt0dSUf+Ug5/+AHk513rrYeAACAGiAABwaOABwAAGDLXXRR8ve/JxMmJB/4QNXVAAAA1AQBODBwpk8vS6F3dFRdCQAAQG1Zvjz52MfK+WmnJePGVVsPAABAjRhddQHAMHb00cm7350sWZKsXFmW72tqqroqAACAoe9zn0v++c9k6tTkne+suhoAAICaoQMcGBidnWW5vl13LX+wmTgxmTu3XAcAAGD9/vWv5BOfKOenn55ss0219QAAANQQHeBA/+voKGH37Nm919rbe1/PmKETHAAAYH3mzEmWLk0e97jkDW+ouhoAAICaogMc6H8NDUlbW9/32trKfQAAAB7rrruSefPK+Zw5Sb0/3QAAAGwOv0UB/a+9vRzru7d06SAWAwAAUEP+8z+Thx9Onv3s5PDDq64GAACg5gjAgf7X0lKO9d1rbh7EYgAAAGrEbbcl3/teOT/rrKSurtp6AAAAapAAHOh/XV1Ja2vf91pby30AAACKjo5k5cpk9OgSgl97bfK0p1VdFQAAQE0SgAP9r6kpmTkzmTWrtxO8pSU57bRyvampyuoAAACGjs7OZO7cZOLEZI89kl13Ta6+ulwHAABgswnAgYHR2JjMmJEsWpTce29y113JoYcmDz5YdWUAAABDQ0dHMmdOMnt20t5errW3l9dz5pT7AAAAbBYBODBwmpqSMWNKJ8PrXpe86lXJhRdWXRUAAMDQ0NCQtLX1fa+trdwHAABgswjAgcFx1FFlvOiipLu72loAAACq1tWV3Hdfb+f3o7W3J0uXDmZFAAAAw4IAHBgcRx+djBuX3HJL8stfVl0NAABANbq7k29/O3nmM5OWlnL0paUlaW4exMIAAACGBwE4MDiamsoy6Ill0AEAgJHp979PnvOcsj3U735XPhx8wgl9v7e1tXSJAwAAsFkE4MDgOfbYMn7jG5byAwAARo6FC5M3vCH5t39LfvWrZOzY5CMfSZ797OTDH05mzertBG9pKa9nziwfJAYAAGCzCMCBwfNv/5YceGCyYkVy+eVVVwMAADCw2tuTk09Opk0rvwPV1SXHHJPcfHNy+ukl4G5sTGbMSBYtShYvLuOMGeU6AAAAm00ADgyeurreLnDLoAMAAFXq6EhWriyh88qV5XV/6epK5s1L9tknmTs3efjh5PnPT66/Pvnyl5Ndd133/U1NyZgxyYQJZdT5DQAAsMUE4MDgetObkoaG5A9/SP7yl6qrAQAARqLOzhJMT5zYe8ydW65vjr5C9GuuKStftbYm99+fTJ+efP/75fqhhw7MzwMAAMAjBODA4Bo/Pnn5y8u5LnAAAGCwdXQkc+Yks2eXJcqTMs6eXa5vaid4XyH6Jz6RPP7xyahRyc47J+edVz74e8QRZUUsAAAABtzoqgsARqC3vS35xjeSSy8tfzDaZpuqKwIAAEaKhoakra3ve21tZc/u970vWbGivHf06MeORx2V/M//lH28e7S3l9fd3clXv5rsuWey/faD8iMBAADQSwAODL4Xv7jseXfXXcl3vpO89rVVVwQAAIwU7e29nd993Vu8OPnpT5Mbbuj7PePHJx/8YNnjuy/nnJOcdlrZyxsAAIBBJwAHBt+oUclb3pKccUZZBl0ADgAADJaWlnL0FYK3tJSlzN/xjuTBB5NVq5KurnXHCROSf/1rwyH60qXlfQAAAAw6AThQjbe+tQTg11yT3HFHssceVVcEAAAMdytXJtdfnxx/fPl95NFaW5M1a8q4sXk2FKI3N/dDsQAAAGyJ+qoLAEaovfZKnv/8sj/el75UdTUAAMBw19mZvPrVydveVgLuWbNKWJ2UcdasZObMpKlp43N1da0/JG9tLfcBAACohAAcqM6xx5bxS19KVq+uthYAAGD4Wr48efnLk+99L1m4MFmwIJkxI1m0qOz5vWhRed3YuGnzNTWVsHxrQnQAAAAGhAAcqM4rX1n+SPSPfyQ/+UnV1QAAAMPRsmXJEUckV1+dbLttcuWVybOeVULqMWPKXt1jxmx+aN3YuHUhOgAAAANCAA5UZ+zY5I1vLOcXXVRtLQAAwPCzdGly2GHJz3+ejBtXQvDnP7//5t/aEB0AAIB+JwAHqtWzDPq3v53cf3+1tQAAAMPH/fcnL3hBct11yQ47lFWnnvGMqqsCAABggAnAgWodemg5Vq5MLrus6moAAIDhYNGi5HnPS66/Phk/PvnZz5InP7nqqgAAABgEAnCgej1d4BdemHR3V1sLAABQ2+6+O3nuc5O//jWZPDn5xS+Sxz++6qoAAAAYJAJwoHpveEOyzTbJX/5SOjQAAAC2xB13JM95TrJgQbLbbiX8PuCAqqsCAABgEAnAgertsENy1FHl/MILq60FAACoTbfckjz72cmttyZ77ZX88pfJvvtWXRUAAACDTAAODA09y6B/5SvJ8uXV1gIAANSW+fNL+P2PfyT771/C76lTq64KAACACgjAgaHhuc9N9twzefDB5JvfrLoaAABgKOvoSFauTBYvTh5+OLn99qS5OTnooLLs+S67VF0hAAAAFRGAA0NDfX3y1reWc8ugAwAA69PZmcydm0ycWI5Jk5LrrkuuvbZ0fk+cWHWFAAAAVEgADgwdb3lLUldXOjZuuaXqagAAgKGmoyOZMyeZPTtpby/X2tuTM85I2tqSMWOqrA4AAIAhQAAODB277ZYcdlg5v+iiamsBAACGnoaGEnT3pa2t3AcAAGBEE4ADQ8uxx5bxy19OVq2qtBQAAGAIeOCB5OKLk+OPT+6+u7fz+9Ha25OlSwezMgAAAIYgATgwtLzsZcn48ck//5lcdVXV1QAAAD06OpKVK5PFi8vY0TFw3+v++8uqUP/+72VP77e8Jfna15Kdd05aWvr+mpaWpLl54GoCAACgJgjAgaFlzJjkTW8q5xdeWG0tAABA0dmZzJ1bwuieY+7ccn1zrS9Iv+++5IILkhe/uMx/7LHJD3+YdHUlBx1UOsA7OpLW1r7nbW0t7wUAAGBEG111AQCPceyxyWc+k3z/+8miReWPXwAAQDU6OkrYPXt277X29t7XM2YkTU2bNldPkN7WVuZoaUlOOCF5//uT5z43+dvfet/7+McnRx+dHHVUMm1a7/WZM8u49hytreV6Y+MW/pAAAAAMFwJwYOg58MDkKU9Jfvvb5L//O/nQh6quCAAARq6GhhI296WtLTnllOSMM0q4PWpUMnp0Gdc+Ro8und2XXZacfnrv17e3l9fd3cnHP17mefWrS+i97759f8/GxhK6n3pq2fO7ubl0fgu/AQAAiAAcGKqOPbYE4BdemHzwg0ldXdUVAQDAyNTeXo713Vu0qOzPfcMN659j/PjkbW9L5s3r+/455yT33pu84hWbVlNPx/mECWUcM2bTvg4AAIBhTwAODE2vfW1ZBvGmm5L//d/kGc+ouiIAABiZWlrK0VcI3tJStix6+cuT5z0vWbUqWb163WPVqmTKlOT++zccpD/4YG+gDQAAAFtoxAfg3/72t3PxxRfn1ltvzbbbbpuDDz4455xzThobG3PKKafk29/+9mO+5oILLsizn/3sCqqFEWT77ZPXvCb58peTiy4SgAMAQFW6uso+3WsvXd6jtTVZs6YsXb4xK1duOEhvbt7KQgEAAGCEB+Bf+MIXcsEFF+S4447LIYcckgceeCDXXXddVq9e/ch7dtttt3zyk59c5+v23nvvwS4VRqZjjy0B+Ne+lnzuc8l221VdEQAAjDy33loC8O7uslR5e3sJrFtbk5kzN33v7a6u8jWzZz/2XmtruW8pcwAAALbSiA3Ab7vttpxzzjn5/Oc/n+c85zmPXD/ssMPWeV9jY2MOOeSQQa4OSFK6vl/ykuS445LRo5PFi8sf2rq6evf8AwAABs5DDyWvelXS0FBWZjrttGTp0tKt3dW16eF3Uv4//MyZ5bytbcuDdAAAANiA+qoLqMq3vvWt7LrrruuE38AQU1dXur//8Idk8uSyt+DEicncuUlnZ9XVAQDA8NfaWjrAOzqSadNKh/aECWXckg+lNjYmM2YkixaVD7guWlReC78BAADoJyO2A/zPf/5z9ttvv3z+85/PJZdckmXLluWggw7KzJkz8/jHP/6R991xxx154hOfmIcffjj77bdf3vOe9+SFL3zhVn3v7u7uLF++fGt/BNayYsWKdUaGh21Wr079pz6VurX3E2xvT2bPTneSNSedlIdHj9jHGAwaz1iAgeU5y1A16lvfyjZf+lK66+ry8Be/mDXbbJP0x++ydXXJqlWp2267dK9aVV77HZkB4hkLMHA8YwEGlufsurq7u1NXV7dJ763r7u7uHuB6hqSXvOQlWbRoUXbeeeeceOKJGTt2bM4777zcfPPNufrqq7PTTjvl4osvzujRo7PPPvtk2bJlufzyy3Pttdfmc5/7XF7ykpds0ff961//mpUrV/bzTwPDz+jRo3PwtGmpnzSphN6P1tKSNffem78uWJBVq1YNen0AADCcNdx7bw54/eszetmy/POtb809731v1SUBAAAwwo0ZMyYHH3zwRt83YgPwww47LAsXLswVV1yRadOmJUna29vz/Oc/P8ccc0ze9773PeZr1qxZk9e97nV56KGH8oMf/GCLvu9f//rXdHd3Z5999tmq+lnXihUrsnDhwkydOjVjx46tuhz6QV1dXRqXLUvdxInrfU/3okXpHDcuI/QxBoPGMxZgYHnOMuSsXp1tXvrSjPrlL7P6CU/Iwz/9adkDHGqQZyzAwPGMBRhYnrPruuWWW1JXV7dJAfiIXTt4++23T0tLyyPhd5K0tLTkgAMOyC233NLn19TX1+fFL35xzj777HR2dqZxC/coq6ury7bbbrtFX8uGjR071r/tcDJqVNLSst4O8LqWlowdM2awq4IRyzMWYGB5zjJkfOITyS9/mTQ1ZdRXv5ptm5urrgi2mmcswMDxjAUYWJ6zxaYuf54k9QNYx5C2oQ7shx9+eBArAdarqytpbe37XmtruQ8AAPSf669PPvKRct7Wluy7b7X1AAAAwGYasQH48573vLS3t2f+/PmPXHvggQdy44035sADD+zza9asWZOrrroq++677xZ3fwOboakpmTkzmTWrdIInZZw1q1xvaqqyOgAAGF46OpI3vCFZtSo56qjkrW+tuiIAAADYbCN2CfQXvvCFOfjgg9Pa2poTTzwx22yzTc4///yMGTMmb3jDG3L33XfnlFNOyRFHHJE99tgjS5cuzeWXX54bbrgh8+bNq7p8GDkaG5MZM5JTT00WL0522CH5f/+vXAcAAPrPBz6Q3HxzsssuyfnnJ5uxvBwAAAAMFSM2AK+vr8/555+fOXPmZNasWenq6sqTnvSkXHbZZZkwYULa29uz3Xbb5Qtf+ELuv//+NDQ05KCDDsoFF1yQZz3rWVWXDyNLT6d3V1cydWpy//3J3/+e7L13pWUBAMCw8Z3v9Ibe//3fyY47Vl0RAAAAbJERG4AnyY477pizzz67z3stLS35whe+MMgVARu0557Jk56UXHVVcs45yWc+U3VFAABQ++65J3n728v5hz6UPP/51dYDAAAAW2HE7gEO1Kj3va+MF12ULFtWbS0AAFDr1qxJjjmmrLL0hCckp59edUUAAACwVQTgQG158YuT/fZLHnywLM0IAABsuc9+NrnmmmTs2OSyy5IxY6quCAAAALaKAByoLfX1yQknlPN580rHCgAAsPn+9Kdk5sxy/pnPJNOmVVoOAAAA9AcBOFB7jjkm2X775KabkquvrroaAACoPcuXJ294Q7JyZfLylyfvfGfVFQEAAEC/EIADtWfcuORtbyvnbW3V1gIAALXoQx9K5s9PJk1KvvjFpK6u6ooAAACgXwjAgdr03veWP9L98IfJzTdXXQ0AANSOq69OPv/5cn7xxcn48dXWAwAAAP1IAA7Upn32SY44opzPm1dtLQAAMNR1dJTlzhctSp7xjOTb307mzEle/OKqKwMAAIB+JQAHatf73lfGL385Wbq00lIAAGDI6uxM5s5NJk4sS57vumty/fW9/38aAAAAhhEBOFC7XvCCZPr05KGHSggOAACsq6OjdHrPnp20t5dr7e3JGWckZ51V7gMAAMAwIgAHalddXdLaWs7nzUtWr662HgAAGGoaGpK2tr7vtbWV+wAAADCMCMCB2vamNyUtLcmttyY//GHV1QAAwNDS3t7b+d3XPVsJAQAAMMwIwIHa1tSUvP3t5Xx9nS0AADBStbSUY333mpsHsRgAAAAYeAJwoPa9971JfX3y4x8nf/tb1dUAAMDQ8dBDyfHH932vtTXp6hrcegAAAGCACcCB2jd1avKyl5XzefMqLQUAAIaM7u7k1FNL0H3aab2d4C0tyaxZycyZZUUlAAAAGEYE4MDw8L73lfG//zt54IFqawEAgKHga19LzjsveeELSxf4okXJ4sVlnDEjaWysukIAAADodwJwYHh4znOSgw9Oli9PLrqo6moAAKBaS5aUzu8kOeqoZOedkzFjkgkTyqjzGwAAgGFKAA4MD3V1vX/gO+ecZPXqausBAIAqnXhict99yUEHJaecUnU1AAAAMGgE4MDw8cY3JjvumCxcmHzve1VXAwAA1bjqquTSS8uHRL/4xdLxDQAAACOEABwYPsaOTd75znLe1lZtLQAAUIVly5J3vaucv//9yVOeUmk5AAAAMNgE4MDw8p73JKNGJT/7WfKXv1RdDQAADK5TT03+8Y9kzz2T00+vuhoAAAAYdAJwYHjZbbfkVa8q5/PmVVsLAAAMpv/93+Scc8r5+ecnTU3V1gMAAAAVEIADw09raxkvvTS5//5qawEAgMHw8MPJ29+edHcnb3lL8sIXVl0RAAAAVEIADgw/z3hGcuihSWdncsEFVVcDAAAD7+MfT+bPTyZOTD71qaqrAQAAgMoIwIHhp66utwv83HOTVauqrQcAAAbSX/+azJlTzs85J9lxx2rrAQAAgAoJwIHh6XWvSyZMSO66K/nOd6quBgAABsbq1cmxx5YPfb7iFclRR1VdEQAAAFRKAA4MT42NybveVc4/97lqawEAgIHyuc8lv/990txcVj+qq6u6IgAAAKiUABwYvt797mT06OTaa5M//rHqagAAoH/ddlvykY+U87PPTqZMqbYeAAAAGAIE4MDwNWVKcvTR5XzevGprAQCA/tTdXVY8WrEied7zkre/veqKAAAAYEgQgAPDW2trGb/yleS++6qtBQAA+suXv5xcc03Z+uf88y19DgAAAP9HAA4Mb095SukC/5//SbbbLlm8OFm5MunoqLoyAADYMv/8Z/KBD5Tz2bOTffapth4AAAAYQgTgwPBWV5dceGHyhz+UJdEnTizH3LlJZ2fV1QEAwOY74YSkvT154hOTE0+suhoAAAAYUkZXXQDAgOroSD75yeSMM3qvtbeXTpkkmTEjaWqqpDQAANhsV16ZfPObyejR5YOeo/1aDwAAAGvTAQ4Mbw0NSVtb3/fa2sp9AAAYCB0dZfudrd2Gp2eeRYuS5z43+fa3k7PPTh7/+H4tFwAAAIYDATgwvLW3l2N995YuHcRiAAAYMTo7y7Y7PVvwbOk2PGvPM2lSsuuuyfXXJ+9618DUDQAAADXOWmnA8NbSUo6+QvCWlqS5eXDrAQBg+OvoKKF1z7Y7ybrb8Lz73cmyZcl225WjqSmp7+Pz6eub54wzyvtt5wMAAACPoQMcGN66upLW1r7vtbaW+wAA0J82tg3PuHHJ05+eTJmSbL99MmpUCbInTkz23rssbX7EEUldne18AAAAYDPpAAeGt6amZObMct7WVjpmWlpK+D1zZtLYWGV1AAAMRxvbhmfJkmSffZJ//StZs6ZcX768HIsXl9dr1pQ9vze2nc+ECf1aOgAAANQ6ATgw/DU2luUhTz21/BFxxx2Tv/9d+A0AwMBobt7wNjyTJyfXXZd0d5c9vh966LHHypXlfbbzAQAAgM0iAAdGhp69Ef/2t+Q//qPstXjbbWVZSQAA6C833JDceWdy/PFlr+5H69mGZ8yY8v9Fx44tR1+d3B0d5f1r7wHe1zwAAADAIwTgwMjyrGeVLpslS5Lf/CZ52tOqrggAgOHi979PXvKSZOedk2uvTerrt24bHtv5AAAAwGarr7oAgEG17bbJy19ezi+/vNpaAAAYPn75y+QFLyj7ejc3J6NHl214Fi0q+3ovWlReb25o3bOdz9bOAwAAACOEABwYeV7/+jL+z/8kq1dXWwsAALXvhz9MDjssWbYsed7zkh//uITgTU1lifIJE8rYsy3P5uqveQAAAGAEEIADI8+LXpTssEPpnvn5z6uuBgCAWvb1r5cVhjo7k5e+NPnBD5Jx46quCgAAAEYsATgw8owZk7z61eXcMugAAGypiy5KXve6pKurjN/6lqXJAQAAoGICcGBk6lkG/ZvfTFaurLYWAABqz+c+lxx7bLJmTfKOdySXXpo0NFRdFQAAAIx4AnBgZHr2s5PJk5P29uRHP6q6GgAAakV3d3LGGcn7319en3RS8l//lYwaVWlZAAAAQCEAB0amUaOS17ymnFsGHQCATdHdncyYkZx2Wnk9e3Zy9tlJXV21dQEAAACPEIADI1fPMuhXXJF0dFRbCwAAQ9vq1cm735188pPl9Wc+U4Jw4TcAAAAMKQJwYOT6t39L9twzWb48+f73q64GAIChpKMjWbkyWby4jL//ffKLX5TA+4tf7F0CHQAAABhSBODAyFVXl7zudeXcMugAAEPDo4PnLVmpZ2vn6OxM5s5NJk7sPa68MvnlL8sHJ489dvNrAgAAAAaFABwY2XqWQf/hD5P29kpLAQAY8foKnufOLdcHa46OjmTOnLK/d8//P2xvT844IznnnOQ5z9ncnwoAAAAYRKOrLgCgUgcfnBx4YHLjjcm3v5289a1VVwQAMDJ1dJSgevbs3mvt7b2vZ8xImpqS7u4SZnd0JA89tO64117JRRclp5/+2Dm6u5OXvjT59KeTFSv6PrbdNvntb5O2tr5rbGtLTj11oP4FAAAAgH4gAAd43euS004ry6ALwAEAqtHQsOHgecaMZM89k3/8I1mz5rHvGT8+WbgwmTev7znmzUtOPjn5yU+SJUv6fs9BB5Vl09e3MlB7e7J0aTJhwkZ+GAAAAKAqAnCAngD8Jz9JFi0qy2QCADC42ts3HDzfd1+y3Xbrht9jx5au8O22Sw45pATbGwuvP/vZZPny8rWPPrbbLpkyJWlp6XuelpakuXlLf0IAAABgEAjAAfbZJ3nyk5Pf/z75+teT44+vuiIAgJGnpWXDwfPkyckVV5Rlypuayjhq1LrvW7lyw3OMH5+88Y0brqOjI2ltXXcp9h6trUlXVzJmzCb8QAAAAEAV6qsuAGBIeP3ry/jVr1ZbBwDASPXAA+v/IGJra7JqVdnje9KkZNy4x4bfSQmnW1vXP0dX18braGpKZs5MZs0qoXlSxlmzyvWmpk35aQAAAICKCMABkuQ1r0nq6pJf/7rsKwkAwODp7Eze+c4SUp922pYHz/0VXjc2lj3HFy0qe4IvWlReNzZu/s8GAAAADCoBOECS7LJL8uxnl3Nd4AAAg+vUU5Pvfjd55SuTE0/cuuC5v8Lrpqay1PmECWXU+Q0AAAA1QQAO0KNnGfTLL6+2DgCAkeQnP0k+/elyfsopyQ47bH3wLLwGAACAEUsADtDjqKOS0aOTP/0pWbCg6moAAIa/f/0rOeaYcn7ccclLX1ptPQAAAEDNE4AD9Bg/PnnRi8q5ZdABAAZWd3cJve++O9lvv+STn6y6IgAAAGAYEIADrG3tZdC7u6utBQBgOLvkkuTrXy8r8Fx2mWXKAQAAgH4hAAdY28tfnjQ2JjffXJZCBwCg/91+e/L/27vzOKvqgn/gn2ETHIVRIxR3IXFD0Sw0tzJLJbPNzPInlbn24Jg+hSKKivRo5JKDqIlWlqalZmYPWrmUZam5W5qmaO7gNoAjyyjz++M8A6LDIszMmbnzfr9e53XOvefMnc/1Vcfxfu73+x01qjg+5ZRk++1LjQMAAABUDgU4wNv17Zt86lPF8RVXlJsFAKASvfVWctBByezZyU47JccfX3YiAAAAoIIowAHeqXka9CuvTBYsKDcLAECl+d73kttvT1ZfvZgGvXv3shMBAAAAFUQBDvBOI0YUH8g+80zy17+WnQYAoHLcfXdy8snF8XnnJRtvXG4eAAAAoOIowAHeqU+f5LOfLY6vvLLUKAAAFaOhITnwwOTNN5MvfrGYBh0AAACglSnAAVrSPA36VVcVH9ICALByvv3t5LHHkoEDkwsvTKqqyk4EAAAAVCAFOEBL9tgjWWutZMaM5JZbyk4DANC5/fa3RemdJJdemqy5Zrl5AAAAgIqlAAdoSc+exdSciWnQAQBWxvTpycEHF8fHHFN80RAAAACgjSjAAZbkgAOK/a9+lcybV24WAIDOqKkpOeSQ5KWXkqFDk//5n7ITAQAAABVOAQ6wJLvskqy7bjJzZnLDDWWnAQDofC66qJj+vFev5PLL5HoscwAAWDlJREFUk969y04EAAAAVDgFOMCSdOuWfOlLxfEVV5SbBQCgs3n00WLK8yQ544xiBDgAAABAG1OAAyxN8zTo11+fvP56uVkAADqLxsbk//2/ZM6c5OMfT44+uuxEAAAAQBehAAdYmu23TwYNKj68/c1vyk4DAHQUDQ3J/PnJjBnFvqGh7EQrpzXez9tf4803k7Fjk+HDk5/8pJhZBwAAAKAd+BQCYGmqqpIvf7k4Ng06AJAkc+cmEycmAwYs2iZOLJ7vjFrj/bzzNQYOTO65J/nDH5L11mu77AAAAADvoAAHWJbmAvx3v0tefbXcLABAuRoaktNPT8aPT+rri+fq64vHp5/e+UaCr+z7eeutZPbsll9jwoTkzDM73z8TAAAAoFNTgAMsyxZbJEOHFmtZ/upXZacBAMrUs2dSV9fyubq6pHv3ohB+j3r37r2SwVbQst5PVVWy667JppsmG26YrL12ssYayaqrJj16FI+7dVv6a/Ts2Xb5AQAAAN5BAQ6wPJpHgU+dWm4OAKBc9fWLRjm3dO7FF5Nddkl22ik55ZTkL38pvkS3JA0N6dOjRzZba6306dFj5dfeXtb63fPmJXfckZx9dnLsscnzzy/9/cyYkbz2WvLvfydPP51Mn148P2dOMfp77bWLa5b2GjNnvvf3BAAAALCCepQdAKBTOOigYiT4HnsUH/yusUbxYXZ1ddnJAID2VFNTbC0VvjU1Sf/+yXPPJS+/nPz1r8mppyarrZZ89KPJJz5R/C2x+ebFyOr/Wze7qq4u3evri5+vrU3GjEmWd0R489rbdXVFpne+xosvFjn+9rdif889RQmeJO97X3LaaUt/P2uvnUyeXIzyXmWVpFevYt+89e5dvL+lvUa/fsv3XgAAAABagQIcYHm8733J3XcnX/tayx8uAwBdQ2Nj8TfA+PHvPldbW+zvuiu56aZiu/nm5JVXkt/+ttiSZODA5Npri8ennbbo55vX3k6S0aOX/UW7hoai/H57lubXaGoqRqHvtde7f+5970s+8pFiq69f+vt5661iCvRl5VjaazQ2FsU5AAAAQDtQgAMsS/OHyxMmLHruvX5ADQBUhurq4gtwTU3JpEktfzFu442TQw8ttgULkvvvT/7wh6IQ//Ofi2nKt9wy2XPPln9HXV1y3HFFed3SqOqmpuJ3XnvtktfenjSpeI3+/YtR3M2F90c+kgwaVIxAbzZmzKLfuyJf9Gv+Z7IyrwEAAADQShTgAMvSs+eSP1yuq0vGjm3fPABAuXr3Tj75yaJgnjmzGFHd2Nhy0dutW7LddsV23HHF2tn331+sq72stbefey75xz9avmarrRatx72k15g9O3niiWT11Zf9fkaPLv6mmTmzmLJ8Se+nLV8DAAAAoBUowAGWpb5+6R8uz5xZjK4CALqO/fcvCt4//KGY0nx5p/ju0yfZccdiFPiy1t7+/veL65q9fdR2r17JOuss/TXWXHP5czXPZtP8N82KTFneGq8BAAAAsJIU4ADLUlOz9A+X+/Vr3zwAQLmmT09eeKEopAcPXrHXWNZa4m+91fL63W9n7W0AAACAd+lWdgCADq/5A+qWNH+4DAB0HffdV+w33TRZbbUVe43mdbPHjSu+UJcU+3HjiuebR1O39WsAAAAAVBgjwAGWpfnD5aRY87u+vvhw+aijiuetbQkAXUtzAb7ttiv3Ov+3bnbT2LFZ8Npr6bbGGqmy9jYAAADASjECHGB5NH+4PH168vzzybPPFut3mlYUALqe++8v9sOGrfxrVVdnzptv5l8vv5w5b765YqO2q6uLv0n69y/2Rn4DAAAAXZgCHGB5NX+4vOaaydZbJyNGJLfdVnYqAKC9tdYI8LeZO3duq70WAAAAQFemAAd4r1ZZJfnoR4vjn/+81CgAQDubPTv597+L41YswAEAAABoHQpwgBVx4IHF/qqrknnzys0CALSfBx4o9uuuW0w5DgAAAECHogAHWBG77Zass05SX5/ccEPZaQCA9tIG058DAAAA0HoU4AAronv35MtfLo5Ngw4AXYcCHAAAAKBDU4ADrKjmadCvvz6ZNavcLABA+1CAAwAAAHRoCnCAFbXttsmQIcncucmvflV2GgCgrc2fn/zzn8WxAhwAAACgQ1KAA6yoqqpFo8BNgw4Ale+f/0waG5M11kg23LDsNAAAAAC0QAEOsDK+8pVif/PNyYsvlpsFAGhbzdOfDxtWfBEOAAAAgA5HAQ6wMgYNSnbYIVmwILnyyrLTAABtyfrfAAAAAB2eAhxgZTWPAjcNOgBUNgU4AAAAQIenAAdYWV/6UtK9e/L3vyf//nfZaQCAtrBgQfLAA8WxAhwAAACgw1KAA6ys978/+cQnimOjwAGgMj3+ePL660nv3smQIWWnAQAAAGAJFOAAraF5GvTLL0+amsrNAgC0vubpz7feOunRo9wsAAAAACyRAhygNXz2s0mfPsUU6HffXXYaAKC1Wf8bAAAAoFNQgAO0htVXTz7zmeLYNOgAUHnuv7/YDxtWZgoAAAAAlkEBDtBamqdBv/LK5K23ys0CALSepiYjwAEAAAA6CQU4QGvZc89kzTWTF19Mbrml7DQAQGt54YVkxoykW7dk6NCy0wAAAACwFApwgNbSq1ey//7FsWnQAaByNI/+3myzZNVVy80CAAAAwFIpwAFaU/M06Ndck8yZU24WAKB1mP4cAAAAoNNQgAO0pp12SjbYIJk9O/ntb8tOAwC0BgU4AAAAQKehAAdoTd26LRoFbhp0AKgMCnAAAACATkMBDtDamgvwqVOT114rNwsAsHLq65MnnyyOhw0rMwkAAAAAy0EBDtDahg4ttvnzk6uvLjsNALAy7r+/2G+4YbLmmqVGAQAAAGDZFOAAbeHAA4u9adABoHMz/TkAAABAp6IAB2gLBxxQ7P/0p+TZZ8vNAgCsOAU4AAAAQKeiAAdoCxtumOyyS9LUlFxxRdlpAIAVpQAHAAAA6FQU4ABtxTToANC5zZmTPPJIcawABwAAAOgUunwBfu211+azn/1shg4dmuHDh+eQQw7J3LlzF56/5ZZbsu+++2bo0KHZc889c80115SYFuhU9tsv6dkzuf/+5OGHy04DALxX//hH8tZbyfvel6y7btlpAAAAAFgOXboAv+CCC3LaaadlxIgRueSSSzJ+/Pist956eeutt5Ikd999d0aNGpVhw4ZlypQp2XvvvTN27NjceOONJScHOoW11kr22qs4vvzycrMAAO/d26c/r6oqNwsAAAAAy6VH2QHKMm3atJx33nk5//zzs9tuuy18fs8991x4fMEFF2TrrbfO+PHjkyQ77LBDnnnmmdTV1WWv5lILYGkOPDC5/vpiGvQJE3x4DgCdifW/AQAAADqdLjsC/Fe/+lXWW2+9xcrvt5s/f37uvPPOdxXdI0aMyBNPPJFnn322PWICnd2nP52stlry1FPJ3/5WdhoA4L1oLsCHDSs1BgAAAADLr8uOAH/ggQey6aab5vzzz8/PfvazzJ49O1tttVXGjBmTbbbZJk8//XQaGxuzySabLPZzgwYNSlKMIF9vvfVW6Hc3NTXljTfeWOn3wCJz5sxZbA8dSa9Pfzo9rrgijZdemkYfoNMJuccCXdJbb6XPgw+mKsmczTZLUxv+/e4+C9B23GMB2o57LEDbcp9dXFNTU6qWc5bdLluAv/TSS/nHP/6Rxx57LCeffHL69OmTCy+8MAcffHB+//vfZ+bMmUmSvn37LvZzzY+bz6+IxsbGPPLIIyseniV66qmnyo4A79L3Ix/JB664IvnlL/PIwQcnPbrsrZdOzj0W6Ep6P/lktpwzJ2/17p2HGxuTdvj73X0WoO24xwK0HfdYgLblPrtIr169luu6LtvCNI/CPvfcc7PZZpslSbbZZpvsvvvuueyyy7Lzzju32e/u2bNnBg8e3Gav3xXNmTMnTz31VDbaaKP06dOn7DiwuA98IE3jx6fnSy9ly+eey4J3LK0AHZ17LNAVdX/wweJgm22y+VZbtenvcp8FaDvusQBtxz0WoG25zy7u8ccfX+5ru2wB3rdv39TU1Cwsv5OkpqYmW2yxRR5//PF86lOfSpLMnj17sZ+bNWtWkqRfv34r/Lurqqqy6qqrrvDPs2R9+vTxz5aO6YADkkmT0vuaa5LPf77sNLBC3GOBLuXhh5Mk3T/4wXa797nPArQd91iAtuMeC9C23GcLyzv9eZJ0a8McHdrSRmDPmzcvG2ywQXr27Jlp06Ytdq758TvXBgdYqgMPLPa//nXS0FBqFABgOdx3X7HfdttycwAAAADwnnTZAvxjH/tY6uvrF1uL+7XXXss///nPbLnllunVq1eGDx+e3/3ud4v93NSpUzNo0KCst9567R0Z6Mw+/OFk0KDkjTeS664rOw0AsDRNTQpwAAAAgE6qyxbge+yxR4YOHZra2tpMnTo1N998c4444oj06tUrX/nKV5IkRx55ZO6///6ccsopufPOO1NXV5ff/va3Oeqoo0pOD3Q6VVXJ/91bcvnl5WYBAJbumWeSV19NevRI2nj9bwAAAABaV5ctwLt165aLLroow4YNy7hx43LsscdmtdVWy+WXX57+/fsnSbbffvtMmjQp99xzT77xjW/kt7/9bSZMmJC999675PRAp9Q8Dfo99xQfqgMAHVPz6O8ttkhWWaXcLAAAAAC8Jz3KDlCmNddcM9///veXes3HP/7xfPzjH2+nREBFGzIkueWWYjr0+vpk/vyksTGpri47GQDwdqY/BwAAAOi0uuwIcIB2N3ducuutyXrrFduAAcnEicXzAEDHoQAHAAAA6LS69AhwgHbT0FCU3aedtui5+vpk/PjiePRoI8EBoKNQgAMAAAB0WkaAA7SHnj2TurqWz9XVFecBgPK98kryzDPF8bBhpUYBAAAA4L1TgAO0h/r6YlvSuZkz2zEMALBEzaO/Bw1K+vYtNwsAAAAA75kCHKA91NQU25LO9evXjmEAgCUy/TkAAABAp6YAB2gPjY1JbW3L52pri/MAQPmaC3DTnwMAAAB0Sj3KDgDQJVRXJ2PGFMd1dcW05zU1yahRyXe+U5wHAMpnBDgAAABAp2YEOEB76d07GT06mT49mTEjef75ZLvtkuOOKzsZAJAkDQ3Jo48WxwpwAAAAgE5JAQ7Qnqqrk169kv79k+eeSz7/+eSCC5LHHy87GQDw0ENJU1MyYECyzjplpwEAAABgBSjAAcoyeHAyYkTxQXtdXdlpAADTnwMAAAB0egpwgDIdc0yx/9GPinXBAYDyKMABAAAAOj0FOECZPv7xZKutijVHL7mk7DQA0LUpwAEAAAA6PQU4QJmqqpJvfas4rqtL3nyz1DgA0GU1NhZrgCcKcAAAAIBOTAEOULYDD0ze977k6aeTa68tOw0AdE3/+lcyb16y+urJJpuUnQYAAACAFaQAByhb797JkUcWxz/4QalRAKDLap7+fNiwpJv/TAIAAADorHyyA9ARfPObSa9eyV//mtx1V9lpAKDrsf43AAAAQEVQgAN0BGuvnXz5y8WxUeAA0P4U4AAAAAAVQQEO0FF861vF/qqrkmefLTUKAHQpTU3J/fcXxwpwAAAAgE5NAQ7QUQwblnz0o8mbbybnnVd2GgDoOp58Mpk5s1iOZIstyk4DAAAAwEpQgAN0JM2jwC+6KGloKDUKAHQZzdOfb7VV0rNnuVkAAAAAWCkKcICOZJ99kkGDktdeS37607LTAEDXYP1vAAAAgIqhAAfoSLp3T44+ujj+wQ+SBQtKjQMAXUJzAT5sWKkxAAAAAFh5CnCAjuZrX0v69k0eeyy58cay0wBA5TMCHAAAAKBiKMABOprVV08OPbQ4PueccrMAQKWbPj154YWkqirZZpuy0wAAAACwkhTgAB3RqFFJt27JTTcl//hH2WkAoHLdf3+x/8AHktVWKzUKAAAAACtPAQ7QEW20UfL5zxfHP/hBmUkAoLKZ/hwAAACgoijAATqqY44p9pddlsyYUW4WAKhUCnAAAACAiqIAB+iodtwx+dCHknnzkh/+sOw0AFCZFOAAAAAAFUUBDtBRVVUtGgU+eXJRhAMArWf27OTf/y6OFeAAAAAAFUEBDtCR7bdfsu66yfTpyS9+UXYaAKgsDzxQ7NddN+nfv9wsAAAAALQKBThAR9azZzJqVHF8zjlJU1O5eQCgkpj+HAAAAKDiKMABOrrDDkv69Enuvz/505/KTgMAlUMBDgAAAFBxFOAAHd2aayZf/Wpx/IMflBoFACqKAhwAAACg4ijAATqDb32r2P/mN8njj5caBQAqwvz5yT//WRwrwAEAAAAqhgIcoDMYMiQZMaJYA7yuruw0AND5Pfpo8e/XwYOTDTcsOw0AAAAArUQBDtBZNI8C/9GPkvr6MpMAQOfW0JB84APFzCoPPJC88UbZiQAAAABoJQpwgM5ijz2SLbcsPrS/5JKy0wBA5zR3bjJxYrLOOskmmyTrrls8nju37GQAAAAAtAIFOEBnUVW1aBT4pEnJm2+WGgcAOp2GhuT005Px4xfNplJfXzw+/fTiPAAAAACdmgIcoDM58MBkxx2Tc88tCvAZM5L5831gDwDLo2fPpK6u5XN1dcV5AAAAADo1BThAZ9KnT3LDDcnddxdTtw4YUGymbgWgo2toKL60VeaXt159ddHI73eqr09mzmzPNAAAAAC0AQU4QGfS0JCcfXYyYYKpWwHoPJrX3W7+4lZ7f3nr+eeTo45KVl89qalp+ZqamqRfv/bJAwAAAECbUYADdCambgWgsylz3e05c5LvfjfZdNPkvPOSm24qivCW1NYmjY1tlwUAAACAdqEAB+hM6utN3QpA51LGl7eampKrrko23zw58cSiZN9hh2SjjZITTkjGjVs0Erympng8ZkxSXd36WQAAAABoVwpwgM6kpmbpU7dWVyd33dWOgQBgGZb15a2XX07+85/W+3333Zd89KPJ/vsXr7vuuslllyV//WuyzTZJ797J6NHJ9OnFeuTTpxePe/duvQwAAAAAlEYBDtCZNDYWU7S2ZNSo5Pe/T4YPT0aMSO65p32zAUBLlvXlrX79ku23T4YOTU45JXnwwWIE93s1fXpyyCHJBz+Y3HZb0qdPcvLJyaOPJgcemFRVLbq2ujrp1Svp37/YG/kNAAAAUDEU4ACdSXV1MUXrkqZuvffepHv35IYbijLh859P/vGPMhMD0NUt7ctbRx1V/Lurvr7499WppxajtDfdNDnuuGJWk3eW4Q0Nyfz5xejt+fOT2bOTiy9OPvCB5JJLiuu//OWi+D7lFOU2AAAAQBejAAfobJY0deuqqybjxyf/+ldy0EHFSLdrr0223jr5yleSxx4rOzkAXVF1dfKd7xRrcb/zy1snnJDsskvx77NLL00+85lklVWSxx9PJk4sZjXZYIPk6KOTv/89mTu3eH7AgEXbxInFz627bvHlr9tvT37+82T99ct81wAAAACURAEO0BktberWwYOTn/60GEn3xS8WI+GuuCLZfPPk4IOTp54qrnvnCLqGhlLeCgAVbsGCYgryD34wef75ltfdXmONZOTI5Ne/LtYE/8Uvki99KVltteTZZ5O6uuJnv/vd4stezWuK19cnEyYU56+/PrnzzuQjHynpjQIAAADQESjAASrVFlskv/xlct99yac/XRQQP/5xsu++RWHQ0gi6uXPLTg1Apbn66uQ3v0m++tXi3zPLWnd7tdWS/fdPrrwyeeml4mdHjUr22CM577yWf+a884qR4t385w0AAABAV+cTIoBKN2xYUR7ccUfyiU8kp52WnHXWu0fQjR+fnH66keAAtJ633irW4U6SY48tRnq/F717F1/imjSp+PdT87+33qm+Ppk5cyWCAgAAAFApFOAAXcXw4cnvf5/stdeSR9DV1SU9e7ZvLgAq15VXJo88UhTf3/rWyr1WTc2iNcRbOtev38q9PgAAAAAVQQEO0NXMnGkEHQBt7803k1NPLY6//e2VL6gbG5Pa2pbP1dYW5wEAAADo8nqUHQCAdtY8gq6lEtwIOgBay2WXJf/+d/K+9y25uH4vqquTMWOK47q64t9jNTXFa48ZU0yXDgAAAECXZwQ4QFdjBB0Aba2xMRk/vjg+7rhktdVa53V7905Gj06mT09mzCj2o0crvwEAAABYyAhwgK5mSSPoRo1Kjj22OA8AK+PHP06efDIZMCD55jdb97Wb/z3Vv3+x79WrdV8fAAAAgE7NCHCAruidI+heeCHZbrtkl12SRx4pOx0Andm8ecmECcXxmDHJqquWmwcAAACALkUBDtBVVVcXo+b6909WWSW55JLkoYeSww9PFiwoOx0AndXFFyfPPJMMHFj8OwUAAAAA2pECHICkqiqZPLkYpffnPyc/+lHZiQDojObMSb773eJ47FhrcwMAAADQ7hTgABQ23DA57bTi+DvfSV58sdw8AHQ+P/xhsazGBhsk3/hG2WkAAAAA6IIU4AAsUltbrAVeX58cc0zZaQDoTBoaktNPL45PPLFYXgMAAAAA2pkCHIBFevRILroo6dYtufLK5MYby04EQGcxeXIyY0ayySbJ175WdhoAAAAAuigFOACL++AHk6OPLo6PPLIY0QcASzN7djJxYnE8blzSs2e5eQAAAADoshTgALzb+PHF+q1PPZWcemrZaQDo6OrqkldeSTbdNDnwwLLTAAAAANCFKcABeLfVViumsk2Ss89O7r+/1DgAdGD19cmZZxbHJ59cLKcBAAAAACVRgAPQsn32SfbbL3nrreTQQ4s9ALzTD35QlOBbbJF86UtlpwEAAACgi1OAA7Bk556b9O2b3H33ohHhANDs1VeTc84pjk85JenevdQ4AAAAAKAAB2DJBg5Mvve94njs2OSZZ8rNA0DHcuaZyaxZydZbJ1/4QtlpAAAAAEABDsAyHHZY8pGPJK+/nhx1VNlpAOgoXnopqasrjk89NenmPy0AAAAAKJ9PqQBYum7dkh/+MOnRI7nuuuTaa8tOBEBHMHFi0tCQfPCDyWc+U3YaAAAAAEiiAAdgeWy1VTJ6dHE8alQx3S0AXdeLLyaTJxfH48cnVVXl5gEAAACA/6MAB2D5nHhiMnhw8vzzyQknlJ0GgDKdcUYyZ04yfHiy995lpwEAAACAhRTgACyfPn2SCy8sjs8/P7njjnLzAFCOZ59d9O+D004z+hsAAACADkUBDsDy+/jHk4MOSpqaksMOSxoby04EQHubNCmZNy/ZZZdkjz3KTgMAAAAAi1GAA/DenHVWstZayUMPJWefXXYaANpDQ0Myf34yfXoyblxy7bXJmWca/Q0AAABAh6MAB+C96d+/KME32yzZYotiFOCMGUUx0tBQdjoAWtvcucnEicmAAcnaayfrrZfce2+y9dZlJwMAAACAd1GAA/DejRyZ3H57ctddRRkyYECxTZxYFCUAdBzNo7dX5MtKDQ3J6acn48cn9fXFc/X1xdrfp5/ui08AAAAAdDgKcADeuzfeSM49N5kwYfFCZPx4hQhAR/L20dvL+2Wlt95K/vOf5C9/Sbp3T+rqWr6uri7p2bNtcgMAAADACupRdgAAOqGePZdeiIwd2755AHi3hoai7B4/ftFzzV9WSpIjjyxm85g2bfHtP/9JGhuTrbZKfvObRV90eqf6+mTmzGJpDAAAAADoIBTgALx39fUKEYCObllfVho9OjniiOTll999vlevpLq6GDFeU9PyPb+mJunXrxUDAwAAAMDKU4AD8N7V1ChEADq6ZX1Z6eWXkz33TJqakk02WXwbOLCY/ryhIamtXXwUebPa2mKkeK9ebfgmAAAAAOC9UYAD8N41NipEADq6ZX1ZaZ11kssuW/prVFcnY8YUx3V1xWvV1BT3+jFjkt69WzMxAAAAAKy0bmUHAKATai5Exo0ripCk2J94YnLsscV5AMrz8svF+t6jRrV8vvnLSsujd+9iuvTp05MZM4r96NHKbwAAAAA6JCPAAVgxzYXI2LHFmt+rr57ccEOyyy7JL36RbL552QkBuqb//Cf55CeTbt2SP/+52K/s6O3mLzb171/szfIBAAAAQAdlBDgAK666uihB+vdPVlklufji5KGHksMPTxYsKDsdQNfz0EPJRz6SPPZY8sYbyaxZRm8DAAAA0KUowAFoHVVVyfnnJ6uuWow4/NGPyk4E0LX8+c/FLBzPP59stVXy178mm2yy+JeVevWyTAUAAAAAFU0BDkDr2XDD5LTTiuPvfKcYaQhA2/v1r5NPfKJYkmKnnZLbbkvWXbfsVAAAAADQ7hTgALSu2tpku+2KtWaPOabsNACV7+KLky98IZk3L9l33+QPf0jWWKPsVAAAAABQCgU4AK2rR4/koouSbt2SK65Ibryx7EQAlampKZkwITn00GTBguQb30iuuSbp06fsZAAAAABQGgU4AK3vgx8sRoInyZFHJg0N5eYBqDRvvZUcdVRy0knF47FjkylTii8hAQAAAEAXpgAHoG2cdlqy/vrJU08lp55adhqAyjFvXvLlLyeTJydVVUldXTESvKqq7GQAAAAAUDoFOABtY7XVinImSc4+O7n//lLjAFSEWbOSESOSq65KevYslpo46qiyUwEAAABAh6EAB6DtfPrTyX77FVP1HnZYsQdg+TU0JPPnJzNmFCO///735Pnniy8ZTZ2afOlLZScEAAAAgA5FAQ5A2zr33KRv36K0Of/8stMAdB5z5yYTJyYDBhTb2msnf/xj8uc/J3fckeyxR9kJAQAAAKDDUYAD0LYGDkzOOKM4PuGE5Jlnys0D0Bk0NCSnn56MH5/U1xfP1dcXa31PmpRstFGJ4QAAAACg41KAA9D2Dj882XHH5PXXrVULsDx69kzq6lo+V1dXnAcAAAAA3kUBDkDb69YtueiipEeP5LrrkmuvLTsRQMdWX79o5HdL52bObMcwAAAAANB5KMABaB9bbZWMHl0cH3VUMmtWuXkAOrK+fZOampbP1dQk/fq1ZxoAAAAA6DQU4AC0nxNPTAYPTp57Lhk7tuw0AB3ThRcmv/tdMmpUy+dra5PGxvbNBAAAAACdhAIcgPbTp09R7CTJ5MnJnXeWmwego/ne95Ijj0yOPz757/9Oxo1bNBK8pqZ4PGZMUl1dZkoAAAAA6LAU4AC0r49/PDnooKSpKTnsMKMYAZLinjhmTFF8J8nnP19Mcz56dDJ9ejJjRrEfPTrp3bvcrAAAAADQgSnAAWh/Z52VrLVW8uCDydlnl50GoFwLFiT/9V/JGWcUj7/3veS7302qqoqR3r16Jf37F3sjvwEAAABgqRTgALS//v2LEjxJTj01mTat3DwAZWlsTEaOTC64oCi8L7ywGOUNAAAAAKwQBTgA5Rg5Mtl992TDDZPnnkvmzy+m+J0/P2loKDsdQNubOzfZb7/k8suTHj2K/eGHl50KAAAAADo1BTgA5aiqSi65JLnttuT3v08GDFi0TZxYFEMAlWr27GTEiOQ3vynW9L722uTLXy47FQAAAAB0ej3KDgBAF9a/f7HW7YQJi56rr0/Gjy+OR4+23i1QeV59Ndl77+Suu5LVVkuuvz756EfLTgUAAAAAFcEIcADK07NnMmlSy+fq6orzAJXkhReS3XYryu8110xuuUX5DQAAAACtSAEOQHnq64ttSedmzmzHMABt7Mknk513Tv7xj2SddYolID70obJTAQAAAEBFUYADUJ6ammJb0rl+/doxDEAramhI5s9PZsxYtP/615Np05JNNkn+8pdkyy3LTgkAAAAAFUcBDkB5GhuT2tqWz9XWFucBOpu5c5OJE5MBAxZtkyYlV12V7LNP8uc/FyU4AAAAANDqepQdAIAurLo6GTOmOK6rK6Y9r6lJRo1Kvv3t4jxAZ9LQUJTf48cveq6+PpkwIamqSn72syXPfAEAAAAArDQjwAEoV+/eyejRyfTpxRTBzz+fbLdd8rGPJU89VXY6gPemZ8/iCz0tmTQpWXXV9s0DAAAAAF2MAhyA8lVXJ716Jf37F/uzzkruuSf52teSBQvKTgew/Orri21J52bObMcwAAAAAND1KMAB6Fi6d08uvbQoxf/0p+Tcc8tOBLD8+vZd8hTnNTVJv37tmQYAAAAAuhwFOAAdz6BBydlnF8djxiQPP1xuHoBlmTEj+eIXk9/9Lhk1quVramuTxsb2zQUAAAAAXUyPsgMAQIsOPTT59a+TG25IDjooueOOYm1dgI6kqSn55S+T//qv5JVXkn/9K/nLX5Ju3Yq1wOvri5HftbXFF3p69y47MQAAAABUtC47AvxXv/pVhgwZ8q7tzDPPXHjNQQcd1OI1TzzxRInJAbqIqqrk4ouTNdZI7r03mTCh7EQAi5s+vRj1fcABRfm99dbJT39aTHM+enRxfsaMYj96tPIbAAAAANpBlx8BfvHFF2f11Vdf+HjAgAGLnd9uu+1y3HHHLfbceuut1y7ZALq8gQOTCy4oyqXvfjf51KeSD3+47FRAV9fUlPziF8VU56+8kvTokZxwQjJ2bNKrV3FNdXWx79+/2Dc/DwAAAAC0qS5fgG+55ZZZc801l3i+b9++GTZsWPsFAmBxX/pSMRX6lVcmI0cWo8FXXbXsVEBXNX168s1vJr/6VfF4m22SH/842XbbcnMBAAAAAEm68BToAHQikycn66yTPPposYYuQHtrakquuCLZYoui/O7RIznllOSuu5TfAAAAANCBdPkR4Pvss09ee+21DBw4MPvvv38OOeSQdO/efeH5u+66K8OGDctbb72VbbbZJkcffXQ+9KEPrdTvbGpqyhtvvLGy0XmbOXPmLLYHKkzv3ul2/vnp/bnPJXV1mfvJT2bBxz5Wdqouwz2WrqSqqiq93nwz3VZZJamvT2pqsmDWrDSdfHJ6nH9+kmTB0KGZ98MfpmmbbZI33yw2WAnuswBtxz0WoO24xwK0LffZxTU1NaWqqmq5rq1qampqauM8HdKf//znPPDAA9lmm21SVVWVW265JVdccUW+/OUvZ9y4cUmSurq6DBw4MBtttFFmzJiRSy65JI8++mh+9rOfZdsVHOnz0EMPZf78+a35VgC6jA1OPz39r7km8wcMyMNXXpm3Vl+97EhABendu3c222ijdJs4MVV1dQsL8KZRo1JVW5umj30sL+y0U178+tfT1LNn2XEBAAAAoEvp1atXhg4duszrumwB3pLvfe97ufTSS/PHP/4x73//+991/o033sg+++yTQYMGZcqUKSv0Ox566KE0NTVl8ODBKxuXt5kzZ06eeuqpbLTRRunTp0/ZcYC28vrr6b3jjuk2bVre/MpXMn8F78W8N+6xdBWrvPVWup15ZqrGj3/XuaYTT8yCI4/MvJqa9g9GxXOfBWg77rEAbcc9FqBtuc8u7vHHH09VVdVyFeBdfgr0t9t7773zox/9KI888kiLBfiqq66a3XbbLb/73e9W6vdUVVVl1VVXXanXoGV9+vTxzxYq2aqrJj/9abLrrunx85+nxxe+kHz+82Wn6jLcY6l48+cndXUtnqo677x0P+mkrNqrVzuHoitxnwVoO+6xAG3HPRagbbnPFpZ3+vMk6daGOQCg9e20UzJ6dHF8+OHJ9Onl5gEqR319sS3p3MyZ7RgGAAAAAFgRCvC3mTp1arp3754tttiixfNvvPFG/vjHPy7X0HoA2tAppyRbb528/HJy6KGJ1TyAlfX668lqqyVLmuK8pibp1689EwEAAAAAK6DLToH+jW98I8OHD8+QIUOSJDfffHN++ctfZuTIkenfv3/uvvvuXHzxxfnEJz6RddddNzNmzMiPf/zjvPTSSzn33HNLTg/Qxa2ySvKznyXbb59cf33yk58kX/962amAzuqxx4rlFL773WTUqGTChHdfU1ubNDYmpkAHAAAAgA6tyxbgG2+8ca655pq8+OKLWbBgQTbaaKOccMIJOeigg5Ik/fv3T2NjY84555zU19enT58+2XbbbXPqqadm6623Ljk9ANl66+S005Ljj0+OPjrZffdkww3LTgV0Ntddl4wcmcyalZx1VnLDDUm3bsVa4PX1xcjv2tpkzJikd++y0wIAAAAAy9BlC/ATTzxxqec33HDDXHLJJe2UBoAV8u1vJ7/5TfLqq8l//pOss86iwqqxMamuLjsh0FG99VZy8snFqO8k2Xnn5Be/KO4bo0cnY8cWa37361fcT5TfAAAAANApWAMcgM6re/fkiiuS225L/vCHZMCARdvEicncuWUnBDqiV15JPvWpReV3bW1yyy3Fl2iSogTv1Svp37/Y+zINAAAAAHQaXXYEOAAVYq21ku99b/E1e+vrk/Hji+PRo5VXwCL33pt84QvJU08lffokU6YkBx5YdioAAAAAoJUYAQ5A59azZzJpUsvn6uqK8wBJcumlyU47FeX3Jpskd9yh/AYAAACACqMAB6Bzq68vtiWdmzmzHcMAHdL8+ck3v5l87WvF0ggjRiR3351svXXZyQAAAACAVqYAB6Bzq6kptiWd69evHcMApWtoKArvGTOK/auvJiNHJhdckFRVJaecklx/fbLGGmUnBQAAAADagAIcgM6tsTGprW35XG1tcR7oGubOTSZOTAYMWLSdc06xTMKHP1wU3yefnHTzJzAAAAAAVKoeZQcAgJVSXZ2MGVMc19UV057X1CSjRiX//d/FeaDyNTQU5ff48Yueq69PJkwojv/3f5P3va+UaAAAAABA+zH8BYDOr3fvZPToZPr0YtrjF15Ittsu2WWX5JFHyk4HtIeePYsvwbTkvPOSvn3bNw8AAAAAUAoFOACVobo66dUr6d8/WWWVZMqU5MEHk69/PXnrrbLTQWV757rbDQ3t+/tfey156aVixHdL6uuTmTPbMxEAAAAAUBIFOACVp6oqueiiYsTnnXcWawADbaOldbcnTiyeb0tvvZXceGOy//7JVlsVSx/U1LR8bU1N0q9f2+YBAAAAADoEBTgAlWm99RYV3yeemPzrX+XmgUrU0JCcfnqx7nbz6Ov6+uLx6ae3zUjwadOSk05KNtoo2Xvv5KqrkuefL77sctRRLf9MbW3S2Nj6WQAAAACADkcBDkDl+vrXkz33TObNSw4+2FTo0NqWtu52XV3SvXtRWC9YsHyvt6Sp1OfMSS67LNl992TQoGTChOTZZ5M11ihK7/vuK86dcEIybtyikeA1NcXjMWOKZRIAAAAAgIqnAAegclVVFWuBr7568re/JeeeW3YiqCz19Utfd/vFF5PPfKb4/+CHP1x8EeWcc5KbbkqmT1/8+iVNpV5fn+y2W3LQQcmttxb/v/7kJ5MrryxGftfVJcOGFa/Ru3cyenTx2jNmFPvRo4vnAQAAAIAuoUfZAQCgTa2/fnLWWclhhyVjxyaf/nTygQ+UnQoqQ/O62y2V4DU1yfvfn7z2WvLGG8nf/15sb9e/fzJ0aHLmmcm11yannbboXPNU6gsWFCO7jzmmmNXhq19NNtxwyZmaR3r371/se/Va4bcHAAAAAHQ+CnAAKt8hhyS//GUx6vTrX0/+9KdiamZg5cybl4waVUxJ/k61tUlTU/LUU8njjyf/+Efy0EOLtieeSF56KXnwwWTTTZNJk1r+Heedl7zwQrLvvkk3kxcBAAAAAEunAAeg8lVVJRdfnGy1VXL77UWhdvTRZaeCzu/73y+K7qT4/1V9fTHyu7a2WHe7eerxzTYrtv32W/SzDQ3Jww8Xa3m/9trSp1KfPXvRiG4AAAAAgKUwjAaArmHDDYtplpOimHv88XLzQGf3858XU5bvtluxxMB7XXe7ujr50IeSz32umCq9pqbl62pqkn79Wjs9AAAAAFChFOAAdB2HHZbsvnsyZ05y8MHF2sLAe/fYY8nhhxfH++2XrL9+sdZ2//7Fvnkd7uXV2LhoJPk71dYW5wEAAAAAloMCHICuo6oqueSSopz785+TyZPLTgSdz9y5yf77J6+/Xoz+PvnklX/N6upiZoZx4xaNBK+pKR6PGfPeC3UAAAAAoMtSgAPQtWy0UbFucZIcf3zyxBOlxoFO55hjkgceKEZ7//znSffurfO6vXsXU6e/16nUAQAAAADeRgEOQNdz+OHJxz6WvPFGcsghpkKH5fWLXyQXXljMpnDZZcnAga37+tXVKzeVOgAAAADQ5SnAAeh6unVbNBX6H/9YFHrA0v3738mhhxbHY8Ykn/xkuXkAAAAAAFqgAAega9p44+SMM4rj0aOTJ58sNw90ZM3rfs+eneyyS3LqqWUnAgAAAABokQIcgK7rm99Mdt01aWhIvvENU6HDkvz3fyf335+8733JFVckPXqUnQgAAAAAoEUKcAC6rm7dkh/9KOnTJ7n11uSii8pOBB3PVVcl559fHP/sZ8m665abBwAAAABgKRTgAHRtgwYVU6Fvtlmy3nrJvHnJjBnJ/PnFyHDoyp54opgdIUmOPz7Za69y8wAAAAAALIMCHABGjUpuvz25885k7bWTAQOKbeLEYu1j6IrmzVu07vdOOyWnnVZ2IgAAAACAZbKAIwDMmZOce24yYcKi5+rrk/Hji+PRo5Pq6lKiQWm+853k3nuTtday7jcAAAAA0GkYAQ4APXsmdXUtn6urK85DV3LNNcmkScXxT3+arL9+uXkAAAAAAJaTAhwA6uuLbUnnZs5sxzBQsmnTFq37/Z3vJCNGlJsHAAAAAOA9UIADQE1NsS3pXL9+7RgGSjRvXvKlLxVf+thxx+S73y07EQAAAADAe6IAB4DGxqS2tuVzo0Yld92VzJrVvpmgvTQ0JPPnJzNmJAsWJGPHJsOHJ1deafp/AAAAAKDTUYADQHV1MmZMMm7copHgNTXF49ra5NBDk498JHnqqRJDQgveXl7Pn188fi/mzk0mTkwGDCi2gQOTe+5Jbrop2WCDtskMAAAAANCGFOAAkCS9eyejRyfTpxdl4vTpix7PnJn885/FqNg77ig7KRTeWV4PGFA8njt3+X6+oSE5/fRk/Phirfuk2E+YkHz/+++9TAcAAAAA6AB6lB0AADqM6upi379/se/VK9lqq2IK9E9/Orn//uSjH01+8pPkgANKCgkpyumJE4vyull9ffG4qan432tdXTEqfN68d++rq5Pf/ra4piV1dcVU6AAAAAAAnYwCHACWZb31kj//OfnKV5Lrr0++/OXksceSk05KqqrKTkdX1LPnksvrSZOS445Lbrwxefnllq/ZaqtipoPmkd/vVF9fzHzQ/GUQAAAAAIBOQgEOAMtjtdWSa68tpkU/++zk5JOLEvzii4vp06E91dcvvbyur0/OPDOZNStZZZViNoO376uri/W+a2pafp2amqRfvzYKDwAAAADQdhTgALC8undPzjorGTIk+eY3k8svT556qijGjZSlPa2++tLL6/79k69+demv0dCQ1NYuPo16s9rapLGxKMwBAAAAADqRbmUHAIBO57DDiuml+/VLbr89GT48efjhslPRVZx1VvL73yejRrV8vrm8Xpbq6mTMmGTcuKI0T4r9uHHF89XVrZUYAAAAAKDdGAEOACtijz2SO+5IPvWpZNq0ZMcdk6uvTj7xibKTUcmmTEm+/e1ks82K//1161asBV5fX5TXtbVFeb280/L37l1M6z92bLHmd79+RXluWn8AAAAAoJMyAhwAVtRmmyV33pnsvHOx1vIxxySPPprMn5/MmFHsGxrKTkmluPLK5PDDi+N990369i3K6+nTi/+9TZ9ePH6v5XV1dTHVef/+xd7IbwAAAACgE1OAA8DKeN/7kptuSo47Lrn11uSyy5IBAxZtEycmc+eWnZLO7vrrk4MOSpqakiOOSM44I6mqUl4DAAAAALyDKdABYGWtskpy0knJ976XTJiw6Pn6+mT8+OJ49Oj3Vk42NCQ9ey6a2rqxUbnZVd1yS/LFLyZvvpkceGAyeXJRfgMAAAAA8C5GgANAa+jZM5k0qeVzdXXFfvfdky99qVhv+Sc/Sf7yl2La6qamxa+fO7cYOW4kOXfcUUx3Pm9e8pnPJD/+cbHuNwAAAAAALTICHABaQ319sS3p3EsvFdutt777/OqrJ4MHF9uJJyZXX52cdtriP7+iI8npvB58MNl772I2gI9/vFgDvGfPslMBAAAAAHRohhABQGuoqSm2JZ1bZ53k+99PzjqrWMN5jz2SDTcsprKePTu5776iHB80aOkjyRWgXcNjjyWf/GTx5Ycdd0x+/eukd++yUwEAAAAAdHhGgANAa2hsTGprF43Ufrva2mL95r32Kra3mzcvmTYtefzx5LXXkldfXfpI8pkzk/79Wzs9HcnTTxdfkJg+PRk2LJk6NVlttbJTAQAAAAB0CgpwAGgN1dXJmDHFcV1dUVbX1BTl95gxSx69u8oqyeabF1uSzJ9f/FxLJXhNTdKvX6tHpwOZPr0ov595JhkyJPnd75Y8swAAAAAAAO9iCnQAaC29exdrdE+fnsyYUexHj35vU1c3jyRvSW1tcZ7K9OqrySc+kfz738X0+H/4Q/L+95edCgAAAACgUzECHABaU3V1sW+eprxXr/f+8y2NJB81asnFOJ3f7NnJiBHJQw8la6+d3HRTsv76ZacCAAAAAOh0jAAHgI7mnSPJX3wx2WGHZOedkwMPTBYsKDshK6uhoZjufsaMYn/vvcX67muuWYz8Hjy47IQAAAAAAJ2SAhwAOqLq6mL0eP/+xTrha62VTJuWXHddMm5c2elYGXPnJhMnJgMGLNpuuim57bbkj39Mttqq7IQAAAAAAJ2WAhwAOoMddkimTCmOv/vd5Mory83DimloSE4/PRk/vpjePin2EyYk552XbLJJmekAAAAAADo9BTgAdBYjRybf/nZx/PWvJ3ffXW4e3ruePYu13VtSV1ecBwAAAABghSnAAaAzOeOMZMSIYhrtz3wmef75shPxXtTXLxr53dK5mTPbMQwAAAAAQOVRgANAZ9K9e3LFFcnmmxfl9+c+l8yZU3YqlldNTbEt6Vy/fu0YBgAAAACg8ijAAaCz6ds3+c1vkjXWSO66Kzn00KSpqexULMuCBcmDDyajRrV8vrY2aWxs30wAAAAAABVGAQ4AndHgwcnVVxcjwi+/PJk4sexELE1TU1F8H3RQUXSPG7doJHhNTfF4zJikurrMlAAAAAAAnZ4CHAA6q913T+rqiuMxY5Lrry83Dy1rakqOOiq54ILk0UeT229PRo9Opk9PZswo9qNHJ717l50UAAAAAKDTU4ADQGf2zW8mRxxRlKxf+Uryj3+UnYi3a2pKjj46mTw5qapKfvSj5LOfLUZ69+qV9O9f7I38BgAAAABoFQpwAOjs6uqSj340ef31ZN99k5dfLjsRSVF+H3NMMmlS8fjii5Ovfa3USAAAAAAAlU4BDgCdXc+exXrgm2ySPPlk8sUvJo2NZafq2pqakmOPTc49t3g8ZUpy8MHlZgIAAAAA6AIU4ABQCdZaK/nNb5LVV0/++MektrbsRF1XU1Pyne8kP/hB8fiHP0wOOaTUSAAAAAAAXYUCHAAqxZZbJj//ebHW9B//mDz6aDJ/fjJjRrFvaCg7YeVrakqOOy4566zi8YUXJocdVm4mAAAAAIAuRAEOAJVkn32Siy5KbrstueyyZMCARdvEicncuWUnrFxNTcmYMcn3v188Pv/85PDDy80EAAAAANDF9Cg7AADQyr785eR730smTFj0XH19Mn58cTx6dFJdXUq0itXUlIwdW/xzT5LzzkuOPLLcTAAAAAAAXZAR4ABQaXr2TCZNavlcXV1xntbT1JScdFJy+unF47q65L/+q9xMAAAAAABdlAIcACpNfX2xLenczJntGKYCNTQsvrb6Qw8l11xTnPvBD5Kjjio1HgAAAABAV6YAB4BKU1NTbEs6169fO4apMHPnFmupv31t9auuKtZc/8lPkqOPLjshAAAAAECXpgAHgErT2JjU1rZ8btSoYgQz711DQzHN+fjxi0bY19cXa61PmpTst1+Z6QAAAAAAiAIcACpPdXUyZkwybtyikeA1NcmJJxbF+Fe/mrz0UpkJV8w7px5v7yK/Z89ife+WTJpkbXUAAAAAgA5AAQ4Alah372T06GT69KIwnj69KL+/8IXk+uuTPfZIXn657JTLr6WpxydOLJ5vL9ZWBwAAAADo8BTgAFCpqquTXr2S/v0X7adMSdZeO3nwwaIEf+WVslMu25KmHh8/vni+PUaCv/BCstpq1lYHAAAAAOjgFOAA0JUMGZLcemsxgvqBB4oS/NVXy061dEuberyurm2nHn/zzeScc5JNN03+8IdiDfWW1NYWa68DAAAAAFAqBTgAdDWbbVaU4O9/f3L//UUJ/tprZadasrKmHr/jjmT77ZNjj01efz258srk+OPfvbb6uHHFmuvV1W2TAwAAAACA5aYAB4CuaPPNixK8f//kvvuST3xiySVz2Wpqlj71+KqrJldckcyb1zq/79VXk8MPTz7ykWKU/BprJBddlFx+eVFyv3Nt9dGjizXXAQAAAAAonQIcALqqLbZIbrkled/7knvuST75yY5Zgr/wwpKnHh81qpia/CtfSTbaKPnud1d8XfOmpuSnPy1GyF90UfH4q19NHn00OfTQpNv//dn0zrXVjfwGAAAAAOgwFOAA0JVttVVRgq+1VvL3vyd77tl2U4qviBdeKMrt2trkpJPePfX4CSckL7+crLtu8uKLyYknJuuvnxx5ZFFcL69HHkk+9rGi8H7ppeLLAX/6U/KTnxRFNwAAAAAAnYICHAC6uqFDk5tvTtZcM7nrrmSvvZJZs8pOlSxYUBTSf/1rcthhyXe+8+6px/v0SQ45JHnyyeSyy5LttkvmzEkuvLAYyf3pTxdTvTc1Fa/Z0JDMn1+8xvz5xfs855xkm22KwrtPn+SMM4pp4Xfdtdz3DwAAAADAe6YABwCKAvjmm4v1ru+4oyjBZ89eeLp3GWtcn3VWMb15nz7J//xPsvrqS556vGfP5MADk7vvTv74x2TffZOqquS3v0123z3Zb7/i/UycmAwYsGj7/veT//f/kkGDirL84YeT444rXh8AAAAAgE5HAQ4AFIYNS266qSjB//a35Igjklmz0qdHj2y21lrp06NHMYK6Pfz978X05kly7rnJ5psv389VVSW77ZZcd13yr38l3/xmUaCPHFmU3+PHL1rnvL4+mTAhqatLbrgh+c1vinXEAQAAAADotBTgAMAi221XjLr+8IeTH/wg+f73UzVgQLqvs06qBgwoSuS5c9s2w+zZyZe/nLz5ZvKFLxRTnK+ITTdNJk9Onn02+eQnk/POa/m6885LBg5c8bwAAAAAAHQYCnAAYHEf/GDy618XI6MnTFh8xPT48cnpp7ftSPBRo5InnkjWXz+ZMqUY1b0y1lyzKNWb38c71dcnM2eu3O8AAAAAAKBDUIADAO+21lpLHjFdV1esud0WLr88+elPk27dkp//vJiOvTXU1BTbks7169c6vwcAAAAAgFIpwAGAd6uvb/8R09OmJUceWRyPG5fsvHPrvXZjY1Jb2/K52triPAAAAAAAnV6PsgMAAB1Q84jplkrwthgx3dhYrPs9e3ZRfI8d27qvX12djBlTHNfVFe+rpqYov8eMSXr3bt3fBwAAAABAKYwABwDebWkjpkeNSv75z+TNN1vv9518cnLXXUUpffnlSY82+I5e797J6NHJ9OnJjBnFfvRo5TcAAAAAQAUxAhwAeLcljZg+6qhi23XXZODA5Mork/79V+533XJLcsYZxfGUKckGG6zc6y1NdXWxb87cq1fb/S4AAAAAANqdEeAAQMv+b8R00/TpeevFF9M0fXpy3HHJvfcmzzxTFNfbb5/cc8+K/46XX07+3/9LmpqSQw9N9tuv9fIDAAAAANDlKMABgCWrrs6cN9/Mv15+OXPefLMYQb3nnsmddyYf+EDy9NPJTjsll1763l+7qSk5+ODkhReSzTZLzjmn9fMDAAAAANClKMABgGWaO3fu4k9suWWxZvc++yTz5iVf+1qxNvj8+cv/opMnJ9dfX0xDfuWVi6YnBwAAAACAFaQABwBWTE1Nct11ySmnFI8nT052370Y0b0sDz6YfPvbxfH3v59ss01bpQQAAAAAoAtRgAMAK65bt+Tkk4uR3P36Jbffnnzwg8lf/7rkn3njjeSAA4qR45/6VHLUUe2XFwAAAACAiqYABwBW3j77JH//e7LFFsUI8I9+NLnggmKd73c69tjkkUeSddZJfvzjpKqq3eMCAAAAAFCZFOAAQOv4wAeSO+9M9tsvaWxMvvnN5MQTk9mzi7XBZ8xI5s5N9tor2Xzz5Kc/Tfr3Lzs1AAAAAAAVRAEOALSe1VZLfvnL5HvfK0aDf+tbycSJyYABxbbOOsk99yR/+1uyxx5lpwUAAAAAoMIowAGA1lVVlYwendx4Y1JXl0yYkNTXF+fq64vHZ5+dNDSUmRIAAAAAgAqkAAcA2saAAcl557V8rq4u6dmzffMAAAAAAFDxFOAAQNuor1808rulczNntmMYAAAAAAC6AgU4ANA2amqKbUnn+vVrxzAAAAAAAHQFCnAAoG00Nia1tS2fq60tzgMAAAAAQCvqUXYAAKBCVVcnY8YUx3V1xbTnNTVF+T1mTNK7d5npAAAAAACoQApwAKDt9O6djB6djB1brPndr18x8lv5DQAAAABAG1CAAwBtq7q62PfvX+x79SovCwAAAAAAFc0a4AAAAAAAAABUBAU4AAAAAAAAABVBAQ4AAAAAAABARVCAAwAAAAAAAFARFOAAAAAAAAAAVAQFOAAAAAAAAAAVQQEOAAAAAAAAQEVQgAMAAAAAAABQERTgAAAAAAAAAFQEBTgAAAAAAAAAFUEBDgAAAAAAAEBF6LIF+K9+9asMGTLkXduZZ5652HVXXXVV9txzzwwdOjT77rtvbr311pISAwAAAAAAALA0PcoOULaLL744q6+++sLHAwYMWHj8v//7vznppJNyxBFHZIcddsjUqVMzatSoXH755Rk2bFgJaQEAAAAAAABYki5fgG+55ZZZc801WzxXV1eXT33qU/nWt76VJNlhhx3y2GOPZfLkyZkyZUo7pgQAAAAAAABgWbrsFOjL8swzz+Spp57K3nvvvdjzI0aMyN/+9rfMnz+/pGQAAAAAAAAAtKTLjwDfZ5998tprr2XgwIHZf//9c8ghh6R79+6ZNm1akmTjjTde7PpBgwalsbExzzzzTAYNGrRCv7OpqSlvvPHGSmdnkTlz5iy2B6D1uMcCtC33WYC24x4L0HbcYwHalvvs4pqamlJVVbVc13bZArx///456qijss0226Sqqiq33HJLfvCDH2T69OkZN25cZs6cmSTp27fvYj/X/Lj5/IpobGzMI488suLhWaKnnnqq7AgAFcs9FqBtuc8CtB33WIC24x4L0LbcZxfp1avXcl3XZQvwXXbZJbvsssvCxzvvvHNWWWWVXHrppTniiCPa9Hf37NkzgwcPbtPf0dXMmTMnTz31VDbaaKP06dOn7DgAFcU9FqBtuc8CtB33WIC24x4L0LbcZxf3+OOPL/e1XbYAb8nee++dH/3oR3nkkUfSr1+/JMns2bPTv3//hdfMmjUrSRaeXxFVVVVZddVVVy4sLerTp49/tgBtxD0WoG25zwK0HfdYgLbjHgvQttxnC8s7/XmSdGvDHJ3aJptskiQL1wJvNm3atPTs2TPrr79+GbEAAAAAAAAAWAIF+NtMnTo13bt3zxZbbJH1118/G220UW688cZ3XbPjjjsu9xzzAAAAAAAAALSPLjsF+je+8Y0MHz48Q4YMSZLcfPPN+eUvf5mRI0cunPL8qKOOyre//e1ssMEGGT58eKZOnZoHH3wwl112WZnRAQAAAAAAAGhBly3AN95441xzzTV58cUXs2DBgmy00UY54YQTctBBBy28Zp999smcOXMyZcqUXHTRRdl4441z3nnnZdttty0xOQAAAAAAAAAt6bIF+Iknnrhc133xi1/MF7/4xTZOAwAAAAAAAMDKsgY4AAAAAAAAABVBAQ4AAAAAAABARVCAAwAAAAAAAFARFOAAAAAAAAAAVAQFOAAAAAAAAAAVQQEOAAAAAAAAQEVQgAMAAAAAAABQERTgAAAAAAAAAFQEBTgAAAAAAAAAFUEBDgAAAAAAAEBFUIADAAAAAAAAUBEU4AAAAAAAAABUBAU4AAAAAAAAABVBAQ4AAAAAAABARVCAAwAAAAAAAFARFOAAAAAAAAAAVAQFOAAAAAAAAAAVQQEOAAAAAAAAQEVQgAMAAAAAAABQERTgAAAAAAAAAFQEBTgAAAAAAAAAFaGqqampqewQXcm9996bpqam9OrVq+woFaWpqSmNjY3p2bNnqqqqyo4DUFHcYwHalvssQNtxjwVoO+6xAG3LfXZx8+fPT1VVVbbbbrtlXtujHfLwNv4H2jaqqqp8qQCgjbjHArQt91mAtuMeC9B23GMB2pb77OKqqqqWu2c1AhwAAAAAAACAimANcAAAAAAAAAAqggIcAAAAAAAAgIqgAAcAAAAAAACgIijAAQAAAAAAAKgICnAAAAAAAAAAKoICHAAAAAAAAICKoAAHAAAAAAAAoCIowAEAAAAAAACoCApwAAAAAAAAACqCAhwAAAAAAACAiqAABwAAAAAAAKAiKMABAAAAAAAAqAgKcDq1J554Il//+tczbNiw7LTTTpk4cWLmz59fdiyATueGG27IkUcemV133TXDhg3LZz7zmVx99dVpampa7Lqrrroqe+65Z4YOHZp99903t956a0mJATqvhoaG7LrrrhkyZEgeeuihxc65zwKsmGuvvTaf/exnM3To0AwfPjyHHHJI5s6du/D8Lbfckn333TdDhw7NnnvumWuuuabEtACdy80335wvfvGL2XbbbbPzzjvn6KOPzjPPPPOu6/wtC7B0//nPfzJu3Lh85jOfyRZbbJF99tmnxeuW5346e/bsnHDCCfnwhz+cbbfdNrW1tZkxY0Zbv4VOQwFOpzVz5sx89atfTWNjYyZNmpRjjjkmv/zlL3PGGWeUHQ2g0/nJT36SPn365Pjjj88FF1yQXXfdNSeddFImT5688Jr//d//zUknnZS99947U6ZMybBhwzJq1Kjcf//95QUH6ITOP//8vPXWW+963n0WYMVccMEFOe200zJixIhccsklGT9+fNZbb72F99q77747o0aNyrBhwzJlypTsvffeGTt2bG688caSkwN0fHfeeWdGjRqVwYMHZ/LkyTnhhBPyr3/9KwcffPBiXzTytyzAsv373//On/70p2y44YYZNGhQi9cs7/30W9/6Vm6//faccsopOfPMM/Pkk0/m0EMPzZtvvtkO76Tjq2p659Au6CR++MMf5sILL8ytt96ampqaJMkvfvGLnHrqqbn11lszYMCAcgMCdCKvvvpq1lxzzcWeO+mkkzJ16tT8/e9/T7du3bLnnntmq622yllnnbXwmgMOOCCrr756pkyZ0t6RATqlJ554Ivvtt1+OO+64nHzyybn66qszdOjQJHGfBVgB06ZNy6c//emcf/752W233Vq85hvf+EYaGhpy5ZVXLnzuv//7v/PII49k6tSp7RUVoFMaN25cbr/99tx0002pqqpKktxxxx356le/mssvvzzbb799En/LAiyPBQsWpFu3Ymzy8ccfn3/84x/57W9/u9g1y3M/ve+++3LAAQfkkksuyc4775yk+Lt4xIgROfvsszNixIh2ekcdlxHgdFq33XZbdtxxx4Xld5LsvffeWbBgQW6//fbyggF0Qu8sv5Nk8803z+uvv5433ngjzzzzTJ566qnsvffei10zYsSI/O1vf7P8BMBymjBhQg444IBsvPHGiz3vPguwYn71q19lvfXWW2L5PX/+/Nx5553Za6+9Fnt+xIgReeKJJ/Lss8+2R0yATuvNN99MdXX1wvI7SVZfffUkWbhsmr9lAZZPc/m9JMt7P73tttvSt2/f7LTTTguv2WSTTbL55pvntttua/3gnZACnE5r2rRp2WSTTRZ7rm/fvunfv3+mTZtWUiqAynHPPfdkwIABWW211RbeV99Z2AwaNCiNjY0trv0FwOJuvPHGPPbYY/mv//qvd51znwVYMQ888EA23XTTnH/++dlxxx2z1VZb5YADDsgDDzyQJHn66afT2Nj4rs8Pmqec9PkBwNJ9/vOfzxNPPJHLL788s2fPzjPPPJOzzz47W2yxRbbbbrsk/pYFaC3Lez+dNm1aNt5448W+nJQUJbi/bwsKcDqtWbNmpW/fvu96vl+/fpk5c2YJiQAqx913352pU6fm4IMPTpKF99V33nebH7vvAizdnDlzcsYZZ+SYY47Jaqut9q7z7rMAK+all17KX/7yl1x33XU5+eSTM3ny5FRVVeXggw/OK6+84v4KsJK23377nHfeeTnrrLOy/fbbZ4899sgrr7ySKVOmpHv37kn8LQvQWpb3fjpr1qyFs3G8nX5sEQU4ALCYF198Mcccc0yGDx+ekSNHlh0HoCJccMEFWWuttfKFL3yh7CgAFaWpqSlvvPFGzj333Oy1117ZbbfdcsEFF6SpqSmXXXZZ2fEAOr177703o0ePzv77759LL7005557bhYsWJDDDjssc+fOLTseALRIAU6n1bdv38yePftdz8+cOTP9+vUrIRFA5zdr1qwceuihqampyaRJkxauS9N8X33nfXfWrFmLnQfg3Z577rn86Ec/Sm1tbWbPnp1Zs2bljTfeSJK88cYbaWhocJ8FWEF9+/ZNTU1NNttss4XP1dTUZIsttsjjjz/u/gqwkiZMmJAddtghxx9/fHbYYYfstddeueiii/Lwww/nuuuuS+IzA4DWsrz30759++b1119/18/rxxZRgNNptbSWwezZs/PSSy+9a20vAJZt7ty5OfzwwzN79uxcfPHFi02j03xffed9d9q0aenZs2fWX3/9ds0K0Jk8++yzaWxszGGHHZYPfehD+dCHPpQjjjgiSTJy5Mh8/etfd58FWEGDBw9e4rl58+Zlgw02SM+ePVu8vybx+QHAMjzxxBOLfckoSdZee+2sscYaefrpp5P4zACgtSzv/XSTTTbJk08+maampsWue/LJJ/19+38U4HRau+66a/76178u/OZLktx4443p1q1bdtpppxKTAXQ+b775Zr71rW9l2rRpufjiizNgwIDFzq+//vrZaKONcuONNy72/NSpU7PjjjumV69e7RkXoFPZfPPN89Of/nSxbcyYMUmSU089NSeffLL7LMAK+tjHPpb6+vo88sgjC5977bXX8s9//jNbbrllevXqleHDh+d3v/vdYj83derUDBo0KOutt157RwboVAYOHJiHH354seeee+65vPbaa1l33XWT+MwAoLUs7/101113zcyZM/O3v/1t4TVPPvlkHn744ey6667tmrmj6lF2AFhRBxxwQH72s5/lv/7rv3L44Ydn+vTpmThxYg444IB3FTcALN2pp56aW2+9Nccff3xef/313H///QvPbbHFFunVq1eOOuqofPvb384GG2yQ4cOHZ+rUqXnwwQetrQiwDH379s3w4cNbPLfllltmyy23TBL3WYAVsMcee2To0KGpra3NMccck1VWWSUXXXRRevXqla985StJkiOPPDIjR47MKaeckr333jt33nlnfvvb3+acc84pOT1Ax3fAAQfkf/7nfzJhwoTsvvvuqa+vzwUXXJC11lore++998Lr/C0LsGxz5szJn/70pyTFl4lef/31hWX3hz/84ay55prLdT/ddttts/POO+eEE07Icccdl1VWWSXnnHNOhgwZkk9+8pOlvLeOpqrpnePjoRN54oknctppp+W+++5LdXV1PvOZz+SYY47xrUKA92j33XfPc8891+K5m2++eeHImKuuuipTpkzJ888/n4033jjHHntsPvaxj7VnVICKcOedd2bkyJG5+uqrM3To0IXPu88CvHevvvpqTj/99Nx6661pbGzM9ttvnzFjxiw2PfrNN9+cH/zgB3nyySczcODAHHbYYdlvv/1KTA3QOTQ1NeXKK6/MFVdckWeeeSbV1dUZNmxYjjnmmAwaNGixa/0tC7B0zz77bD7+8Y+3eO6nP/3pwi/PL8/9dPbs2Tn99NPzhz/8IW+++WZ23nnnnHjiiQaI/h8FOAAAAAAAAAAVwRrgAAAAAAAAAFQEBTgAAAAAAAAAFUEBDgAAAAAAAEBFUIADAAAAAAAAUBEU4AAAAAAAAABUBAU4AAAAAAAAABVBAQ4AAAAAAABARVCAAwAAAAAAAFARFOAAAAAAAAAAVIQeZQcAAAAAWsejjz6ayZMn56GHHsrLL7+cmpqaDB48OLvvvnsOOuigJMmFF16YwYMHZ4899ig5LQAAALS+qqampqayQwAAAAAr5957783IkSMzcODAfPazn03//v3zwgsv5IEHHsjTTz+dP/zhD0mSbbfdNnvuuWfOOOOMkhMDAABA6zMCHAAAACrAhRdemNVXXz1XX311+vbtu9i5V155paRUAAAA0L6sAQ4AAAAV4Omnn87gwYPfVX4nyVprrZUkGTJkSN54441ce+21GTJkSIYMGZLjjz9+4XXTp0/PmDFj8pGPfCRbbbVVPvWpT+Xqq69e7LXuvPPODBkyJFOnTs3ZZ5+dnXbaKcOGDcsRRxyRF154oW3fJAAAACyDEeAAAABQAdZdd93cd999eeyxx7Lpppu2eM3EiRNz4oknZuutt87++++fJNlggw2SJC+//HL233//VFVV5cADD8yaa66Z2267LWPHjs3rr7+er33ta4u91gUXXJCqqqoceuiheeWVV3LppZfma1/7Wq677rr07t27Td8rAAAALIk1wAEAAKAC3H777Tn00EOTJFtvvXU++MEPZscdd8zw4cPTs2fPhdctaQ3wsWPH5k9/+lOuv/76rLHGGgufP/bYY3PbbbflL3/5S3r37p0777wzI0eOzIABAzJ16tSsttpqSZIbbrgh3/rWtzJ27NiMHDmyHd4xAAAAvJsp0AEAAKAC7LTTTrnyyiuz++6751//+lcuvvjifOMb38iuu+6am2++eak/29TUlN///vfZfffd09TUlFdffXXhtvPOO2f27Nn55z//udjPfPazn11YfifJXnvtlf79++dPf/pTm7w/AAAAWB6mQAcAAIAKsfXWW+e8887L/Pnz869//Ss33XRTfvKTn+Too4/Or3/96wwePLjFn3v11Vcza9as/OIXv8gvfvGLJV7zdhtuuOFij6uqqrLhhhvmueeea503AwAAACtAAQ4AAAAVplevXtl6662z9dZbZ6ONNsqYMWNy4403ZtSoUS1ev2DBgiTJvvvum8997nMtXjNkyJA2ywsAAACtRQEOAAAAFWyrrbZKksyYMWOJ16y55pqprq7OggUL8pGPfGS5Xvc///nPYo+bmpryn//8R1EOAABAqawBDgAAABXgjjvuSFNT07ueb16Te5NNNkmSrLrqqpk1a9Zi13Tv3j177rlnfve73+Wxxx5712u8c/rzJPn1r3+d119/feHjG2+8MS+99FJ23XXXlXofAAAAsDKqmlr6r2MAAACgU9lnn30yZ86cfOITn8gmm2ySxsbG3Hvvvbnhhhuy9tpr59e//nX69u2bww47LH//+99TW1ub97///VlvvfWyzTbb5OWXX87++++fV199NV/84hczePDgzJw5M//85z/zt7/9LXfddVeS5M4778zIkSOz6aabpqqqKp///Ofzyiuv5NJLL83aa6+d6667Ln369Cn5nwYAAABdlQIcAAAAKsBtt92WG2+8Mffdd19efPHFNDY2ZuDAgdl1111z5JFHZq211kqSTJs2LePGjctDDz2UuXPn5nOf+1zOOOOMJMkrr7ySyZMn55ZbbsnLL7+cmpqaDB48OCNGjMj++++fZFEBfvbZZ+fRRx/N1VdfnYaGhuywww45+eSTM3DgwNL+GQAAAIACHAAAAFhuzQX4ueeem7322qvsOAAAALAYa4ADAAAAAAAAUBEU4AAAAAAAAABUBAU4AAAAAAAAABXBGuAAAAAAAAAAVAQjwAEAAAAAAACoCApwAAAAAAAAACqCAhwAAAAAAACAiqAABwAAAAAAAKAiKMABAAAAAAAAqAgKcAAAAAAAAAAqggIcAAAAAAAAgIqgAAcAAAAAAACgIvx/Fcqq95AEIX0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQo8TbhFCDMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_next,loss_steps_i,loss_steps_d  = pgd(mals[13:14].to(torch.float32).to(device), mals_y[13:14].to(device), model_AT_rFGSM, insertion_array, removal_array, k=1000, step_length=.002, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nrS8svYvGNW",
        "outputId": "bbea56f9-cab8-40eb-8f56-a09996dc3357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 0.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Convert the loss data to DataFrames for seaborn\n",
        "df_i = pd.DataFrame({\n",
        "    'Step': range(len(loss_steps_i)),\n",
        "    'Loss': loss_steps_i\n",
        "})\n",
        "\n",
        "df_d = pd.DataFrame({\n",
        "    'Step': range(len(loss_steps_d)),\n",
        "    'Loss': loss_steps_d\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 1, figsize=(20, 25))\n",
        "\n",
        "# Plot loss_steps_i\n",
        "sns.lineplot(data=df_i, x='Step', y='Loss', ax=axes[0], marker='o', color='blue')\n",
        "axes[0].set_title('Loss Steps I')\n",
        "axes[0].set_xlabel('Step')\n",
        "axes[0].set_ylabel('Loss')\n",
        "\n",
        "# Plot loss_steps_d\n",
        "sns.lineplot(data=df_d, x='Step', y='Loss', ax=axes[1], marker='o', color='red')\n",
        "axes[1].set_title('Loss Steps D')\n",
        "axes[1].set_xlabel('Step')\n",
        "axes[1].set_ylabel('Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "957dab94-12d4-4337-f01d-ac7f3335174e",
        "id": "DFWBmwEnBgUc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x2500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #loss = criterion(y_model, y.view(-1).long())\n",
        "        print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print(loss)\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "        #print(torch.abs(gradients).sum())\n",
        "\n",
        "        #pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "        #print('nonzero(grad4insertion)',torch.count_nonzero(grad4insertion))\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "        #print('nonzero(grad4removal)',torch.count_nonzero(grad4removal))\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  gradients * tensor\n",
        "        #print(torch.abs(gradients).sum())\n",
        "        #print('nonzero(gradients)',torch.count_nonzero(gradients))\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            #perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            #perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            #perturbation[torch.isnan(perturbation)] = 0.\n",
        "            #perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        if t==549:\n",
        "          a = x_next\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return a\n"
      ],
      "metadata": {
        "id": "j9d7zN33GqvA"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = pgd(mals[13:14].to(torch.float32).to(device), mals_y[13:14].to(device), model_AT_rFGSM, insertion_array, removal_array, k=1000, step_length=.001, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHXSqCtFGLc5",
        "outputId": "6cb318a4-e5cf-4864-fd6c-f50400647cb0"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********  0\n",
            "loss_mal :  tensor([38.1857], grad_fn=<NllLossBackward0>)\n",
            "***********  1\n",
            "loss_mal :  tensor([38.0078], grad_fn=<NllLossBackward0>)\n",
            "***********  2\n",
            "loss_mal :  tensor([37.8264], grad_fn=<NllLossBackward0>)\n",
            "***********  3\n",
            "loss_mal :  tensor([37.6393], grad_fn=<NllLossBackward0>)\n",
            "***********  4\n",
            "loss_mal :  tensor([37.4498], grad_fn=<NllLossBackward0>)\n",
            "***********  5\n",
            "loss_mal :  tensor([37.2559], grad_fn=<NllLossBackward0>)\n",
            "***********  6\n",
            "loss_mal :  tensor([37.0543], grad_fn=<NllLossBackward0>)\n",
            "***********  7\n",
            "loss_mal :  tensor([36.8450], grad_fn=<NllLossBackward0>)\n",
            "***********  8\n",
            "loss_mal :  tensor([36.6358], grad_fn=<NllLossBackward0>)\n",
            "***********  9\n",
            "loss_mal :  tensor([36.4265], grad_fn=<NllLossBackward0>)\n",
            "***********  10\n",
            "loss_mal :  tensor([36.2152], grad_fn=<NllLossBackward0>)\n",
            "***********  11\n",
            "loss_mal :  tensor([36.0035], grad_fn=<NllLossBackward0>)\n",
            "***********  12\n",
            "loss_mal :  tensor([35.7917], grad_fn=<NllLossBackward0>)\n",
            "***********  13\n",
            "loss_mal :  tensor([35.5800], grad_fn=<NllLossBackward0>)\n",
            "***********  14\n",
            "loss_mal :  tensor([35.3682], grad_fn=<NllLossBackward0>)\n",
            "***********  15\n",
            "loss_mal :  tensor([35.1561], grad_fn=<NllLossBackward0>)\n",
            "***********  16\n",
            "loss_mal :  tensor([34.9430], grad_fn=<NllLossBackward0>)\n",
            "***********  17\n",
            "loss_mal :  tensor([34.7298], grad_fn=<NllLossBackward0>)\n",
            "***********  18\n",
            "loss_mal :  tensor([34.5167], grad_fn=<NllLossBackward0>)\n",
            "***********  19\n",
            "loss_mal :  tensor([34.3069], grad_fn=<NllLossBackward0>)\n",
            "***********  20\n",
            "loss_mal :  tensor([34.1011], grad_fn=<NllLossBackward0>)\n",
            "***********  21\n",
            "loss_mal :  tensor([33.8870], grad_fn=<NllLossBackward0>)\n",
            "***********  22\n",
            "loss_mal :  tensor([33.6801], grad_fn=<NllLossBackward0>)\n",
            "***********  23\n",
            "loss_mal :  tensor([33.4754], grad_fn=<NllLossBackward0>)\n",
            "***********  24\n",
            "loss_mal :  tensor([33.2616], grad_fn=<NllLossBackward0>)\n",
            "***********  25\n",
            "loss_mal :  tensor([33.0346], grad_fn=<NllLossBackward0>)\n",
            "***********  26\n",
            "loss_mal :  tensor([32.8077], grad_fn=<NllLossBackward0>)\n",
            "***********  27\n",
            "loss_mal :  tensor([32.5807], grad_fn=<NllLossBackward0>)\n",
            "***********  28\n",
            "loss_mal :  tensor([32.3519], grad_fn=<NllLossBackward0>)\n",
            "***********  29\n",
            "loss_mal :  tensor([32.1223], grad_fn=<NllLossBackward0>)\n",
            "***********  30\n",
            "loss_mal :  tensor([31.8928], grad_fn=<NllLossBackward0>)\n",
            "***********  31\n",
            "loss_mal :  tensor([31.6631], grad_fn=<NllLossBackward0>)\n",
            "***********  32\n",
            "loss_mal :  tensor([31.4331], grad_fn=<NllLossBackward0>)\n",
            "***********  33\n",
            "loss_mal :  tensor([31.2031], grad_fn=<NllLossBackward0>)\n",
            "***********  34\n",
            "loss_mal :  tensor([30.9735], grad_fn=<NllLossBackward0>)\n",
            "***********  35\n",
            "loss_mal :  tensor([30.7443], grad_fn=<NllLossBackward0>)\n",
            "***********  36\n",
            "loss_mal :  tensor([30.5139], grad_fn=<NllLossBackward0>)\n",
            "***********  37\n",
            "loss_mal :  tensor([30.2834], grad_fn=<NllLossBackward0>)\n",
            "***********  38\n",
            "loss_mal :  tensor([30.0536], grad_fn=<NllLossBackward0>)\n",
            "***********  39\n",
            "loss_mal :  tensor([29.8244], grad_fn=<NllLossBackward0>)\n",
            "***********  40\n",
            "loss_mal :  tensor([29.5952], grad_fn=<NllLossBackward0>)\n",
            "***********  41\n",
            "loss_mal :  tensor([29.3672], grad_fn=<NllLossBackward0>)\n",
            "***********  42\n",
            "loss_mal :  tensor([29.1398], grad_fn=<NllLossBackward0>)\n",
            "***********  43\n",
            "loss_mal :  tensor([28.9128], grad_fn=<NllLossBackward0>)\n",
            "***********  44\n",
            "loss_mal :  tensor([28.6856], grad_fn=<NllLossBackward0>)\n",
            "***********  45\n",
            "loss_mal :  tensor([28.4578], grad_fn=<NllLossBackward0>)\n",
            "***********  46\n",
            "loss_mal :  tensor([28.2300], grad_fn=<NllLossBackward0>)\n",
            "***********  47\n",
            "loss_mal :  tensor([28.0022], grad_fn=<NllLossBackward0>)\n",
            "***********  48\n",
            "loss_mal :  tensor([27.7744], grad_fn=<NllLossBackward0>)\n",
            "***********  49\n",
            "loss_mal :  tensor([27.5466], grad_fn=<NllLossBackward0>)\n",
            "***********  50\n",
            "loss_mal :  tensor([27.3151], grad_fn=<NllLossBackward0>)\n",
            "***********  51\n",
            "loss_mal :  tensor([27.0798], grad_fn=<NllLossBackward0>)\n",
            "***********  52\n",
            "loss_mal :  tensor([26.8445], grad_fn=<NllLossBackward0>)\n",
            "***********  53\n",
            "loss_mal :  tensor([26.6092], grad_fn=<NllLossBackward0>)\n",
            "***********  54\n",
            "loss_mal :  tensor([26.3733], grad_fn=<NllLossBackward0>)\n",
            "***********  55\n",
            "loss_mal :  tensor([26.1380], grad_fn=<NllLossBackward0>)\n",
            "***********  56\n",
            "loss_mal :  tensor([25.9030], grad_fn=<NllLossBackward0>)\n",
            "***********  57\n",
            "loss_mal :  tensor([25.6681], grad_fn=<NllLossBackward0>)\n",
            "***********  58\n",
            "loss_mal :  tensor([25.4347], grad_fn=<NllLossBackward0>)\n",
            "***********  59\n",
            "loss_mal :  tensor([25.1963], grad_fn=<NllLossBackward0>)\n",
            "***********  60\n",
            "loss_mal :  tensor([24.9579], grad_fn=<NllLossBackward0>)\n",
            "***********  61\n",
            "loss_mal :  tensor([24.7195], grad_fn=<NllLossBackward0>)\n",
            "***********  62\n",
            "loss_mal :  tensor([24.4836], grad_fn=<NllLossBackward0>)\n",
            "***********  63\n",
            "loss_mal :  tensor([24.2510], grad_fn=<NllLossBackward0>)\n",
            "***********  64\n",
            "loss_mal :  tensor([24.0202], grad_fn=<NllLossBackward0>)\n",
            "***********  65\n",
            "loss_mal :  tensor([23.7905], grad_fn=<NllLossBackward0>)\n",
            "***********  66\n",
            "loss_mal :  tensor([23.5594], grad_fn=<NllLossBackward0>)\n",
            "***********  67\n",
            "loss_mal :  tensor([23.3275], grad_fn=<NllLossBackward0>)\n",
            "***********  68\n",
            "loss_mal :  tensor([23.0944], grad_fn=<NllLossBackward0>)\n",
            "***********  69\n",
            "loss_mal :  tensor([22.8645], grad_fn=<NllLossBackward0>)\n",
            "***********  70\n",
            "loss_mal :  tensor([22.6347], grad_fn=<NllLossBackward0>)\n",
            "***********  71\n",
            "loss_mal :  tensor([22.4100], grad_fn=<NllLossBackward0>)\n",
            "***********  72\n",
            "loss_mal :  tensor([22.1840], grad_fn=<NllLossBackward0>)\n",
            "***********  73\n",
            "loss_mal :  tensor([21.9498], grad_fn=<NllLossBackward0>)\n",
            "***********  74\n",
            "loss_mal :  tensor([21.7623], grad_fn=<NllLossBackward0>)\n",
            "***********  75\n",
            "loss_mal :  tensor([21.5759], grad_fn=<NllLossBackward0>)\n",
            "***********  76\n",
            "loss_mal :  tensor([21.3939], grad_fn=<NllLossBackward0>)\n",
            "***********  77\n",
            "loss_mal :  tensor([21.2121], grad_fn=<NllLossBackward0>)\n",
            "***********  78\n",
            "loss_mal :  tensor([21.0304], grad_fn=<NllLossBackward0>)\n",
            "***********  79\n",
            "loss_mal :  tensor([20.8486], grad_fn=<NllLossBackward0>)\n",
            "***********  80\n",
            "loss_mal :  tensor([20.6669], grad_fn=<NllLossBackward0>)\n",
            "***********  81\n",
            "loss_mal :  tensor([20.4926], grad_fn=<NllLossBackward0>)\n",
            "***********  82\n",
            "loss_mal :  tensor([20.3215], grad_fn=<NllLossBackward0>)\n",
            "***********  83\n",
            "loss_mal :  tensor([20.1504], grad_fn=<NllLossBackward0>)\n",
            "***********  84\n",
            "loss_mal :  tensor([19.9793], grad_fn=<NllLossBackward0>)\n",
            "***********  85\n",
            "loss_mal :  tensor([19.8082], grad_fn=<NllLossBackward0>)\n",
            "***********  86\n",
            "loss_mal :  tensor([19.6371], grad_fn=<NllLossBackward0>)\n",
            "***********  87\n",
            "loss_mal :  tensor([19.4785], grad_fn=<NllLossBackward0>)\n",
            "***********  88\n",
            "loss_mal :  tensor([19.3182], grad_fn=<NllLossBackward0>)\n",
            "***********  89\n",
            "loss_mal :  tensor([19.1584], grad_fn=<NllLossBackward0>)\n",
            "***********  90\n",
            "loss_mal :  tensor([19.0031], grad_fn=<NllLossBackward0>)\n",
            "***********  91\n",
            "loss_mal :  tensor([18.8478], grad_fn=<NllLossBackward0>)\n",
            "***********  92\n",
            "loss_mal :  tensor([18.7042], grad_fn=<NllLossBackward0>)\n",
            "***********  93\n",
            "loss_mal :  tensor([18.6333], grad_fn=<NllLossBackward0>)\n",
            "***********  94\n",
            "loss_mal :  tensor([18.4785], grad_fn=<NllLossBackward0>)\n",
            "***********  95\n",
            "loss_mal :  tensor([18.3599], grad_fn=<NllLossBackward0>)\n",
            "***********  96\n",
            "loss_mal :  tensor([18.2694], grad_fn=<NllLossBackward0>)\n",
            "***********  97\n",
            "loss_mal :  tensor([18.1180], grad_fn=<NllLossBackward0>)\n",
            "***********  98\n",
            "loss_mal :  tensor([18.0234], grad_fn=<NllLossBackward0>)\n",
            "***********  99\n",
            "loss_mal :  tensor([17.9095], grad_fn=<NllLossBackward0>)\n",
            "***********  100\n",
            "loss_mal :  tensor([17.8083], grad_fn=<NllLossBackward0>)\n",
            "***********  101\n",
            "loss_mal :  tensor([17.7656], grad_fn=<NllLossBackward0>)\n",
            "***********  102\n",
            "loss_mal :  tensor([17.6359], grad_fn=<NllLossBackward0>)\n",
            "***********  103\n",
            "loss_mal :  tensor([17.5547], grad_fn=<NllLossBackward0>)\n",
            "***********  104\n",
            "loss_mal :  tensor([17.4920], grad_fn=<NllLossBackward0>)\n",
            "***********  105\n",
            "loss_mal :  tensor([17.3832], grad_fn=<NllLossBackward0>)\n",
            "***********  106\n",
            "loss_mal :  tensor([17.3663], grad_fn=<NllLossBackward0>)\n",
            "***********  107\n",
            "loss_mal :  tensor([17.3377], grad_fn=<NllLossBackward0>)\n",
            "***********  108\n",
            "loss_mal :  tensor([17.2866], grad_fn=<NllLossBackward0>)\n",
            "***********  109\n",
            "loss_mal :  tensor([17.2539], grad_fn=<NllLossBackward0>)\n",
            "***********  110\n",
            "loss_mal :  tensor([17.2410], grad_fn=<NllLossBackward0>)\n",
            "***********  111\n",
            "loss_mal :  tensor([17.1920], grad_fn=<NllLossBackward0>)\n",
            "***********  112\n",
            "loss_mal :  tensor([17.1646], grad_fn=<NllLossBackward0>)\n",
            "***********  113\n",
            "loss_mal :  tensor([17.1712], grad_fn=<NllLossBackward0>)\n",
            "***********  114\n",
            "loss_mal :  tensor([17.1304], grad_fn=<NllLossBackward0>)\n",
            "***********  115\n",
            "loss_mal :  tensor([17.1048], grad_fn=<NllLossBackward0>)\n",
            "***********  116\n",
            "loss_mal :  tensor([17.1113], grad_fn=<NllLossBackward0>)\n",
            "***********  117\n",
            "loss_mal :  tensor([17.0705], grad_fn=<NllLossBackward0>)\n",
            "***********  118\n",
            "loss_mal :  tensor([17.0495], grad_fn=<NllLossBackward0>)\n",
            "***********  119\n",
            "loss_mal :  tensor([17.0499], grad_fn=<NllLossBackward0>)\n",
            "***********  120\n",
            "loss_mal :  tensor([17.0097], grad_fn=<NllLossBackward0>)\n",
            "***********  121\n",
            "loss_mal :  tensor([16.9948], grad_fn=<NllLossBackward0>)\n",
            "***********  122\n",
            "loss_mal :  tensor([16.9916], grad_fn=<NllLossBackward0>)\n",
            "***********  123\n",
            "loss_mal :  tensor([16.9514], grad_fn=<NllLossBackward0>)\n",
            "***********  124\n",
            "loss_mal :  tensor([16.9410], grad_fn=<NllLossBackward0>)\n",
            "***********  125\n",
            "loss_mal :  tensor([16.9332], grad_fn=<NllLossBackward0>)\n",
            "***********  126\n",
            "loss_mal :  tensor([16.8930], grad_fn=<NllLossBackward0>)\n",
            "***********  127\n",
            "loss_mal :  tensor([16.8872], grad_fn=<NllLossBackward0>)\n",
            "***********  128\n",
            "loss_mal :  tensor([16.8749], grad_fn=<NllLossBackward0>)\n",
            "***********  129\n",
            "loss_mal :  tensor([16.8347], grad_fn=<NllLossBackward0>)\n",
            "***********  130\n",
            "loss_mal :  tensor([16.8334], grad_fn=<NllLossBackward0>)\n",
            "***********  131\n",
            "loss_mal :  tensor([16.8166], grad_fn=<NllLossBackward0>)\n",
            "***********  132\n",
            "loss_mal :  tensor([16.7764], grad_fn=<NllLossBackward0>)\n",
            "***********  133\n",
            "loss_mal :  tensor([16.7803], grad_fn=<NllLossBackward0>)\n",
            "***********  134\n",
            "loss_mal :  tensor([16.7594], grad_fn=<NllLossBackward0>)\n",
            "***********  135\n",
            "loss_mal :  tensor([16.7227], grad_fn=<NllLossBackward0>)\n",
            "***********  136\n",
            "loss_mal :  tensor([16.7413], grad_fn=<NllLossBackward0>)\n",
            "***********  137\n",
            "loss_mal :  tensor([16.7011], grad_fn=<NllLossBackward0>)\n",
            "***********  138\n",
            "loss_mal :  tensor([16.6689], grad_fn=<NllLossBackward0>)\n",
            "***********  139\n",
            "loss_mal :  tensor([16.6830], grad_fn=<NllLossBackward0>)\n",
            "***********  140\n",
            "loss_mal :  tensor([16.6428], grad_fn=<NllLossBackward0>)\n",
            "***********  141\n",
            "loss_mal :  tensor([16.6151], grad_fn=<NllLossBackward0>)\n",
            "***********  142\n",
            "loss_mal :  tensor([16.6190], grad_fn=<NllLossBackward0>)\n",
            "***********  143\n",
            "loss_mal :  tensor([16.5782], grad_fn=<NllLossBackward0>)\n",
            "***********  144\n",
            "loss_mal :  tensor([16.5385], grad_fn=<NllLossBackward0>)\n",
            "***********  145\n",
            "loss_mal :  tensor([16.5539], grad_fn=<NllLossBackward0>)\n",
            "***********  146\n",
            "loss_mal :  tensor([16.5131], grad_fn=<NllLossBackward0>)\n",
            "***********  147\n",
            "loss_mal :  tensor([16.4723], grad_fn=<NllLossBackward0>)\n",
            "***********  148\n",
            "loss_mal :  tensor([16.4612], grad_fn=<NllLossBackward0>)\n",
            "***********  149\n",
            "loss_mal :  tensor([16.4479], grad_fn=<NllLossBackward0>)\n",
            "***********  150\n",
            "loss_mal :  tensor([16.4082], grad_fn=<NllLossBackward0>)\n",
            "***********  151\n",
            "loss_mal :  tensor([16.3923], grad_fn=<NllLossBackward0>)\n",
            "***********  152\n",
            "loss_mal :  tensor([16.3916], grad_fn=<NllLossBackward0>)\n",
            "***********  153\n",
            "loss_mal :  tensor([16.3508], grad_fn=<NllLossBackward0>)\n",
            "***********  154\n",
            "loss_mal :  tensor([16.3156], grad_fn=<NllLossBackward0>)\n",
            "***********  155\n",
            "loss_mal :  tensor([16.3258], grad_fn=<NllLossBackward0>)\n",
            "***********  156\n",
            "loss_mal :  tensor([16.2850], grad_fn=<NllLossBackward0>)\n",
            "***********  157\n",
            "loss_mal :  tensor([16.2441], grad_fn=<NllLossBackward0>)\n",
            "***********  158\n",
            "loss_mal :  tensor([16.2381], grad_fn=<NllLossBackward0>)\n",
            "***********  159\n",
            "loss_mal :  tensor([16.2191], grad_fn=<NllLossBackward0>)\n",
            "***********  160\n",
            "loss_mal :  tensor([16.1783], grad_fn=<NllLossBackward0>)\n",
            "***********  161\n",
            "loss_mal :  tensor([16.1635], grad_fn=<NllLossBackward0>)\n",
            "***********  162\n",
            "loss_mal :  tensor([16.1536], grad_fn=<NllLossBackward0>)\n",
            "***********  163\n",
            "loss_mal :  tensor([16.1127], grad_fn=<NllLossBackward0>)\n",
            "***********  164\n",
            "loss_mal :  tensor([16.0915], grad_fn=<NllLossBackward0>)\n",
            "***********  165\n",
            "loss_mal :  tensor([16.0881], grad_fn=<NllLossBackward0>)\n",
            "***********  166\n",
            "loss_mal :  tensor([16.0472], grad_fn=<NllLossBackward0>)\n",
            "***********  167\n",
            "loss_mal :  tensor([16.0199], grad_fn=<NllLossBackward0>)\n",
            "***********  168\n",
            "loss_mal :  tensor([16.0230], grad_fn=<NllLossBackward0>)\n",
            "***********  169\n",
            "loss_mal :  tensor([15.9864], grad_fn=<NllLossBackward0>)\n",
            "***********  170\n",
            "loss_mal :  tensor([15.9569], grad_fn=<NllLossBackward0>)\n",
            "***********  171\n",
            "loss_mal :  tensor([15.9571], grad_fn=<NllLossBackward0>)\n",
            "***********  172\n",
            "loss_mal :  tensor([15.9430], grad_fn=<NllLossBackward0>)\n",
            "***********  173\n",
            "loss_mal :  tensor([15.9051], grad_fn=<NllLossBackward0>)\n",
            "***********  174\n",
            "loss_mal :  tensor([15.8840], grad_fn=<NllLossBackward0>)\n",
            "***********  175\n",
            "loss_mal :  tensor([15.8916], grad_fn=<NllLossBackward0>)\n",
            "***********  176\n",
            "loss_mal :  tensor([15.8537], grad_fn=<NllLossBackward0>)\n",
            "***********  177\n",
            "loss_mal :  tensor([15.8158], grad_fn=<NllLossBackward0>)\n",
            "***********  178\n",
            "loss_mal :  tensor([15.8144], grad_fn=<NllLossBackward0>)\n",
            "***********  179\n",
            "loss_mal :  tensor([15.8022], grad_fn=<NllLossBackward0>)\n",
            "***********  180\n",
            "loss_mal :  tensor([15.7644], grad_fn=<NllLossBackward0>)\n",
            "***********  181\n",
            "loss_mal :  tensor([15.7440], grad_fn=<NllLossBackward0>)\n",
            "***********  182\n",
            "loss_mal :  tensor([15.7508], grad_fn=<NllLossBackward0>)\n",
            "***********  183\n",
            "loss_mal :  tensor([15.7129], grad_fn=<NllLossBackward0>)\n",
            "***********  184\n",
            "loss_mal :  tensor([15.6751], grad_fn=<NllLossBackward0>)\n",
            "***********  185\n",
            "loss_mal :  tensor([15.6763], grad_fn=<NllLossBackward0>)\n",
            "***********  186\n",
            "loss_mal :  tensor([15.6646], grad_fn=<NllLossBackward0>)\n",
            "***********  187\n",
            "loss_mal :  tensor([15.6319], grad_fn=<NllLossBackward0>)\n",
            "***********  188\n",
            "loss_mal :  tensor([15.6261], grad_fn=<NllLossBackward0>)\n",
            "***********  189\n",
            "loss_mal :  tensor([15.6426], grad_fn=<NllLossBackward0>)\n",
            "***********  190\n",
            "loss_mal :  tensor([15.6143], grad_fn=<NllLossBackward0>)\n",
            "***********  191\n",
            "loss_mal :  tensor([15.5802], grad_fn=<NllLossBackward0>)\n",
            "***********  192\n",
            "loss_mal :  tensor([15.6151], grad_fn=<NllLossBackward0>)\n",
            "***********  193\n",
            "loss_mal :  tensor([15.5807], grad_fn=<NllLossBackward0>)\n",
            "***********  194\n",
            "loss_mal :  tensor([15.5511], grad_fn=<NllLossBackward0>)\n",
            "***********  195\n",
            "loss_mal :  tensor([15.5531], grad_fn=<NllLossBackward0>)\n",
            "***********  196\n",
            "loss_mal :  tensor([15.5674], grad_fn=<NllLossBackward0>)\n",
            "***********  197\n",
            "loss_mal :  tensor([15.5330], grad_fn=<NllLossBackward0>)\n",
            "***********  198\n",
            "loss_mal :  tensor([15.5041], grad_fn=<NllLossBackward0>)\n",
            "***********  199\n",
            "loss_mal :  tensor([15.5303], grad_fn=<NllLossBackward0>)\n",
            "***********  200\n",
            "loss_mal :  tensor([15.4974], grad_fn=<NllLossBackward0>)\n",
            "***********  201\n",
            "loss_mal :  tensor([15.4705], grad_fn=<NllLossBackward0>)\n",
            "***********  202\n",
            "loss_mal :  tensor([15.4614], grad_fn=<NllLossBackward0>)\n",
            "***********  203\n",
            "loss_mal :  tensor([15.4762], grad_fn=<NllLossBackward0>)\n",
            "***********  204\n",
            "loss_mal :  tensor([15.4418], grad_fn=<NllLossBackward0>)\n",
            "***********  205\n",
            "loss_mal :  tensor([15.4120], grad_fn=<NllLossBackward0>)\n",
            "***********  206\n",
            "loss_mal :  tensor([15.4400], grad_fn=<NllLossBackward0>)\n",
            "***********  207\n",
            "loss_mal :  tensor([15.4130], grad_fn=<NllLossBackward0>)\n",
            "***********  208\n",
            "loss_mal :  tensor([15.3804], grad_fn=<NllLossBackward0>)\n",
            "***********  209\n",
            "loss_mal :  tensor([15.3700], grad_fn=<NllLossBackward0>)\n",
            "***********  210\n",
            "loss_mal :  tensor([15.3915], grad_fn=<NllLossBackward0>)\n",
            "***********  211\n",
            "loss_mal :  tensor([15.4630], grad_fn=<NllLossBackward0>)\n",
            "***********  212\n",
            "loss_mal :  tensor([15.4642], grad_fn=<NllLossBackward0>)\n",
            "***********  213\n",
            "loss_mal :  tensor([15.4300], grad_fn=<NllLossBackward0>)\n",
            "***********  214\n",
            "loss_mal :  tensor([15.4209], grad_fn=<NllLossBackward0>)\n",
            "***********  215\n",
            "loss_mal :  tensor([15.4309], grad_fn=<NllLossBackward0>)\n",
            "***********  216\n",
            "loss_mal :  tensor([15.3989], grad_fn=<NllLossBackward0>)\n",
            "***********  217\n",
            "loss_mal :  tensor([15.3768], grad_fn=<NllLossBackward0>)\n",
            "***********  218\n",
            "loss_mal :  tensor([15.4022], grad_fn=<NllLossBackward0>)\n",
            "***********  219\n",
            "loss_mal :  tensor([15.3680], grad_fn=<NllLossBackward0>)\n",
            "***********  220\n",
            "loss_mal :  tensor([15.3374], grad_fn=<NllLossBackward0>)\n",
            "***********  221\n",
            "loss_mal :  tensor([15.3360], grad_fn=<NllLossBackward0>)\n",
            "***********  222\n",
            "loss_mal :  tensor([15.3420], grad_fn=<NllLossBackward0>)\n",
            "***********  223\n",
            "loss_mal :  tensor([15.3145], grad_fn=<NllLossBackward0>)\n",
            "***********  224\n",
            "loss_mal :  tensor([15.2864], grad_fn=<NllLossBackward0>)\n",
            "***********  225\n",
            "loss_mal :  tensor([15.3137], grad_fn=<NllLossBackward0>)\n",
            "***********  226\n",
            "loss_mal :  tensor([15.2795], grad_fn=<NllLossBackward0>)\n",
            "***********  227\n",
            "loss_mal :  tensor([15.2512], grad_fn=<NllLossBackward0>)\n",
            "***********  228\n",
            "loss_mal :  tensor([15.2578], grad_fn=<NllLossBackward0>)\n",
            "***********  229\n",
            "loss_mal :  tensor([15.2640], grad_fn=<NllLossBackward0>)\n",
            "***********  230\n",
            "loss_mal :  tensor([15.2299], grad_fn=<NllLossBackward0>)\n",
            "***********  231\n",
            "loss_mal :  tensor([15.2094], grad_fn=<NllLossBackward0>)\n",
            "***********  232\n",
            "loss_mal :  tensor([15.2302], grad_fn=<NllLossBackward0>)\n",
            "***********  233\n",
            "loss_mal :  tensor([15.2033], grad_fn=<NllLossBackward0>)\n",
            "***********  234\n",
            "loss_mal :  tensor([15.1720], grad_fn=<NllLossBackward0>)\n",
            "***********  235\n",
            "loss_mal :  tensor([15.1678], grad_fn=<NllLossBackward0>)\n",
            "***********  236\n",
            "loss_mal :  tensor([15.1778], grad_fn=<NllLossBackward0>)\n",
            "***********  237\n",
            "loss_mal :  tensor([15.1436], grad_fn=<NllLossBackward0>)\n",
            "***********  238\n",
            "loss_mal :  tensor([15.1195], grad_fn=<NllLossBackward0>)\n",
            "***********  239\n",
            "loss_mal :  tensor([15.1462], grad_fn=<NllLossBackward0>)\n",
            "***********  240\n",
            "loss_mal :  tensor([15.1170], grad_fn=<NllLossBackward0>)\n",
            "***********  241\n",
            "loss_mal :  tensor([15.0871], grad_fn=<NllLossBackward0>)\n",
            "***********  242\n",
            "loss_mal :  tensor([15.0792], grad_fn=<NllLossBackward0>)\n",
            "***********  243\n",
            "loss_mal :  tensor([15.0914], grad_fn=<NllLossBackward0>)\n",
            "***********  244\n",
            "loss_mal :  tensor([15.0572], grad_fn=<NllLossBackward0>)\n",
            "***********  245\n",
            "loss_mal :  tensor([15.0326], grad_fn=<NllLossBackward0>)\n",
            "***********  246\n",
            "loss_mal :  tensor([15.0549], grad_fn=<NllLossBackward0>)\n",
            "***********  247\n",
            "loss_mal :  tensor([15.0229], grad_fn=<NllLossBackward0>)\n",
            "***********  248\n",
            "loss_mal :  tensor([14.9972], grad_fn=<NllLossBackward0>)\n",
            "***********  249\n",
            "loss_mal :  tensor([15.0025], grad_fn=<NllLossBackward0>)\n",
            "***********  250\n",
            "loss_mal :  tensor([15.0013], grad_fn=<NllLossBackward0>)\n",
            "***********  251\n",
            "loss_mal :  tensor([14.9732], grad_fn=<NllLossBackward0>)\n",
            "***********  252\n",
            "loss_mal :  tensor([14.9630], grad_fn=<NllLossBackward0>)\n",
            "***********  253\n",
            "loss_mal :  tensor([14.9741], grad_fn=<NllLossBackward0>)\n",
            "***********  254\n",
            "loss_mal :  tensor([14.9451], grad_fn=<NllLossBackward0>)\n",
            "***********  255\n",
            "loss_mal :  tensor([14.9290], grad_fn=<NllLossBackward0>)\n",
            "***********  256\n",
            "loss_mal :  tensor([14.9622], grad_fn=<NllLossBackward0>)\n",
            "***********  257\n",
            "loss_mal :  tensor([14.9294], grad_fn=<NllLossBackward0>)\n",
            "***********  258\n",
            "loss_mal :  tensor([14.8970], grad_fn=<NllLossBackward0>)\n",
            "***********  259\n",
            "loss_mal :  tensor([14.9087], grad_fn=<NllLossBackward0>)\n",
            "***********  260\n",
            "loss_mal :  tensor([14.9016], grad_fn=<NllLossBackward0>)\n",
            "***********  261\n",
            "loss_mal :  tensor([15.0348], grad_fn=<NllLossBackward0>)\n",
            "***********  262\n",
            "loss_mal :  tensor([14.9848], grad_fn=<NllLossBackward0>)\n",
            "***********  263\n",
            "loss_mal :  tensor([14.9796], grad_fn=<NllLossBackward0>)\n",
            "***********  264\n",
            "loss_mal :  tensor([14.9929], grad_fn=<NllLossBackward0>)\n",
            "***********  265\n",
            "loss_mal :  tensor([14.9609], grad_fn=<NllLossBackward0>)\n",
            "***********  266\n",
            "loss_mal :  tensor([14.9327], grad_fn=<NllLossBackward0>)\n",
            "***********  267\n",
            "loss_mal :  tensor([14.9633], grad_fn=<NllLossBackward0>)\n",
            "***********  268\n",
            "loss_mal :  tensor([14.9496], grad_fn=<NllLossBackward0>)\n",
            "***********  269\n",
            "loss_mal :  tensor([14.9176], grad_fn=<NllLossBackward0>)\n",
            "***********  270\n",
            "loss_mal :  tensor([14.9056], grad_fn=<NllLossBackward0>)\n",
            "***********  271\n",
            "loss_mal :  tensor([14.9184], grad_fn=<NllLossBackward0>)\n",
            "***********  272\n",
            "loss_mal :  tensor([14.8865], grad_fn=<NllLossBackward0>)\n",
            "***********  273\n",
            "loss_mal :  tensor([14.8606], grad_fn=<NllLossBackward0>)\n",
            "***********  274\n",
            "loss_mal :  tensor([14.8870], grad_fn=<NllLossBackward0>)\n",
            "***********  275\n",
            "loss_mal :  tensor([14.8584], grad_fn=<NllLossBackward0>)\n",
            "***********  276\n",
            "loss_mal :  tensor([14.8325], grad_fn=<NllLossBackward0>)\n",
            "***********  277\n",
            "loss_mal :  tensor([14.8293], grad_fn=<NllLossBackward0>)\n",
            "***********  278\n",
            "loss_mal :  tensor([14.8334], grad_fn=<NllLossBackward0>)\n",
            "***********  279\n",
            "loss_mal :  tensor([14.8015], grad_fn=<NllLossBackward0>)\n",
            "***********  280\n",
            "loss_mal :  tensor([14.7869], grad_fn=<NllLossBackward0>)\n",
            "***********  281\n",
            "loss_mal :  tensor([14.8069], grad_fn=<NllLossBackward0>)\n",
            "***********  282\n",
            "loss_mal :  tensor([14.7750], grad_fn=<NllLossBackward0>)\n",
            "***********  283\n",
            "loss_mal :  tensor([14.7513], grad_fn=<NllLossBackward0>)\n",
            "***********  284\n",
            "loss_mal :  tensor([14.7795], grad_fn=<NllLossBackward0>)\n",
            "***********  285\n",
            "loss_mal :  tensor([14.7536], grad_fn=<NllLossBackward0>)\n",
            "***********  286\n",
            "loss_mal :  tensor([14.7245], grad_fn=<NllLossBackward0>)\n",
            "***********  287\n",
            "loss_mal :  tensor([14.7130], grad_fn=<NllLossBackward0>)\n",
            "***********  288\n",
            "loss_mal :  tensor([14.7343], grad_fn=<NllLossBackward0>)\n",
            "***********  289\n",
            "loss_mal :  tensor([14.7045], grad_fn=<NllLossBackward0>)\n",
            "***********  290\n",
            "loss_mal :  tensor([14.8533], grad_fn=<NllLossBackward0>)\n",
            "***********  291\n",
            "loss_mal :  tensor([14.7929], grad_fn=<NllLossBackward0>)\n",
            "***********  292\n",
            "loss_mal :  tensor([14.8245], grad_fn=<NllLossBackward0>)\n",
            "***********  293\n",
            "loss_mal :  tensor([14.7923], grad_fn=<NllLossBackward0>)\n",
            "***********  294\n",
            "loss_mal :  tensor([14.7600], grad_fn=<NllLossBackward0>)\n",
            "***********  295\n",
            "loss_mal :  tensor([14.7631], grad_fn=<NllLossBackward0>)\n",
            "***********  296\n",
            "loss_mal :  tensor([14.7633], grad_fn=<NllLossBackward0>)\n",
            "***********  297\n",
            "loss_mal :  tensor([14.7482], grad_fn=<NllLossBackward0>)\n",
            "***********  298\n",
            "loss_mal :  tensor([14.7258], grad_fn=<NllLossBackward0>)\n",
            "***********  299\n",
            "loss_mal :  tensor([14.7492], grad_fn=<NllLossBackward0>)\n",
            "***********  300\n",
            "loss_mal :  tensor([14.7170], grad_fn=<NllLossBackward0>)\n",
            "***********  301\n",
            "loss_mal :  tensor([14.6849], grad_fn=<NllLossBackward0>)\n",
            "***********  302\n",
            "loss_mal :  tensor([14.6958], grad_fn=<NllLossBackward0>)\n",
            "***********  303\n",
            "loss_mal :  tensor([14.6900], grad_fn=<NllLossBackward0>)\n",
            "***********  304\n",
            "loss_mal :  tensor([14.6757], grad_fn=<NllLossBackward0>)\n",
            "***********  305\n",
            "loss_mal :  tensor([14.6572], grad_fn=<NllLossBackward0>)\n",
            "***********  306\n",
            "loss_mal :  tensor([14.6767], grad_fn=<NllLossBackward0>)\n",
            "***********  307\n",
            "loss_mal :  tensor([14.6447], grad_fn=<NllLossBackward0>)\n",
            "***********  308\n",
            "loss_mal :  tensor([14.6127], grad_fn=<NllLossBackward0>)\n",
            "***********  309\n",
            "loss_mal :  tensor([14.6256], grad_fn=<NllLossBackward0>)\n",
            "***********  310\n",
            "loss_mal :  tensor([14.6175], grad_fn=<NllLossBackward0>)\n",
            "***********  311\n",
            "loss_mal :  tensor([14.5855], grad_fn=<NllLossBackward0>)\n",
            "***********  312\n",
            "loss_mal :  tensor([14.5925], grad_fn=<NllLossBackward0>)\n",
            "***********  313\n",
            "loss_mal :  tensor([14.5865], grad_fn=<NllLossBackward0>)\n",
            "***********  314\n",
            "loss_mal :  tensor([14.5553], grad_fn=<NllLossBackward0>)\n",
            "***********  315\n",
            "loss_mal :  tensor([14.5597], grad_fn=<NllLossBackward0>)\n",
            "***********  316\n",
            "loss_mal :  tensor([14.5774], grad_fn=<NllLossBackward0>)\n",
            "***********  317\n",
            "loss_mal :  tensor([14.6689], grad_fn=<NllLossBackward0>)\n",
            "***********  318\n",
            "loss_mal :  tensor([14.6600], grad_fn=<NllLossBackward0>)\n",
            "***********  319\n",
            "loss_mal :  tensor([14.6278], grad_fn=<NllLossBackward0>)\n",
            "***********  320\n",
            "loss_mal :  tensor([14.6247], grad_fn=<NllLossBackward0>)\n",
            "***********  321\n",
            "loss_mal :  tensor([14.6315], grad_fn=<NllLossBackward0>)\n",
            "***********  322\n",
            "loss_mal :  tensor([14.6160], grad_fn=<NllLossBackward0>)\n",
            "***********  323\n",
            "loss_mal :  tensor([14.5850], grad_fn=<NllLossBackward0>)\n",
            "***********  324\n",
            "loss_mal :  tensor([14.6169], grad_fn=<NllLossBackward0>)\n",
            "***********  325\n",
            "loss_mal :  tensor([14.5847], grad_fn=<NllLossBackward0>)\n",
            "***********  326\n",
            "loss_mal :  tensor([14.5529], grad_fn=<NllLossBackward0>)\n",
            "***********  327\n",
            "loss_mal :  tensor([14.5546], grad_fn=<NllLossBackward0>)\n",
            "***********  328\n",
            "loss_mal :  tensor([14.5556], grad_fn=<NllLossBackward0>)\n",
            "***********  329\n",
            "loss_mal :  tensor([14.5430], grad_fn=<NllLossBackward0>)\n",
            "***********  330\n",
            "loss_mal :  tensor([14.5205], grad_fn=<NllLossBackward0>)\n",
            "***********  331\n",
            "loss_mal :  tensor([14.5440], grad_fn=<NllLossBackward0>)\n",
            "***********  332\n",
            "loss_mal :  tensor([14.5119], grad_fn=<NllLossBackward0>)\n",
            "***********  333\n",
            "loss_mal :  tensor([14.4800], grad_fn=<NllLossBackward0>)\n",
            "***********  334\n",
            "loss_mal :  tensor([14.4894], grad_fn=<NllLossBackward0>)\n",
            "***********  335\n",
            "loss_mal :  tensor([14.4870], grad_fn=<NllLossBackward0>)\n",
            "***********  336\n",
            "loss_mal :  tensor([14.4707], grad_fn=<NllLossBackward0>)\n",
            "***********  337\n",
            "loss_mal :  tensor([14.4515], grad_fn=<NllLossBackward0>)\n",
            "***********  338\n",
            "loss_mal :  tensor([14.4739], grad_fn=<NllLossBackward0>)\n",
            "***********  339\n",
            "loss_mal :  tensor([14.4417], grad_fn=<NllLossBackward0>)\n",
            "***********  340\n",
            "loss_mal :  tensor([14.4098], grad_fn=<NllLossBackward0>)\n",
            "***********  341\n",
            "loss_mal :  tensor([14.4213], grad_fn=<NllLossBackward0>)\n",
            "***********  342\n",
            "loss_mal :  tensor([14.4185], grad_fn=<NllLossBackward0>)\n",
            "***********  343\n",
            "loss_mal :  tensor([14.5363], grad_fn=<NllLossBackward0>)\n",
            "***********  344\n",
            "loss_mal :  tensor([14.5027], grad_fn=<NllLossBackward0>)\n",
            "***********  345\n",
            "loss_mal :  tensor([14.4806], grad_fn=<NllLossBackward0>)\n",
            "***********  346\n",
            "loss_mal :  tensor([14.5066], grad_fn=<NllLossBackward0>)\n",
            "***********  347\n",
            "loss_mal :  tensor([14.4787], grad_fn=<NllLossBackward0>)\n",
            "***********  348\n",
            "loss_mal :  tensor([14.4469], grad_fn=<NllLossBackward0>)\n",
            "***********  349\n",
            "loss_mal :  tensor([14.4432], grad_fn=<NllLossBackward0>)\n",
            "***********  350\n",
            "loss_mal :  tensor([14.4575], grad_fn=<NllLossBackward0>)\n",
            "***********  351\n",
            "loss_mal :  tensor([14.4253], grad_fn=<NllLossBackward0>)\n",
            "***********  352\n",
            "loss_mal :  tensor([14.3996], grad_fn=<NllLossBackward0>)\n",
            "***********  353\n",
            "loss_mal :  tensor([14.4262], grad_fn=<NllLossBackward0>)\n",
            "***********  354\n",
            "loss_mal :  tensor([14.3952], grad_fn=<NllLossBackward0>)\n",
            "***********  355\n",
            "loss_mal :  tensor([14.3695], grad_fn=<NllLossBackward0>)\n",
            "***********  356\n",
            "loss_mal :  tensor([14.3733], grad_fn=<NllLossBackward0>)\n",
            "***********  357\n",
            "loss_mal :  tensor([14.3731], grad_fn=<NllLossBackward0>)\n",
            "***********  358\n",
            "loss_mal :  tensor([14.3409], grad_fn=<NllLossBackward0>)\n",
            "***********  359\n",
            "loss_mal :  tensor([14.3338], grad_fn=<NllLossBackward0>)\n",
            "***********  360\n",
            "loss_mal :  tensor([14.3429], grad_fn=<NllLossBackward0>)\n",
            "***********  361\n",
            "loss_mal :  tensor([14.3175], grad_fn=<NllLossBackward0>)\n",
            "***********  362\n",
            "loss_mal :  tensor([14.2903], grad_fn=<NllLossBackward0>)\n",
            "***********  363\n",
            "loss_mal :  tensor([14.3206], grad_fn=<NllLossBackward0>)\n",
            "***********  364\n",
            "loss_mal :  tensor([14.2887], grad_fn=<NllLossBackward0>)\n",
            "***********  365\n",
            "loss_mal :  tensor([14.2568], grad_fn=<NllLossBackward0>)\n",
            "***********  366\n",
            "loss_mal :  tensor([14.2672], grad_fn=<NllLossBackward0>)\n",
            "***********  367\n",
            "loss_mal :  tensor([14.2623], grad_fn=<NllLossBackward0>)\n",
            "***********  368\n",
            "loss_mal :  tensor([14.4000], grad_fn=<NllLossBackward0>)\n",
            "***********  369\n",
            "loss_mal :  tensor([14.3492], grad_fn=<NllLossBackward0>)\n",
            "***********  370\n",
            "loss_mal :  tensor([14.3393], grad_fn=<NllLossBackward0>)\n",
            "***********  371\n",
            "loss_mal :  tensor([14.3573], grad_fn=<NllLossBackward0>)\n",
            "***********  372\n",
            "loss_mal :  tensor([14.3292], grad_fn=<NllLossBackward0>)\n",
            "***********  373\n",
            "loss_mal :  tensor([14.3029], grad_fn=<NllLossBackward0>)\n",
            "***********  374\n",
            "loss_mal :  tensor([14.3041], grad_fn=<NllLossBackward0>)\n",
            "***********  375\n",
            "loss_mal :  tensor([14.3038], grad_fn=<NllLossBackward0>)\n",
            "***********  376\n",
            "loss_mal :  tensor([14.2716], grad_fn=<NllLossBackward0>)\n",
            "***********  377\n",
            "loss_mal :  tensor([14.2609], grad_fn=<NllLossBackward0>)\n",
            "***********  378\n",
            "loss_mal :  tensor([14.2754], grad_fn=<NllLossBackward0>)\n",
            "***********  379\n",
            "loss_mal :  tensor([14.2497], grad_fn=<NllLossBackward0>)\n",
            "***********  380\n",
            "loss_mal :  tensor([14.2212], grad_fn=<NllLossBackward0>)\n",
            "***********  381\n",
            "loss_mal :  tensor([14.2493], grad_fn=<NllLossBackward0>)\n",
            "***********  382\n",
            "loss_mal :  tensor([14.2173], grad_fn=<NllLossBackward0>)\n",
            "***********  383\n",
            "loss_mal :  tensor([14.1884], grad_fn=<NllLossBackward0>)\n",
            "***********  384\n",
            "loss_mal :  tensor([14.1936], grad_fn=<NllLossBackward0>)\n",
            "***********  385\n",
            "loss_mal :  tensor([14.2085], grad_fn=<NllLossBackward0>)\n",
            "***********  386\n",
            "loss_mal :  tensor([14.1842], grad_fn=<NllLossBackward0>)\n",
            "***********  387\n",
            "loss_mal :  tensor([14.1561], grad_fn=<NllLossBackward0>)\n",
            "***********  388\n",
            "loss_mal :  tensor([14.1937], grad_fn=<NllLossBackward0>)\n",
            "***********  389\n",
            "loss_mal :  tensor([14.1639], grad_fn=<NllLossBackward0>)\n",
            "***********  390\n",
            "loss_mal :  tensor([14.1396], grad_fn=<NllLossBackward0>)\n",
            "***********  391\n",
            "loss_mal :  tensor([14.1491], grad_fn=<NllLossBackward0>)\n",
            "***********  392\n",
            "loss_mal :  tensor([14.1640], grad_fn=<NllLossBackward0>)\n",
            "***********  393\n",
            "loss_mal :  tensor([14.1346], grad_fn=<NllLossBackward0>)\n",
            "***********  394\n",
            "loss_mal :  tensor([14.1112], grad_fn=<NllLossBackward0>)\n",
            "***********  395\n",
            "loss_mal :  tensor([14.1461], grad_fn=<NllLossBackward0>)\n",
            "***********  396\n",
            "loss_mal :  tensor([14.1216], grad_fn=<NllLossBackward0>)\n",
            "***********  397\n",
            "loss_mal :  tensor([14.0927], grad_fn=<NllLossBackward0>)\n",
            "***********  398\n",
            "loss_mal :  tensor([14.0896], grad_fn=<NllLossBackward0>)\n",
            "***********  399\n",
            "loss_mal :  tensor([14.1188], grad_fn=<NllLossBackward0>)\n",
            "***********  400\n",
            "loss_mal :  tensor([14.2044], grad_fn=<NllLossBackward0>)\n",
            "***********  401\n",
            "loss_mal :  tensor([14.2024], grad_fn=<NllLossBackward0>)\n",
            "***********  402\n",
            "loss_mal :  tensor([14.1779], grad_fn=<NllLossBackward0>)\n",
            "***********  403\n",
            "loss_mal :  tensor([14.1701], grad_fn=<NllLossBackward0>)\n",
            "***********  404\n",
            "loss_mal :  tensor([14.1869], grad_fn=<NllLossBackward0>)\n",
            "***********  405\n",
            "loss_mal :  tensor([14.1571], grad_fn=<NllLossBackward0>)\n",
            "***********  406\n",
            "loss_mal :  tensor([14.1393], grad_fn=<NllLossBackward0>)\n",
            "***********  407\n",
            "loss_mal :  tensor([14.1636], grad_fn=<NllLossBackward0>)\n",
            "***********  408\n",
            "loss_mal :  tensor([14.1359], grad_fn=<NllLossBackward0>)\n",
            "***********  409\n",
            "loss_mal :  tensor([14.1140], grad_fn=<NllLossBackward0>)\n",
            "***********  410\n",
            "loss_mal :  tensor([14.1220], grad_fn=<NllLossBackward0>)\n",
            "***********  411\n",
            "loss_mal :  tensor([14.1244], grad_fn=<NllLossBackward0>)\n",
            "***********  412\n",
            "loss_mal :  tensor([14.1012], grad_fn=<NllLossBackward0>)\n",
            "***********  413\n",
            "loss_mal :  tensor([14.0855], grad_fn=<NllLossBackward0>)\n",
            "***********  414\n",
            "loss_mal :  tensor([14.1088], grad_fn=<NllLossBackward0>)\n",
            "***********  415\n",
            "loss_mal :  tensor([14.0839], grad_fn=<NllLossBackward0>)\n",
            "***********  416\n",
            "loss_mal :  tensor([14.0592], grad_fn=<NllLossBackward0>)\n",
            "***********  417\n",
            "loss_mal :  tensor([14.0635], grad_fn=<NllLossBackward0>)\n",
            "***********  418\n",
            "loss_mal :  tensor([14.0704], grad_fn=<NllLossBackward0>)\n",
            "***********  419\n",
            "loss_mal :  tensor([14.0462], grad_fn=<NllLossBackward0>)\n",
            "***********  420\n",
            "loss_mal :  tensor([14.0265], grad_fn=<NllLossBackward0>)\n",
            "***********  421\n",
            "loss_mal :  tensor([14.0598], grad_fn=<NllLossBackward0>)\n",
            "***********  422\n",
            "loss_mal :  tensor([14.0300], grad_fn=<NllLossBackward0>)\n",
            "***********  423\n",
            "loss_mal :  tensor([14.0015], grad_fn=<NllLossBackward0>)\n",
            "***********  424\n",
            "loss_mal :  tensor([14.0168], grad_fn=<NllLossBackward0>)\n",
            "***********  425\n",
            "loss_mal :  tensor([14.0170], grad_fn=<NllLossBackward0>)\n",
            "***********  426\n",
            "loss_mal :  tensor([13.9909], grad_fn=<NllLossBackward0>)\n",
            "***********  427\n",
            "loss_mal :  tensor([13.9774], grad_fn=<NllLossBackward0>)\n",
            "***********  428\n",
            "loss_mal :  tensor([14.0067], grad_fn=<NllLossBackward0>)\n",
            "***********  429\n",
            "loss_mal :  tensor([13.9771], grad_fn=<NllLossBackward0>)\n",
            "***********  430\n",
            "loss_mal :  tensor([13.9505], grad_fn=<NllLossBackward0>)\n",
            "***********  431\n",
            "loss_mal :  tensor([13.9597], grad_fn=<NllLossBackward0>)\n",
            "***********  432\n",
            "loss_mal :  tensor([13.9641], grad_fn=<NllLossBackward0>)\n",
            "***********  433\n",
            "loss_mal :  tensor([13.9376], grad_fn=<NllLossBackward0>)\n",
            "***********  434\n",
            "loss_mal :  tensor([13.9207], grad_fn=<NllLossBackward0>)\n",
            "***********  435\n",
            "loss_mal :  tensor([13.9538], grad_fn=<NllLossBackward0>)\n",
            "***********  436\n",
            "loss_mal :  tensor([13.9258], grad_fn=<NllLossBackward0>)\n",
            "***********  437\n",
            "loss_mal :  tensor([13.9016], grad_fn=<NllLossBackward0>)\n",
            "***********  438\n",
            "loss_mal :  tensor([13.9027], grad_fn=<NllLossBackward0>)\n",
            "***********  439\n",
            "loss_mal :  tensor([13.9112], grad_fn=<NllLossBackward0>)\n",
            "***********  440\n",
            "loss_mal :  tensor([13.8843], grad_fn=<NllLossBackward0>)\n",
            "***********  441\n",
            "loss_mal :  tensor([13.8621], grad_fn=<NllLossBackward0>)\n",
            "***********  442\n",
            "loss_mal :  tensor([13.8794], grad_fn=<NllLossBackward0>)\n",
            "***********  443\n",
            "loss_mal :  tensor([13.8673], grad_fn=<NllLossBackward0>)\n",
            "***********  444\n",
            "loss_mal :  tensor([13.8377], grad_fn=<NllLossBackward0>)\n",
            "***********  445\n",
            "loss_mal :  tensor([13.8474], grad_fn=<NllLossBackward0>)\n",
            "***********  446\n",
            "loss_mal :  tensor([13.8594], grad_fn=<NllLossBackward0>)\n",
            "***********  447\n",
            "loss_mal :  tensor([13.9408], grad_fn=<NllLossBackward0>)\n",
            "***********  448\n",
            "loss_mal :  tensor([13.9406], grad_fn=<NllLossBackward0>)\n",
            "***********  449\n",
            "loss_mal :  tensor([13.9108], grad_fn=<NllLossBackward0>)\n",
            "***********  450\n",
            "loss_mal :  tensor([13.9051], grad_fn=<NllLossBackward0>)\n",
            "***********  451\n",
            "loss_mal :  tensor([13.9182], grad_fn=<NllLossBackward0>)\n",
            "***********  452\n",
            "loss_mal :  tensor([13.8928], grad_fn=<NllLossBackward0>)\n",
            "***********  453\n",
            "loss_mal :  tensor([13.8807], grad_fn=<NllLossBackward0>)\n",
            "***********  454\n",
            "loss_mal :  tensor([13.9181], grad_fn=<NllLossBackward0>)\n",
            "***********  455\n",
            "loss_mal :  tensor([13.8883], grad_fn=<NllLossBackward0>)\n",
            "***********  456\n",
            "loss_mal :  tensor([13.8587], grad_fn=<NllLossBackward0>)\n",
            "***********  457\n",
            "loss_mal :  tensor([13.8622], grad_fn=<NllLossBackward0>)\n",
            "***********  458\n",
            "loss_mal :  tensor([13.8640], grad_fn=<NllLossBackward0>)\n",
            "***********  459\n",
            "loss_mal :  tensor([13.8356], grad_fn=<NllLossBackward0>)\n",
            "***********  460\n",
            "loss_mal :  tensor([13.8261], grad_fn=<NllLossBackward0>)\n",
            "***********  461\n",
            "loss_mal :  tensor([13.8536], grad_fn=<NllLossBackward0>)\n",
            "***********  462\n",
            "loss_mal :  tensor([13.8242], grad_fn=<NllLossBackward0>)\n",
            "***********  463\n",
            "loss_mal :  tensor([13.8019], grad_fn=<NllLossBackward0>)\n",
            "***********  464\n",
            "loss_mal :  tensor([13.8098], grad_fn=<NllLossBackward0>)\n",
            "***********  465\n",
            "loss_mal :  tensor([13.8152], grad_fn=<NllLossBackward0>)\n",
            "***********  466\n",
            "loss_mal :  tensor([13.7858], grad_fn=<NllLossBackward0>)\n",
            "***********  467\n",
            "loss_mal :  tensor([13.7780], grad_fn=<NllLossBackward0>)\n",
            "***********  468\n",
            "loss_mal :  tensor([13.8022], grad_fn=<NllLossBackward0>)\n",
            "***********  469\n",
            "loss_mal :  tensor([13.7767], grad_fn=<NllLossBackward0>)\n",
            "***********  470\n",
            "loss_mal :  tensor([13.7471], grad_fn=<NllLossBackward0>)\n",
            "***********  471\n",
            "loss_mal :  tensor([13.7630], grad_fn=<NllLossBackward0>)\n",
            "***********  472\n",
            "loss_mal :  tensor([13.7611], grad_fn=<NllLossBackward0>)\n",
            "***********  473\n",
            "loss_mal :  tensor([13.7376], grad_fn=<NllLossBackward0>)\n",
            "***********  474\n",
            "loss_mal :  tensor([13.7304], grad_fn=<NllLossBackward0>)\n",
            "***********  475\n",
            "loss_mal :  tensor([13.7470], grad_fn=<NllLossBackward0>)\n",
            "***********  476\n",
            "loss_mal :  tensor([13.7196], grad_fn=<NllLossBackward0>)\n",
            "***********  477\n",
            "loss_mal :  tensor([13.6974], grad_fn=<NllLossBackward0>)\n",
            "***********  478\n",
            "loss_mal :  tensor([13.7106], grad_fn=<NllLossBackward0>)\n",
            "***********  479\n",
            "loss_mal :  tensor([13.7100], grad_fn=<NllLossBackward0>)\n",
            "***********  480\n",
            "loss_mal :  tensor([13.6849], grad_fn=<NllLossBackward0>)\n",
            "***********  481\n",
            "loss_mal :  tensor([13.6735], grad_fn=<NllLossBackward0>)\n",
            "***********  482\n",
            "loss_mal :  tensor([13.6941], grad_fn=<NllLossBackward0>)\n",
            "***********  483\n",
            "loss_mal :  tensor([13.6692], grad_fn=<NllLossBackward0>)\n",
            "***********  484\n",
            "loss_mal :  tensor([13.6556], grad_fn=<NllLossBackward0>)\n",
            "***********  485\n",
            "loss_mal :  tensor([13.6661], grad_fn=<NllLossBackward0>)\n",
            "***********  486\n",
            "loss_mal :  tensor([13.6705], grad_fn=<NllLossBackward0>)\n",
            "***********  487\n",
            "loss_mal :  tensor([13.7860], grad_fn=<NllLossBackward0>)\n",
            "***********  488\n",
            "loss_mal :  tensor([13.7615], grad_fn=<NllLossBackward0>)\n",
            "***********  489\n",
            "loss_mal :  tensor([13.7376], grad_fn=<NllLossBackward0>)\n",
            "***********  490\n",
            "loss_mal :  tensor([13.7506], grad_fn=<NllLossBackward0>)\n",
            "***********  491\n",
            "loss_mal :  tensor([13.7492], grad_fn=<NllLossBackward0>)\n",
            "***********  492\n",
            "loss_mal :  tensor([13.7194], grad_fn=<NllLossBackward0>)\n",
            "***********  493\n",
            "loss_mal :  tensor([13.7196], grad_fn=<NllLossBackward0>)\n",
            "***********  494\n",
            "loss_mal :  tensor([13.7359], grad_fn=<NllLossBackward0>)\n",
            "***********  495\n",
            "loss_mal :  tensor([13.7106], grad_fn=<NllLossBackward0>)\n",
            "***********  496\n",
            "loss_mal :  tensor([13.6889], grad_fn=<NllLossBackward0>)\n",
            "***********  497\n",
            "loss_mal :  tensor([13.7241], grad_fn=<NllLossBackward0>)\n",
            "***********  498\n",
            "loss_mal :  tensor([13.6946], grad_fn=<NllLossBackward0>)\n",
            "***********  499\n",
            "loss_mal :  tensor([13.6662], grad_fn=<NllLossBackward0>)\n",
            "***********  500\n",
            "loss_mal :  tensor([13.6764], grad_fn=<NllLossBackward0>)\n",
            "***********  501\n",
            "loss_mal :  tensor([13.6816], grad_fn=<NllLossBackward0>)\n",
            "***********  502\n",
            "loss_mal :  tensor([13.6787], grad_fn=<NllLossBackward0>)\n",
            "***********  503\n",
            "loss_mal :  tensor([13.6998], grad_fn=<NllLossBackward0>)\n",
            "***********  504\n",
            "loss_mal :  tensor([13.7316], grad_fn=<NllLossBackward0>)\n",
            "***********  505\n",
            "loss_mal :  tensor([13.7216], grad_fn=<NllLossBackward0>)\n",
            "***********  506\n",
            "loss_mal :  tensor([13.7649], grad_fn=<NllLossBackward0>)\n",
            "***********  507\n",
            "loss_mal :  tensor([13.7768], grad_fn=<NllLossBackward0>)\n",
            "***********  508\n",
            "loss_mal :  tensor([13.7782], grad_fn=<NllLossBackward0>)\n",
            "***********  509\n",
            "loss_mal :  tensor([13.8239], grad_fn=<NllLossBackward0>)\n",
            "***********  510\n",
            "loss_mal :  tensor([13.8379], grad_fn=<NllLossBackward0>)\n",
            "***********  511\n",
            "loss_mal :  tensor([13.9533], grad_fn=<NllLossBackward0>)\n",
            "***********  512\n",
            "loss_mal :  tensor([13.9361], grad_fn=<NllLossBackward0>)\n",
            "***********  513\n",
            "loss_mal :  tensor([13.9776], grad_fn=<NllLossBackward0>)\n",
            "***********  514\n",
            "loss_mal :  tensor([13.9888], grad_fn=<NllLossBackward0>)\n",
            "***********  515\n",
            "loss_mal :  tensor([14.0032], grad_fn=<NllLossBackward0>)\n",
            "***********  516\n",
            "loss_mal :  tensor([14.0418], grad_fn=<NllLossBackward0>)\n",
            "***********  517\n",
            "loss_mal :  tensor([14.0337], grad_fn=<NllLossBackward0>)\n",
            "***********  518\n",
            "loss_mal :  tensor([14.0766], grad_fn=<NllLossBackward0>)\n",
            "***********  519\n",
            "loss_mal :  tensor([14.0859], grad_fn=<NllLossBackward0>)\n",
            "***********  520\n",
            "loss_mal :  tensor([14.1104], grad_fn=<NllLossBackward0>)\n",
            "***********  521\n",
            "loss_mal :  tensor([14.1570], grad_fn=<NllLossBackward0>)\n",
            "***********  522\n",
            "loss_mal :  tensor([14.1802], grad_fn=<NllLossBackward0>)\n",
            "***********  523\n",
            "loss_mal :  tensor([14.2023], grad_fn=<NllLossBackward0>)\n",
            "***********  524\n",
            "loss_mal :  tensor([14.2009], grad_fn=<NllLossBackward0>)\n",
            "***********  525\n",
            "loss_mal :  tensor([14.2186], grad_fn=<NllLossBackward0>)\n",
            "***********  526\n",
            "loss_mal :  tensor([14.2581], grad_fn=<NllLossBackward0>)\n",
            "***********  527\n",
            "loss_mal :  tensor([14.4329], grad_fn=<NllLossBackward0>)\n",
            "***********  528\n",
            "loss_mal :  tensor([14.3905], grad_fn=<NllLossBackward0>)\n",
            "***********  529\n",
            "loss_mal :  tensor([14.4004], grad_fn=<NllLossBackward0>)\n",
            "***********  530\n",
            "loss_mal :  tensor([14.4395], grad_fn=<NllLossBackward0>)\n",
            "***********  531\n",
            "loss_mal :  tensor([14.4413], grad_fn=<NllLossBackward0>)\n",
            "***********  532\n",
            "loss_mal :  tensor([14.4874], grad_fn=<NllLossBackward0>)\n",
            "***********  533\n",
            "loss_mal :  tensor([14.4966], grad_fn=<NllLossBackward0>)\n",
            "***********  534\n",
            "loss_mal :  tensor([14.5056], grad_fn=<NllLossBackward0>)\n",
            "***********  535\n",
            "loss_mal :  tensor([14.5494], grad_fn=<NllLossBackward0>)\n",
            "***********  536\n",
            "loss_mal :  tensor([14.5416], grad_fn=<NllLossBackward0>)\n",
            "***********  537\n",
            "loss_mal :  tensor([14.5712], grad_fn=<NllLossBackward0>)\n",
            "***********  538\n",
            "loss_mal :  tensor([14.5997], grad_fn=<NllLossBackward0>)\n",
            "***********  539\n",
            "loss_mal :  tensor([14.6033], grad_fn=<NllLossBackward0>)\n",
            "***********  540\n",
            "loss_mal :  tensor([14.7481], grad_fn=<NllLossBackward0>)\n",
            "***********  541\n",
            "loss_mal :  tensor([14.7146], grad_fn=<NllLossBackward0>)\n",
            "***********  542\n",
            "loss_mal :  tensor([14.7765], grad_fn=<NllLossBackward0>)\n",
            "***********  543\n",
            "loss_mal :  tensor([14.7632], grad_fn=<NllLossBackward0>)\n",
            "***********  544\n",
            "loss_mal :  tensor([14.8065], grad_fn=<NllLossBackward0>)\n",
            "***********  545\n",
            "loss_mal :  tensor([14.8061], grad_fn=<NllLossBackward0>)\n",
            "***********  546\n",
            "loss_mal :  tensor([14.8374], grad_fn=<NllLossBackward0>)\n",
            "***********  547\n",
            "loss_mal :  tensor([14.8552], grad_fn=<NllLossBackward0>)\n",
            "***********  548\n",
            "loss_mal :  tensor([14.8533], grad_fn=<NllLossBackward0>)\n",
            "***********  549\n",
            "loss_mal :  tensor([14.9158], grad_fn=<NllLossBackward0>)\n",
            "***********  550\n",
            "loss_mal :  tensor([15.0326], grad_fn=<NllLossBackward0>)\n",
            "***********  551\n",
            "loss_mal :  tensor([15.0301], grad_fn=<NllLossBackward0>)\n",
            "***********  552\n",
            "loss_mal :  tensor([15.0252], grad_fn=<NllLossBackward0>)\n",
            "***********  553\n",
            "loss_mal :  tensor([15.0616], grad_fn=<NllLossBackward0>)\n",
            "***********  554\n",
            "loss_mal :  tensor([15.0797], grad_fn=<NllLossBackward0>)\n",
            "***********  555\n",
            "loss_mal :  tensor([15.0931], grad_fn=<NllLossBackward0>)\n",
            "***********  556\n",
            "loss_mal :  tensor([15.1296], grad_fn=<NllLossBackward0>)\n",
            "***********  557\n",
            "loss_mal :  tensor([15.1270], grad_fn=<NllLossBackward0>)\n",
            "***********  558\n",
            "loss_mal :  tensor([15.1546], grad_fn=<NllLossBackward0>)\n",
            "***********  559\n",
            "loss_mal :  tensor([15.1825], grad_fn=<NllLossBackward0>)\n",
            "***********  560\n",
            "loss_mal :  tensor([15.1781], grad_fn=<NllLossBackward0>)\n",
            "***********  561\n",
            "loss_mal :  tensor([15.2058], grad_fn=<NllLossBackward0>)\n",
            "***********  562\n",
            "loss_mal :  tensor([15.2355], grad_fn=<NllLossBackward0>)\n",
            "***********  563\n",
            "loss_mal :  tensor([15.3785], grad_fn=<NllLossBackward0>)\n",
            "***********  564\n",
            "loss_mal :  tensor([15.3586], grad_fn=<NllLossBackward0>)\n",
            "***********  565\n",
            "loss_mal :  tensor([15.3727], grad_fn=<NllLossBackward0>)\n",
            "***********  566\n",
            "loss_mal :  tensor([15.3979], grad_fn=<NllLossBackward0>)\n",
            "***********  567\n",
            "loss_mal :  tensor([15.3992], grad_fn=<NllLossBackward0>)\n",
            "***********  568\n",
            "loss_mal :  tensor([15.4424], grad_fn=<NllLossBackward0>)\n",
            "***********  569\n",
            "loss_mal :  tensor([15.4331], grad_fn=<NllLossBackward0>)\n",
            "***********  570\n",
            "loss_mal :  tensor([15.4620], grad_fn=<NllLossBackward0>)\n",
            "***********  571\n",
            "loss_mal :  tensor([15.4944], grad_fn=<NllLossBackward0>)\n",
            "***********  572\n",
            "loss_mal :  tensor([15.4884], grad_fn=<NllLossBackward0>)\n",
            "***********  573\n",
            "loss_mal :  tensor([15.5319], grad_fn=<NllLossBackward0>)\n",
            "***********  574\n",
            "loss_mal :  tensor([15.5506], grad_fn=<NllLossBackward0>)\n",
            "***********  575\n",
            "loss_mal :  tensor([15.7119], grad_fn=<NllLossBackward0>)\n",
            "***********  576\n",
            "loss_mal :  tensor([15.6652], grad_fn=<NllLossBackward0>)\n",
            "***********  577\n",
            "loss_mal :  tensor([15.6883], grad_fn=<NllLossBackward0>)\n",
            "***********  578\n",
            "loss_mal :  tensor([15.7091], grad_fn=<NllLossBackward0>)\n",
            "***********  579\n",
            "loss_mal :  tensor([15.7184], grad_fn=<NllLossBackward0>)\n",
            "***********  580\n",
            "loss_mal :  tensor([15.7514], grad_fn=<NllLossBackward0>)\n",
            "***********  581\n",
            "loss_mal :  tensor([15.7442], grad_fn=<NllLossBackward0>)\n",
            "***********  582\n",
            "loss_mal :  tensor([15.7810], grad_fn=<NllLossBackward0>)\n",
            "***********  583\n",
            "loss_mal :  tensor([15.8045], grad_fn=<NllLossBackward0>)\n",
            "***********  584\n",
            "loss_mal :  tensor([15.7990], grad_fn=<NllLossBackward0>)\n",
            "***********  585\n",
            "loss_mal :  tensor([15.8625], grad_fn=<NllLossBackward0>)\n",
            "***********  586\n",
            "loss_mal :  tensor([15.8631], grad_fn=<NllLossBackward0>)\n",
            "***********  587\n",
            "loss_mal :  tensor([15.8960], grad_fn=<NllLossBackward0>)\n",
            "***********  588\n",
            "loss_mal :  tensor([15.9224], grad_fn=<NllLossBackward0>)\n",
            "***********  589\n",
            "loss_mal :  tensor([15.9163], grad_fn=<NllLossBackward0>)\n",
            "***********  590\n",
            "loss_mal :  tensor([15.9646], grad_fn=<NllLossBackward0>)\n",
            "***********  591\n",
            "loss_mal :  tensor([15.9731], grad_fn=<NllLossBackward0>)\n",
            "***********  592\n",
            "loss_mal :  tensor([16.0845], grad_fn=<NllLossBackward0>)\n",
            "***********  593\n",
            "loss_mal :  tensor([16.0716], grad_fn=<NllLossBackward0>)\n",
            "***********  594\n",
            "loss_mal :  tensor([16.0996], grad_fn=<NllLossBackward0>)\n",
            "***********  595\n",
            "loss_mal :  tensor([16.1247], grad_fn=<NllLossBackward0>)\n",
            "***********  596\n",
            "loss_mal :  tensor([16.1245], grad_fn=<NllLossBackward0>)\n",
            "***********  597\n",
            "loss_mal :  tensor([16.1642], grad_fn=<NllLossBackward0>)\n",
            "***********  598\n",
            "loss_mal :  tensor([16.1615], grad_fn=<NllLossBackward0>)\n",
            "***********  599\n",
            "loss_mal :  tensor([16.1742], grad_fn=<NllLossBackward0>)\n",
            "***********  600\n",
            "loss_mal :  tensor([16.2273], grad_fn=<NllLossBackward0>)\n",
            "***********  601\n",
            "loss_mal :  tensor([16.3405], grad_fn=<NllLossBackward0>)\n",
            "***********  602\n",
            "loss_mal :  tensor([16.3342], grad_fn=<NllLossBackward0>)\n",
            "***********  603\n",
            "loss_mal :  tensor([16.3288], grad_fn=<NllLossBackward0>)\n",
            "***********  604\n",
            "loss_mal :  tensor([16.3663], grad_fn=<NllLossBackward0>)\n",
            "***********  605\n",
            "loss_mal :  tensor([16.3817], grad_fn=<NllLossBackward0>)\n",
            "***********  606\n",
            "loss_mal :  tensor([16.3936], grad_fn=<NllLossBackward0>)\n",
            "***********  607\n",
            "loss_mal :  tensor([16.4243], grad_fn=<NllLossBackward0>)\n",
            "***********  608\n",
            "loss_mal :  tensor([16.4210], grad_fn=<NllLossBackward0>)\n",
            "***********  609\n",
            "loss_mal :  tensor([16.4497], grad_fn=<NllLossBackward0>)\n",
            "***********  610\n",
            "loss_mal :  tensor([16.4850], grad_fn=<NllLossBackward0>)\n",
            "***********  611\n",
            "loss_mal :  tensor([16.6121], grad_fn=<NllLossBackward0>)\n",
            "***********  612\n",
            "loss_mal :  tensor([16.6017], grad_fn=<NllLossBackward0>)\n",
            "***********  613\n",
            "loss_mal :  tensor([16.6379], grad_fn=<NllLossBackward0>)\n",
            "***********  614\n",
            "loss_mal :  tensor([16.6455], grad_fn=<NllLossBackward0>)\n",
            "***********  615\n",
            "loss_mal :  tensor([16.6752], grad_fn=<NllLossBackward0>)\n",
            "***********  616\n",
            "loss_mal :  tensor([16.6947], grad_fn=<NllLossBackward0>)\n",
            "***********  617\n",
            "loss_mal :  tensor([16.7105], grad_fn=<NllLossBackward0>)\n",
            "***********  618\n",
            "loss_mal :  tensor([16.7438], grad_fn=<NllLossBackward0>)\n",
            "***********  619\n",
            "loss_mal :  tensor([16.7489], grad_fn=<NllLossBackward0>)\n",
            "***********  620\n",
            "loss_mal :  tensor([16.7883], grad_fn=<NllLossBackward0>)\n",
            "***********  621\n",
            "loss_mal :  tensor([16.8017], grad_fn=<NllLossBackward0>)\n",
            "***********  622\n",
            "loss_mal :  tensor([16.9407], grad_fn=<NllLossBackward0>)\n",
            "***********  623\n",
            "loss_mal :  tensor([16.9046], grad_fn=<NllLossBackward0>)\n",
            "***********  624\n",
            "loss_mal :  tensor([16.9749], grad_fn=<NllLossBackward0>)\n",
            "***********  625\n",
            "loss_mal :  tensor([16.9495], grad_fn=<NllLossBackward0>)\n",
            "***********  626\n",
            "loss_mal :  tensor([16.9941], grad_fn=<NllLossBackward0>)\n",
            "***********  627\n",
            "loss_mal :  tensor([17.0089], grad_fn=<NllLossBackward0>)\n",
            "***********  628\n",
            "loss_mal :  tensor([17.0282], grad_fn=<NllLossBackward0>)\n",
            "***********  629\n",
            "loss_mal :  tensor([17.0616], grad_fn=<NllLossBackward0>)\n",
            "***********  630\n",
            "loss_mal :  tensor([17.0688], grad_fn=<NllLossBackward0>)\n",
            "***********  631\n",
            "loss_mal :  tensor([17.0893], grad_fn=<NllLossBackward0>)\n",
            "***********  632\n",
            "loss_mal :  tensor([17.1394], grad_fn=<NllLossBackward0>)\n",
            "***********  633\n",
            "loss_mal :  tensor([17.2719], grad_fn=<NllLossBackward0>)\n",
            "***********  634\n",
            "loss_mal :  tensor([17.2596], grad_fn=<NllLossBackward0>)\n",
            "***********  635\n",
            "loss_mal :  tensor([17.2663], grad_fn=<NllLossBackward0>)\n",
            "***********  636\n",
            "loss_mal :  tensor([17.3010], grad_fn=<NllLossBackward0>)\n",
            "***********  637\n",
            "loss_mal :  tensor([17.3059], grad_fn=<NllLossBackward0>)\n",
            "***********  638\n",
            "loss_mal :  tensor([17.3502], grad_fn=<NllLossBackward0>)\n",
            "***********  639\n",
            "loss_mal :  tensor([17.3451], grad_fn=<NllLossBackward0>)\n",
            "***********  640\n",
            "loss_mal :  tensor([17.3674], grad_fn=<NllLossBackward0>)\n",
            "***********  641\n",
            "loss_mal :  tensor([17.4062], grad_fn=<NllLossBackward0>)\n",
            "***********  642\n",
            "loss_mal :  tensor([17.4063], grad_fn=<NllLossBackward0>)\n",
            "***********  643\n",
            "loss_mal :  tensor([17.4142], grad_fn=<NllLossBackward0>)\n",
            "***********  644\n",
            "loss_mal :  tensor([17.4459], grad_fn=<NllLossBackward0>)\n",
            "***********  645\n",
            "loss_mal :  tensor([17.4637], grad_fn=<NllLossBackward0>)\n",
            "***********  646\n",
            "loss_mal :  tensor([17.5180], grad_fn=<NllLossBackward0>)\n",
            "***********  647\n",
            "loss_mal :  tensor([17.5135], grad_fn=<NllLossBackward0>)\n",
            "***********  648\n",
            "loss_mal :  tensor([17.5458], grad_fn=<NllLossBackward0>)\n",
            "***********  649\n",
            "loss_mal :  tensor([17.5656], grad_fn=<NllLossBackward0>)\n",
            "***********  650\n",
            "loss_mal :  tensor([17.5834], grad_fn=<NllLossBackward0>)\n",
            "***********  651\n",
            "loss_mal :  tensor([17.6184], grad_fn=<NllLossBackward0>)\n",
            "***********  652\n",
            "loss_mal :  tensor([17.6138], grad_fn=<NllLossBackward0>)\n",
            "***********  653\n",
            "loss_mal :  tensor([17.6842], grad_fn=<NllLossBackward0>)\n",
            "***********  654\n",
            "loss_mal :  tensor([17.6975], grad_fn=<NllLossBackward0>)\n",
            "***********  655\n",
            "loss_mal :  tensor([17.7506], grad_fn=<NllLossBackward0>)\n",
            "***********  656\n",
            "loss_mal :  tensor([17.7434], grad_fn=<NllLossBackward0>)\n",
            "***********  657\n",
            "loss_mal :  tensor([17.7565], grad_fn=<NllLossBackward0>)\n",
            "***********  658\n",
            "loss_mal :  tensor([17.7982], grad_fn=<NllLossBackward0>)\n",
            "***********  659\n",
            "loss_mal :  tensor([17.7934], grad_fn=<NllLossBackward0>)\n",
            "***********  660\n",
            "loss_mal :  tensor([17.8526], grad_fn=<NllLossBackward0>)\n",
            "***********  661\n",
            "loss_mal :  tensor([17.8451], grad_fn=<NllLossBackward0>)\n",
            "***********  662\n",
            "loss_mal :  tensor([17.8781], grad_fn=<NllLossBackward0>)\n",
            "***********  663\n",
            "loss_mal :  tensor([17.9226], grad_fn=<NllLossBackward0>)\n",
            "***********  664\n",
            "loss_mal :  tensor([17.9362], grad_fn=<NllLossBackward0>)\n",
            "***********  665\n",
            "loss_mal :  tensor([17.9748], grad_fn=<NllLossBackward0>)\n",
            "***********  666\n",
            "loss_mal :  tensor([17.9751], grad_fn=<NllLossBackward0>)\n",
            "***********  667\n",
            "loss_mal :  tensor([18.0102], grad_fn=<NllLossBackward0>)\n",
            "***********  668\n",
            "loss_mal :  tensor([18.0402], grad_fn=<NllLossBackward0>)\n",
            "***********  669\n",
            "loss_mal :  tensor([18.0401], grad_fn=<NllLossBackward0>)\n",
            "***********  670\n",
            "loss_mal :  tensor([18.0863], grad_fn=<NllLossBackward0>)\n",
            "***********  671\n",
            "loss_mal :  tensor([18.0854], grad_fn=<NllLossBackward0>)\n",
            "***********  672\n",
            "loss_mal :  tensor([18.1197], grad_fn=<NllLossBackward0>)\n",
            "***********  673\n",
            "loss_mal :  tensor([18.1598], grad_fn=<NllLossBackward0>)\n",
            "***********  674\n",
            "loss_mal :  tensor([18.1980], grad_fn=<NllLossBackward0>)\n",
            "***********  675\n",
            "loss_mal :  tensor([18.2130], grad_fn=<NllLossBackward0>)\n",
            "***********  676\n",
            "loss_mal :  tensor([18.2235], grad_fn=<NllLossBackward0>)\n",
            "***********  677\n",
            "loss_mal :  tensor([18.2677], grad_fn=<NllLossBackward0>)\n",
            "***********  678\n",
            "loss_mal :  tensor([18.2658], grad_fn=<NllLossBackward0>)\n",
            "***********  679\n",
            "loss_mal :  tensor([18.2967], grad_fn=<NllLossBackward0>)\n",
            "***********  680\n",
            "loss_mal :  tensor([18.3392], grad_fn=<NllLossBackward0>)\n",
            "***********  681\n",
            "loss_mal :  tensor([18.5224], grad_fn=<NllLossBackward0>)\n",
            "***********  682\n",
            "loss_mal :  tensor([18.4815], grad_fn=<NllLossBackward0>)\n",
            "***********  683\n",
            "loss_mal :  tensor([18.5388], grad_fn=<NllLossBackward0>)\n",
            "***********  684\n",
            "loss_mal :  tensor([18.5342], grad_fn=<NllLossBackward0>)\n",
            "***********  685\n",
            "loss_mal :  tensor([18.5362], grad_fn=<NllLossBackward0>)\n",
            "***********  686\n",
            "loss_mal :  tensor([18.5873], grad_fn=<NllLossBackward0>)\n",
            "***********  687\n",
            "loss_mal :  tensor([18.5841], grad_fn=<NllLossBackward0>)\n",
            "***********  688\n",
            "loss_mal :  tensor([18.5953], grad_fn=<NllLossBackward0>)\n",
            "***********  689\n",
            "loss_mal :  tensor([18.6422], grad_fn=<NllLossBackward0>)\n",
            "***********  690\n",
            "loss_mal :  tensor([18.6526], grad_fn=<NllLossBackward0>)\n",
            "***********  691\n",
            "loss_mal :  tensor([18.6604], grad_fn=<NllLossBackward0>)\n",
            "***********  692\n",
            "loss_mal :  tensor([18.7016], grad_fn=<NllLossBackward0>)\n",
            "***********  693\n",
            "loss_mal :  tensor([18.7023], grad_fn=<NllLossBackward0>)\n",
            "***********  694\n",
            "loss_mal :  tensor([18.7078], grad_fn=<NllLossBackward0>)\n",
            "***********  695\n",
            "loss_mal :  tensor([18.7568], grad_fn=<NllLossBackward0>)\n",
            "***********  696\n",
            "loss_mal :  tensor([18.7558], grad_fn=<NllLossBackward0>)\n",
            "***********  697\n",
            "loss_mal :  tensor([18.7646], grad_fn=<NllLossBackward0>)\n",
            "***********  698\n",
            "loss_mal :  tensor([18.8276], grad_fn=<NllLossBackward0>)\n",
            "***********  699\n",
            "loss_mal :  tensor([18.9954], grad_fn=<NllLossBackward0>)\n",
            "***********  700\n",
            "loss_mal :  tensor([18.9689], grad_fn=<NllLossBackward0>)\n",
            "***********  701\n",
            "loss_mal :  tensor([18.9738], grad_fn=<NllLossBackward0>)\n",
            "***********  702\n",
            "loss_mal :  tensor([19.0246], grad_fn=<NllLossBackward0>)\n",
            "***********  703\n",
            "loss_mal :  tensor([19.0320], grad_fn=<NllLossBackward0>)\n",
            "***********  704\n",
            "loss_mal :  tensor([19.0395], grad_fn=<NllLossBackward0>)\n",
            "***********  705\n",
            "loss_mal :  tensor([19.0863], grad_fn=<NllLossBackward0>)\n",
            "***********  706\n",
            "loss_mal :  tensor([19.0868], grad_fn=<NllLossBackward0>)\n",
            "***********  707\n",
            "loss_mal :  tensor([19.0872], grad_fn=<NllLossBackward0>)\n",
            "***********  708\n",
            "loss_mal :  tensor([19.1395], grad_fn=<NllLossBackward0>)\n",
            "***********  709\n",
            "loss_mal :  tensor([19.1312], grad_fn=<NllLossBackward0>)\n",
            "***********  710\n",
            "loss_mal :  tensor([19.1706], grad_fn=<NllLossBackward0>)\n",
            "***********  711\n",
            "loss_mal :  tensor([19.1977], grad_fn=<NllLossBackward0>)\n",
            "***********  712\n",
            "loss_mal :  tensor([19.2034], grad_fn=<NllLossBackward0>)\n",
            "***********  713\n",
            "loss_mal :  tensor([19.2432], grad_fn=<NllLossBackward0>)\n",
            "***********  714\n",
            "loss_mal :  tensor([19.2577], grad_fn=<NllLossBackward0>)\n",
            "***********  715\n",
            "loss_mal :  tensor([19.3940], grad_fn=<NllLossBackward0>)\n",
            "***********  716\n",
            "loss_mal :  tensor([19.3612], grad_fn=<NllLossBackward0>)\n",
            "***********  717\n",
            "loss_mal :  tensor([19.4296], grad_fn=<NllLossBackward0>)\n",
            "***********  718\n",
            "loss_mal :  tensor([19.4115], grad_fn=<NllLossBackward0>)\n",
            "***********  719\n",
            "loss_mal :  tensor([19.4669], grad_fn=<NllLossBackward0>)\n",
            "***********  720\n",
            "loss_mal :  tensor([19.4575], grad_fn=<NllLossBackward0>)\n",
            "***********  721\n",
            "loss_mal :  tensor([19.5065], grad_fn=<NllLossBackward0>)\n",
            "***********  722\n",
            "loss_mal :  tensor([19.5080], grad_fn=<NllLossBackward0>)\n",
            "***********  723\n",
            "loss_mal :  tensor([19.5457], grad_fn=<NllLossBackward0>)\n",
            "***********  724\n",
            "loss_mal :  tensor([19.5773], grad_fn=<NllLossBackward0>)\n",
            "***********  725\n",
            "loss_mal :  tensor([19.6294], grad_fn=<NllLossBackward0>)\n",
            "***********  726\n",
            "loss_mal :  tensor([19.6280], grad_fn=<NllLossBackward0>)\n",
            "***********  727\n",
            "loss_mal :  tensor([19.6667], grad_fn=<NllLossBackward0>)\n",
            "***********  728\n",
            "loss_mal :  tensor([19.6740], grad_fn=<NllLossBackward0>)\n",
            "***********  729\n",
            "loss_mal :  tensor([19.7063], grad_fn=<NllLossBackward0>)\n",
            "***********  730\n",
            "loss_mal :  tensor([19.7317], grad_fn=<NllLossBackward0>)\n",
            "***********  731\n",
            "loss_mal :  tensor([19.9381], grad_fn=<NllLossBackward0>)\n",
            "***********  732\n",
            "loss_mal :  tensor([19.8970], grad_fn=<NllLossBackward0>)\n",
            "***********  733\n",
            "loss_mal :  tensor([19.9309], grad_fn=<NllLossBackward0>)\n",
            "***********  734\n",
            "loss_mal :  tensor([19.9397], grad_fn=<NllLossBackward0>)\n",
            "***********  735\n",
            "loss_mal :  tensor([19.9513], grad_fn=<NllLossBackward0>)\n",
            "***********  736\n",
            "loss_mal :  tensor([19.9898], grad_fn=<NllLossBackward0>)\n",
            "***********  737\n",
            "loss_mal :  tensor([19.9903], grad_fn=<NllLossBackward0>)\n",
            "***********  738\n",
            "loss_mal :  tensor([20.0464], grad_fn=<NllLossBackward0>)\n",
            "***********  739\n",
            "loss_mal :  tensor([20.0498], grad_fn=<NllLossBackward0>)\n",
            "***********  740\n",
            "loss_mal :  tensor([20.0521], grad_fn=<NllLossBackward0>)\n",
            "***********  741\n",
            "loss_mal :  tensor([20.1080], grad_fn=<NllLossBackward0>)\n",
            "***********  742\n",
            "loss_mal :  tensor([20.1081], grad_fn=<NllLossBackward0>)\n",
            "***********  743\n",
            "loss_mal :  tensor([20.1085], grad_fn=<NllLossBackward0>)\n",
            "***********  744\n",
            "loss_mal :  tensor([20.1653], grad_fn=<NllLossBackward0>)\n",
            "***********  745\n",
            "loss_mal :  tensor([20.1705], grad_fn=<NllLossBackward0>)\n",
            "***********  746\n",
            "loss_mal :  tensor([20.1619], grad_fn=<NllLossBackward0>)\n",
            "***********  747\n",
            "loss_mal :  tensor([20.1934], grad_fn=<NllLossBackward0>)\n",
            "***********  748\n",
            "loss_mal :  tensor([20.2255], grad_fn=<NllLossBackward0>)\n",
            "***********  749\n",
            "loss_mal :  tensor([20.4245], grad_fn=<NllLossBackward0>)\n",
            "***********  750\n",
            "loss_mal :  tensor([20.3959], grad_fn=<NllLossBackward0>)\n",
            "***********  751\n",
            "loss_mal :  tensor([20.4278], grad_fn=<NllLossBackward0>)\n",
            "***********  752\n",
            "loss_mal :  tensor([20.4231], grad_fn=<NllLossBackward0>)\n",
            "***********  753\n",
            "loss_mal :  tensor([20.4529], grad_fn=<NllLossBackward0>)\n",
            "***********  754\n",
            "loss_mal :  tensor([20.4752], grad_fn=<NllLossBackward0>)\n",
            "***********  755\n",
            "loss_mal :  tensor([20.4804], grad_fn=<NllLossBackward0>)\n",
            "***********  756\n",
            "loss_mal :  tensor([20.5309], grad_fn=<NllLossBackward0>)\n",
            "***********  757\n",
            "loss_mal :  tensor([20.5343], grad_fn=<NllLossBackward0>)\n",
            "***********  758\n",
            "loss_mal :  tensor([20.5886], grad_fn=<NllLossBackward0>)\n",
            "***********  759\n",
            "loss_mal :  tensor([20.6160], grad_fn=<NllLossBackward0>)\n",
            "***********  760\n",
            "loss_mal :  tensor([20.6196], grad_fn=<NllLossBackward0>)\n",
            "***********  761\n",
            "loss_mal :  tensor([20.6688], grad_fn=<NllLossBackward0>)\n",
            "***********  762\n",
            "loss_mal :  tensor([20.6668], grad_fn=<NllLossBackward0>)\n",
            "***********  763\n",
            "loss_mal :  tensor([20.6989], grad_fn=<NllLossBackward0>)\n",
            "***********  764\n",
            "loss_mal :  tensor([20.7434], grad_fn=<NllLossBackward0>)\n",
            "***********  765\n",
            "loss_mal :  tensor([20.8753], grad_fn=<NllLossBackward0>)\n",
            "***********  766\n",
            "loss_mal :  tensor([20.8660], grad_fn=<NllLossBackward0>)\n",
            "***********  767\n",
            "loss_mal :  tensor([20.8693], grad_fn=<NllLossBackward0>)\n",
            "***********  768\n",
            "loss_mal :  tensor([20.9161], grad_fn=<NllLossBackward0>)\n",
            "***********  769\n",
            "loss_mal :  tensor([20.9112], grad_fn=<NllLossBackward0>)\n",
            "***********  770\n",
            "loss_mal :  tensor([20.9661], grad_fn=<NllLossBackward0>)\n",
            "***********  771\n",
            "loss_mal :  tensor([20.9723], grad_fn=<NllLossBackward0>)\n",
            "***********  772\n",
            "loss_mal :  tensor([21.0268], grad_fn=<NllLossBackward0>)\n",
            "***********  773\n",
            "loss_mal :  tensor([21.0499], grad_fn=<NllLossBackward0>)\n",
            "***********  774\n",
            "loss_mal :  tensor([21.0615], grad_fn=<NllLossBackward0>)\n",
            "***********  775\n",
            "loss_mal :  tensor([21.1046], grad_fn=<NllLossBackward0>)\n",
            "***********  776\n",
            "loss_mal :  tensor([21.1052], grad_fn=<NllLossBackward0>)\n",
            "***********  777\n",
            "loss_mal :  tensor([21.1406], grad_fn=<NllLossBackward0>)\n",
            "***********  778\n",
            "loss_mal :  tensor([21.1720], grad_fn=<NllLossBackward0>)\n",
            "***********  779\n",
            "loss_mal :  tensor([21.1794], grad_fn=<NllLossBackward0>)\n",
            "***********  780\n",
            "loss_mal :  tensor([21.2478], grad_fn=<NllLossBackward0>)\n",
            "***********  781\n",
            "loss_mal :  tensor([21.2725], grad_fn=<NllLossBackward0>)\n",
            "***********  782\n",
            "loss_mal :  tensor([21.3000], grad_fn=<NllLossBackward0>)\n",
            "***********  783\n",
            "loss_mal :  tensor([21.2968], grad_fn=<NllLossBackward0>)\n",
            "***********  784\n",
            "loss_mal :  tensor([21.3113], grad_fn=<NllLossBackward0>)\n",
            "***********  785\n",
            "loss_mal :  tensor([21.3558], grad_fn=<NllLossBackward0>)\n",
            "***********  786\n",
            "loss_mal :  tensor([21.3539], grad_fn=<NllLossBackward0>)\n",
            "***********  787\n",
            "loss_mal :  tensor([21.3782], grad_fn=<NllLossBackward0>)\n",
            "***********  788\n",
            "loss_mal :  tensor([21.4158], grad_fn=<NllLossBackward0>)\n",
            "***********  789\n",
            "loss_mal :  tensor([21.4157], grad_fn=<NllLossBackward0>)\n",
            "***********  790\n",
            "loss_mal :  tensor([21.4369], grad_fn=<NllLossBackward0>)\n",
            "***********  791\n",
            "loss_mal :  tensor([21.4859], grad_fn=<NllLossBackward0>)\n",
            "***********  792\n",
            "loss_mal :  tensor([21.6740], grad_fn=<NllLossBackward0>)\n",
            "***********  793\n",
            "loss_mal :  tensor([21.6440], grad_fn=<NllLossBackward0>)\n",
            "***********  794\n",
            "loss_mal :  tensor([21.6804], grad_fn=<NllLossBackward0>)\n",
            "***********  795\n",
            "loss_mal :  tensor([21.6922], grad_fn=<NllLossBackward0>)\n",
            "***********  796\n",
            "loss_mal :  tensor([21.7196], grad_fn=<NllLossBackward0>)\n",
            "***********  797\n",
            "loss_mal :  tensor([21.7497], grad_fn=<NllLossBackward0>)\n",
            "***********  798\n",
            "loss_mal :  tensor([21.7502], grad_fn=<NllLossBackward0>)\n",
            "***********  799\n",
            "loss_mal :  tensor([21.7825], grad_fn=<NllLossBackward0>)\n",
            "***********  800\n",
            "loss_mal :  tensor([21.8018], grad_fn=<NllLossBackward0>)\n",
            "***********  801\n",
            "loss_mal :  tensor([21.8282], grad_fn=<NllLossBackward0>)\n",
            "***********  802\n",
            "loss_mal :  tensor([21.8545], grad_fn=<NllLossBackward0>)\n",
            "***********  803\n",
            "loss_mal :  tensor([21.8502], grad_fn=<NllLossBackward0>)\n",
            "***********  804\n",
            "loss_mal :  tensor([21.9135], grad_fn=<NllLossBackward0>)\n",
            "***********  805\n",
            "loss_mal :  tensor([21.9030], grad_fn=<NllLossBackward0>)\n",
            "***********  806\n",
            "loss_mal :  tensor([21.9591], grad_fn=<NllLossBackward0>)\n",
            "***********  807\n",
            "loss_mal :  tensor([21.9640], grad_fn=<NllLossBackward0>)\n",
            "***********  808\n",
            "loss_mal :  tensor([21.9756], grad_fn=<NllLossBackward0>)\n",
            "***********  809\n",
            "loss_mal :  tensor([22.0175], grad_fn=<NllLossBackward0>)\n",
            "***********  810\n",
            "loss_mal :  tensor([22.1660], grad_fn=<NllLossBackward0>)\n",
            "***********  811\n",
            "loss_mal :  tensor([22.1513], grad_fn=<NllLossBackward0>)\n",
            "***********  812\n",
            "loss_mal :  tensor([22.1657], grad_fn=<NllLossBackward0>)\n",
            "***********  813\n",
            "loss_mal :  tensor([22.2005], grad_fn=<NllLossBackward0>)\n",
            "***********  814\n",
            "loss_mal :  tensor([22.2126], grad_fn=<NllLossBackward0>)\n",
            "***********  815\n",
            "loss_mal :  tensor([22.2563], grad_fn=<NllLossBackward0>)\n",
            "***********  816\n",
            "loss_mal :  tensor([22.2601], grad_fn=<NllLossBackward0>)\n",
            "***********  817\n",
            "loss_mal :  tensor([22.2769], grad_fn=<NllLossBackward0>)\n",
            "***********  818\n",
            "loss_mal :  tensor([22.3162], grad_fn=<NllLossBackward0>)\n",
            "***********  819\n",
            "loss_mal :  tensor([22.3212], grad_fn=<NllLossBackward0>)\n",
            "***********  820\n",
            "loss_mal :  tensor([22.3686], grad_fn=<NllLossBackward0>)\n",
            "***********  821\n",
            "loss_mal :  tensor([22.3698], grad_fn=<NllLossBackward0>)\n",
            "***********  822\n",
            "loss_mal :  tensor([22.4185], grad_fn=<NllLossBackward0>)\n",
            "***********  823\n",
            "loss_mal :  tensor([22.4456], grad_fn=<NllLossBackward0>)\n",
            "***********  824\n",
            "loss_mal :  tensor([22.4451], grad_fn=<NllLossBackward0>)\n",
            "***********  825\n",
            "loss_mal :  tensor([22.5019], grad_fn=<NllLossBackward0>)\n",
            "***********  826\n",
            "loss_mal :  tensor([22.6810], grad_fn=<NllLossBackward0>)\n",
            "***********  827\n",
            "loss_mal :  tensor([22.6477], grad_fn=<NllLossBackward0>)\n",
            "***********  828\n",
            "loss_mal :  tensor([22.6661], grad_fn=<NllLossBackward0>)\n",
            "***********  829\n",
            "loss_mal :  tensor([22.6999], grad_fn=<NllLossBackward0>)\n",
            "***********  830\n",
            "loss_mal :  tensor([22.7147], grad_fn=<NllLossBackward0>)\n",
            "***********  831\n",
            "loss_mal :  tensor([22.7561], grad_fn=<NllLossBackward0>)\n",
            "***********  832\n",
            "loss_mal :  tensor([22.7596], grad_fn=<NllLossBackward0>)\n",
            "***********  833\n",
            "loss_mal :  tensor([22.7787], grad_fn=<NllLossBackward0>)\n",
            "***********  834\n",
            "loss_mal :  tensor([22.8163], grad_fn=<NllLossBackward0>)\n",
            "***********  835\n",
            "loss_mal :  tensor([22.8247], grad_fn=<NllLossBackward0>)\n",
            "***********  836\n",
            "loss_mal :  tensor([22.8749], grad_fn=<NllLossBackward0>)\n",
            "***********  837\n",
            "loss_mal :  tensor([22.8799], grad_fn=<NllLossBackward0>)\n",
            "***********  838\n",
            "loss_mal :  tensor([22.9319], grad_fn=<NllLossBackward0>)\n",
            "***********  839\n",
            "loss_mal :  tensor([22.9366], grad_fn=<NllLossBackward0>)\n",
            "***********  840\n",
            "loss_mal :  tensor([22.9445], grad_fn=<NllLossBackward0>)\n",
            "***********  841\n",
            "loss_mal :  tensor([22.9923], grad_fn=<NllLossBackward0>)\n",
            "***********  842\n",
            "loss_mal :  tensor([22.9897], grad_fn=<NllLossBackward0>)\n",
            "***********  843\n",
            "loss_mal :  tensor([23.0285], grad_fn=<NllLossBackward0>)\n",
            "***********  844\n",
            "loss_mal :  tensor([23.0564], grad_fn=<NllLossBackward0>)\n",
            "***********  845\n",
            "loss_mal :  tensor([23.0805], grad_fn=<NllLossBackward0>)\n",
            "***********  846\n",
            "loss_mal :  tensor([23.1362], grad_fn=<NllLossBackward0>)\n",
            "***********  847\n",
            "loss_mal :  tensor([23.1353], grad_fn=<NllLossBackward0>)\n",
            "***********  848\n",
            "loss_mal :  tensor([23.1794], grad_fn=<NllLossBackward0>)\n",
            "***********  849\n",
            "loss_mal :  tensor([23.2018], grad_fn=<NllLossBackward0>)\n",
            "***********  850\n",
            "loss_mal :  tensor([23.2180], grad_fn=<NllLossBackward0>)\n",
            "***********  851\n",
            "loss_mal :  tensor([23.2489], grad_fn=<NllLossBackward0>)\n",
            "***********  852\n",
            "loss_mal :  tensor([23.2675], grad_fn=<NllLossBackward0>)\n",
            "***********  853\n",
            "loss_mal :  tensor([23.3105], grad_fn=<NllLossBackward0>)\n",
            "***********  854\n",
            "loss_mal :  tensor([23.3035], grad_fn=<NllLossBackward0>)\n",
            "***********  855\n",
            "loss_mal :  tensor([23.3670], grad_fn=<NllLossBackward0>)\n",
            "***********  856\n",
            "loss_mal :  tensor([23.3792], grad_fn=<NllLossBackward0>)\n",
            "***********  857\n",
            "loss_mal :  tensor([23.4076], grad_fn=<NllLossBackward0>)\n",
            "***********  858\n",
            "loss_mal :  tensor([23.4281], grad_fn=<NllLossBackward0>)\n",
            "***********  859\n",
            "loss_mal :  tensor([23.4571], grad_fn=<NllLossBackward0>)\n",
            "***********  860\n",
            "loss_mal :  tensor([23.4808], grad_fn=<NllLossBackward0>)\n",
            "***********  861\n",
            "loss_mal :  tensor([23.5038], grad_fn=<NllLossBackward0>)\n",
            "***********  862\n",
            "loss_mal :  tensor([23.5383], grad_fn=<NllLossBackward0>)\n",
            "***********  863\n",
            "loss_mal :  tensor([23.5397], grad_fn=<NllLossBackward0>)\n",
            "***********  864\n",
            "loss_mal :  tensor([23.5874], grad_fn=<NllLossBackward0>)\n",
            "***********  865\n",
            "loss_mal :  tensor([23.6079], grad_fn=<NllLossBackward0>)\n",
            "***********  866\n",
            "loss_mal :  tensor([23.7553], grad_fn=<NllLossBackward0>)\n",
            "***********  867\n",
            "loss_mal :  tensor([23.7294], grad_fn=<NllLossBackward0>)\n",
            "***********  868\n",
            "loss_mal :  tensor([23.7615], grad_fn=<NllLossBackward0>)\n",
            "***********  869\n",
            "loss_mal :  tensor([23.7922], grad_fn=<NllLossBackward0>)\n",
            "***********  870\n",
            "loss_mal :  tensor([23.8284], grad_fn=<NllLossBackward0>)\n",
            "***********  871\n",
            "loss_mal :  tensor([23.8678], grad_fn=<NllLossBackward0>)\n",
            "***********  872\n",
            "loss_mal :  tensor([23.8702], grad_fn=<NllLossBackward0>)\n",
            "***********  873\n",
            "loss_mal :  tensor([23.8916], grad_fn=<NllLossBackward0>)\n",
            "***********  874\n",
            "loss_mal :  tensor([23.9266], grad_fn=<NllLossBackward0>)\n",
            "***********  875\n",
            "loss_mal :  tensor([23.9264], grad_fn=<NllLossBackward0>)\n",
            "***********  876\n",
            "loss_mal :  tensor([23.9384], grad_fn=<NllLossBackward0>)\n",
            "***********  877\n",
            "loss_mal :  tensor([23.9864], grad_fn=<NllLossBackward0>)\n",
            "***********  878\n",
            "loss_mal :  tensor([23.9815], grad_fn=<NllLossBackward0>)\n",
            "***********  879\n",
            "loss_mal :  tensor([24.0049], grad_fn=<NllLossBackward0>)\n",
            "***********  880\n",
            "loss_mal :  tensor([24.0318], grad_fn=<NllLossBackward0>)\n",
            "***********  881\n",
            "loss_mal :  tensor([24.0551], grad_fn=<NllLossBackward0>)\n",
            "***********  882\n",
            "loss_mal :  tensor([24.0891], grad_fn=<NllLossBackward0>)\n",
            "***********  883\n",
            "loss_mal :  tensor([24.2058], grad_fn=<NllLossBackward0>)\n",
            "***********  884\n",
            "loss_mal :  tensor([24.2210], grad_fn=<NllLossBackward0>)\n",
            "***********  885\n",
            "loss_mal :  tensor([24.2522], grad_fn=<NllLossBackward0>)\n",
            "***********  886\n",
            "loss_mal :  tensor([24.2742], grad_fn=<NllLossBackward0>)\n",
            "***********  887\n",
            "loss_mal :  tensor([24.3013], grad_fn=<NllLossBackward0>)\n",
            "***********  888\n",
            "loss_mal :  tensor([24.3394], grad_fn=<NllLossBackward0>)\n",
            "***********  889\n",
            "loss_mal :  tensor([24.3350], grad_fn=<NllLossBackward0>)\n",
            "***********  890\n",
            "loss_mal :  tensor([24.3791], grad_fn=<NllLossBackward0>)\n",
            "***********  891\n",
            "loss_mal :  tensor([24.3924], grad_fn=<NllLossBackward0>)\n",
            "***********  892\n",
            "loss_mal :  tensor([24.4297], grad_fn=<NllLossBackward0>)\n",
            "***********  893\n",
            "loss_mal :  tensor([24.4457], grad_fn=<NllLossBackward0>)\n",
            "***********  894\n",
            "loss_mal :  tensor([24.4455], grad_fn=<NllLossBackward0>)\n",
            "***********  895\n",
            "loss_mal :  tensor([24.4781], grad_fn=<NllLossBackward0>)\n",
            "***********  896\n",
            "loss_mal :  tensor([24.5117], grad_fn=<NllLossBackward0>)\n",
            "***********  897\n",
            "loss_mal :  tensor([24.5072], grad_fn=<NllLossBackward0>)\n",
            "***********  898\n",
            "loss_mal :  tensor([24.5416], grad_fn=<NllLossBackward0>)\n",
            "***********  899\n",
            "loss_mal :  tensor([24.5703], grad_fn=<NllLossBackward0>)\n",
            "***********  900\n",
            "loss_mal :  tensor([24.5756], grad_fn=<NllLossBackward0>)\n",
            "***********  901\n",
            "loss_mal :  tensor([24.6375], grad_fn=<NllLossBackward0>)\n",
            "***********  902\n",
            "loss_mal :  tensor([24.8188], grad_fn=<NllLossBackward0>)\n",
            "***********  903\n",
            "loss_mal :  tensor([24.7855], grad_fn=<NllLossBackward0>)\n",
            "***********  904\n",
            "loss_mal :  tensor([24.8194], grad_fn=<NllLossBackward0>)\n",
            "***********  905\n",
            "loss_mal :  tensor([24.8424], grad_fn=<NllLossBackward0>)\n",
            "***********  906\n",
            "loss_mal :  tensor([24.8526], grad_fn=<NllLossBackward0>)\n",
            "***********  907\n",
            "loss_mal :  tensor([24.9001], grad_fn=<NllLossBackward0>)\n",
            "***********  908\n",
            "loss_mal :  tensor([24.9104], grad_fn=<NllLossBackward0>)\n",
            "***********  909\n",
            "loss_mal :  tensor([24.9304], grad_fn=<NllLossBackward0>)\n",
            "***********  910\n",
            "loss_mal :  tensor([24.9706], grad_fn=<NllLossBackward0>)\n",
            "***********  911\n",
            "loss_mal :  tensor([24.9771], grad_fn=<NllLossBackward0>)\n",
            "***********  912\n",
            "loss_mal :  tensor([25.0237], grad_fn=<NllLossBackward0>)\n",
            "***********  913\n",
            "loss_mal :  tensor([25.0315], grad_fn=<NllLossBackward0>)\n",
            "***********  914\n",
            "loss_mal :  tensor([25.0909], grad_fn=<NllLossBackward0>)\n",
            "***********  915\n",
            "loss_mal :  tensor([25.1114], grad_fn=<NllLossBackward0>)\n",
            "***********  916\n",
            "loss_mal :  tensor([25.1288], grad_fn=<NllLossBackward0>)\n",
            "***********  917\n",
            "loss_mal :  tensor([25.1698], grad_fn=<NllLossBackward0>)\n",
            "***********  918\n",
            "loss_mal :  tensor([25.1710], grad_fn=<NllLossBackward0>)\n",
            "***********  919\n",
            "loss_mal :  tensor([25.2100], grad_fn=<NllLossBackward0>)\n",
            "***********  920\n",
            "loss_mal :  tensor([25.2322], grad_fn=<NllLossBackward0>)\n",
            "***********  921\n",
            "loss_mal :  tensor([25.3643], grad_fn=<NllLossBackward0>)\n",
            "***********  922\n",
            "loss_mal :  tensor([25.3656], grad_fn=<NllLossBackward0>)\n",
            "***********  923\n",
            "loss_mal :  tensor([25.3676], grad_fn=<NllLossBackward0>)\n",
            "***********  924\n",
            "loss_mal :  tensor([25.4242], grad_fn=<NllLossBackward0>)\n",
            "***********  925\n",
            "loss_mal :  tensor([25.4304], grad_fn=<NllLossBackward0>)\n",
            "***********  926\n",
            "loss_mal :  tensor([25.4850], grad_fn=<NllLossBackward0>)\n",
            "***********  927\n",
            "loss_mal :  tensor([25.5125], grad_fn=<NllLossBackward0>)\n",
            "***********  928\n",
            "loss_mal :  tensor([25.5238], grad_fn=<NllLossBackward0>)\n",
            "***********  929\n",
            "loss_mal :  tensor([25.5689], grad_fn=<NllLossBackward0>)\n",
            "***********  930\n",
            "loss_mal :  tensor([25.5755], grad_fn=<NllLossBackward0>)\n",
            "***********  931\n",
            "loss_mal :  tensor([25.5873], grad_fn=<NllLossBackward0>)\n",
            "***********  932\n",
            "loss_mal :  tensor([25.6321], grad_fn=<NllLossBackward0>)\n",
            "***********  933\n",
            "loss_mal :  tensor([25.6369], grad_fn=<NllLossBackward0>)\n",
            "***********  934\n",
            "loss_mal :  tensor([25.6662], grad_fn=<NllLossBackward0>)\n",
            "***********  935\n",
            "loss_mal :  tensor([25.7036], grad_fn=<NllLossBackward0>)\n",
            "***********  936\n",
            "loss_mal :  tensor([25.7095], grad_fn=<NllLossBackward0>)\n",
            "***********  937\n",
            "loss_mal :  tensor([25.7789], grad_fn=<NllLossBackward0>)\n",
            "***********  938\n",
            "loss_mal :  tensor([25.7727], grad_fn=<NllLossBackward0>)\n",
            "***********  939\n",
            "loss_mal :  tensor([25.7944], grad_fn=<NllLossBackward0>)\n",
            "***********  940\n",
            "loss_mal :  tensor([25.8295], grad_fn=<NllLossBackward0>)\n",
            "***********  941\n",
            "loss_mal :  tensor([25.8417], grad_fn=<NllLossBackward0>)\n",
            "***********  942\n",
            "loss_mal :  tensor([25.8852], grad_fn=<NllLossBackward0>)\n",
            "***********  943\n",
            "loss_mal :  tensor([25.8789], grad_fn=<NllLossBackward0>)\n",
            "***********  944\n",
            "loss_mal :  tensor([25.8831], grad_fn=<NllLossBackward0>)\n",
            "***********  945\n",
            "loss_mal :  tensor([25.9470], grad_fn=<NllLossBackward0>)\n",
            "***********  946\n",
            "loss_mal :  tensor([25.9674], grad_fn=<NllLossBackward0>)\n",
            "***********  947\n",
            "loss_mal :  tensor([25.9738], grad_fn=<NllLossBackward0>)\n",
            "***********  948\n",
            "loss_mal :  tensor([26.0305], grad_fn=<NllLossBackward0>)\n",
            "***********  949\n",
            "loss_mal :  tensor([26.1431], grad_fn=<NllLossBackward0>)\n",
            "***********  950\n",
            "loss_mal :  tensor([26.1572], grad_fn=<NllLossBackward0>)\n",
            "***********  951\n",
            "loss_mal :  tensor([26.1690], grad_fn=<NllLossBackward0>)\n",
            "***********  952\n",
            "loss_mal :  tensor([26.1853], grad_fn=<NllLossBackward0>)\n",
            "***********  953\n",
            "loss_mal :  tensor([26.2327], grad_fn=<NllLossBackward0>)\n",
            "***********  954\n",
            "loss_mal :  tensor([26.2347], grad_fn=<NllLossBackward0>)\n",
            "***********  955\n",
            "loss_mal :  tensor([26.2519], grad_fn=<NllLossBackward0>)\n",
            "***********  956\n",
            "loss_mal :  tensor([26.2955], grad_fn=<NllLossBackward0>)\n",
            "***********  957\n",
            "loss_mal :  tensor([26.3149], grad_fn=<NllLossBackward0>)\n",
            "***********  958\n",
            "loss_mal :  tensor([26.3463], grad_fn=<NllLossBackward0>)\n",
            "***********  959\n",
            "loss_mal :  tensor([26.3870], grad_fn=<NllLossBackward0>)\n",
            "***********  960\n",
            "loss_mal :  tensor([26.3860], grad_fn=<NllLossBackward0>)\n",
            "***********  961\n",
            "loss_mal :  tensor([26.3874], grad_fn=<NllLossBackward0>)\n",
            "***********  962\n",
            "loss_mal :  tensor([26.4204], grad_fn=<NllLossBackward0>)\n",
            "***********  963\n",
            "loss_mal :  tensor([26.4583], grad_fn=<NllLossBackward0>)\n",
            "***********  964\n",
            "loss_mal :  tensor([26.4674], grad_fn=<NllLossBackward0>)\n",
            "***********  965\n",
            "loss_mal :  tensor([26.4816], grad_fn=<NllLossBackward0>)\n",
            "***********  966\n",
            "loss_mal :  tensor([26.5342], grad_fn=<NllLossBackward0>)\n",
            "***********  967\n",
            "loss_mal :  tensor([26.5371], grad_fn=<NllLossBackward0>)\n",
            "***********  968\n",
            "loss_mal :  tensor([26.6594], grad_fn=<NllLossBackward0>)\n",
            "***********  969\n",
            "loss_mal :  tensor([26.6650], grad_fn=<NllLossBackward0>)\n",
            "***********  970\n",
            "loss_mal :  tensor([26.7071], grad_fn=<NllLossBackward0>)\n",
            "***********  971\n",
            "loss_mal :  tensor([26.7271], grad_fn=<NllLossBackward0>)\n",
            "***********  972\n",
            "loss_mal :  tensor([26.7242], grad_fn=<NllLossBackward0>)\n",
            "***********  973\n",
            "loss_mal :  tensor([26.7356], grad_fn=<NllLossBackward0>)\n",
            "***********  974\n",
            "loss_mal :  tensor([26.7830], grad_fn=<NllLossBackward0>)\n",
            "***********  975\n",
            "loss_mal :  tensor([26.7880], grad_fn=<NllLossBackward0>)\n",
            "***********  976\n",
            "loss_mal :  tensor([26.8376], grad_fn=<NllLossBackward0>)\n",
            "***********  977\n",
            "loss_mal :  tensor([26.8724], grad_fn=<NllLossBackward0>)\n",
            "***********  978\n",
            "loss_mal :  tensor([26.8757], grad_fn=<NllLossBackward0>)\n",
            "***********  979\n",
            "loss_mal :  tensor([26.9310], grad_fn=<NllLossBackward0>)\n",
            "***********  980\n",
            "loss_mal :  tensor([26.9361], grad_fn=<NllLossBackward0>)\n",
            "***********  981\n",
            "loss_mal :  tensor([26.9423], grad_fn=<NllLossBackward0>)\n",
            "***********  982\n",
            "loss_mal :  tensor([26.9802], grad_fn=<NllLossBackward0>)\n",
            "***********  983\n",
            "loss_mal :  tensor([27.0056], grad_fn=<NllLossBackward0>)\n",
            "***********  984\n",
            "loss_mal :  tensor([27.1367], grad_fn=<NllLossBackward0>)\n",
            "***********  985\n",
            "loss_mal :  tensor([27.1484], grad_fn=<NllLossBackward0>)\n",
            "***********  986\n",
            "loss_mal :  tensor([27.1437], grad_fn=<NllLossBackward0>)\n",
            "***********  987\n",
            "loss_mal :  tensor([27.1619], grad_fn=<NllLossBackward0>)\n",
            "***********  988\n",
            "loss_mal :  tensor([27.2133], grad_fn=<NllLossBackward0>)\n",
            "***********  989\n",
            "loss_mal :  tensor([27.2150], grad_fn=<NllLossBackward0>)\n",
            "***********  990\n",
            "loss_mal :  tensor([27.2148], grad_fn=<NllLossBackward0>)\n",
            "***********  991\n",
            "loss_mal :  tensor([27.2415], grad_fn=<NllLossBackward0>)\n",
            "***********  992\n",
            "loss_mal :  tensor([27.2806], grad_fn=<NllLossBackward0>)\n",
            "***********  993\n",
            "loss_mal :  tensor([27.2846], grad_fn=<NllLossBackward0>)\n",
            "***********  994\n",
            "loss_mal :  tensor([27.2975], grad_fn=<NllLossBackward0>)\n",
            "***********  995\n",
            "loss_mal :  tensor([27.3482], grad_fn=<NllLossBackward0>)\n",
            "***********  996\n",
            "loss_mal :  tensor([27.3560], grad_fn=<NllLossBackward0>)\n",
            "***********  997\n",
            "loss_mal :  tensor([27.3824], grad_fn=<NllLossBackward0>)\n",
            "***********  998\n",
            "loss_mal :  tensor([27.4110], grad_fn=<NllLossBackward0>)\n",
            "***********  999\n",
            "loss_mal :  tensor([27.4234], grad_fn=<NllLossBackward0>)\n",
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_AT_rFGSM.eval()\n",
        "step_length = 0.001\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "y = mals_y[13:14].to(torch.float32).to(device)\n",
        "x_var = a.clone().detach().requires_grad_(True)\n",
        "y_model = model_AT_rFGSM(x_var)\n",
        "loss = criterion(y_model, y.view(-1).long())\n",
        "#loss = criterion(y_model, y.view(-1).long())\n",
        "loss_mal = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "print('loss_mal : ',loss_mal)\n",
        "#print(loss)\n",
        "\n",
        "# Compute gradient\n",
        "grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "gradients = grad_vars[0].data\n",
        "\n",
        "pos_insertion = (x_var <= 0.5) * 1\n",
        "grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "#print('nonzero(grad4insertion)',torch.count_nonzero(grad4insertion))\n",
        "\n",
        "pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "#print('nonzero(grad4removal)',torch.count_nonzero(grad4removal))\n",
        "print('nonzero',(torch.count_nonzero(grad4removal) + torch.count_nonzero(grad4insertion)))\n",
        "gradients = (grad4removal + grad4insertion)\n",
        "perturbation = torch.sign(gradients).float()\n",
        "x_next = torch.clamp(a + perturbation * step_length, min=0., max=1.)\n",
        "loss_total = criterion(model_AT_rFGSM(x_next),  torch.zeros_like(y.view(-1).long()))\n",
        "print('diff total ',loss_total-loss_mal )\n",
        "\n",
        "dif = 0.\n",
        "for i in range(10000):\n",
        "\n",
        "  tensor = torch.zeros_like(gradients)\n",
        "  tensor[0,i:i+1]= 1\n",
        "\n",
        "  gradients = (grad4removal + grad4insertion) * tensor\n",
        "  perturbation = torch.sign(gradients).float()\n",
        "  x_next = torch.clamp(a + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "  loss_updated = criterion(model_AT_rFGSM(x_next),  torch.zeros_like(y.view(-1).long()))\n",
        "  delta = loss_updated - loss_mal\n",
        "  dif += delta\n",
        "  if loss_updated > loss_mal:\n",
        "    print('****** i')\n",
        "    print('nonzero with tensor',(torch.count_nonzero(grad4removal* tensor) + torch.count_nonzero(grad4insertion* tensor)))\n",
        "\n",
        "\n",
        "print('sum of each features update loss : ',dif)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d8eaeb-0bf2-4cec-f8fa-6dd80428825e",
        "id": "P8XGlTWek2ZH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_mal :  tensor([14.9158], grad_fn=<NllLossBackward0>)\n",
            "nonzero tensor(230)\n",
            "diff total  tensor([0.1168], grad_fn=<SubBackward0>)\n",
            "sum of each features update loss :  tensor([-0.0871], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G96z4Qmtk139"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tsWSFfB8nWHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd2(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    #insertion_array_updated = torch.bitwise_or(insertion_array.to(torch.uint8), x.squeeze().to(torch.uint8) )\n",
        "    #removal_array_updated = torch.bitwise_or(removal_array.to(torch.uint8), (1 - x.squeeze().to(torch.uint8)) )\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    loss_steps_i = []\n",
        "    loss_steps_d = []\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        loss_steps_i.append(loss.detach().item())\n",
        "        loss_steps_d.append(criterion(y_model, torch.zeros_like(y.view(-1).long())).detach().item())\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        #pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next,loss_steps_i,loss_steps_d"
      ],
      "metadata": {
        "id": "53qbdF1xG7KK"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_next,loss_steps_i,loss_steps_d = pgd2(mals[13:14].to(torch.float32).to(device), mals_y[13:14].to(device), model_AT_rFGSM, insertion_array, removal_array, k=1000, step_length=.002, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1da6e24e-a99f-4626-af77-3f4b9877e32c",
        "id": "pL6R5YeRG7KP"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Convert the loss data to DataFrames for seaborn\n",
        "df_i = pd.DataFrame({\n",
        "    'Step': range(len(loss_steps_i)),\n",
        "    'Loss': loss_steps_i\n",
        "})\n",
        "\n",
        "df_d = pd.DataFrame({\n",
        "    'Step': range(len(loss_steps_d)),\n",
        "    'Loss': loss_steps_d\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 1, figsize=(20, 25))\n",
        "\n",
        "# Plot loss_steps_i\n",
        "sns.lineplot(data=df_i, x='Step', y='Loss', ax=axes[0], marker='o', color='blue')\n",
        "axes[0].set_title('Loss Steps I')\n",
        "axes[0].set_xlabel('Step')\n",
        "axes[0].set_ylabel('Loss')\n",
        "\n",
        "# Plot loss_steps_d\n",
        "sns.lineplot(data=df_d, x='Step', y='Loss', ax=axes[1], marker='o', color='red')\n",
        "axes[1].set_title('Loss Steps D')\n",
        "axes[1].set_xlabel('Step')\n",
        "axes[1].set_ylabel('Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bfdf4060-898e-4387-8e1e-6a31a5c296ea",
        "id": "ZLfcBBhlG7KQ"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x2500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB8AAAAmzCAYAAABjyUM4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzde3RU9b338U8gCYFwGTAhAcJFsBp5QCtYKFoQFLxCLR6t4u2gYrV9QtRq03ARFfJARFQIOdrWGz1VUUtbq1StKCJWsdS7HE2xCFrEJAQzBCYZSSTPH9/zc++ZTEKAhJDwfq3FmszMnj17BlfOWf3w+X7jamtrawUAAAAAAAAAAAAAQCvXrqUvAAAAAAAAAAAAAACApkAADgAAAAAAAAAAAABoEwjAAQAAAAAAAAAAAABtAgE4AAAAAAAAAAAAAKBNIAAHAAAAAAAAAAAAALQJBOAAAAAAAAAAAAAAgDaBABwAAAAAAAAAAAAA0CYQgAMAAAAAAAAAAAAA2gQCcAAAAAAAAAAAAABAm0AADgAAAAAAAAAAAABoEwjAAQAAAACI4Y9//KOOO+44ffjhhy19Kfv01ltvadq0aRo9erSGDh2qsWPH6vrrr9ezzz777TFVVVVaunSp/v73v7fgle7bFVdcoYkTJ7b0ZQAAAAAAWikCcAAAAAAAWrHnn39el19+uXbs2KErr7xSt956q374wx9q586deuqpp749rqqqSoWFhVq/fn0LXi0AAAAAAM0rvqUvAAAAAAAAHLjCwkIdc8wxevLJJ5WYmBjx3I4dO1roqgAAAAAAaBk0wAEAAAAAOAgfffSRpk2bpmHDhumkk07Sf/7nf+q9996LOKa6ulqFhYU688wzNXToUI0cOVJTpkzR66+//u0x27dv14wZMzRmzBgNGTJEP/jBD/TTn/5UW7dubfD9P//8cw0dOrRO+C1JRx11lCRp69atGjVqlCQLzI877jgdd9xxWrp06bfHbtq0SdnZ2RoxYoSGDh2qCy64QC+//HLE+dxY+H/84x+aM2eORo4cqWHDhiknJ0c7d+6MOPbDDz/UNddco5EjR+qEE07Q6aefrhkzZuz7CwUAAAAA4CDQAAcAAAAA4AB98sknuuyyy5ScnKxp06YpPj5eTz75pK644go9+uijOvHEEyVZ6PzrX/9aF110kU444QTt3r1bGzZs0P/8z//o1FNPlSRNnz5d//rXv3T55ZerT58++uqrr/T666/ryy+/VEZGRr3X0Lt3b61bt07FxcVKT0+PeUyPHj10++236/bbb9eECRM0YcIESdJxxx337eeYMmWK0tLSdO2116pTp056/vnn9X//7//V0qVLvz3emTt3rrp27aqsrCxt3rxZy5cv17Zt2/S73/1OcXFx2rFjh6655hp1795dP/nJT9S1a1dt3bpVq1atOujvHAAAAACAhhCAAwAAAABwgBYvXqzq6motX75cffv2lST96Ec/0tlnn6277rpLjz76qCRpzZo1Ou200zRv3ryY56moqNC7776rnJwcXXPNNd8+ft111+3zGq699lrNmjVL48eP17BhwzR8+HCdeuqpGjZsmNq1s8FvnTp10llnnaXbb79dxx13nM4///yIc/y///f/1KtXL/3hD3/4tkl+6aWXasqUKVq0aFGdADwhIUHLli1TQkKCJAvh77rrLq1evVpnnHGG3n33Xe3cuVMPPfSQhg4d+u3rbrrppn1+HgAAAAAADgYj0AEAAAAAOADffPONXn/9dY0fP/7b8FuSevbsqYkTJ+rtt9/W7t27JUldu3bVJ598oi1btsQ8V1JSkhISErR+/fo6o8T35cILL9SDDz6okSNH6p133tF9992nyy67TGeeeabeeeedfb4+GAzqzTff1DnnnKPdu3frq6++0ldffaXy8nL94Ac/0JYtW1RSUhLxmosvvvjb8FuSpkyZovj4eL366quSpC5dukiy4L+6unq/Pg8AAAAAAAeDABwAAAAAgAPw1VdfqaqqSkcffXSd5wYNGqS9e/fqyy+/lCRlZ2dr165dOuusszRp0iTdeeedKioq+vb4xMRE3XLLLVq7dq1OPfVUXXbZZXrggQe0ffv2Rl3L6NGj9dBDD+kf//iHHnvsMV122WXatm2brr/+eu3YsaPB137++eeqra3VkiVLNGrUqIg/bkd49Dn69+8fcT85OVmpqan64osvJEkjRozQWWedpcLCQn3/+9/XT3/6U/3hD3/Qnj17GvV5AAAAAAA4UIxABwAAAACgmX3ve9/TqlWr9PLLL+v111/XihUr9Nvf/lZ33HGHLrroIknS1KlTdfrpp+ull17S3/72Ny1ZskS/+c1v9Nvf/laDBw9u1Pt07NhRJ598sk4++WR1795dhYWFWrt2rSZPnlzva/bu3StJuvrqqzV69OiYx/Tr12+/Pm9cXJwKCgr03nvv6ZVXXtFrr72mmTNn6pFHHtGTTz6p5OTk/TofAAAAAACNRQMcAAAAAIAD0KNHD3Xs2FGbN2+u89ynn36qdu3aqVevXt8+FggE9B//8R+65557tGbNGh133HHfNqydfv366eqrr9bDDz+slStXqrq6Wg8//PABXd+QIUMk6dsWeVxcXMzj3Pj2hIQEnXLKKTH/dO7cOeI1n332WcT9UCik7du3q0+fPhGPf/e739VNN92kP/7xj1q0aJE++eQTPffccwf0eQAAAAAAaAwCcAAAAAAADkD79u116qmn6uWXX9bWrVu/fbysrEwrV67U8OHDvw2Oy8vLI16bnJysfv36fTsSvKqqSl9//XXEMf369VNycvI+x4avW7cu5uNuH7cb0d6xY0dJUkVFRcRxRx11lEaMGKEnn3xSpaWldc7z1Vdf1XnsySefjNjtvXz5ctXU1GjMmDGSpJ07d6q2tjbiNccff7wkMQYdAAAAANCsGIEOAAAAAEAD/vCHP+i1116r8/iVV16pG2+8UW+88YYuvfRSXXrppWrfvr2efPJJ7dmzR7/4xS++Pfa8887TiBEj9H/+z/9RIBDQhx9+qL/+9a+6/PLLJUlbtmzR1KlTdfbZZ+uYY45R+/bt9dJLL6msrEznnXdeg9f3s5/9TBkZGRo3bpz69u2rqqoqvfHGG3rllVc0dOhQjRs3TpKUlJSkY445Rs8//7wGDBigQCCg73znOzr22GN122236dJLL9WkSZP04x//WH379lVZWZnee+89FRcX65lnnol4z+rqak2dOlXnnHOONm/erMcff1zDhw/XGWecIUn605/+pOXLl2v8+PHq16+fQqGQnnrqKXXu3PnbkBwAAAAAgOYQVxv9T7IBAAAAAID++Mc/asaMGfU+/+qrryo9PV0fffSR7r77br3zzjuqra3VCSecoJtuukknnXTSt8fef//9Wr16tbZs2aI9e/aod+/eOv/883XNNdcoISFB5eXlWrp0qdatW6fi4mK1b99eAwcO1FVXXaVzzjmnwev8y1/+opdfflkffvihSktLVVtbq759+2r8+PG69tprI8aXv/vuu5o3b542btyo6upqZWVlafr06ZKkf//73yosLNTrr7+uYDCoHj16aPDgwZo8ebLOOuusiO/k0Ucf1bPPPqsXXnhB1dXVOuOMMzR79mwFAgFJ0kcffaSHHnpI77zzjsrKytSlSxedcMIJysrK+nY0e32uuOIKlZeXa+XKlQ0eBwAAAABALATgAAAAAACgUVwAvmLFCg0dOrSlLwcAAAAAgDrYAQ4AAAAAAAAAAAAAaBMIwAEAAAAAAAAAAAAAbQIBOAAAAAAAAAAAAACgTWAHOAAAAAAAAAAAAACgTaABDgAAAAAAAAAAAABoE+Jb+gIOB++++65qa2uVkJDQ0pcCAAAAAAAAAAAAAPCprq5WXFycTjrppH0eSwNcUm1trZgE3zxqa2u1Z88evl8AaEb8rgWA5sfvWgA4NPh9CwDNj9+1AND8+F3b9PYnz6UBLn3b/B46dGgLX0nbU1lZqY8//ljHHHOMOnXq1NKXAwBtEr9rAaD58bsWAA4Nft8CQPPjdy0AND9+1za9Dz/8sNHH0gAHAAAAAAAAAAAAALQJBOAAAAAAAAAAAAAAgDaBABwAAAAAAAAAAAAA0CYQgAMAAAAAAAAAAAAA2gQCcAAAAAAAAAAAAABAm0AADgAAAAAAAAAAAABoEwjAAQAAAAAAAAAAAABtAgE4AAAAAAAAAAAAAKBNIAAHAAAAAAAAAAAAALQJBOAAAAAAAAAAAAAAgDaBABwAAAAAAAAAAAAA0CYQgAMAAAAAAAAAAAAA2gQCcAAAAAAAAAAAAABAm0AADgAAAAAAAAAAAABoEwjAAQAAAAAAAAAAAABtAgE4AAAAAAAAAAAAAKBNIAAHAAAAAAAAAAAAALQJBOAAAAAAAAAAAAAAgDaBABwAAAAAAAAAAAAA0CYQgAMAAAAAAAAAAAAA2gQCcAAAAAAAAAAAAABAm0AADgAAAAAAAAAAAABoEwjAAQAAAAAAAAAAAABtAgE4AAAAAAAAAAAAAKBNIAAHAAAAAAAAAAAAALQJBOAAAAAAAAAAAAAAgDaBABwAAAAAAAAAAAAA0CYQgAMAAAAAAAAAAAAA2gQCcAAAAAAAAAAAAABAm0AADgAAAAAAAAAAAABoEwjAAQAAAAAAAAAAAABtAgE4AAAAAAAAAAAAAKBNIAAHAAAAAAAAAAAAALQJBOAAAAAAAAAAAAAAgDaBABwAAAAAAAAAAAAA0CYQgAMAAAAAAAAAAAAA2gQCcAAAAAAAAAAAAABAm0AADgAAAAAAAAAAAABoEwjAAQAAAAAAAAAAAABtAgE4AAAAAAAAAAAAAKBNIAAHAAAAAAAAAAAAALQJBOAAAAAAAAAAAAAAgDaBABwAAAAAAAAAAAAA0CYQgAMAAAAAAAAAAAAA2gQCcAAAAAAAAAAAAABAm0AADgAAAAAAAAAAAABoEwjAAQAAAAAAAAAAAABtQnxLXwAAAAAAAAAAAAAANJXKSumbb6TERGnnTikQkMJhKSlJ2rXLHu/QQQoG7bnqaik5uYUvGk2GBjgAAAAAAAAAAACANuHrry3QXrhQSk+XTjtNqqiQFi2SRo+W9u6159LSvD8LF1pAjraBBjgAAAAAAAAAAACAVq+iQvrsM+mpp6S8PCkzU3r+eWnpUmnePOnpp6WCAnvOiY+X/vhHqVMnKSuLJnhbQAAOAAAAAAAAAAAAoNWqrJTi4qSEBGngQKmw0MLv116zQLugQEpJkcaPl6ZOtddkZkr5+fZYaanUs6e1w6OFQnbeUMjO5Uaq79lDWH64YgQ6AAAAAAAAAAAAgCYXCllQXFpqt5WVB/a6igq73bHDdnj7n6uqkjZtkh56yJ4vLbXd3vn50hNPSMXFdn/UKGn3bvs5M1Nau1Z66y0pI8NC8xNPlP77vyNHoYfD0sMPW+j9u99Jq1Z5IXhCglRSYtcQfU01NR2UlJTUDN8oGoMAHAAAAAAAAAAAADgCuaA5VrAcCh3cuauqbLf26NHSunW2l/urr/YdhIfD3o7u006zsDnW/u7TTrNrXLLEAuzFi6Xu3a3JPWiQNbsXL7b7gYD0059KXbvaz/n53ij09HQbjf7++9K550q1tRaUh0LSggVSv37S8uXSlCkWtLv3HD1a2rBB+uYbuz7/TvG7726nAQMyFRcXd3BfIg4IATgAAAAAAAAAAABwBPAH3v6A2h8sp6XZY59+euDh+K5dFh4/9ZS0cqU1rU8/XXr77cggPPo8LnSeO9drcRcU2P7umTO90DoYlO65R0pKkp591q5r0ybppZfs9sYbIx/LyZHGjJFWr7afx4/3xqS7Jvjpp0sffGDfRUWFNbwfe8yOHTTI3rt/f7tdscI+V2qqNH++d72S3c6dG6eFC9upujqxyf8OsW8E4AAAAAAAAAAAAMBhqLENbXdceXnd8eDuORd4p6VJf/ubF9xGB8uZmRbuPvVU7HA8Lc3uf/113VHloZD9iY+Xli71AmwXGLuR4337eufxjxxPSLDjJW9nd2Fh5M/uudNOs/HmGzd6Le/cXDv/lVdas9s9dt119t3ccov3sz9gj76+Sy6RvvxS6thRKiuTxo61Fri7hvx86cEHLRh31+Sua8gQuy0oiFNSElFsS+BbBwAAAAAAAAAAAA4zbhT4vkJo/57qe+7xRo7X1Ng474ULbXe1C7zj4+sPlqXI8eDR4bhkwfKYMXXHka9aJbVrZyF2cXHk+/jPGdmUtsa3C/ODwcj3cfu8/T+757Zvt+C7psZa3llZUlGRdOqp0vr1FshnZ9tjEyfaZy0u9n52Y9L917dihbRsmfT88/YeVVXW8i4ttTC8tNT7XCtXeteUmWlj1LdskZ55xm4fecT+0QIOPQJwAAAAAAAAAAAA4DBSUeGNAq8vhB42zEJet6c6usn8yiv23H33WYPZhdwNBcsNta4lC3pfey1y9Hd6uj325pt2/JdfWjB97LGRgbH/PH4FBRaaS9bYDgTs5y5dpF697H5xsdfylux+Soq0Zo0F37m5FnbPnm3PTZgg/fjH1vieM0f6+GPpr3+1Y9ets5/dmHR3fS++6I1Dz8iQXnjB9n6/+qoXhvs/l2uejxgR+bqBA+327belTp2a4r8G7C8CcAAAAAAAAAAAAOAwUFlpQasbBd5QCP3RR9a4dnuq/U3mX/3KC72jQ25/mBwdLDcUjmdmWjP6oYe80d/usaVLLaDfuNELpidOjAyM3XmiBYMW+Eu2Hzw728775z/b+bKy7Jzbt9tzko0ld/u+s7OlCy+09xs+XNq61f48+6zUvr3t/C4psWb8zJkWiM+fb2PO09O964v+hwa5udINN0iffGIt+0svtfd0n8s1zx94IHa7PS/P/j72tS8dTY8AHAAAAAAAAAAAADhIsfZhN+a4igpvR/emTRYwFxdbiDpqlD0fHUIvXWpN5oqKuqO5o0Pv6JDbhccuWN6zR5o+3Z6rLxx3oXtamo34Li31mt9pad7ebn8wPW2a3brA2L2/5O3KHjXKGutdu9qec8mC56eesnNefLF0003WLn/mGbvm2bOtdd25s3TttdITT0gjR0qrV0vf/a79o4BAQEpMtAZ2crL93KOHfVc5OXbd7dpZ4P7DH9r1+f+hQWamhdedOlnwnZwszZghff6597mysqQ775SOO66hdnvct+12HDoE4AAAAAAAAAAAADjiNTbAjsXt647e0R0O13/caafZ3u5Fi2x/9uLFNj578WJvtPbDD3tjwf0h9AsveM9Fj+aODr39gbfjRoY/+aT0u99ZAD57duQ+bf/r8vMtaC4u9kZ/L1rkPeZvd+fm2ujw5culr76yUeOffmrn8e/Kfv556eWXbV+323Oeny/96EfSd75joXJRkY0Sv/tua2OPGWPf25o1NnZ87Fipb19p3Dj7xwB9+th5kpPr/7vyB+Jdu9p5v/hCKi/39nm7keZ9+ti498GDpT/9SZo6VerWTerf3xrjl11W9/P7BYP2d4xDiwAcAAAAAAAAAAAAR7ToAHvwYGnZsroBdiz+fd0uCI2Pl/74R2nJEi9ID4Uij3Pjyu+/34LclSstwN60yRutvWSJ7aaODqFnzvSemzIlcjR3rNDbvyM7ELBg+fLLrb18220WLLvx4Sef7I0Kf/xx6Ze/lM480wvma2osID79dO8xf7u7qMjO16+f9L3v2Xc4cKA0a5Y1ud96y15bU2Ofae5cGxfuxqhv22Z7xINBa4qfcoo9584dCnn7x9evlyZPlgYMsGu85579/7tPSrKd3e5zuL8X/0jzTZss7F60yFrj3bpZm3zqVG9PeSyBgB2LQyuutra2tqUvoqV9+OGHkqShQ4e28JW0PZWVlfr44491/PHHq1OnTi19OQDQJvG7FgCaH79rAeDQ4PctADQ/fteirQqFbG/2rl3W7u3QwcLLQMAC2KSk2M/t2WON4bvvtkDVjb6eMMH2cXftas3g7t0t+IxuFofDUm2t1Lu31x7Oz7dx2qWlFqru3St16WLvlZbmjTZfvdrC04wMG+89YoQ1ozMypOOPl155xcaMu1HjycnS0KHS++/be2dk2HNr11rbesoUC45//3sLb12TuaDAG4t+113SGWfYd9G5s7WTe/f2Pk9Kih3Xt6+F5SedZM3ncePsuKeftgD7/fctoE9P9x7Ly6v797JggQXvnTrZ39HChTbe3H2e3r0jP6sLvd334L6bgQO963PPxWpdBwK27zsx8cD+GyostH80sD/nd59r7ty6x8+ZU6ucnLgGG+lonP3Jc2mAAwAAAAAAAAAA4LBT367s6BHlrr3txmjHGjE+erQF1f7ndu+24xMSLCR2gfFnn9m5lyyxY9PTY480D4Ui93X7R2dnZFhom5HhNcndePLMTGt7l5XZ/S5dLAj3jx/ftctrQRcVSVdfbUH8pk3S3//ujev2t62Tky24du3t4mJ7buRIa1W/+qo1r1evtj3Ww4ZZwB+9mzsvz0L0Y4+1MP+GG7zjXJN81Ci7bv9jrl0u2e2cOdJPfmLht+R9z/4mezBo77d9uxc4+9vr0fvL/bvNYzmYkePJyTau3X23jT2/2w8+Z070569Vbm7D49jRPAjAAQAAAAAAAAAA0KxcmL1jh4W70T9Hh9xVVbGDbP+O7cJCe70bKz5zZuToajfK+ve/t13XbsS2e27rVum//9sLYt3x/fvXHYEdDNp7LFhgzXDXOL/33oZHZweD0qBBNiq8e3fvuF//2u6PGCH9+c+2VzsrywuTL73Udk+7QHXdOhulHQhY0Ox/rqjIxoD372+B93/+p/SLX9he6+eft/a2C73797dR6c8+K/3zn5Fj0v0B/okn2ojvgoLIQNoF7scfb4F9drb3mBuhvnmzBe5Tp9qocicYtNHw48d7o9MHDbKA3n03jvserr/e9n27a4wOxKMd7MjxDh0O7PxJSVJOjrXDS0ulkpJaXX99haRGzNBHkyMABwAAAAAAAAAAwH6Lbmi7Rrb/uR07vDDb39BuqK29apW349kfTM+b5zWoH3vMQtHERHsuJcWC1cJCe393/8UXbdx2Wpod53/OBdMuiB0/3kaJ+8/jjh8yxG5feEGKi7NW9xdfePu6c3Lqvi4zU3ruOXv8zjsjj1u40NvzXVAgXXyxBb4XXmgB9fHHS998Y49JkSH0unUWVLvnnLIyacMGe21cnJ1n0iTbj+1C77KyyNfk5lrDe84c++5dgN+xY2TT2t/yLi62wP2ii6Sbb/ba5pMnW3C+cqX9/Z97rgXKTiBgrXL/nvMbb7T7/iBeigzVzzjDa7X7W/KxZGfbqPqDUV1d97ttzPmTk+2/x9RUqaamStu2/Utsom4ZBOAAAAAAAAAAAABHmFiN7OgWdmMb2mlpFrT++991n/vb37ww29/Qrq+tff/90tixXpA8apR01lne/XPPlf7xDxsDfuaZ1jQOBuuOxnb3Z86MHLftnisvjx3E+oPfzEzbb71li+2h3rJFWrHCrvX2272mcG6udN11kaOzXZt60yZvlHn0cXfeKR13nH02f+C7erX03e9aWzo31xut7ULoW2+1IDorK/bY8RkzLIxNSbFAPDr0luy5lBR7j1mzLJifMMH7nqOb1rFa3s8+a6/3N58/+sjGsP/wh/ZcZqb3ntXV9rj/e7vkEvv7mD+/7hj14mLp7bdtdH3Hjt77jB7tBeL1ffaDUf9I86Y5P5pfXC3/9GC/lqZj/1RWVurjjz/W8ccfr05uyQMAoEnxuxYAmh+/awHg0OD3LQA0P37XQrJ91AsWWJt55UoLngsLLYhcu9ZGhf/1r/U/N3Kk9OabFl5nZtrzp5xiwfCIEd5zKSkWGmdkWBga6+dgMPK4jAwLVo8+2s79+us2crxvX7v/xhs2dnzevPrP7z+nZK3k99+P/VxGhn22116zYHPoUDv29NO9z//iixa4TpggtW9vxweDFo6/9ZZ91lGjLLju1SvyuZdfll55xXuN/zj/Z/VLSbHji4ttTHmHDjZufedOG78dDtvI7d277fEOHbznqqu9gHbWLG8EeceO9qddO7uGOXOsKV5aav9QQbLzuZ+lyM8XfX23324jzmOFwS+/LH3/+3buXr2sye6OC4elTz6RnnrK++/n+eelRx6xxxYssO95+3ZrUv/rX9J3vuPtEfdzY+hjffamcDDn53dt09ufPJcGOAAAAAAAAAAAQCsRPXbcNbJjjSGP9bry8n3vzJ43r3ENbddyTk21oPW++yLb2/5Wdn0/Rx/XpYuFptG7sgMB6Z57LPhdutRe5x8L7v/ZPff3v9vndS1v/3MvvWSPu73Wo0dbu/iyyyJHk69YYUH4Z59ZeLt9e+RY8BtusNbyxx/bPxrIyoocx/7LX1qI7d573TrvuOLiyF3ejhtlXlMjde4cOVo7MVHq2tVue/Sw78v/nD+gPeYYb3z75Mn2WT77zBrxzz5rAfzAgVKfPvadBgKx93BHt8x/9jPpmmtih8HhsPTqq965e/WyaQDh/12FnZRk4+Zdg7u4WDrnHGn6dBupftVVNrL9ssvs+zvuuNjht1T3e2nqZnZznx/Nhwa4aIA3J/6FCwA0P37XAkDz43ctABwa/L4FgObH79rWxTVQQyEL3775xgui/Y3spUstlHVjpW+80VrB7vXudY895jWhG2phN/Tc6NEWSvfubQ3hjz6ykNS1t595xoJP6cAa4MuWWSP6L3/xzrtsmb3PDTdYSO3OL3khfEGBtHGj9NBD0l13Wbv9rrusiZyW5n1f/kb7G29YI3rRIu+x116T/vQn6cor7f6yZdaCPvnkyM+anm7f6YQJ1lDv2tWawp072+vHjZNOOME+l79N3lCbPtqcOTb2+0CD1y++sP8mysulo46q29CP9sEHFpLPnRv5/S5YYGPod+2y89XXhA6FLOz2v76hzxLdsHbN9uZqdB9K/K5tejTAAQAAAAAAAAAAWqnKStuj/fDDFgbec4+0apW3Szu6re32VS9bZs3i8nJvD7f/df791g21sOt7Lj3drikQsBbv+PEW7Lpjo/dG+1vZKSnWns7Otp/37LHWr/+4nBw757XXRu7Kdruzg8HI80veburTTrPP/8gjFj6vWWOjuF9/3d7Tv8P6iy9sN7kLV3Ny7NrXrrUge8oU+97j4+16li+324UL7TrvuMOOfesta0+npkqDB0tPPmmB+uTJdo3HHmvfzfr1kfuzn3/e3vuqqyxAb4591uGw9JvfWFj/9tuxG/rRpk2L3Dku2ffy7rtSXJx99w01oRMS7L/JWAoK7Hm/+prtNK5xsGiAiwZ4c+JfuABA8+N3LQA0P37XAsChwe9bAGh+/K49fPnb2v/6l+1DPvlkC1l/9av6G9Mu/Pa3m11ruaHXHUgDfM0aaw+ffLL0739L551njWL/sdF7o90e7/btpd/+1sLlL7+0c02ZEtmK9u/7jt6p7e6//HLsBvPTT1vQO29e5DVEfzeu5V1Q4DXnBw2ysPnyy605L1lAP26c9Oij0g9/6DXbo/eQR3Mt6mHDIhvj7h8T+Pd719RYoz0xsWn3Wfub2NH71f0N/Vh27Ki7c7yx1+LfJ17f86mp+/1xWiV+1zY9GuAAAAAAAAAAAACHMbeTe8cOr639+uvSkiUWULrWsQtt62trS14bPC/Pay3Het2+dmY39NzatTbKu7DQWsKXXGLnr6mJPDZ6b3RRkY1dv+ceG2F+/vnWjL7ttrqt6KQkbye2f1e25N3ftKnuXupBg6Qzz7RA279/W4psfm/das3vpUu95rxk55w2zVrSbod6dbUF3z172t+Pa56XlVlg7faQ+6Wk2N9dQYF9D9OmeXvGHbffu6zMPkd1tT3elPum/U1s/38D0Q39aIFA7J3jjb2W6B3i0c9167Y/nwI4cATgAAAAAAAAAAAAh1A4bIH36NG2Vzk/30aVn3KK9OyzFlj6x5VHB5f++9GB774CT39APX9+ZJicm2sh9ZIldt8/mvv++6WKCjtvUZFde0mJvd5/zuJiC5xHjpS2bbNrOfVUL5Ddtcsa4O48kydLAwZIkyZJ/fvb7u7s7LrX6q5vyhT7xwEjR1qgvXmzBcru2qJDf8l7n+HDLWRvzJju5GT7Lj791N7Thfyxzu/4n3PBe2lp84w4b0gw6F2f/7+B6H/YEM0fyB+I6mrv766pzw3sDwJwAAAAAAAAAACAQyQUkhYssBHnf/mLjYx27e3t26WNG2O3jutra0cHstGB5xtvRIaS/kb0mjX22C9+Ya/7299sD/O119q48bFjpZNOsqD517+Wunf3QtyiIumcc2yP94UXShMnei3rl16y3dZvvmljuLdvj319jr8VPX++t4c6Okx/9VVrEU+dao1v9x299po1lwOBhlvOCQn1h9eSPb5zp3c/Kcna5TNnSp9/bt/jpZfWf/7o9y4qsu/hjDPsHwxs3WrvX1Jie8eTkmJfx8HyN7Gj/9uJ/kcF7vimCOSTk+0chzLsB2IhAAcAAAAAAAAAADgEKiu98dT5+dITT1hoGh9vIW/37t5IcX/rWIrdhs7Oli67LDJ09QeemZkWSv/855GBZ3Gx9O67FiBfc4101VXWzF682Jro8+dbcL1+vdfQPvNM6ZNP6obpo0d7Yfopp3iPn3WWjUx/4gmpR4/6A1m/lBTpiiukDz+0gLikxEavn3mmtHevfc7ERAvpExPt+DPOsOfrG9/uV1xs/2Bgf8Z0d+pkbfyrr7bnrr/eriVW07mszBrj0c+tW2dBeGGhN168OcPg6CZ2Qw39kpKmDeSTkry/u0MR9gOxxLf0BQAAAAAAAAAAALRmoZAF28GghajV1XUDznDYGsCdOnl7uk880cLnY4+1sPCDDyy4zc214Hf5ci/ILCy04PKuu6yRvGuXvceUKV4gO3euHetef9FF0r33SitWWOt861ZrY6emSv/6l/TOOzZy/emnbez5r35lr7388shrLyuzP1dfbUF3ba3twHZj1t99Vzr7bAuoL7lEevLJyNe+9559rry8yOtznys9XVq0yALzigoLuPfu9fZQS/ZzLC7Mzs21Hert2lmAv3Kld3739/Kzn9k/MPB/V35uTHes93J/n+65GTPstqDAO392tv1d1vfcDTccmiDYNbHdNRQVWUP/gQekW2+1lnu3bvZZe/aM/FxN9f7Svv/ugOZCAxwAAAAAAAAAAOAAVFZai3rhQhtl/n/+jzRunIWu4bB3XChkAXOfPtbydoH3pk3WVp440YJIt5PbjRTv18/CxJtusibtK6/Y+VevlqZNs53ZZ59toa6/5V1UZCH2ccfZtUTv2h4wwMabjxplx7gd4g3tt5as3VxRYaF7fQ3fLl3qvm7Rosj2ugtkL7rIXv/OO9Y2793brqF3b/tO/d9hfY4+2prg8fHSf/2XXctrr0nt28duInfp0jRjuutrOnfocHi0oKOv4bXXbJy7+0cFzd1CB1oSDXAAAAAAAAAAAID9UFkpxcVZgP3UU9awXrbMguTSUguzP/9cysiwxndCgvTMM9KPf2wtbxd4u1Hma9fauc4801reCxbY49u3W9t6+3Zr606YYI3rsjLvWh58ULrnnrot7549pfLyyDDbNbmd7dulwYO90Ds+3rsu/+syM21k+/jxFuYffbRdT6yGb9++db+v1avtc917b90GcnW1hd3+RnYw6N3PyWk4qM3KslZ8aamF56453qOHd0z0dbpweNasyGvZ34C6oabz4dCCPhyuAWgJNMABAAAAAAAAAAAaKRy2sHrxYmngQOnFFy3AfustG+H9wQd2nBt1HgpZoLtxo9fynjbNzpGVZW3oMWMswJ0500Lyq66ylvZll1kzu39/6ZtvpA0b7NxDhlhL2t/ejm55DxtmbeeG9l2npkoffeSF3rH2Z2dmep8vI8Oa7r161d/Qzsio+1gwaNf3y1/WbSC7neixFBTY8w39XTzxhL3nwIH71xx3u7hpQwNtDwE4AAAAAAAAAABAI7hR5gMH2o5pF1oXFFgDe+VKLwj/5z8ttH78cdtpXVNj4bJrebvXzpljre4JE6Qf/cgbL/7RRzbyPCvLmsnp6dYi37JFev55C5Tff1/avbtuy3vDBnv/l17ydohHy8qyndk7dkhvvumF3rm5kePK8/Pt8+Xlee/jGtoLFth34peRYSPJhwyxW79YYXwwWP/I9WDQGtr1/V0sWCDNm9e46wJw5CAABwAAAAAAAAAArVYoJO3ZY4Hynj31B5+NPa4hbpR5aanX6HYNbBcUuyA8NVV64AELtTdutIDZhcsXXmj7r884w3Z6b9tmofezz9oI8VjN5D17bE/26afbnumFC23neOfO9be858+394zedz17tu0V79nTAvXjjosM48eMkUaOtOs6+2z7fLHEamh/97t2ThfW/+lP1iKXbP95tECg4ZZ6t26xnzuY5jiAto0AHAAAAAAAAAAAtErhsAXBaWneH/8I7MpKC7qrqho+rrHv5Q++a2qkv//d9mzHx0cG4Q8+KA0aZH8KCqSLL/aC74kTpeHDbVf3738vnXKK7a3u2bP+Udyu7ZyX5zXO8/JsjHr0yHInJUW64grpX/+yfdclJXb927bZtcbFSX/8ozW2+/aVxo6V/uM/7Li1a62pXltrDezGNrTDYem//ssbSZ6RIb39tp0vMzN20F1dXX9LPTvbnq/vvQ+kOQ6g7SMABwAAAAAAAAAArYZrcu/caaHw3LmRI7Dvu8+C3XDYwtNNm6wJHeu4hx5qXBN81y7b+d29uzfKPCvLmtSpqdKxx1q47ILwlSttFPnYsd5+7jFjLPhevdpa0pIFwo3ZP+3azikpXtDuRI8sz8y09//8c2uZH3OMHeda5Vu2SCedZJ/HPz58/XrpxBOle+/19mN36tT4hrYL6aO/57w8u/YFC2I3wJOTpRkz6rbU58yxx+v7bg60OQ6g7SMABwAAAAAAAAAALc4F2zt2WOAca1x5OCw9/LBUUWEBrX8Edmam9PTTFvCeeab0yScWcg8aFBkY+48791wLlysqvFDdvXd5ud1WVFiwfeedXvDtQuczzpDeeEP64Q+twe2C8I0bLWwuLfXC4KIiafJkacAAadIku62sbNx349rO6emR53TndeH6F19I77xjYXbv3nZ8797Wdv/6a/v+/vxnqX37+seH5+fb53Ua29BuaCR5YaHtOO/dO/bzSUmRLfWSEruflFT/d3KgzXEAbR8BOAAAAAAAAAAAOCRihdw7dngjykePtnHgscaV79plLeJ+/aQnnrBd1S4Izsy0MdtvvWUt5o4dbQT3ypWRgbH/uIwMC8B37pSWLZN277ZjHnnEHvvd77yg3b2Xf4e3G2U+cqQ0fbr06af2mBuP/uqrFkBHt5TLyqQNG+yYxraUXdu5uNjOH31OF67/7W8WYEe3sOfOtVB+wQLpscfqhuh+0ePDG9vQ3tdI8u3bpV696v+MrnUevfu8oeMPpDkOoO0jAAcAAAAAAAAAAM3O7euODrn/9jdvRLl/v3X0uPL4eAtvx4+38d3+IDg/33tdx47W3vbv6451XDDo3e/f33ZyL1pkP7vHnnjC2sjuHLFGmdfW2rkHDZJuvNGC8Kws6ec/t888fXrs72N/Wsqu7VxW1vDO79GjY7ewU1LsHwQUFNQfojuxxoc3pqG9r5HkqanWEm9KB9IcB9D2EYADAAAAAAAAAIAmE6vlXV7u7Yf2h9xuZ3ZhYez91k56uoWbHTta0LlpkxcER7+uuNh2Tbsmdn3HufvLl9vtoEHez+528WI7bs0aL3SOHmVeWGgheKdOdn2DBtln/PGPpeuvt0D2YFvK/rbz/PmRO7/dOW+/3b7vWC1s/+j0hkJ0qf5gfl8N7YZGkmdlSatW2ffT1Pa3OQ6g7Yvf9yEAAAAAAAAAAAD75lreL7xg48cXLrQw+f33LfR2ofPUqXa8P5gdMiT2aO7MTDtP7942Kt21l3NzbZx5Wlrk61zAe/TR3r7utWvtPLt3e8e593aN8epqL2CPDto/+8wLdwsL7Rw1NRaEZ2dHBrudOtltTo41nnfvlm65RZo1y0aLd+tm77W/LWXXdp41y86ZkyPdemvkORMS7LuJ/g79rW83yn3t2sjPEwjYZ5kx48Aa1C6kl+zv2p1z+nT7M2aM9Jvf7P95AWB/0QAHAAAAAAAAAAAHzDW+/S3vvDxp6VJvJLkLqP2BtyR16WJ7oevbb+12dr/xhjWIp0zx2stuHPnAgVKfPpGvy821Hd+33OLt6x49Wura1TvOvV9VldcYdwF7dNA+ZYoF+SNH2qj0zZulbdukX/yi/lazayb36GGfsylayvs6Z30t7LIyG83unvOPct+61f40xfjwWCPJzz3X3quoqP4R6QDQlAjAAQAAAAAAAADAAXGN78GDLYQtKJBGjbLAc+lSO8YfbPt/zsyU/vxn29OdlRV7NLd/Z/ctt1iA69rYs2fb+W6+2YJWf/BbVCSdeqr0j39YIOua4qtXe+d37+dC9U2bvJ9jBe39+knjxtlo9/R0a4B36XKovunG8Y9Kjx65fuyxkc8VFUlXXeWNn2+q8eHRI8kffdS+6yFDLLgHgOZGAA4AAAAAAAAAwBGustL2R3/9tTV39+yxZnd9QiE73jW+/S3vvDxp+/a6I8ldyP3GGxZWu3D74ou9QHv+fOmGG2y096BBkTu7/UF0crIdV1IivfOO9PTTdn7/XuziYunVV+2zdOhgo8JdiD57tjRihNS5s+3r/vxzrzHuAvbooH3yZOnEE6Vnn7XzH27htxOrhZ2TY99BrOeysuy55vKTn0hbtkjPPGOheEP/XQFAUyAABwAAAAAAAADgCFVZaS3u6mprcqenW1N60iTp3/+28NgF4lVVdnxVlYXS7dpZgC15o8wHDbIx4d271x1Jnp0tLVliY7dvvlk680w7jwu2TzvNxpx37SrddJP08ceRO7slO3byZKl/f2tjV1ZakJ6dHTnS240ov/pqO5fkNdDde61ZY83wsWOlvn0tHK6ttVZ0t27SFVfY7c9/bkFxSYn00Ue2v/xgxoQfCtEtbH+zu6Hnmlo4LK1YYf+4YOBA+29r4UJ7HACaCwE4AAAAAAAAAABHmLi4OIXDNvZ740Zp0SJrbqenW1v76adt5/Xo0dK6dTbuu7bWjp8/38Zau5a3f5T5jTdaYB49ytyF3D/8oYXgl19uO8P94faJJ0p33WUhaY8e0mWXWWgda290WZkF8Z06eSG8C8cHDLAAf+hQG7fetas97/ZjFxVZC3n+fGuvr1/vheqjR0uPPGKft3t3C4jdbc+ezR8YtyWhkE0ImDfP+3sOBu07X7CAJjiA5kMADgAAAAAAAABAKxcKRba1KyvrP7Zr166qrk7SkiXWyh00yJrYmZnWiE5NtXD4qaeklSult96SXnlF3x5fWOg1vgOByFHml1xiIfr8+d74cBdgV1dbuF1QYKF6t27ec/5d3y5U/6//kv75z8gg3e/GG+1Yf4guWTi+YYOF9du3S8cfb4+7/dj5+ZGj1aNfd9ttUnz8fnz5iCkhwfvHCdEKCux5AGgOBOAAAAAAAAAAALRi4bCNlXZt7epq6auv6u7xDoWk+PiOysg4RgkJtpO5vNzb3Z2fLz34oBeIu1D6V7+yMeErV3rH/vKXFk7n5HhhclGRXUNJiXT22XVHkn/wge3hDgYj94KnpNQNpGPtB3dheSAgzZkjXXll/Q1xyT5Hz54WyDtJSdL06fZdRAfnTjBo14mDE+sfJ/if4zsG0FwIwAEAAAAAAAAAaKUqKmyctL+tnZFhO63dvuWvv/ZC8lWr4vTQQ3EqLo7Txo023rtnTwuLx4/3Qu74eC+UHjXK3mfjxshjr71Wuu66yFHmRUXSOedYyHzhhbZPe8AAG2f+m99E7gZ3e8Fvu80L1qXIQNyNTvcH6Vu32g7xF1/0xpr7ZWbaCPcPP7Rznnde5D8E6NSp4eA8ELB2Og4O3zGAlkIADgAAAAAAAABAKxQOe2Omo0eISxZi//GP0r/+ZSH5ffdZk3vxYguya2qshb1pk7e724Xcxx5r99PTpYcftsDSHe+OXb9emjjRAmt/0Oma4C60/p//sbC6f38bse5Gmrtwe+BAqU8f7xzp6ZGBePRu7wEDrOHeubM31nzOHHu9G+P+9ttS79523l69LPwPh71rjBWcO9nZ9jwODt8xgJZCAA4AAAAAAAAAQCsTCkkPPSQVF0e2tSWvAb1li/Tcc9J3vmPh+KhR0u7dFni78eO5udYYv/JKC55dyD1xogXhixbZ7u8XX/SOd3u+AwEbuf7Xv9bd0+1C68JCC6oHDpQuuMCCz+xsL7AuKrJ2+CefeGFpcbG9d3R72O3orqmxPeWdO9vjSUk2ir2kRHrjDXvPefO8AD0YlObOtX8E4Jrg0cG55I1WnzHDnsfB4TsG0FIIwAEAAAAAAAAAaEVCIWt+33tvZFs7GPQa0J99Jq1ZIx11lDei/Kc/lbp2tRDSjR+/8EIbWb5+ve0Mz86256ZNswD99NMtUPYff955Fja7wNo9F2tP9w032Mjxdv+bRhQV2TlcYF1aardHH+2FpS6Ejw7VnawsadWqyIA8OVlKTLTbgoLYrysosO/N8Qfn7jpycuxxNA2+YwAtgQAcAAAAAAAAAIBWIhyW/vxnC7Vdk9u1tQMBG4W+fLk0ZYr05pvWwu7WzfZ2jxkjrV5tAbJ/t/bq1dIxx0gdOliY/eMf2zkrKqRduyw8jz4+IcGOnTPHGttjxkgjR0rbtlnIGR109urlfYaKCi+wTk21206dIsPS0aOlmTPrtodnz7awfcYM+8zRgkGv+R3ruZ07Ix+Lvg5ayU2P7xjAoUYADgAAAAAAAABAKxAK2Rjv7t0jm9zTplkYnpNjo9AHDbK284oV0qOP2l5vt7f7llu8tnZxsQXkJ54orVwp1dZKHTvaeV57zUanBwJeAO3fxX322dKPfmTnKymx1vmZZ0p791owHR10pqd7P6el1f8ZXVjao4d3La49/OWX0rBhFrb/85/Wbo/mv95Yz3Xr1uivGwDQShGAAwAAAAAAAABwmAiFbBR5aandup3VkrWuH3ssdpO7tFS66SZrOI8da2PL8/MtCL/4Ym9vt2trDx8ubd0qbd4svf++1L+/1L69vY8LoVNSpOpqb9S543Zxf//7Ntq8Me1efwM8VnO7Pv728D/+YXvEi4osIHfX6xfrep3sbHseANC2EYADAAAAAAAAAHAIRYfcu3bZbVWVtHChNaTdn4ULbey5ZCO8O3aM3eSeMEG68koLrUtLpfh4a4MXFlpgPHq0t7fb3+SeNMlu33ordjicnOzt5o7e7z1jRuPHWTe2Ad6Qnj3t8w0ZIh13XOxjmup6AQCtV3xLXwAAAAAAAAAAAG1dKGQN7m++sVD7hRek+fOlU06RFi2SRoywnd15ed5rjj/e2tq1tdJXX1mQW1VlQbBrci9YYE3u7dutJV1cbGHzscdaEO72YRcVSeecY6PN9+61YLysTKqp8XZqu33d0dxu7lmzrGHerZuF5fUdH0uvXhZep6db4H4gevWStmyxz5WWZt9prEC7Ka4XANB60QAHAAAAAAAAAKAJuGb3jh1eq3vHDq/ZvWqVhd5PPWU7t1NT7f5993ljyyUpM1N66SXp5Zdtt/bYsRacFxVJU6bYc278eXST+89/tlB70iQLyv37sF0T3I0/Ly21VnhOzr7DYf8o8oZGndfnvPMsvH7mGemaayJHuzdGOCzdfbeUkSENHCj16RPZjm/q6wUAtF4E4AAAAAAAAAAAHKRw2ALZ0aOtYe1Gmf/tb3VD7vx86cEHpUGD7H56utfWzsy00NuF43PnSjNneru8s7Olzz7zxp8HAtbk3rpV+tnPpGuvlbp0kW64Qfr0UwvK/VxoXlhY+21I3NzhcDgsLVvmhde9ejUcXkcLhazpPneu12gPBu3+ggX7H6YDANo2AnAAAAAAAAAAAOoRva+7oqJuy7u83AtoXVidlxe5h9uF3O6xlSu90Lu42GtrR4fjKSmRu7zHjJH69bPQ+oYbrMHt/vib3B072jlmzoy1D7tW2dlSp06H5vs72PA6IcG+01gKCux5AAAcAnAAAAAAAAAAAGJwo8vT0qTTTrN90osWRba8Bw+2FnVBQWRYLUU2u13I7XZzb9zohd5lZTbWPCenbjjuP4fkNbj795fGjZNOOMFC9VhN7k6dLAjPybGA3Eae1+r66yskNbJ+fZCaIrwOBr3PH+u5nTsP8OIAAG0SATgAAAAAAAAAAFF27YpsLufnW2A7b561u5cutduOHesPq/3NbhdyT5xoj9XUeLu8JSk3V7r+emuT+8Nx/zn8ysqkDRss2O7cueHP4t+HXVNTpW3b/qXa2tqm/Lrq1RThdSBQ9/P7n+vW7UCuDADQVhGAAwAAAAAAAADgEwpZq3rpUrvvb3aPGmVjyN1z/oA6Oqx2obc/5J42Tdq0yR7LzbVd3kuWWLO8QwcXUnuvS0mRtm+342LJzpaqq5vz2zg4TRFeV1e33s8PADj0CMABAAAAAAAAAPBJTLQw2zWX/c3uvDwLpN1z/pA7JcV2gk+f7p3LhdyzZ9s5x4yRvvrKdnP/+Mf2uquvltavl/r0kV54wQvHb7pJevNN6Zln7LHZs6N3eUszZtQdfX44aYrwOjnZPmfdXeaH/+cHABx68S19AQAAAAAAAAAAHC6+/lrascNrcvv3dw8aJI0cace55yQLq9eulS66SPrd7ywAr621xnhRkY09f+AB6dZbpS+/tKB8/Xrp0kul3r2lu+6yUev+c0nS++9Lr7xiY9fvv99Gsm/dagF8WpqFx0lJh/b72V8uvJZshHwwaN9ddrY93tjrT0qyXeazZtnY9G7dWsfnBwAcejTAAQAAAAAAAACQVFEhLV5s4eqaNd7octfyvvFGa4L7x5pLFnKff7507LHSbbdZy3v4cAurN2+WVq+WPvvMRptfcYU0YIA0dqz08ss29rygIPJcY8bYqPVTT/VGrRcVSZMn22snTZKGDm094a8Lr0tK7PsrKbH7+3v9/l3miYk0vwEAsdEABwAAAAAAAAC0GaGQlJDgNY2rqxsXlIbD9rr8fCkz0wJrN7q7sNCa2a+9Zue65BJp5UrvuWBQiouzoDwYtD+TJ1vTOz3dGuRlZRaGl5fbz5J0zDHe8X5FRdIvfyk9+2zd58rKvNfv3GlhcGvg/g7c9SYmtty1AADaNhrgAAAAAAAAAIA2IRyWFi608eDuz8KF9nhDQiHpoYe8vd+5udKUKdLy5TbyfOtW6fnnpc6dbcf32WfXbXm/+KKFu25HtWRB9YYNdhsI2PPFxd7zbsy6/zVOcXHd8/kFAtZUBwAAkQjAAQAAAAAAAACtXkWF7cieO9dC7JQUKSNDuu8+ezwUqv+1CQnSvfd6gbQbQ96vnzRunDWtjzpKatdO2r3bAvIf/1i66iobSX7ZZTbGvKoqcjS6X1aWtGqVheEpKdKQIVLfvtZQd01zv7Iy6dNPYz8n2ePV1fv5JQEAcAQgAAcAAAAAAAAAtGpufHlBgY0vf/ppacsW6Zln7PZ737PnQyFrcJeW2m1lpfT113Z/06bI3d5u5/aECban+667rFGekWH7u//jP2yX9YYN1v7+7nctfM/OlubM8ZrbgYA0e7Y9/sAD3jU984zUpYuNTp8xo+5r5syxneL1PTdjBjuwAQCIhQAcAAAAAAAAANBq+ceXp6dLa9dKb71lQfXAgXb76afSN99449F/9CPpjTfsscWLpe7dLVjOzbWgevZsL3CeNUtaskSaN8/bx71+vXTiidYaT0y0FviFF0p/+Ys1x6+91sLx0lIbkT5smDR1qrRsmb3WXVtamnTnnRaC5+R4rykpsfsdOkhJSbGfS0o69N81AACtAQE4AAAAAAAAAKDV8o8vX7TIWuB5eV5YHQxK/fvbGPSnnpJWrJBefln6+99tpHl+vtf8dqPP3W7vzz6zBvjSpbHfOz/fa2FXVkpffmnnKC+3YDw1VRo/XrrgAuknP4l9bXPnSvPn2333msTEyHZ3cnL9zwEAgEgE4AAAAAAAAACAVsk/vnztWun006XCwshjUlIshP7rX+2Y1FQLnB99VNq+3UJof/O7uNhGn594ovT66/a8C6yjBYO2Hzwjw+7v2mW3PXp4xyQkeNcQfW1OQYEdBwAADh4BOAAAAAAAAACg1QmFIseX33+/VFFRN6w+/niprEyaOVN68EFp0CALort0kXr1stdGN783b5bef1+aNMmed+PQowUCUrduFsT7de/u/ZyWZqPZS0v3HaQDAICDRwAOAAAAAAAAAGh1EhIix5evWyd17uyF1ZmZ0tNPSy+8YAH0+PHSypVeEP3LX0r//Ke9VrIQfPJk2+c9aZKF5HFxUnW1tcNjyc625+PivMc6dJA6dvTu9+xprfKePfcdpAMAgIMX39IXAAAAAAAAAACAEwpZuB0MWjBcXV1353VlpbR7tze+fO1ae3zNGgu0V6ywx5Yvt/OVl0udOkkbN1oQPWiQBeKnn26huGSBdzAo1dRIP/qRhdsuyJ4xw24LCrzrys62x5OSpKOO8q6te/fIQDwtzRroL70kTZ8uzZtX9zO7ID0x8eC+OwAAQAMcAAAAAAAAAHCIhELSnj3Wwt6zx+77hcPSwoUWGrs/Cxfa4/5jli61UDx6fPm4cRZKP/WUhdX9+9vt1Km2+7umxoLoG2+0a1i/vu7o861bpWHDpKoq7z2TkqScHKmkxF5XUmL3k5LseX8A7t//LVngnpIiPfSQdMst0pw5XhM8ELD7M2bUDfkBAMCBoQEOAAAAAAAAAGh2Ltyur0UdCtnzc+d6rwkGvfs5OXbrjsnMtLZ3Xp43vjwlRTrrLAubly+3dvjUqXaetWvt/XJzpddeiwzQ3WvT021ceU2Nhdx+LqBOTbVbf1s7JcX72b//W7LrueIKC847dJBuvlmaNct2fnfrZs1vF6QDAICDRwMcAAAAAAAAANCsQiFpwQILroNBe8yF20uWWNs6IcHCcb+UFNvHfeqp9nx8vHdMbq4F2rNne43qmhpp1CgLlzt29PZ9S3ZsVpZ04YXSeedZwO3f7V1WJm3YYLduJHlj1dcAD4elZcukjAxp4EAL2O++W9q714L0xESa3wAANDUCcAAAAAAAAABAs6msjB1uZ2ZKTz9te7Hj4iLDavfcZ59JTzwhvf66jTffts07xj/63I0vLy62xncgYKF6z55eOO4/fvVqu6bc3KYZSe5vgLsAvKHQf8GCuuPfAQBA0yAABwAAAAAAAAA0i3DYwml/uC1ZwL12rfTWW9Irr0j33mujwwOBus+5EHnjxshAW/LGlw8YIF12mQXpycnW3r7sMtv3nZUV+/inn5Zqaxve7d1Y/ga4G4EeK/R3CgrseQAA0PQOqwD8+eef109/+lONGTNG3/3ud3X++edrxYoVqq2tjTju97//vc466ywNHTpUP/zhD/XKK6+00BUDAAAAAAAAAGIJhWy8eZ8+Xrjt5OdbCPyrX0ljx9pebxdWu+deflk64wypsNBek5Iibd8eObbcKSuzY93Y8uRka3F//nndMemBgPSzn0nXXCN16mTHJiYe3Ejyo46y6xsyxMadSxb4+0N/v2DQxrQDAICmF9/SF+C3bNky9enTR7m5uerevbveeOMN3XrrrSouLlbW//4zvb/85S+69dZbdf311+v73/++nnvuOWVlZemxxx7Td7/73Zb9AAAAAAAAAABwBAmFrMkcDFqw/M03tt/a7et+5hnpxz+WPvjAwu28PAuKx4+3UeUZGV47PDdXeu01C6Dnz5eef96C7WDQa4UvX27n2bvXgnH3vllZ0s03R4bXSUnS1Vfbtfz859Ktt3rHV1fvf8u7ISefLG3ZYp+lVy/7XgIB+xMrBA8EpG7dmu79AQCA57BqgN9///265557dO6552rUqFG6+eabdeGFF+qRRx7R3r17JUkFBQU677zzdOONN+r73/++5s6dq6FDh+q//uu/WvjqAQAAAAAAAODIEQ5bc3v0aGndOqmmxkaKL1zo7et2Y8vnz/ea2BMmSBUVFgwXF3tjzYuKLLAuL5dmzpR+/WuvOe5a4TfcUHfv97Zt9ronnqh7ja7d3b273fbseeAt74a+h1//2sL8gQMtAF+4UPr669htdcked211AADQtA6rALxHjx51Hjv++OO1e/duVVZW6t///re2bNmic845J+KYc889V+vWrdOePXsO1aUCAAAAAAAAwBGrosJ2cz/1lLRyZf37umtqbLT5mWdacH3aadJDD3nt6LIy6Y03vKB43TprRo8f741Fz8mx+24Uun+P96RJ0tCh9l4dOx767yEU8j6za3oHg3Z/2TJrtc+ZEzl+fc4cG8/elCE8AADwHFYj0GN5++23lZaWps6dO+vtt9+WJB199NERxwwaNEjV1dX697//rUGDBh3Q+9TW1qqysvKgrxeRqqqqIm4BAE2P37UA0Pz4XQsAhwa/b4HDW1xcnOLi4lRb20EJCVJBQZyWLfN2eefmSpdfbseWlXk7vXNzbXy5ZK3o+fNtZHhWlrRihbW5R4zwxpr//e/Sscd6Y9Fff91a4dGjxMvK7I9ku8FTUvaosrLmEH0bJiGhowoK4mI+N2eOdN11tbr55r2aNavdt+PXw+G9qq3do8rK2kN6rQ6/awGg+fG7tunV1tYqLi72/82NdlgH4G+99Zaee+45/fKXv5Qk7dy5U5LUtWvXiOPcfff8gaiurtbHH398wK9Hw7Zs2dLSlwAAbR6/awGg+fG7FgAODX7fAoePpKQk9ejRVz16dFZFRa2Sktrpt7+VzjsvTvHxsXd5O/7ge+JE6bbb7PjLL5fS0+25iy6S7r3XgvAFC2yseUWFdNRR3lj0iROl1asb3qedmirt2PG5Pv74wP834v2VlJSko47KVDDYPubzwaBUXr5XZWVFqqmpUXx8vL78skY1NYc2pK8Pv2sBoPnxu7ZpJSYmNuq4wzYALy4u1k033aSRI0fqyiuvbPb3S0hI0DHHHNPs73Okqaqq0pYtWzRgwAB1bIkZRABwBOB3LQA0P37XAsChwe9b4PBiLauk/92/HafHH5fGjpUWL5auuspa2i70jo/3dnm7kLqoyMaeL1hgYXg4XKtgUAoG4xQMSuefb8F2YaG9ZvJkKSXFwvGlS20s+ty5Nhb9r3+1xnheXt3rzMqSVq2SBg/uo+OP731IvhsnPr5dg8F89+7t1KXL0XWfbEH8rgWA5sfv2qb3r3/9q9HHHpYBeEVFha699loFAgEtXbpU7drZqvJu3bpJknbt2qXU1NSI4/3PH4i4uDh16tTpIK4aDenYsSPfLwA0M37XAkDz43ctABwa/L4FWl4oJH3zjbRokYXQKSm2v7u4WNq0ycabT5zohd7+kef+kNrt654/X7rkkjj16eOF5Lt2SV9+GRkeu7HmP/2p1x4vKPDa5HFxFo67ceJZWRaUjxkjrVnTUYf6V0co5AX10bKzperqOCUnH56/z/hdCwDNj9+1Taex488lqV0zXscBCYfDuu6667Rr1y49+OCD6tKly7fPDRw4UJL06aefRrzm008/VUJCgvr27XtIrxUAAAAAAAAA2ppw2FrZ7dpZ+CxZK3v7di/wzs2Vpk2zMDwry47JzbXQd/ZsO0ay21tvtWM/+ECqrrZjJAvT3fmiudHnt9wilZRY+N2tm3e/tNTC82HDLPwuKpJ69Gje7yWW5GRpxgzb9+3/zHPm2OPJyYf+mgAAONIdVgF4TU2NbrzxRn366ad68MEHlZaWFvF83759NWDAAL3wwgsRjz/33HMaNWpUo+e+AwAAAAAAAAA8oZC0Z49UXm4jyx99NHKnd3GxtcDXrLHA2403Ly2VZs60wLe42B77wQ8snC4pkb74wh4bM0ZKSIgMjGtqvNZ4LGefbSF8YqLt+E5MlLp29e6vXStdcIFdSyBg528JSUlSTo4XzJeU2P2kpJa5HgAAjnSH1Qj0O+64Q6+88opyc3O1e/duvffee98+N3jwYCUmJmr69Om65ZZb1K9fP40cOVLPPfecPvjgAz366KMtd+EAAAAAAAAA0EqFw9LChdJjj0nvv2+t7+OPl3r18saVuxHnn33mNbgLC6UJE6Qzz5SWLJFmzbKx5l26SF99ZWH1unUWqBcVWYAueYHxrFnS7t32etc2d6PNs7MtKG8oRD7qKO9nd+6W4prebnMnXS0AAFrOYRWAv/7665Kk/Pz8Os+9/PLLysjI0MSJE1VVVaUHHnhAv/nNb3T00UersLBQJ5100qG+XAAAAAAAAABoNUIha0mHQhbYBoNSx47enu8hQ6zBnJ4u/fnP0saNkTu93R7u5culkSPt/vbtUlqatHev/Vm82IJx/47uJ56wBrg/sHaBsRtb7gLxnTtt1Hl19b4b1L17W/Cdni5lZDTtdwUAAFqvwyoAX716daOOu+iii3TRRRc189UAAAAAAAAAQNsQDksPPyxdcok1rV98Ubr9dumMM7w9324n96JF9tiKFRZ4SxZqu73cDzwgHXecjUvv08dGp0vW9HZhuWQhuLu/YEHDLe0DaVB36SJt2WKhfVqaF+wDAIAj22G1AxwAAAAAAAAA0HRCIRtLvmCB1K+fF2yvXClt2mQ7ut2e77IyC7xPP90LvMeMkYYPl7ZulTZvllavlo4+Wpo82c7j9nonJHhBejQ3Kr1r16b7XOGwdNdd1vweONCC+IUL7XEAAHBkIwAHAAAAAAAAgDYoHLbwuV072+89frzdz8+3sPqOO6zxHQh4r7n/fqmiwgvFi4os7B4wQJo0yW537pT+/W/b3+0Eg95rorkd4nFxTfO5QiEL9OfO9d4zGLT7CxbY8wAA4MhFAA4AAAAAAAAAbUQoZCPJy8stDH70URsR3rGj3cbHe0F4WZn00ku2p9tZt07q3DkyFJfs2A0bpJoa29tdXGzndAKBuq/xP9fQ+PP91VDbvKDAngcAAEcuAnAAAAAAAAAAaMVc6F1VZWPABw+2HdoFBd5e76oquz32WAvCXXM6N1fKzpZmz7aguqxMWrNGmj499ntNny69+64d16mT93h1tZ0nlqws6a23mu7z7qttvnNn070XAABofQjAAQAAAAAAAKCVCoelhx+OHAvu2t5u9PhLL0lTptjtxImRY89j7fmeMEH65S+9UFyy29mzpZtvtveTIgPw5GRpxgxpzpy6r8nOtn3hTWVfbfNu3ZruvQAAQOtDAA4AAAAAAAAArZALvY85RkpKkpYutcdd69uFxK7l/dln0rRp0qZNkWPP/Xu+V66U/vlPe83w4dKXX1qYvm2bNGyYtHixPS9FjkCX7BpycqSSksjXjBkjff11033uhtrm2dn2PAAAOHIRgAMAAAAAAABAK5SQID32mHTaaRZ6u7HgsXZ7f/CBNHWqtaP795dmzqzb1r7+eunii6Xf/95C7smTpWeflVJTpYceki64QPr0U6my0l7jb4A7yck2fj01VXr8cXtNUZF01FFN97nra5vPmWOPJyc33XsBAIDWhwAcAAAAAAAAAFqRUMhazuXl1sLevj2y8S15re8lS6S1a22vd//+Up8+0rnn2kjyW26RvvjCxp5/8YXX1v7kE2nXLjtPQoLdxsXZbWVlwwG4X9euUkqKNGSI1LdvU34DddvmJSV2Pympad8HAAC0PvEtfQEAAAAAAAAAgMapqpIWLrTm9/vv2/2UFAu4s7KkvDw7zu32fv55qbDQe1yyhvgbb1hj+uyzpZ/8RIqPl957z54fNEjavdt+7tzZbl3YXVlp7ynVHYEe7ZRTpC1bLKDu1cuC+6ZsZ7tzpababWJi050bAAC0XjTAAQAAAAAAAKAV2LXLdn7PnWt7vF96SZoyxW43bbLG9+zZXhO8ulpKS5MKCmKfr6BA+u53bXy6C78lqaLCa4B36WK3/gC8MQ3wcFj6zW+kjAxp4EALwBcutMcBAACaEwE4AAAAAAAAABxmQiFpzx5rT+/ZY6F0fLy0dKl3jBtz/tlnFoQvXy6NHClt3WpjzTdssNe53eDRgkHpq6+k9PTIx3ftargBvq8APBTygnr33sGg3V+wwJ4HAABoLgTgAAAAAAAAAHAYCYetLZ2WJp12mgXSiYnW1PaH2W7Meb9+Ng78iiukM8+0gLpnT2n9emtw+3eD+wUC0lFH2Xn9/AF4dAM8FNr3CPSEhIZb526vOAAAQHMgAAcAAAAAAACAFuKa3jt2WPDsH3Oenm47vJ94QiopsVA7OswuKpImT5ZOOMEC6cREC9D795fOPVd67TXbDR5Ldrb05Ze2E9zvq6+kr7+2n10D3IXd5eVSba39XF8DPBhsuHW+c2f93wcAAMDBIgAHAAAAAAAAgGYUHXK7n6uqrOk9erS0d69UWCi1a2ct6cxMC6/T0qR775VSUqQ1a+oPsy+7TPr0U/s5ELBQOxSSbrnFgu45c7zwPBCw+zNmeE1vv/Jy7+foEeg7dnjP1dcADwQabp136xb7OQAAgKZAAA4AAAAAAAAAzcSNM3chtxtt/re/SfPnW9N75kwLvR9/3HZ+B4NSfr41v4uLpU2bpJdestvsbGn27Mgw+9ZbpV/8Qnr/fXusc2cpKcl+3rDBxqT//OfWIi8ttducHDvmqKPqv/bERPsjeQG4a4bHx9c/yry62q4zluxsex4AAKC5xLf0BQAAAAAAAABAWxIKWTgcCkmLF1vI/fTTFnLn5Vmbe/x4aerUyJ/j423M+aBB9tiJJ0pXXWUhd26utHattHy5NHKk3d++3cL0r7+WLr7YdoBLUlycnefzz+3+pk1S1672eGqqPeaC7bQ0qX17qXt3G5v+2WfeSHTX/pbqjjuvb/y5ZPvIZ8ywnwsKLNAPBCz8njHDC+cBAACaAw1wAAAAAAAAADhIbsy5G2s+eLCFzAUFXshdWGjHpqd7TW//z2Vl0htvSDfdZI+55ndWlu36HjNG6tdPGjfORpGnp1sI/p//KT33XGSbu2dP7+eUFAu/Y2nfXlq5UtqyRVqxwm7/9CcbwX6gAbhkIXdOTuzWOQAAQHOiAQ4AAAAAAAAAB8GNOR8xQnrzTWt5DxniBdv+nyUba96zp7Wi/T+np0vDh1vDOyEhsvktWYA+ebI1xG+6SbrkEumssywElxoOwBu69tdfl6ZM8ZraWVn2ntdc4x0XHXjXt//bLznZbqNb5wAAAM2JBjgAAAAAAAAA7CfX+C4vlxYskO67Txo71mt51xdyS9b0ds1u/8/5+dK991oAXlJiI8Nd83v4cGnrVmnzZunDD23c+dix9vyOHXbeHj2860tL836uLwAPheza8/K8cD4YtPsFBRayOx06RLbI99UABwAAaCkE4AAAAAAAAACwH1zj2z/m3D/KXKo/5HZycy3gnj1bmj9fuuEG6cwzLUAvKpLOOUeaPt2eLy625veJJ0p/+YsF0V27esG3s78N8IQEu/ZYCgstiHfi4iJDbwJwAABwuGIEOgAAAAAAAAA0QmWlVFMj3X23NHdu5Gjz+Hiv5e1CcP/48vnzvfB66VILuSdOlH7zGwu5g0GposJ7bVGRNHq0NbS3brUx57162fu7Pdo9e0pffmk/JyRE7uxuTAAeDHrvF+u5HTvqht6hkP3cmBHoAAAALYEGOAAAAAAAAADsw9dfW/jsb037R5vHanm78eWnnWbPde1qY8VLSiy4XrNG+vRTadw46aKLLMB2Y9Ld6ydPlgYMkC67zMJzt1dbigy5jzoqckR5z54WfA8ZIvXrF/szBQKR7xf9nL9RLtEABwAArQMBOAAAAAAAAAA0IBSSNm6U/vu/LfSONeZcihxr7oLlrl2l731PWrTI9nL36GGj01essFB95kxp3TrptdfqBuhOWZk0frxUXR35uD8A9+//lix037JFeuYZ6cYbvea2X3W1XW8sWVnSRx9FPkYADgAAWgMCcAAAAAAAAABoQEKCNHCgtHix1/h2/KF3cbE1vkeOlLZts6b3Sy95I9NdcL5pk+33vvtuG3Eefa45c7z3CATs/owZke1vqW4D3AmHpQcflDIy7Lp79bKd5eFw5OuTk+280e83e7Zdx5o1kcf7Q29GoAMAgMMVATgAAAAAAAAA/K9QSNqzx3Z779lje7937bL7mzbVP+Z8+HALvVevlk4/3W5//nOpXTvb+R1LYaE0YYK3o9udy41JLy2125wcb++3X6wAPBSyUN0fuAeDdn/BgrpN8KQkO797v23bpGHD7DoqKyOPpQEOAABaAwJwAAAAAAAAAEe8ykqpqsqa0mlpNkJ81Srbq925s9f8jjXmvLhYevddO8d110mnnipNnCi9/76Fyi6IjhYMStu3S+np3mOffSZ16yYlJkqpqXYb3fx2Yo1A9+8oj1ZQYM9HS0723q+gQLrgAgvjO3eOPI4AHAAAtAYE4AAAAAAAAACOWC743rRJmj/fmtLp6dLatdKbb0pjx0pffil9+qk1v/2N761bpc2brTV91VXSD34gPfus9N57du7i4roj0/0CAQudd++2FviQIdIJJ1jo3hg9e3qv69fPHgsGGw7cd+5s+Jzt23s/d+kS+Rwj0AEAQGtAAA4AAAAAAADgiBQOW/C9eLHtyi4stMfz860JvWKFtHKl9MwzUp8+0i23eLu+J0+WTjxR+stfbFT6uedaOO5XVlZ3ZLpfVpa0bp20bJm0ZYu9z+rVdceU12f4cO91OTn2ukCg4cC9W7eGz+kPvaMb4P7QmwY4AAA4XBGAAwAAAAAAADjihELSkiUWfK9c6Y0qT0mRxo+3MNwF4TfcYGPN//EP6Re/sAC8pET6n/+RvvMd6cILpY8/rvse3btLd94p3XijNGeOF0wHAhak33STdNJJ0ssvSxkZdi19+tgY9nC44esPh6Vf/cp7Xe/e9rqvv7YR7bFkZ0vV1Q2f1x+AN9QAJwAHAACHq/iWvgAAAAAAAAAAONQSEqw5/eMfSxs3eqPKR42SKiqk+HgLwqdOteOLiqQJEywgP/54a0f/6U/SZZdZ0ztaZqb0wAMWcJeXW3t81iwbQd6tm/TKK9Lbb9uo9bw873XBoI1hl6zVHWv/dyhkYbc7zv+6lBTbUy5ZeB8M2ufKzpZmzJCSkhr+Xrp2tXOkp1uA78cIdAAA0BrQAAcAAAAAAABwRKmstGDYBd81NTaq/I47pIcftsD42GO9VrhfWZn02mvS88/bc+npdc+fmWnB9osvWkO7b1/b0X3vvRYwJyZaw/yUU7yx69EKCiykjyUhwZ6PZc4c2+Odk2Mt9dJSu83J2Xf4LUnf/a43Vv3kkyPHsdMABwAArQEBOAAAAAAAAIAjQmWlVFUlLV1qzWoXfGdlWWv6iissWH7xRWniRK8VHksgYA3p4uLIx1JSLNQuKJDmzfMC9GDQ3mP+fAuVjz1W2r69bsDuBIPWFq/vuX29LjnZgvbUVLuN1SSPFg5ba92NVU9PjxzHTgAOAABaA0agAwAAAAAAAGj1QiFrRruR3+GwNZ537bLbdu2kTZukp56ykeOZmV7wvXatBbodOlg4np5uj23aZMf4R5Q7bp+2C4czM6Vf/1oaPtzGp194YezrLCiwUehXXWV7uwOB2GF2IGCj0mMJBA7sdQ1paKy6ZA1yRqADAIDWgAY4AAAAAAAAgFYpFJL27LFW98KFUlqadNpp1oBetEgaPVrau9eC7MWLrdXsRo7n5lqIfeGF1vYeOVLavdtC36IiacwYGx8+c6aNFXdN8EDA7s+YYa3qtDRv5PlLL0lnnilt21Z/Qzs9XfrmGwviV62ygD0WF7DHUl1tz+/v6xrS0Fh1N469Rw/vMRrgAADgcEUDHAAAAAAAAECrEw5b6D1ihPTmm15Le9kyC2zz8qSnn5YefNBC4ZUrpUsu8YJpF3IvWGBh+I4dkc3qoiJpwgRp1ChrQefk2Aj1bt0sYHb7tNPS7Dn3nikp3uj0WCH4okVSfr6NR3fBuWTBvGuvZ2dbwF7fzu7kZHtesvdt7Osa0pix6gTgAACgNaABDgAAAAAAAKBVCYUsuL7vPmnsWK/VnZIijR9v993PK1dak3vjxro7vYuKpMmTpQEDpClTpK+/rtusXrfOgvDCQm+vtn+f9rHHeu8pSWVl3l7xaCkp0umne01rF8IPHy5t3Spt3iyVlFigvq8QOynJjispsc/X2NfVx4X/9T0XPVadEegAAOBwRQAOAAAAAAAAoFVx47rT0y38dc1l/333swu+a2rqD6bLyqQzzpDat7cGdayR5zfcELv1PGiQtH17ZHvajVefPTvyPLffbjvJ/cf6Q/hJk6SKisiAvSEukE9NrRvM76/GjFWPj7cQf8gQqUuXA38vAACA5sQIdAAAAAAAAACHjVDIAm431jsctlbzrl0W8iYlSeXl9nx8fOS48eJi77772R985+bWP3L8ppu8gDsnR5o1y8Z+R488j3bBBVLv3pEjz/3j1bdujRydnpAQezx6WZlda9euTfdd7o/GjFU/8URpyxb7hwW9e9vf1cGE7gAAAM2BBjgAAAAAAACAFhUKSXv2SFVVttc7LU067TQLoBctkkaPlvbuteeOP17q3NnC2ehx4/77/p9dI/vCC6WJE72R41u3WlB+6qnSf/+3dz2NbVaHw9ITT0irVtVtlrtmd/To9MY0rVtKQ2PVw2HpgQekjAxp4EBr2C9caI8DAAAcTmiAAwAAAAAAADjkXNP7m28sSB0xQnrzTSkvz55ftsyayHl50tNPez9LXrCdl1e31e3ux8VJ8+fbDnDJgu8ZM+x51x7/7DPpootsz7fb4b0/179woTRvnpSZWX+z/IYbItvjjWlatyQX9qem2m1iovdZ5871jgsGvfs5OTTBAQDA4YMGOAAAAAAAAIBDKhy2QHXVKgup77tPGjvWC6FTUqTx4+2+/2fHv2O7uNjGjY8cKW3bJr36qoWxV10lrV5tzfJf/MIeHz7cQvfu3aXHH7fm97p1ds5evfbvM7g95JI38tw1yzdvjmxPR2uoaX048n/WaAUF9jwAAMDhggY4AAAAAAAAgEPGtYnvu89C38svt7HapaXeXuz0dO/+kCGRz0l1d2zv3m0t6rVrpaVLLdQuK7PwPD3d2t5bt0rnniv99rfWao4Obfc3AA8G617T5Mnee77yiv1cn1hN68NV9GeNfm7nTu9zAAAAtDQCcAAAAAAAAADNzo08j4+31rA/9I6Pl3r2tBA7GLRWt7vv/zlW4DxokPTBB9a6vvBCqaLCguchQ+y1GzZ4r9mzx/v56KMjr29/A/BAoO41SRa819RIXbvu3/kOZ/V9Vvdct26H9noAAAAawgh0AAAAAAAAAM3KjTwfN87GlEeH3GVl3l5vyQLs7dttzHn0c9GmTJFefFEqL5f69rV94Vu2SM88Y7d/+pPt6JbseWfAAO+9hgyx1vb+qK6264slO9uebyuOpM8KAABaPxrgAAAAAAAAAJqNG3k+d66FzbFC77w82+u9dq2N0p4yRVq+3J7bu9f2hK9caecrLLQAPRCw57OzbRz6r35lY8cLCqSpUyOPWbvWjsnI8K6rVy8LyU8/3Zro7drZtbrR5PuSnCzNmGE/FxR475edbY8frvu8D8SR9FkBAEDrRwMcAAAAAAAAQLNwY88LCux+dJs7N9dC1NmzrRE+Zoz0ox9ZyH3DDXZ/+HBp9WobX56TY8dt2WI7vUeOtGOKiqQ+fex98vK8Ud3BoN0vKLB94f4AfM8eaf16e2zgQCktzYL6cLjxny8pya6ppMRC9JISu98WA+Ej6bMCAIDWjQY4AAAAAAAAgCYXDkvLlknnnhu5O9o1vSULuseMke66S5o1y17TqZMXmLs93ykpNqK8qkp6/33piiukjz+2QF2y5/v0sfPFUlhogfnGjXbftdLz8rxjgkFrqUsW7O5PE1yy5rokJSY27nWt0ZH0WQEAQOtFAxwAAAAAAADAAauslHbtskZ1ebl3u2CBdPvt3shzp6jIa3Zv3Sq9+qo0YYJUUyM9+qj0xReRgblkQfeGDdKmTbYbvLzcC78lC8d37Kj7Oic+3p5zO8D9rfRoBQX2PAAAAFonAnAAAAAAAAAAB+Trr6XqaumRR6SdO6V77pEGD7ZmcEFB3ZHnjmt2FxRYsL1xo3TnndIdd9QNzP0CAWsfFxdHPl5cLB11VN3XZWZKTz9tI9MTEqSuXS2wDwbrD8uDQfssAAAAaJ0IwAEAAAAAAADst1DIgutFi6T+/b392x072o5oFzD793y7gDoQsPvTptntoEE2pry+wNzJypJWrYpsf0t2/513Il+XmWmj1t96y/Z8p6XZn6VL7f0bCtm7dTvQbwUAAAAtjQAcAAAAAAAAwH5LSJAGDpSWL5fGj/f2bxcXR7a4o0eeb94slZRII0fa47t2NS4wv/VW6YYbpIcfttdGW7w48nX5+V4o784dDNr5//lPOzaW7GxrtQMAAKB1IgAHAAAAAAAA0GihkAXELriObnzHanG7kecDBkgrV0qvvCJNmmSPFxfbWPN9BeY/+Ym915Il9vq//MXOkZJir1u/3l53+ul2/Nlne6F8tGnTLAifMycyZJ8zR5oxQ0pObvKvDQAAAIcIATgAAAAAAACACKGQtGePBdt79kgVFXZbVSUtXCgdf7yUlGRN76qqunu762txX3+9dPHF0o03esfuKzB/4w3pm2+k3/zGRpmfe660erU0bpz0wAO23/tPf5J69LDX/b//ZzvId+6sf8/3+vV23Tk5FpaXltptTo59LgAAALReBOAAAAAAAAAAvg29XcidliaddpoFycuWSV9+aWPF586VNm2y0HrTJmnKlNgBtmtxb9tmf7ZulYYNs8eLiiLf2wXm0Y3sn/3MWt75+dK8eVJ6uu31fvNNqXdvu5+RIb39tvTii7b3u1Mn7/UN7fnu3Nma3omJ1kBPTKT5DQAA0BbEt/QFAAAAAAAAADg0QiHb3R0MWghcXW2hbzhsofeIERYu5+XZ8cuW2Y7vSy+14woKvHPl5kqvvy7dcov02996O7ULC+381dUWeldWStddJ61bZ23vWFxg/vrr0qxZFrp362bnSEjw3te/19sJBr37CxZITz5pP1dX2zXNnVv3/dye78TEA/oaAQAAcBijAQ4AAAAAAAAcAVzI7Zrdq1ZJ7dpJ5eUWHN93nzR2rLc3OyVFGj9eGjRIeuIJ29XtHyleVCSdeqr0j39IU6daYP3zn9so8a++kj76SLrgAhspfvXV3q5uP9fWdufr0qVuIzsYtD/ueurb611YKE2YYJ9PstfOmMGebwAAgCMNATgAAAAAAADQxoVCFnLPnRs5RnzoUAuaCwrs8dJSL+QeNUravdtC8cWL6+75lqzRfdNNFkzHxUndu0t790r33mtBdJ8+3ojytWttRPmAAd7rhw71fu7Wzdre0dwo8+jrixYMStu3ewG4ZOE7e74BAACOLATgAAAAAAAAQBvl9nrHx3tjxAsKpKVLbWx4x45eqFxc7IXcmZnSww9bKF1a6u38dnu+MzOlp5+WtmyRnnlGevVV2x3ugvZ587yg2o0oLyiQ7rpLOuYYa3MPGWIhu5OaGvszuFHm/uuLJRCwc3zzTeTj7PkGAAA4shCAAwAAAAAAAG2QG3k+bpy0bZsF0aNG2a7tpUvtGH+oXFbmhdz5+dKSJdIrr1jzOhCwnd/Z2fb42rXSW29Zu3vgQKl3b+n++yP3dfulpEirV0unny7deacXnC9YIP3pTxaoxxqRLnmjzH/2M2nNGi+Ej5aVZWPda2sP+qsDAABAK0YADgAAAAAAALQSrtFdWmq3lZWxj9m1yxt5vnGjF3Ln5dmYcNfO9ofekoXcN9wgnXmm7dT++c8tSJ8+3XZ0jxkj/fCH9lxeXmTL+9FH644o9zfFn3zSxqT/+c9ecN6rl/Txx9Lrr0ujR9f/ud0o8wkTpJkz6+71nj3bwnl2ewMAAIAAHAAAAAAAAGgFXKN79Ghp3TobDV5RYWH3nj3Sjh02hrywUGrXzmtiu5A7J0caOdL2dPvHiLtm9+zZ1gi/+moLsYNBC70vv9xeO2eOvWdqauyWd3Fx5LkzMyOb4m+/Lc2fb6F8MGjPL1tm711ZaWPTQ6H6P78bZd6xY+Re7+JiadgwC+eLiqROnZri2wYAAEBrRQAOAAAAAAAAHObcbu2nnpJWrrRQ+fTTpfbtLRRPS5P+9jcLmGM1sXNzpeuuk8rLIxvfktfsHj7cRqU/8IB01FFekP3cc9L3vieNGCF98IG0e3fkuR0XtGdn2/38fAvK8/JsB/n48RbOS3XD8b59bdT6woUW9O+Lf693MChdcIF9DvccAAAAjlzxLX0BAAAAAAAAABrmdmsvW+aFyk8/7f2ckmIB89SpFja7kecuqC4qkiZOtD3c8+dbiC5ZIB0MWov67bels86Sbr5ZuvRSC8nz8iJff9xx0nvvRZ7bb/5829PdqZN3PZKF2/5Q3h+OO8GgtcMla3g3NsiObnwTgAMAABzZaIADAAAAAAAAh7lgMLJF7QJv16j2B8wpKTYSffr0yHOsWyf99a8WcrvG99at0ubNdnvhhbaL+7HHLATPzq67a/vii20Mumt5Rzv7bKm21t47FPIC7+JiL5SPvvZoBQUW+DdWdADOCHQAAIAjGw1wAAAAAAAA4DAWDktdukjHHuuF3EOGSNu3xw6Y8/Ol3/3OQujaWq/lHQhIn30mzZghxcVJV11lofqxx1q7e9o06bLL7HxlZdI110gPPyzNmiXt3Cl162bhd3KynUOysNqdOzvbHk9Ksufi472muBuPnpVlzfXoEe1+waC9X2pq476f9u3tPd3odBrgAAAARzYa4AAAAAAAAMBhyu3+fvFFC6ldyN2li9Srl9fOdgFzTo61q2+7LXbL++ij7ficHKmkRProI2nVKumEEyzw/sEPLKDeskVaskTq2tVC79RU27ntwuWkJO8cpaV2m5Pjhd9S3aZ4bq7dv/RS73PEEghY2L4//KE3DXAAAIAjGw1wAAAAAAAA4DAUCnm7v9PTpbVrpU2brEV98snSP/8Zuac7N9dGmJeXW4s6GJQmT7aR4+np1hIvK7Nwu39/e01qqvTOO9LYsdKJJ0pHHWWB+9Sp9Te7HRc6u6Z2YmLd5/1NcbdH/IEHpL177bxu57dfdraF59Hna0hysrRjR+R1AQAA4MhEAA4AAAAAAAC0MBd2u9D5m2+kZcukc8/1wuwxY6SlS6WZM+01Y8dKK1faz4WFFjBff72NP3ejxyULvcvK7OdAIHK0eDhsje+lS+393nrLC9QlO4cLqXNy9j9cdk3xAx2j3lg0wAEAAOAwAh0AAAAAAABoQeGwtHChlJZmf9assRb27bdHjgovKpImTJCuvdYC4/XrvTHnX3xhIfejj1qY7h897qSkSPPm2Xkkb7z6vHm2r3v8eAvSYykosID+QCQnW5v7QMao7897xPoZAAAARx4CcAAAAAAAAOAQCoWkPXss9N21y0LouXNtTPlzz1mze+lSb693Vlbk6//6V9vNHQhYmD1jhlRZKS1ebAH6qafaa2691Y7JzLSm+OefSxddJA0eHDleXbL3Li31WuPRgkFrcDe1+sLxAzlPrJ8BAABw5CEABwAAAAAAAA4Rf9v7Rz/yQujMTG/H9xdfeEF0bq61uWfP9prgNTXSp596Le/8fGnJEhtdHgxaKD5mjDRsmPTll7bje/16qXdvC7p79bJx5260umT7wf1t82iBgI0vP1y50DsuTurQoWWvBQAAAC2LABwAAAAAAABoRq7xXV4e2fZeudJa3sGghdgFBdIdd9Qde+7GnG/dKm3ebKPCBw2y5nd+fuzR5UVF0uTJFqrn59t7urA7GLTx6l26eO9TX9vcyc623d2HKxeAJydbCA4AAIAjFwE4AAAAAAAA0Exc43vwYBvx7UaO5+dLv/611L27hdkuxI4VRLswe8AAC82rq6VOnWxX9vTpFrDHGl2ekmLj0N17+pWVSatXR+4Kj9U2DwSkOXMsbD+cR4u7a+vUqWWvAwAAAC0vvqUvAAAAAAAAAGjt3E7tYNBC43DYmsiLFln7esgQb8d2SooF3lOnWjB+442R+7dzc625LVko7s55/fXSZZdFBtGdOknx8fZ8dAi+r73et9xi49ElC8mLiqSJE6UHHrD94Tt32tjz6moL2w9nycn2vQ4b1tJXAgAAgJZGAxwAAAAAAAA4CP693qedZsHxffdJ7dp57Wv/ju1Ro6SKCgumc3OlSy6xsHpfY89HjpR+97u6719dHdnkdoqLI88b6/n27aWcHDt/aan02mvWSE9MlFJT7fZwbn4706ZJW7ZIv/qVjZsPhVr6igAAANBSCMABAAAAAACARnC7vEtL7baiQtq1y9vr7d/l/dhjke1rN9r8jjukhx+2UDoQsLB79GgLoP0hdvTY87VrbQx5Skrd60pOtufmzIkcXf6zn0k1NbHDccnb652c3PoCb79wWPrjH6WMDPu+0tLsHySEwy19ZQAAAGgJBOAAAAAAAADAPjSm5e1GmxcWRja+ndxc6Yor7PgXX/T2fBcVSeecY/u8o/dv33qrjT0fNUp65hnpggtit5uTkiKb3CUldr9Ll9jheGvY690YoZD9A4R587x/bBAM2j9IWLCAJjgAAMCRiAAcAAAAAAAAqEco1PiWt3/ntmt8u5BbsscSE6WlSy0Mz872Au+iIum886SLLrLw/PPPpW3bpKuvlu6919rNAwdKvXrV326ur8ldXzh+uO/1boyEBO8fIEQrKLDnAQAAcGSJb+kLAAAAAAAAAA5H4bC1ubOy6ra8p06V4uO9lncwGNn6dvu916611xUWRgbkwaDt+V6wwPZ8b99uwXVNjR3To4dUWWnvO2+ed02u3SxZiN3YBrc7LjXVbhMTD/x7OZy477K+53bu9D4zAAAAjgw0wAEAAAAAAID/5fZ8l5dbOP3oo41veUffLyqykHv4cAu5X3xR6tPHG0Xu3/M9aZKNN09MlB58UDrxRGto025umNulXt9z3bodwosBAADAYYEAHAAAAAAAAJC353vwYAuiCwpsh3avXl7IGr3bO3qUubt/663eaPOrrrJzdegg/etfkWPRJQvON2yQFi2y8epz50odO0YG79Fcu/lIV11t33cs2dn2PAAAAI4sBOAAAAAAAAA4okXv+Xbhc3q69Oc/Sxs3Nr7l/dJLUteuFnpv3Spt3my3mZnSqadKixdHBuaS3ebnS8cd5zW+o4P2aLSbTXKyNGOGNGdO5Pc5Z4493tgR8QAAAGg72AEOAAAAAACAI1asPd8ufF60yB5bsSJyl7fb7R0XJy1d6rW8c3Kk666Txo2T1q2zfeHp6Xa+sjJ7fU2NBeYLF0olJdbi7tbNHvfvs/YH7Xl5da/btZvbyi7vg5GUZN/9rFne91ldbY8DAADgyEMDHAAAAAAAAEekUCj2nu+yMgu4Tz/dAu/olvfzz1uz+OabpS++sJb3tm1ey/uTT6QhQ+xcxcUWgqek2P0tW+x8P/uZhdepqXbbqVPdfdbR49Ul2s31SU6O/D75bgAAAI5cNMABAAAAAADQ5oVCUkKCjTpPTLR93PHx1vCOj/fGjbsQ/P77pWHDvPtFRdLkyZGt7n/8Q7rySqm83MLyF1+0Uebjx9tjXbvaexUXWzC7apUF10VFUvfuda/R7bOeO9d7zzFjLKTfts2uPRCg3QwAAAA0hAY4AAAAAAAA2rRw2EaOjx4t7d1rP48bZ6FyMFh3r3dmpvTTn1pIHb2Du6xM2rDBRpanpkoff2z3XWv8rbcsDO/QQbrrLqlXL+noo6WMDOntt+2YzMzYAXisfdbFxdK779q49Z49aTcDAAAA+0IDHAAAAAAAAG1WKGSB99y50tNPW+M7L8+a3P7Wt9vrnZoqTZlix+3dW/8O7qwsa3S73d6XXead2/8+TjDo3V+wQFq2LPb1ss8aAAAAODgE4AAAAAAAAGizEhIsjE5JsdHkU6fa4/7Wd16eN278+edt73denjW116614wsLLcQOBKTp0+3PmDH2XEqKdNppFoJHv0+0wkLbI756df3X7Breqal2m5h4cN8BAAAAcCQhAAcAAAAAAECbEL3nOynJdnEHg9KQIVJpqbfTW/Ja35IF02VlFjoXFNhj/h3cW7dK27dLvXtLH34onX++7Q53O8HduWO9j18waOfp27fZvgYAAADgiMYOcAAAAAAAALR6sfZ8H3+81LmztbaLi72R544LuIcPt4C7qMhCdH94XVQkTZ4sDRggTZpkz3fubOPPn3lG2rJFuvNO79yx3scvELCQvR3/qxwAAADQLPh/tQEAAAAAANCqhULW0p47V5o509u/vWmTN+bcP/LczwXcBQXSRx9JXbrEDq/LyqROnaxV/thjUkaGNHCg3a5bJ+3Z0/D7OG53eIcOTf41AAAAABAj0AEAAAAAANDK1bfnW4occz5/vrRypf3s3+mdlSVNm2Zt8Ece8faCR3vwQSk/X5o3z3ssGLRjU1OlX/7Smt31vY9/d/js2U39LQAAAACQaIADAAAAAACglQsG7Y9/F7fjH3P+yiu2Gzwnx0aVl5RIX35pY9PHjLFj77pLys6Wbr3Va4IHAhZ8H3ectx882m23Wfh9003Sa69J7dvb+5SU2DWVlEjnnuu9T/fuzfqVAAAAAEcsAnAAAAAAAAC0GqGQjRvfuVPatUv6+mupW7eG928XFUkzZkiVldI990j9+tno8smTpXvvlU46yTt21SoLqSdNigyvp0/3gvZYgkE7tksXC9l79PB+Tk2128cft2uR7HkAAAAATY8AHAAAAAAAAK1COCw9/LC0e7fdX7jQWt8vvLDv/dv5+dbenjvXwuqyMumNNywQf/JJ6e677bhduyyk/v3vI8PrTp0sWI+1H1yyx1NTrfldnx49bEz7kCF2CwAAAKDpEYADAAAAAADgsFdRIS1YYO3trVulRYts93YwaHu+s7Ntr/bjj9su7jlzvLB60CDpzDOlpUu982VmSk8/LW3ZIp13nu0OX7nSHpdiN7Srq+19YsnKkv72t4Y/wyWX2Ps984zUt6+12QEAAAA0LQJwAAAAAAAAHLYqK6WqKikhQXrsMQuqBw2SCgu9Y9ye79NOk5Ytkx55RPre9ywo37xZ2rDBAnQ3vjwzU1q7VnrrLRuFPnCglJYmvfmmPZ6ZGXtHd3KyjVL3h+uBgDR/vu3+fuqp+j9HOGzhvHu/9HRrsIfDTfM9AQAAADDxLX0BAAAAAAAAgJOUlPTtz+GwtGmT9Oqr1tLu2FEqL7cmdvQu7qIia1TPn2/NcMnGjKen2/HvvmthdTDojUN3x0n2uLu/YIG9Jvb1STk50qxZNoo9IcH+FBdbwzwUsqDcLxSysHvevMj3mzvXfs7JqfsaAAAAAAeGBjgAAAAAAABaXCgkxcd3VM+emfrmm47atUtassTa0osXSz17WhO8e3f7OXoXd0qKtcP9zfCyMmt///Of0urVNr481nF+hYXShAnWCK9PcrK3F3zRIqlXL+noo6U+fWK3uhMSLHCPpaDAngcAAADQNAjAAQAAAAAA0KLCYQuOR4+Ok9RehYVxatfOdmWXlloL/KWXpClT7HbTJtu57ZeebsdGN8MlC71//WvbDX777fUfJ9nj27fbaxoSCllTfO5c71yu1b1gQeR+72Cw4ffbubPh9wIAAADQeATgAAAAAAAAOORCIWnPHhtp7oLkmTOtEf344xZSb9zotb1zc63B/dlntkf7lluk2bO9JnhVlYXg/mZ4Zqb09NPSli3WJo+Pl6680pra0Q1yJxCQUlOlzp0bvv79aXUHAg2/X7duDb8XAAAAgMYjAAcAAAAAAMAh5RrfgwfbKPGCgsjR5MXFFnzX1FjjOyvLdnyPGSP162d7uPfutd3ZxcVSSYmNOi8vl6ZPt/fIzJTWrpXeessC84EDbaz5okXS119bmB5LVpa0alX9gbWzP63u6ur63y87u/594wAAAAD2X3xLXwAAAAAAAADaplDImtDBoAXKLuhduNAa30OGeOPI/T9LXvCdm2tBtmTh+OTJ0qBB0k03SddcI3XoYGFzcrKF4jNmSHFx0ogRFqzn5XnX40aUp6TYeSU7xl1fVpYF0uPGSR9+2PBnc63uWCF4dKs7OdmuK/r9srPt8aSkRn2dAAAAABqBBjgAAAAAAACanGt5p6XZn9GjbXy5f3S4a3oHApE/S97I8wsvlCZOlIYPl7ZutT8ffSRNnWrBcTgs/e531vLu21caO1b68Y+lCRMsMI9lzhypfXtrkJeUWPBeUiKdcop0/vlSjx4Wojdkf1vdSUl13y8nh/AbAAAAaGoE4AAAAAAAAGgyoZC0a5e31zsYtHHkK1dKq1dLX3zhtabLyrymt/9nyRt5Pny4ve5737PwvHt3G5uenGzvtWCBNG+ed87166VLLpG2b9/3iPLkZDtXaqrdduhg48//+79tP3koVP/ndK3uOXO80D4QsPszZtjzsV7jf79YxwAAAAA4OATgAAAAAAAAaBLhsLWu27XzWt6SlJ9v9++4I7LlLXlN79mzpfnzvZ8DAQvBb7lFWr5c6trVQuNOnbzX+tvkfsXF3ojyWKJHlLtrX7PGmuRHH22t9YUL7fH60OoGAAAADj8E4AAAAAAAADhoro396KORu7xTUqTx4y0Yj255S5FN7zVr7LGcnFoVF9vxH30knXuuhd3RjexgMHbL271PY0eUx2qSu33hCxbsuwlOqxsAAAA4fBCAAwAAAAAAYL+EQjYifMcOG3e+Z48UH29t7Ohd3unpkYG4v/Htjikult55x857zTXS//2/tdq1S1q82JrYffpIvXrVbWQ31PKeP9/eqzEjyutrkkv2eELC/n9HAAAAAFoGATgAAAAAAAAaLRy2IHr0aGnvXvt53Dhp2zYLuaNb3tGBuL/xvXWrtHmzjQ4fOdLO+eyz0oUXxmnJEikvr+FGdnV1/S3vs8+WamsbN6K8via5e27nzgP8sgAAAAAccgTgAAAAAAAAaJBrfJeXWwA9d640c6a1o/PypI0bI0Nuf8u7pib22PPJk6UBA6SVK6X335cmTrTHU1KkM86IU2Fh7GvxN7KTk63NXV/Lu1Onxo0o39994QAAAAAOXwTgAAAAAAAAqMOF3lVV1vIePNgC5IKCyL3eUt3Wd1GRdP750mmnWcv7+9+3wNwF1Skp0imnSD//uXTxxTau3IkemR4tupGdlNS4lndDGmqSR+8LBwAAAHB4i2/pCwAAAAAAAMDhxY05HzFCevNNa3kPGeIF0/6fndxcae1aa1oPGiSNHWvHtG/vhdE33ijdcosF6Tt3Sl27Sq+9ZmPQHf/I9FgheKxGtmt1p6babWLi/n1e1ySXLOAPBu19srPt8f0J0wEAAAC0LBrgAAAAAAAAqDPm/L77LMR2LW9/MB2911uy1vfUqdLVV0vr10sZGdK550q7dkl3323nqqmxYD09XUpLk3r1stB81SopM9POE90mj9ZcjeymaJIDAAAAaHkE4AAAAAAAAEc41/j2jzmPHkXuD6brC6l/8hPpzjttR3gwKOXn27nmzYvcGe7OGQza/YICC90TEmqVkiI99JC1xevb7R1rj3dTaMy+cAAAAACHN0agAwAAAAAAHKFCIWnvXmnRIgut/aPN4+PrjiJ3Y84l29u9cqX9XFhox48fby1wydsTPnVq5M+xFBZKX3xhTfCTT7Zr6NRJuvlmadYsG5ferZs1v2lkAwAAAGgIDXAAAAAAAIAjUDhswXO7dtbAliJHm8dqeRcVSWPGSMOHS2vW2GO/+IX05ZfS++9boO7Ccn+DPLpNHi093W5fftlGpw8cKPXubaPT9+6lkQ0AAACg8QjAAQAAAAAAjhDRe74ffbT+MeeSNb6zs6XZs71R5NXV0tatUmWldM010qBBUt++0iWXSF26eMfta2e436JFNjp93ry4iPHoc+fadYZCzfBlAAAAAGiTCMABAAAAAADaMBd6V1XV3fMdK5j2h97Fxdb4HjnSWt5ffSV99JF0wQVSx47S9ddLo0bZ6157TXrnHS88b8zOcMnGo59+utdCj1ZQICUkNOlXAgAAAKANYwc4AAAAAABAGxUOW+g9YoT05ptSXl7knm/JC6bz8uy+G3O+YIG0bZsdd9RRFqLfe6+0dKmNLF+0yILr4cOlzp1tf/eTT0q33mrnKSz0dobHxdXdGR4MWvB+221SRUX949GDQdsBnpraXN8SAAAAgLaEBjgAAAAAAEAbEj3m/L77pLFjLXSW6ra+9zXm/Lrr7DU2otzC77VrLVDv3dvuZ2RIb79t4ffUqRaKl5TYcd26SbfcYg3x9u2lnBx7rrTUbk8+2QL0+sajBwJ2DgAAAABoDAJwAAAAAACANsI1vv1jztPTG97z7Rrfw4dLX3wh7djhjTlPSrIx5z17egF6fr6dNy9PEfu68/Ls8Wuvla66yt4/NdVuu3a12x49bE+4/7kHHpBefTX2eHTJwvnq6mb80gAAAAC0KQTgAAAAAAAAbUAoZI3vuXNtP7cLvfe15zsQsBB8wQKppkZaskRKS5P69LFm96ef2gjyYND2dY8f74Xh0QoLpQkTpIEDG3/d2dnWUJ8xQ5ozp/bb6wwEpDlz7PHk5AP4QgAAAAAckQjAAQAAAAAAWrlQSEpIsAa2FBl6Rze+pcjW97Zt9uell2yv99y5kc3uO+6wBncgULdNHi0YlLZvlwYNatx1h8PSH/5gYfvw4dL3vhenbdtqVVJSq5ISG5eelHQg3wgAAACAIxUBOAAAAAAAQCsWDkvLltn48vrGnP/mN9Ivf2mNatewLi6W3nnHwvNf/MIC9KVL656/rExavdqa2rHa5H6BgI02b8zIctdYnzfPrruoSJo0SerXL06//72dg+Y3AAAAgP0V39IXAAAAAAAAgAMTCtnO7/vuk6ZOtQDaheC5udLatRZIT5kiPfyw9L3vSVu3Wks7LU3as0e69FLp888t6K6v2X3LLdLbb0u1tdKaNRas5+XVPS4rS1q1SoqL2/e1+xvrfmVl0uzZcbr22kZ9BQAAAAAQgQAcAAAAAACglXEjz+PjLUQOBr3Gtwum3Zjz55+33dxz59rjKSk2yry4WPrpT6Vrr7U/PXpEBuh+xcV2e8EF0uDB0umnS+3aee8dCNh7Z2fbe44ate/PEAw2PEp9504L7wEAAABgfzACHQAAAAAAoBUJh631PXWq9NVXkY3v7Gxp9mxvRHl1tTW9/U3rsjJpwwa7XbpUmjDBHv/73yP3hPtlZUkvvig99ZSUmCh17Gj7uUtKbCd4SYmF4mPGWPBe34h0v0Cg4VHq3brt+xwAAAAAEI0AHAAAAAAAoJVwe7OfespCbX+I7Brfw4fbmPPNm6WPP7YmdUNN6+3brRE+c6YF6Lfe6p0zELBAPTtbmjFD6trVe21ysoXhqal2e889dg3udftSXW3njSU7u7ZRe8QBAAAAIBoBOAAAAAAAQCvh9mbn50tLllgr29/aLiqSJk+WBgyQVq60vd3dujXctE5NtRHnb75pAfoPf+g1u7dtk4YN85rd/gA8WvfusX+uT3Kyhepz5kQG7nPm1Co3154HAAAAgP1FAA4AAAAAAHAYC4WkPXukXbsslI6Pl8aPt73escaeBwLSz34mXXKJhcvvvdfwaPNVq2wcumQh94oVXrN7xQrb++2a3Q0F4D16eD83pgEuSUlJ0aPUa3X99RWSwo07AQAAAABEiW/pCwAAAAAAAEBs4bD08MPSlCnWiO7eXTr2WAuLg0H7M2aMjUXfutXGmaemSlVV0tVXS2+9Jf3619Jdd0nt2ll7PBi0gDory8LzMWMi39O/e7tDh8jnmjoAl7ymd2qqVFlZpW3b/qVu3Y5v/AkAAAAAwIcGOAAAAAAAwGHI7fvu10964gkbU/7SS9LEiVLPnpG7v93Y80mTpBNPlDp2lNats7D7vfcs5L7mGq9pvXVr5GhzP3947Q/DpX2PQE9JkYYMsTAbAAAAAFoCATgAAAAAAMBhKCFBeuwxG3e+eLGF3vPnS9OmSZs21R1rXlYmbdggXXqpN9a8tlb67DMLubdt80abn3JK5GhzP3/ovT8B+LnnSlu2SM88I510kgX4AAAAAHCoEYADAAAAAAAcJqL3fXfsaLebNln7+8wzrbVdWirNnGk7vv27v2fPlm68UZoxwztnMGi3/jB77976r8HfAI8OvOsLwMNh6be/lTIypIEDpfR0aeFCexwAAAAADiUCcAAAAAAAgBbgwu7SUrutqrJ93xUVUny8jRSvqvLGnefm2s7uCy+ULrpIOuMMadw4a3aXlNjtsGHSFVfEbnb7g+2GdnTvbwPcjWqfN88L24NBae5ce5wmOAAAAIBDiQAcAAAAAADgEKqstGB74UIpLc3+rFkTe9/3lCl2m5VlofaYMdLw4bbD+/HHpZEjpZoaC8l/9Ssba/7KK7Hft74AvGfPyOP2NwBPSJAKCmK/Z0GBPQ8AAAAAhwoBOAAAAAAAwCESDts48/nzrSGdni69+qo0dqwF2tH7vrOzbYd3draNNy8uliZPlk48UfrLX6S4OKlLFzt3YqLdVlXVfd/ERCkpybvvD8D794881v9ccrLU7n//16OEBKlDh7rnDga95nes53burOfLAAAAAIBmQAAOAAAAAABwCIRC0pIltiO7sFDKzJRef1066aSG933362dB9A032Kjz4mLpo4+kqVMjQ+3u3et/70DAwvJYx/bsae/t+FvfcXFe67tr18hz+M9d30j1QKBuixwAAAAAmhMBOAAAAAAAwCGQkCA984yF3MGglJ9vo8wLCxve933VVdbSPv98G3MeCFijOzk58vwN7fWOfs5/v1s37367dlLnzpHHugA71vhzSaqutmuNJTvbngcAAACAQyW+pS8AAAAAAACgrfv6a2n7dmnjRgu5Bw2yceeS7QIfPDhy33denrW/FyywkHz7dtsVXl0dewy5dHABePfu0pdf2s/RLe9u3aSUFGnYsNjnTk6WZsywnwsKLNwPBCz8njEjsqUOAAAAAM2NBjgAAAAAAEAzCoVsr3f37lJNjYXcN94olZd7bXDX+N7Xvu/6WthS3RHoRx3l/dyYBnhKijRiRN3zFhRIW7ZId98t7dljnydaUpKUk2Mj2ktL7TYnh/AbAAAAwKFHAA4AAAAAANBMQiEbfZ6f77W7c3OlSy6xgNqNPC8qir3vu6Qk9r7vWKJD7gED6n/OH5YPHGgt9C1bpF//OjLkDoell1+WMjLsfGlpdmw4XPf9k5NtNHtqauwR7QAAAABwKBCAAwAAAAAANINwWFq2TPrii8iW94UXSuedZ+H2p59aKC5ZCD55su37HjfO9n137tz4MDm6AX700d7PsRrgKSnSpEkWxv/1r3VD7l27bAT7vHl2/ZLdzp1rj8dqggMAAABASyMABwAAAAAAaGKhkIXEt99et+X9/9m7++i47/pO9G87lipHSTQJkqOASY3DZg0XGojL4wE3tIa2bFJWtDw1tBtoYFmOogSSCilJXdb4xMalN0RR27tLYcNCSm/K6TYhUB68eTALuTfldgsNVAQch9bJ+kHAGDP2JHLs+8d3h5FsOU+2NbL0ep3zO7+Z3+87M9/hcObEfvvz+axcmdxxR6kM/1f/KrnqqtLyvBFS79+fvPGNybvfnZx88pP/zI6OqfPBHy8Af97zSsX3Jz6RfOQjh4fcf/qnyaJFpf35dEZGyv4BAABmGwE4AAAAwDHW1lZC4vHx5OtfL5XfSbPKe9my5Nd+Lfn0p5OTTirzsrdvP/r52ZOD7mXLSpX3C16QnHVW83q9nvzZn5XZ4osXTx9y9/aW/TRC8UNVq8nu3U99fwAAAMfbolZvAAAAAGCuqVbLsWJFqfh+6UuTAweS0dFyvVHl/fa3Tw26e3rKub396X3u6aeXAH3FitJm/d/9uxKqn3VWs2X5xo2ljfkLXlDuTRdyb9/erFyf7n6lknR1Pb09AgAAHE8qwAEAAACOsUqlHBs2JNdfn7z85SUI37Yt2bq1nN/4xhKKH+vPXbEi2by5tDdfujRZvrwE4KOjzcr0ZGrIfajx8eSuu5qV64caGEgmJo7t3gEAAI4FFeAAAAAAx1i9ngwNJatXJ5dcUqqo+/pKS/JGe/H9+0u19rFUqZTPHRlJ1q1rXq9WS7v1iy9uVnSPjyebNiX9/VPXNjzwQHmvpLxftVref2AgGR5+ei3aAQAAjjcBOAAAAMAxUKuVCuvHHks++cnkXe9KfvzjqS3Ex8fL0bB7d7Pt+bFw9tnN0P1Q27eXFumT25oPDZVq8aTZnr0Rcr/rXSXkHhxMrrmm7LWrq1R+C78BAIDZSgt0AAAAgKNUr5eW4//rf5W25wMDyYUXlorv6VqMJ8dnjvbAQAnip5vb3aj4ntzWfGwsWbWqtGd/+OFSkb5jRwm9GyF3Z2eZSd7TU86dncd2zwAAAMeSABwAAADgKNRqycc+lrztbWWmdmPG9j33JF/6UmkxPp1jPUe7Xk8+97kSUB8pdL/uulL1vWZNc8327cn//J/JggVl/0JuAADgRCYABwAAADgKbW3J8uXJX/5lCZMnV18PDZWg+9prm4FzpVIC6OHhYxc012rJ+vXlPRtzvafza7+WHDxYKrx37Eh27jy84hsAAOBEZgY4AAAAwNNQq5Vq6Wo1ueCC5PLLk3e8Y+qM7UaL8fXrk23bkr17j88c7ba2ZuX54831Hh6e+rmN+ePt7cduLwAAAK2kAhwAAADgKdi7N9m3L9m4MXne85JTTimV1Fu2TF99PTaW9PWVMLoxT/tYtxivVg8P3VeuLKH71q2lMl2VNwAAMB+oAAcAAAB4kvbtK0H3Lbck69aVa3ffXSrAK5XHr76+/PLjF0BXKodXnvf1Jd3dybnnJnfeaa43AAAwP6gABwAAAHgCe/cme/YkN9xQ5n2Pjjbvvf/9Sb2eXHbZ9NXXDz+cXHnl8a2+npgoIfuhxseT1avLfQAAgPlAAA4AAADwOOr15MEHk0WLks99rrQ7b1RaJyX0fvvbS4vxNWtKu/G+vuS885LPfz5ZsCA57bTju8fOzjLfe82aUgmelPOaNeW66m8AAGC+EIADAAAAHEGtVqq+ly0rwfb99ydLljRD5oYvfCF5yUuSV74y2bGjhOTf+U5yySUzN3e7o6OE8I3P37HD3G8AAGD+EYADAAAAHEFbW3LbbcmuXSX43r8/2bQp6e8/fO3YWPI//kcJynt6kvb2ma+87uwsn9uqzwcAAGi1Ra3eAAAAAMBs9MgjJfi+//6kuzu5664SfA8NJZs3lzWjo6UdeqVSZnD39yf/8i/J2We3cOMAAADzmApwAAAAgEPUaslHP5qcfnqz6nvLlhJy/9ZvJRdemKxcmWzbVo4dO5Jf+qVk1SotxwEAAFpJBTgAAADAIdrakg0bkhUrplZ9f+YzycteVp7v2pUsXJicckryR3+U/MEfJAcPJmec0erdAwAAzF8CcAAAAIBJ9u5NfvrT0tp8crvzCy9MhoeT17wm+clPkmc9q4Tgv/IrSb1ewu+kVI0DAADQGlqgAwAAAPNarZY8+mjywx8m+/YlN96YdHaWud5jY6Wt+cqVyR13JC96UXnNaaeVKvGHHiprfvCDcr2zM/m5n2vVNwEAAEAADgAAAMw7jdB7375k48bk1a9O9uwpbc+HhsrM7/7+snZsLOnrS5YtSy66KBkdbb5PT085q/4GAACYHbRABwAAAOaVer2E3hdckNx5Z/JXf5V89aulentkpKyZ3Pp8dLS0Q9+/P3njG5PLL086Osq97u6p723+NwAAQGupAAcAAADmjVotWb8++cpXkle+srQ737Ah+cu/TLZvL0F3MrX1+bZtydat5f7gYDP8TkpoPvm5ABwAAKC1BOAAAADAnNdoeb5oUanyXrcu2bWrPF+9OvnoR5MlS8rc74bJrc8vvjhZsKAE3pMtWDC1ClwADgAA0FoCcAAAAGBOa7Q8f81rkocfLqH3y15W5nWfe26yc2eyZcvUud+TjY8nv/IrycTE9O/fmAOemAEOAADQamaAAwAAAHNSrZYcOJB85CPJ2rXJK16RnHVWM/T+1reSCy9sVn5PN/e7UkkGBpL3v//w6u8GFeAAAACzhwpwAAAAYM6p10uIfdJJpeX5ihXJrbcm99/fDL2vuy659NJS/d3fP/3c74cfTi65JPnCF478WQJwAACA2UMADgAAAMwpP/lJsn598rWvJXv2lEruDRtKEP6WtzRD79e9rgTeO3cmV1+drFmTbN9e5n6fd15y++2livz1rz9y9XcytQW6ABwAAKC1BOAAAADAnLB3b7JvX9LWVsLu//AfktNOS845J1m9ulSEN6q8G6H3m9+cvOlNZcZ3Y0b4jh3Jffclz3pW8upXl9csXXrkz+3uLscLXpD09s7c9wUAAOBwAnAAAADghFevl6ruj3+8VHEvWlSC7jvuSK64ogTe1WpZOzaWvPa1JfR+9atLu/O/+Zvkla8sM8OXLEkefDB54xuT8fESbD/72Uf+7De/uay/7bZSVV6rHfevCwAAwBEIwAEAAIATWq2W3HBDsnx58tGPlgD73HNL6H3VVclb31oqsyuVqa+7554ShJ93XnL66Ul7e7PV+bJlJRRvBNtdXdMH2/V6cvPNpUJ8+fLkrLOSjRvLdQAAAGaeABwAAAA4YdVqpeX5bbeVwHvLlmTTpuTCC0sQvn17qfLesSMZGJj+PS6+OHn00cOvf+MbzWD7zDMPD7ZrtTJr/EMfalaXV6vJ2rXlukpwAACAmScABwAAAE44jXnfN92UPPRQcv/9JfCuVJKhoeTSS0sY3t9fWp7/+q8nl12WXHttsxK8UknWrEmGh5uV30kz2F637vGD7cas8emMjJT7AAAAzCwBOAAAAHDCaATfW7Yk112XfPCDJfjev79UfjcC71WrSkX41VeXkLtRCf6ylyUPP1wqwnfsSAYHk46OqZ/xZIPtarUZkB+qWk127z423xkAAIAnb1GrNwAAAADwZNTrJfi+/fbSznx0tATNjeB7aCjZvLmsHR0t871f97oyH/yaa0og3dWVTEyU0Dwpc78P9WSC7Z6eUkFeqUy/tlIpnwUAAMDMUgEOAAAAzHq1Wgmyly8vAfjOnc3geWioBOK/9Vtl9vfKlcm2beX43OeSZz+7BN09PeU8ud35dBrB9pHuNYLtiYkjzxUfGCj3AQAAmFkCcAAAAGDWa2tLbrutBN+T530nzZbnK1cmd9yRvOhFZf3ppz+5wPtQTzbY7uws88PXrHniueIAAADMDAE4AAAAMKvt3VuqvRvB9+R53w1jY0lfX7JsWakQf/TR5OSTn97nPZVgu6OjzBHfsaOE80eaKw4AAMDMMAMcAAAAmHVqtVLF/dhjyY03JpddNjX4PnTed7VaQur3vjd5y1uSU045us9vBNuHzg6fLthuBOI9PeU83VxxAAAAZoYAHAAAAJhV6vVk48bkpS9N/p//J/nsZ5N3v7uE4JOD7wsvLBXZ27aVALy7O7n77uTii5Mvf/no9yHYBgAAOPFogQ4AAADMGrVasn598qd/mlxwQanu3rAh+dSnSgD+W79Vgu/GvO+XvKRUiv/gB8l73pP86q+Wim0AAADmJxXgAAAAQMs1Wp4vWpSMjCRLl5aZ2osWJatXl+d/9mclHB8aSnbtKq/73veSZzwjecMbkp/+tFzr7m7d9wAAAKC1BOAAAABASzVanm/alHz606Wd+aJFyZIlybnnliC8Wi1HX18JuHt7k+3bk/Hx5KGHyvP77ivv94xntPDLAAAA0FJaoAMAAAAtUasle/aUqu61a5ODB5OzzkoqlRJsb9pU2p0vWVKuNYyPl7B7fLxcP+OMEoY3qAAHAACYvwTgAAAAwIyr18t874ULS8vzFSuSW29N7r8/6e8va4aGkksvTbZsaV471MBA8sMfljC8QQAOAAAwfwnAAQAAgBlVq5Wq709/utnefMOGEoS/5S0l1L722lLVvWpV8qMfJVdfnaxZ06wEr1TK8+Hh5LHHpr6/ABwAAGD+MgMcAAAAmBG1WtLWVuZ7j4w053yfc06yenVyySUlDF+1qgTk27Ylu3YlPT3JgQPJxRcn11yT7N6ddHUlExNJR0dpgT6ZGeAAAADzlwpwAAAA4Lir15ONG5PXvCZ5+OESdDfmfF9xRbMSPEnGxpK+vmTZsuSii8p5167k859P2ttLIN7ennR2lvWnnNJ8nKgABwAAmM8E4AAAAMBxU6sle/aUiu61a5ODB5Ozzmq2Mh8aSt761qS3t3mtYXw8ue++ZP/+Enp3dBz5c3p7m48F4AAAAPOXABwAAAA4Lur1ZHQ0WbiwtDxfsSK59dbk/vuT/v6yZmwsefWrkx07yuzv6fT3J1/5Sqn0PpIzzyzB9wteUMJyAAAA5iczwAEAAIBjpjHnu1ZLPvrR5K//Onnzm0t785tuKkH4Zz+bbN5c1o+OlhD8LW8pIXdS1lSrpSK8v78E46tWJddff+TP/chHkl/4hdJKvVIpnz+5LToAAADzgwpwAAAA4JhozPl+/vPLjO6RkWT79mTJkuScc5LVq5uB96pVycqVybZtydatyR13lErxN72pVIPv3Jk89FBy/vll7dhYeZ8jfe7f/m2ydGmyfHmpBt+4sVwHAABgflEBDgAAAByVWi05cKBUYa9dW9qQ79xZqriTZNOm5Iorpl4bG0v6+krb8t7eEpT/3d8lX/hCs435FVckN9zQ/JzpWpvXaiXs/tCHmteq1bKPJBkcVAkOAAAwn6gABwAAAJ62Q+d8J82q70qlPB8aSt761hJ0N641jI8n992X7N9fAu6TTmreO/nkqWunC8Db2pqfe6iRkXIfAACA+UMADgAAADwttVqyfn3yta8lP/lJs7p7fLxUfff3l+djY8mrX11amw8MTP9e/f1lBnhHR/PamWc2H5966tR7DdVq83Onu7d791P7TgAAAJzYtEAHAAAAnpJarVRWL1qUfPGLyec/X9qMVyrNMHpoKNm8uTxuzP1+y1tKyJ2U6uxqtbymv78E46tWJWvWND9ncgA+XfV3Ul4/+XMPvdfV9fS/JwAAACceFeAAAADAk1avl5nbr3lN8vDDydVXlzndX/5ys+I7KYH3qlXJypVl3cMPJ3fdldx7b/Jv/22pBt+5M3nooeT888vasbHkjDOa7/FkAvCJiSNXlQ8MlPsAAADMHyrAAQAAgCdUqyUHDiQf+Uiydm3yilckZ51VZn1fckmZ7z254rtaLbPA/+f/LO3P//2/TxYvTm65JbnsslIB3tOTvPvdycc+1vycyQH4kiVJd3d57+c+d/p9dXYmw8Pl8eSq8oGBcn26tukAAADMXQJwAAAA4HHV6yXU7u8vIfOKFcmttyb//M/JySc353CvWlVmgm/bluzaVQLuej15xzuSz30uWbq0vN/kau6f+7mpnzU5AH/Ws5IHHyyV4r29JYTv7Dx8fx0dyeBgcs01ZeZ3V1ep/BZ+AwAAzD9aoAMAAABHVKuVUPvTny5BdLWabNhQgvBLLilhdqVS1o6NJX19ybJlyUUXJeedV0Loe+4p97dtK+fu7ub7T36cNAPwej25/voSmi9fnjzzmaX1er0+/T47O5P29rKf9vbpg3IAAADmPgE4AAAAcJhaLXn00WTRohJ2b99eWpKfc06yenWpCL/nntL2/NAZ3OPjyX33JW97W/KVr5Tnk02uAJ/8eMGCUr3dCN3Xri2Be1LOa9eW67Xa8fjGAAAAzAUCcAAAAGCKer1UW7/mNcnDD5fweXw82bQpueKKZiV4UsLv/v7k2mubleCVSnnemMN9qCNVgFcqycKFSVtbCd2nMzJS7gMAAMB0BOAAAABAklJZvWdPs/r64MHkrLOawfbQUPLWt5Z53JPbnq9alaxcWVqcb92a7NiRvOxl5frY2OGfM7nqe3IA3mh/3pgpPp1qtcz5BgAAgOkIwAEAAIDU66Wt+cKFpcp6xYrk1luT++8vFd5JCbNf/eoScE9uez559vfnP5/s3VtmgE8XfidTQ+/JYXgjAK9UmgH7oSqV0iYdAAAAprOo1RsAAAAAWqtWKy3P//qvkze/uVRZ33RTCcI/+9ky5zspAfnYWPKWt5TZ3klZU62WYPq9701+7/dKi/IFC0oF+XSe8Yzm4+7ucvT2Js95Trk2MVEC9rVrD3/twEC5395+bL47AAAAc4sAHAAAAOa5xsztRYuSJUuSc85JVq9OLrmkhNurVpW26Nu2Jbt2lartgwdLlfc115SW5F1dJZju6CjvecYZyQ9/WB7//M8nP/hBeXz66eVzGk45JXnwwTJX/MwzSxjf2dmcHT45YG/MFG98BgAAABxKC3QAAACYp2q1Elr/+MclZB4fTzZtSq64ogTSjTnck1ucX3RROY+PJ5/6VKnE7ukp587O5ntPbm3+/OdPf71eTz7ykWTp0mT58uRZzyqV6PV6CbkHB0u79Z07y3lwUPgNAADA4xOAAwAAwDxUr5ew+XnPK1XYjZnbQ0PJW99aWpIfOod7fDy5775k//4SZC98nL9VmBx0r1hR2py/4AXJv/pX5VqtVqrK165tBu3Vanm+fn2zEvxIATsAAABMRwAOAAAA80itluzZ0wyft2wpVd/9/eX+2Fjy6leXiuuBgenfo7+/zAD/uZ878uc0AvAVK5JLLy1tzm+7LbnllrKHRtv16YyMlPsAAADwVAnAAQAAYJ6o15PR0VK53QifV6woFeBXX52sWVOqvsfGkre8Jbnyyua1pJyvvbY5i7u7+8if1dNT3nvz5uQv/3Jqm/ObbirV3o3K70NVq2WuOAAAADxVAnAAAACYBxotxz/96eZ870ZAfdddyQUXJC9+cbJtWzm++tXkH/8x+dVfbc7hfvjh5Pzzk1WrSkj+RAH4H/9xCdo/9KGpbc4/+MHk1FMPb7HeUKkkXV3H7rsDAAAwfyxq9QYAAACA46fRbnzRohJGL1qULFlSQuYNG8q1devK2r6+Emr39iYXX1wC8j/5k9LuvKenhOe/8zvN937GM478ub/7u8nP/3x5n0ONjyd33FEqydeuPfz+wEAyMVHmfgMAAMBTIQAHAACAOapeTzZuLDO+P/3pZhX217+eDA0lq1cnl1wy9TXj4+X48IdLJfgXvtC8d9ppU9ceqQK8Xk++9KXk3/ybI7c5v+qq5O//vjweGSnrKpVme/WOjqf0VQEAACCJABwAAADmpFqthN9r15agulH13dubrFyZvPzlyY9//PhzuHftSpYta15bsmTqmukC8Mbn/umfJu94R/nM6T5j+/bkpJOSwcHkmmvKzO+urlL5LfwGAADg6TIDHAAAAOaYRtvzkZHyfHy8VIH395e259dfX2Z7d3c//hzunp7SMr3h0AB8uhbojc+d/JnTabQ57+wsrc57esq5s/OpflsAAABoUgEOAAAAc0i9ntx0U/L61zcrr1esSE45Jbn66vL8kkvKvS99qQTUjRngk/X3l9nfJ5/cvDY5AF+4cPrwvFptfu7QULJ5c3k8OqrNOQAAAMefCnAAAACYI2q1ZP365IMfbLY8X7GihNB33ZX83u9NbXs+NFTC6GuvbYbZlUp53gipJ1d5d3YmixeXyvFXvaqE4IeqVJrvNTaWrFpVWq5v25Zs3Zrs2FHangu/AQAAOB4E4AAAADBHTNd+fMOGcm3dulLR3dX1xAH1y15Wro+NTZ3zvWBB8t/+W/Lgg8knP5k8+mgJ3SebmCjhecPYWNLXV2aJf/7zzbbnAAAAcDwIwAEAAOAEV6uVYPnQ6u7LL09e97rSfjyZfi73oQH17t3JRReV68nUALxeT/7H/0iWLk2e85zkzDOTjRvL9YbOzlI5vmbN1Kry9763VKALvwEAADieBOAAAABwAqvXSwj9vOeVOd+T53J///vJT37SDMWTI7c9bwTUbW1T378RgDfaq69b13y/ajVZu7Zcn1wJ3tFR2pzv2JHs3KntOQAAADNHAA4AAAAnqEYovXZtsmVLs7q7Mff7zjsPD8UPbXt+aEB92mnJokXN9Y0Z4I326tMZGTk8OO/sTNrbk56eclb5DQAAwEwQgAMAAMAJqFY7PJRuVHf/3/93uX711Ye3PE+abc9HR5tBdSOgXriwhNbd3ckv/mJy8snlerU6tZJ8smq1tE4HAACAVlv0xEsAAACA2aReT266KXn966eG0mNjyRvekNxxR3Pu99BQqQZPyrVqtVSEDwyUGeHTtSX/1KeSl7882bUrefTRMl+8UinHdCF4pZJ0dR277wcAAABPlwpwAAAAOIE02p5/8IPJkiVT25snyZ49yf/6X82g+tCW51u3Jtu3H3kmd72e3H13snRp8pznJGeeWWaMP/JICc2nMzBQQnIAAABotVlVAf6DH/wgH//4x/PNb34z3/ve97J8+fLcfvvtU9b8zu/8Tu69997DXvuFL3wh55xzzkxtFQAAAFqi0fa8Wm22N1+3rnl/+/ZmMD45BO/rK23Nzz23zAafbiZ3rVbC7g99qHmtWi0zxru7SzV50vz8RiX58PD0YToAAADMtFkVgH/ve9/L3XffnfPOOy8HDhzIwYMHp113/vnn5wMf+MCUa0uXLp2JLQIAAEBLTZ7FPV178/37ky1bSjC9du3U146PJ+99b6nWbm8//L0PnSk+2Zo1yb//96Vy/Jpryszvrq7yXsJvAAAAZotZFYD/8i//clavXp0kGRoayn333TftutNOOy0vetGLZnBnAAAAMDtMnsXdaG++fn1pb75rV/LMZyZf/3qZ7508tWrtyeH6dPd27056esrzxnm6IB0AAABaZVbNAF+4cFZtBwAAAGadiYmps7gb7c2XLUtuv71UhF92WakOHxxMduxIdu4s5yPN/W5ohOtHutfVdey+BwAAABwPs6oC/Mm6995786IXvSiPPfZYzjvvvFx++eV5yUteclTvefDgwezdu/cY7ZCGffv2TTkDcOz5rQU4/vzWMpssXLggQ0MdOXgwufHGBT+r7v6DP0guvrhUZN92W3LmmQezf/+BLFz4aE45Jdm//2AWLEge74++jz32cxkYWJi1axccdm9g4GDq9QPZv/+R4/bdwO8twPHntxbg+PNbe+wdPHgwCxYc/mfV6ZxwAfhLXvKSvOENb8iyZcuyc+fOfPzjH8873vGOfOpTn8qLX/zip/2+ExMT+ad/+qdjuFMme/DBB1u9BYA5z28twPHnt5bZoqOjI6985Yp84AMnZdeuEnY/9tiCfPjDzVnglcqCDAwszOBgex58cCz1ev1Jve/g4IokCzMysmBS6/SDGRw88KTfB46W31uA489vLcDx57f22Gp/kjO4Fhw8ePDgcd7L09KYAX777bc/7rq9e/fmwgsvzDnnnJOPfexjT+uz/vEf/zEHDx7Mc5/73Kf1eo5s3759efDBB7Ns2bIsXry41dsBmJP81gIcf35rmW3q9eQ1r1mcRx5ZkLa25M/+7GC+9KUFWbfu8LVr1hzMlVceyKJFT65ye8GCBZmYaE9Hx8KfBeD1+oG0tT2aWfpXCMwhfm8Bjj+/tQDHn9/aY+/73/9+FixYkBe+8IVPuPaEqwA/1Mknn5xf+qVfype+9KWjep8FCxbk5JNPPka74lCLFy/2vy/Acea3FuD481vLbFCrJZ2dya23Jj09yd13J+efvyAXXTT9+pGRBbnmmpPS3v7k/7/b+PuZJUvKub39pCT+0oaZ4/cW4PjzWwtw/PmtPXaebPvzJFl4HPcBAAAAHEP79iUbNya9vclznpMsXZo88ECyZ09pez6dajXZvXsmdwkAAACtc8IH4Hv37s1dd931pMrdAQAA4ES1Z0+yfn2ydm0z7K5Wk//4H5PTTiutyqdTqSRdXTOzRwAAAGi1WdUCfd++fbn77ruTJA899FB++tOf5otf/GKS5KUvfWkeeOCB/Pmf/3le+9rX5lnPelZ27tyZ//Jf/kt27dqVG264oZVbBwAAgONmz55k0aLkxhsPvzc+ntxxRzIwUMLxQw0MJBMTSXv78d8nAAAAtNqsCsB/+MMf5vLLL59yrfH8v/7X/5re3t5MTEzk+uuvT7VazeLFi/PiF784//E//sf8wi/8Qiu2DAAAAMdVvZ60tSX/638duc35VVcl/9//Vx6PjJR1lUoJv4eHk46OGdosAAAAtNisCsCXLl2a7373u4+75uMf//gM7QYAAABaq1ZLbropufDCZMmSEmpPF4Jv317O/+7fJddcU2Z+d3WVym/hNwAAAPPJCT8DHAAAAOaqtrbk+uuT7u7krruS/v7p1w0MJHfemYyNlVbnPT3l3Nk5o9sFAACAlptVFeAAAABA8cgjya5dyZYtyaZNyQ9+UILuJBkdbbY5v+yy5Pd/P3nJS5JPfrKVOwYAAIDWUwEOAAAAs0ytlnz0o8npp5eQe2goedvbks98JnnZy5Jt25KtW5OHH07e//7kXe9KFi1Kzjyz1TsHAACA1hKAAwAAwCzT1pZs2FAqv/v7S2vzVauSs89OXvOa5Ic/THp7kwMHShD+53+e3HZbctZZJTwHAACA+UoADgAAALNMtVqOoaHS9vzaa5Pt25O+vuS885LPfS5ZsCA56aTkv/23ZOnSZPnyUgG+cWNSr7f6GwAAAEBrmAEOAAAAs0ylUo5G5ff69aXt+a5dSU9Pqfzev7+E3R/6UPN11Wqydm15PDiYdHa2YPMAAADQQirAAQAAYJap10vld1JC8L6+ZNmy5KKLktHRZOHC0iZ9ZGT614+MlPsAAAAw3wjAAQAAYBbZty/55CfL7O9rry2V4Emp+H7jG5PLLy+V3Y026dOpVpPdu2dmvwAAADCbCMABAABgltizp7Q7Hxgorc9Xriytz7duTR5+OLnyyqSjo6xttEmfTqWSdHXN0KYBAABgFhGAAwAAwCxQqyWLFiU33lieH9r6/IUvbIbfSTIx0WyTfqiBgXIfAAAA5ptFrd4AAAAAkLS3l2rvQ9uaj4+XIyltzXt6yuPOzmR4uDweGSmvq1RK+D08PDUsBwAAgPlCAA4AAACzQLWaLFlSQuzpZntP19a8oyMZHEyuuaaE411dpfJb+A0AAMB8pQU6AAAAzAJdXclddyX9/dPfP1Jb887OUj3e01POnZ3HdZsAAAAwq6kABwAAgFmgXk8eeKA513t0tNnW/LLLkqGhZPHiVu4QAAAAZj8BOAAAALRYvZ588pPJW9+afOYzycteVgLvXbuSM89M9u8XfgMAAMCToQU6AAAAtFCtlqxfXyq/V61Kzj47ec1rkh/+MOntLeH3qae2epcAAABwYlABDgAAAC3U1paMjJTHY2NJX1/S3V3C7337ku98p7X7AwAAgBOJABwAAABaqFotx2Tj4+VIkt27k56emd4VAAAAnJi0QAcAAIAWqlTKcaR7XV0zuBkAAAA4wQnAAQAAoAX27i0tzr/73aS/f/o1AwPJxMTM7gsAAABOZFqgAwAAwAzbty/ZsiW55Zbks59NNm8u10dHSzv0SqWE38PDSUdHK3cKAAAAJxYBOAAAAMygPXuSP/mT5LLLmoH3qlXJ+vXJtm3Jrl1l5veBA8JvAAAAeKq0QAcAAIAZUqslixYln/tcsnNnCb+TZGws6etLli1LLrqonOv1Fm4UAAAATlACcAAAAJgh7e3J9u3J/fcnS5aUVueTjY8n992X7N+fdHW1ZIsAAABwQhOAAwAAwAypVkvwvX9/smlT0t8//bqBgWRiYka3BgAAAHOCGeAAAAAwQ7q6kq98pQTfQ0PJ5s3lemMWeKVSwu/hYfO/AQAA4OkQgAMAAMAMmZhIHnighNxJcuGFJezetq0E4D09ZY3wGwAAAJ4eLdABAABghnR2Jr/3e8lnPpO87GXJHXckL3pRsnBhqf5uby9rAAAAgKdHBTgAAADMkFotWbQo+e3fTk47Ldm9O3nWs5JHHxV8AwAAwLGgAhwAAACOs717k337ko0bk97e0ur8+c9Pbrkleewx4TcAAAAcKyrAAQAA4Diq15MtW0rYvW5d8/qWLUl/f7JzZzI4KAQHAACAY0EFOAAAABwntVpyww3J8uXJ6Oj0a0ZGkra2md0XAAAAzFUCcAAAADhO2tqS224rVd7V6vRrqtUyCxwAAAA4egJwAAAAOE6q1eT++5MlS5JKZfo1lUrS1TWDmwIAAIA5TAAOAAAAx0G9npx6arJ/f7JpU5n3PZ2BgWRiYmb3BgAAAHPVolZvAAAAAOaaWi3ZuDE5//wSfA8NJZs3l3ujo6UyvFIp4ffwcNLR0crdAgAAwNwhAAcAAIBjrK0tGRlJenubwfeFF5awe9u2EoD39JTKb+E3AAAAHDtaoAMAAMAxVq2WY2wsWbUqWbkyueOO5EUvKvdPOSVpb086O1u4SQAAAJiDVIADAADAMVaplKMRgvf1Jd3dpSJ8377kO99p8QYBAABgjlIBDgAAAMdQrZZ897tl9vdk4+PJffclF19cWp8DAAAAx54KcAAAADiG2tqSSy9Nbr+9PB8dLZXglUoJxYeGksWLW7lDAAAAmLtUgAMAAMAxVK0m997bnP29bVuydWs5n39+aYEOAAAAHB8qwAEAAOAYqdeTU08t1d6Hzv7evj3Zvz/ZsaPVuwQAAIC5SwU4AAAAHAO1WrJ+ffLlL0+d/92Y/T0+ngwMmP8NAAAAx5MKcAAAADgG2tqSkZFS7b15c7k2ef73wEAyPJx0dLRylwAAADC3CcABAADgGKhWm8eqVaUafNu2ZNeupKcnefRR4TcAAAAcbwJwAAAAOAYqlXJUq4fP/963L/nOd1q8QQAAAJgHzAAHAACAo1SvJ9/97tTZ30lz/vfFF5v9DQAAADNBBTgAAAAchVot2bgxueUWs78BAACg1QTgAAAAcBTa2pKRkSPP/j5wQPgNAAAAM0UADgAAAEehWi1Hcvjs7+3by+zvU09t5Q4BAABg/jADHAAAAI5CpVKOyRqzv/fvT7q6WrErAAAAmJ8E4AAAAHAUJibKnO/pDAyU+wAAAMDM0AIdAAAAjsKCBcnQUHncmAVeqZTwe3jY/G8AAACYSQJwAAAAeJrq9eTDH06++MUSdm/bVgLwnp5S+S38BgAAgJklAAcAAICnoVZLNm5M1q4tz/v6ku7upLc3efvbk/7+1u4PAAAA5iMzwAEAAOBpaGsrLc8nGx9P7rsv2bCh3AcAAABmlgAcAAAAnoZqtRxHurd79wxuBgAAAEgiAAcAAICnpVIpx5HudXXN4GYAAACAJAJwAAAAeMpqteSRR5KBgenvDwwkExMzuycAAABAAA4AAABPSb2efOITJeB+//uTa69tVoJXKsmaNcnwcNLZ2cpdAgAAwPwkAAcAAIAnqVZL1q9Pzj47uf765OUvT1auTLZtS7ZuLec3vjE5cKDVOwUAAID5aVGrNwAAAAAnira25Oabk8HB5JJLkmo16etLuruT3t5k+/Zk//5kx45W7xQAAADmJwE4AAAAPEnVarJ4cbJzZ3ncMD5ejobdu5OenpneHQAAAKAFOgAAADwJtVqZ8b1vX7JkSXPu96EqlaSrawY3BgAAAPyMABwAAACeQL2ebNyYfOUrydvelmzalPT3T792YCCZmJjZ/QEAAACFFugAAADwOGq1En6vXZusWJFs3px85jMl6E6S0dHSDr1SKdeGh5OOjlbuGAAAAOYvFeAAAADwONrakpGR8nhsLFm1Kjn77KSzM7n88mTHjuYxOCj8BgAAgFZSAQ4AAACPo1otR8PYWNLXl3R3J729yV13lZngSdLe3oINAgAAAD8jAAcAAIBp1Gql+rurq7Q3nxyCJ8n4eLJ/f3Lqqa3YHQAAADAdLdABAADgEPV6mft95pnJF7+Y9Pc373V3Jy94QTkPDCQTE63bJwAAADCVCnAAAACYpFYr4ffateX50FCyeXPS05Occ05ywQXJzp2l/fn+/WUWOAAAADA7qAAHAACA/63R9nxkpHltbCy55JLkne9M7r03Wbo0Wb48eeYzk498pFSLAwAAALODABwAAABSguybbkoeeujwed/vfnfy4Q+XqvDGvWq1PF+/vgTnAAAAQOsJwAEAAJj3arUSZH/wg8mSJUml0rzX3Z2sXp2Mjk7/2pGRUjUOAAAAtJ4AHAAAgHmrVksefTRZtKgE2ePjyaZNSX9/c01vb5n5fWhVeEO1muzePRO7BQAAAJ6IABwAAIB5qV5PPvGJ5Kc/TXbtagbcQ0PJwEBy7bWlEnz79sOrwierVJKurpnZMwAAAPD4BOAAAADMK7VasmdPaXl+9tnJf/pPyemnNwPusbFk1apk5coyD3xsLDlwoITi0xkYSCYmZmz7AAAAwOMQgAMAADBv1OtllvfChcnNN5fZ3hs3Ht72fGwsGR5O9u5NbrihrOvvb1aFJ+W8Zk1Z19nZim8DAAAAHGpRqzcAAAAAM6FWK2H3X/918uY3J4sXN2d7Dw0lmzeXdaOj5dpHPlLmgn/oQ+X6qlWlanzbtnK/p6dUfnd0tOgLAQAAAIcRgAMAADAvtLWVQHvRojLTe9++5mzvRtvzRsD9wx8mvb3J29/efP3YWNLXl3R3J+eem9x5p8pvAAAAmG20QAcAAGBeqFbLMT5eWp6/7W1TW583Au5ly5Irrkh27SrrDzU+nnz968nu3TO2dQAAAOBJEoADAAAw59XryamnNud3Dw0lAwPJD35QzpNne+/fn7zkJaXSu3HtUJVK0tV1/PcNAAAAPDUCcAAAAOa0Wq20Nv/yl6dWe69alZx9dmlj/r73JTt2lOPhh5MVK8pM8Mb6Qw0MlPnfAAAAwOxiBjgAAABzWmP2d29vCbWTZHS0hODveEepBv/3/z5pby8zwf/P/zO58spmCL5wYXl9tVoqvwcGkuHhpKOjld8KAAAAmI4AHAAAgDmtMfu7Wi1V3+vXJ9u2lRnfPT3Jvn3J2rUl+E5K6/OkBOS/+ZvJ3/5tcs01ZeZ3V1ep/BZ+AwAAwOwkAAcAAGBOq1TKUa2WULuvr4Tcvb0l/P7mN8ss8Iazz27eb2srLdKTEpYnpVIcAAAAmJ3MAAcAAGBOq9dL2/LJxseT++5Lfvu3k698Zeq9F7wgefDB5Lbbks99rswQBwAAAE4MAnAAAADmrHo9+eQnk/7+5NprSyV4Us7XXpu8//1lnndXV3P9jTcmS5cmy5cnz3pWsnFjuQ4AAADMfgJwAAAA5qRarcz7Hhgos79Xriyzv7duTR5+OHnnO0sAPjZWAvDG+rVrS7v0pJzXri3XVYIDAADA7CcABwAAYE5qa0tGRsrjxuzvZcuSiy5KXvjCZMmS0gY9SZ75zKnrDzUyUu4DAAAAs9uiVm8AAAAAjodqtVnJ3TA+Xo4k2bUr2b+/PO7tnX795PfavTvp6TkuWwUAAACOERXgAAAAzEmVSnPm93T3enqSf/mX8vykk554fWNOOAAAADB7CcABAACYk+r1Mv97OpddlnzlK81q8I6OZGLiyOsHBsp9AAAAYHbTAh0AAIA5p15PPvnJpL8/OXAgGR0tbcwrlRJmv+99ySte0Vzf1ZV0dibDw+X5yMjU9cPDJSQHAAAAZjcBOAAAAHNKrZZs3JisXZv86Z8m69cn27aVmd9nnlkquW+6KRkba76m0d68oyMZHEyuuabM/O7qKuuF3wAAAHBiEIADAAAwp7S1lQrupITcfX1Jd3fS25vs25d85ztlzWSTZ393dpZzT085t7cf9y0DAAAAx4gAHAAAgDmlWi3HZOPjzXnfu3cnZ5019X6jAhwAAAA4sS1s9QYAAADgWKnVklNPnVrRPVmlUsLuQyvABeAAAAAwNwjAAQAAmDPa2pI77kj6+6e/PzBQZnqffHLzWkeHNucAAAAwV2iBDgAAwAlv795k4cLS5vyqq5LNm8v10dHSDr1SKaH40FCyePHUCnHV3wAAADB3qAAHAADghFavJ1u2JNdfn5x+erJ9e7JqVbJyZbJtW7J1azm/9KXJSSeV1wjAAQAAYG4SgAMAAHBCqtWSPXuSG25Ili9PNm5MNm0qld5jY0lfX7JsWXLRReX8jW+U9ufJ1AD8SPPCAQAAgBOPABwAAIATTr1e2psvXJjcdluyc2dpdT40VOZ8X3ttCbbHx0v193vfmwwPJ52d5fWnndZ8LxXgAAAAMHeYAQ4AAMAJpVYr1d5//dfJm9+c3H9/smRJCbzHxkr78/XrS/C9a1fS05McOJB0dDTf46STkuc8pwTiz352y74KAAAAcIypAAcAAOCE0taWjIyUWd9LliT79zdbnyeHtz9vVIpPVqsl//iPpXp8dLQ8BwAAAE58AnAAAABOGI880mx3Pj7eDL4PbX2elGD8jW9MLr+82fo8Ke3TN25Mli4ts8Of+czyvF5vxTcCAAAAjiUt0AEAADgh1GqlWru/v4TcjZnfmzeX+xdeWOZ8b9tW7vX0JBMTU1ufN9qnr13bvFatNp8PDk4NywEAAIATiwpwAAAATghtbcmGDYe3O1+1Klm5MrnjjuQlLynrTj89aW8/PMxutE+fzshIuQ8AAACcuFSAAwAAMOs98kiya9fhVd+jo6UV+h/9UfLP/5xcemkJvtvbp3+farUcR7q3e3epHAcAAABOTCrAAQAAmNVqteSjHy1V3ZVKs+r7l34p+cEPSsvz//bfkve8pwTlj6dSac4In+5eV9cx3ToAAAAwwwTgAAAAzGrTtT5PkvPOK5Xfvb3JmWeW46MfTer1I7/XxEQyMDD9vYGBch8AAAA4cWmBDgAAwKzWaFs+ufX5y19eZnavWzd13dq15fHg4OHzv5NybXi4PB4ZKa+pVEr4PTycdHQct68BAAAAzAAV4AAAAMxatVqzbXmj9fkrXpG89rVl/vd0RkZK1fiRdHSUgHzHjmTnznIeHBR+AwAAwFwgAAcAAGBWqteTT3yihOCXXVaujY0lH/hA8vDDpXp7OtVqsnv34793Z2fS3p709JTzdNXiAAAAwIlHAA4AAMCsU6sl69cnZ5+dfOpTJQC/9tpSCb59ewmuK5XpX1upJF1dM7hZAAAAYNYQgAMAADDrtLUlN9+crF6d/OEfltbnK1cm27Ylf/d3yWOPlbnd0xkYSCYmZna/AAAAwOywqNUbAAAAgENVq8nixWVGd7Vajr6+pLs76e1NTj01ufXWsnZkpNyvVEr4PTxsnjcAAADMVwJwAAAAZo1arVR/d3Ul+/YlS5aUYLsx73t8vBxJcuGFyaZNyTXXlJnfXV2l8lv4DQAAAPOXFugAAADMCvV68olPlDD7u99N3va2EnD390+//td+LVm4MGlvLzPB29uTzs6Z3TMAAAAwu6gABwAAoOVqtWTjxuT880tL889+Ntm8OfnMZ5qzvkdHtToHAAAAHp8KcAAAAFqurS25+eZk9eoSdI+NJatWJWefXaq6L7882bGjeQwOCr8BAACAw6kABwAAoOWq1WTx4mTnzua877GxpK8v6e5OenvLfO+vf720Om9vb+VuAQAAgNlKAA4AAEBL1Wqlrfm+fcmSJeVxIwRPkvHxclQqySmntGaPAAAAwIlBC3QAAABapl4vs7+/9rXk4ouTTZuS/v7p1w4MlCpwAAAAgCMRgAMAANAStVqyfn1yyy3Jeecl739/8oMflKD72mtLxXdSzmvWJMPDZR44AAAAwJEIwAEAAJhRtVry6KPJokXJyEiyYUNy/fXJy1+enH12CbkvvzzZsSPZvr2cBweTjo5W7xwAAACY7cwABwAAYMY0Wp5v2pR8+tMlBF+9OrnkkjL3u68v6e5OentLu/NnPCO5806V3wAAAMCTIwAHAABgRtRqJfxeu7aE3EuWJOeem+zcWcLvhvHxcjTs3p309Mz4dgEAAIATkBboAAAAzIi2ttLyPCkB96ZNyYUXliC8Me/7UJVK0tU1UzsEAAAATnQCcAAAAI67vXtLlXej0nvFiuSUU5IrrkgeeCDp75/+dQMDpRU6AAAAwJMhAAcAAOC4qteTG28sc7wrlRJ+b96c3HVXcsEFyY4dydVXJ2vWNCvBK5XyfHjY/G8AAADgyTMDHAAAgONm8tzvFStKpfcv/mJphb5uXVnz2tcmr3hFWTM0lOzZUwLwiYmko6Ol2wcAAABOMAJwAAAAjpvJc7+HhpKvfrVUdF9yydR199xTgvBzzkn+6Z/K69rbZ3y7AAAAwAlOC3QAAACOm8lzv8fGkne+M/nxj5vXDrVly5HvAQAAADwRATgAAADHTaXSnOudlErvrq6p1w5d39V1/PcFAAAAzE0CcAAAAI65Wi159NFyHhhoXh8fTzZtKrPApzMwUGZ/AwAAADwdZoADAABwTNXrycaNyRe/mFx3XZn9nZRZ4NVqufblLycLFzavVSol/B4eTjo6Wrh5AAAA4ISmAhwAAIBjplZL1q9Pbrkluf325K67kgsuSF784mTbtnJs3pw88EBy6aXJjh3Jzp3lPDgo/AYAAACOjgAcAACAY6atrVR1b9hQzuvWJffem/T1Ja99bTI2lhw4kJx5ZrJkSWl33tOTtLcnnZ2t3j0AAABwohOAAwAAcMxUq8miRcnq1cnoaPP6ihXJrbeWivBnPjN51rOS3t7SKr1eb9VuAQAAgLnGDHAAAACOib17yyzvc88tbc2r1ea9yRXhDdVqsnZteTw4qAIcAAAAOHoqwAEAADhq9XqyZUtpcX7hhaW9eaVS7nV3H14RPtnISGmdDgAAAHC0BOAAAAAclVotueGGZPny5F3vSi69tITh/f3lfm/v4RXhk1Wrye7dM7VbAAAAYC4TgAMAAPC01GrJo4+Wmd+33VZC7nvvTVatKo+vvjpZsybZt29qRfihKpWkq2smdw4AAADMVQJwAAAAnrJ6PfnEJ5Kf/jTZtSu5//5myD02lrz2tcmv/Ery6lcn3/xmWT8wMP17DQwkExMzun0AAABgjhKAAwAA8JTUasn69cnZZyf/6T8lp5+e7N+fbNrUbHueJPfcU4LwZcuSL30pGRoqFeGNSvBKpTwfHk46O1vwRQAAAIA5Z1GrNwAAAMCJpa0tufnmZHAwueSS5PnPL8H30FCyeXNZMzpaZntXKsl735u88Y1JR0d5zTXXlJnfXV2l8rujo4VfBgAAAJhTBOAAAAA8aXv3lrbnixeXOd/V6tTg+8ILS0X3tm3lXk/P1JC7Uend01PO7e0z/Q0AAACAuUwLdAAAAJ6Uej258cbklFOSffumzvxetSpZuTK5447kRS8q6yuVEnBrbw4AAADMFAE4AAAAT6gx9/umm5JHHkl++7enzvweG0v6+sq874suKi3QAQAAAGaaABwAAIAn1NaWjIwkGzYkn/pUctllyQ9+kAwMJNdeW6q9k2T//jLv+/LLVX4DAAAAM08ADgAAwBOqVpNFi5LVq5M//MPS8vzss0vIffnlyY4dycMPl/PgYHPmNwAAAMBMWtTqDQAAADD7VSrJuecmO3eWMLxaLS3Pu7uT3t5kYqJUid95Z7kGAAAA0AoCcAAAAJ7QxETyG7+RLFlSwvBqtVwfHy9HUq6fdlqLNggAAAAQLdABAAB4Anv3lvPAQPLAA0l///TrBgZKUA4AAADQKgJwAAAAjuiRR0qovWFDcsEFZcb31Vcna9aUiu+knNesSYaHy0xwAAAAgFbRAh0AAIBp1Wql4vuWW5J168q11742ecUrkrVrk6GhZM+eEoBPTCQdHS3dLgAAAIAKcAAAAKbX1pYsX56Mjk69fs89JQh/4QuT009P2ttVfgMAAACzgwAcAACAKWq1UtG9Z0+yc2dSrU6/bsuWI98DAAAAaAUBOAAAAD9TrycbNybPe15pab5kSXPW96EqlaSrayZ3BwAAAPD4BOAAAAAkKZXf69eX+d5btiSbNpVzf//06wcGSqU4AAAAwGyxqNUbAAAAYHZoa0tGRsrjFSuSU05Jzj03ueqqcm10tLQ8r1RK+D08XKrEAQAAAGYLFeAAAAAkKeF2tVrC782bk7vuSn7pl5K/+7vk938/2b69zATfsSMZHBR+AwAAALOPABwAAIAkpbK7Ukk2bCiV4OvWJffem7z2tck555TzbbeVtuedna3eLQAAAMDhBOAAAACkVkseeSQZGkpWry7tzicbH0+++tXSDr2trTV7BAAAAHgiAnAAAIB5rl5PPvGJ5O//Prn88uTHPy6t0KdTrSa7d8/k7gAAAACePAE4AADAPFarJR/7WPLWtyabNiVveEPS3V1aoU+nUkm6umZyhwAAAABPngAcAABgHmtrS5Yvb878/vKXky99Kenvn379wECZAQ4AAAAwGy1q9QYAAABond27kwsuSN7+9ua1oaFk8+byeHS0tD2vVEr4PTycdHS0YKMAAAAAT4IAHAAAYB6rVJJt26bO/B4bS1atStavL/dqtbJuYkL4DQAAAMxuWqADAADMQ7Va8uij5dzbe/jM77GxpK8vOe+85PTTk/b2pLOzJVsFAAAAeNIE4AAAAPNMvZ5s3JiceWbyileU55ddNv3aiy8uQTkAAADAiUALdAAAgHmkVivh99q15Xm1mrz3vcnHPpYsWJCMjJj5DQAAAJy4BOAAAADzRK2WtLWVkDtJVqxINmxIVq9Ofvzj5Mork2uuSXbvTrq6zPwGAAAATjxaoAMAAMwD9Xpy003JQw+VCu8VK5LNm5NvfCNZujR59rOTn//55Prrk9NOM/MbAAAAODEJwAEAAOa4Wi1Zvz754AeTJUtKe/MNG0ol+Lp1JRBPynloKLnuuvIaAAAAgBPNUQXgDz/8cL7xjW9MuTY2NpbBwcFcccUV2bRp01FtDgAAgKPXaHs+Pp5s2pQMDpa256Oj068fGSmvAQAAADjRHNUM8HXr1mXv3r256aabkiTj4+P53d/93UxMTKSzszNf+tKXcsMNN+R1r3vdsdgrAAAAT0O12mx7fsopyfveV8LwRuX3dOt37056emZujwAAAADHwlFVgH/rW9/KK1/5yp89/5u/+ZvU6/Xceuut2bx5c17xilfkE5/4xFFvEgAAgKeuVksefTTp6kpe+tIy8/uuu5I3vCHp7i6t0KdTqZTXAAAAAJxojioA3717d57xjGf87Pldd92Vl7zkJTn77LOzcOHCvPa1r80DDzxw1JsEAADgqanXk40bkzPPTL74xeRjH2vO/P7yl5MvfSnp75/+tQMDycTEzO4XAAAA4Fg4qgD8jDPOyMMPP5wk+clPfpJ/+Id/yKtf/eqf3X/ssceyf//+o9shAAAAT0mtlqxfn6xdW9qZf/jDyb/+11Nnfg8NlaD72mubleCVSrJmTTI8nHR2tmDjAAAAAEfpqGaAv/KVr8ynPvWpnHLKKfl//9//NwcPHsyv/Mqv/Oz+97///Zx11llHvUkAAACenFotaWsr1d4Ne/Yk27dPnfk9NpasWlWC8m3byusqlVL53dEx07sGAAAAODaOqgL8yiuvzPLly/PhD384X/va1zI4OJhnP/vZSZJHH300f/u3f5tXvOIVx2SjAAAAPL56PbnppuShh5ph94oVpRX6WWcdPvN7bCzp60vOOy85/fSkvV3lNwAAAHBiO6oK8O7u7vzlX/5l9uzZk5/7uZ9Le3v7z+4dOHAgn/zkJ9Pb23vUmwQAAODx1Wol6P7TP00uuaSE3b29yebNpRr8wIEy83vdusNfe/HFyaOPlspxAAAAgBPZUQXgDaeeeuph1zo6OrJixYpj8fYAAAA8gUbb82o12bSphN2/+Ivl2rp1pRJ88+aydnS0rKtUyhzw4WFtzwEAAIC54agC8HvuuSff/va3c+mll/7s2mc/+9mMjo7m0UcfzYUXXpgPfOADOemkk456owAAAByuViuty3/842bb86Gh5KtfLe3ML7mkXDt05veuXckzn2nmNwAAADC3HNUM8BtvvDFjY2M/e/7d7343f/iHf5gzzjgjL33pS/OpT30qH//4x496kwAAAByuXi9tz5/3vOSUU5ozvsfGkne+c2oo3rje15csW5ZcdFHyk5+Y+Q0AAADMLUcVgG/ZsiUveMELfvb81ltvzSmnnJKbb745H/3oR/OmN70pt95661FvEgAAgKlqtVLNvXZtsmVLs+15wz33JF1dzVB8svHxUgV+2mkztl0AAACAGXFUAfi+fftyyimn/Oz5V7/61bzqVa/K4sWLkyQvfOEL8/DDDx/dDgEAAJiiVmvO/G4YGkouvzy59tpm6P13f5dcdtn07zEwUNqfAwAAAMwlRxWAn3XWWfnHf/zHJMkPfvCDfO9738urXvWqn93fvXt32tvbj26HAAAA/Ey9ntx0U/LQQ8325itWJBs2JCefnLz//ck//3Op8H7hC5Ph4WTNmmYoXqmU58PD2p8DAAAAc89RBeAXXXRRbrnllrznPe/J7/3e76Wrqyu/8iu/8rP73/72t7Ns2bKj3SMAAABptj3/4AeTJUtKmL1iRbJ5c/KNbySveU2yf3+ZC97bm3R3JxdckPzmbyY7diQ7d5bz4GDS0dHiLwMAAABwHCw6mhe/5z3vycTERO6+++6cddZZ2bBhQ07730PkqtVq7r333vzu7/7uMdkoAADAfNdoe16tNmd+/+Ivlmvr1iV/8zfNxw333pucd16pEL/ssqS9vRwAAAAAc9FRBeCLFi3K+973vrzvfe877F6lUsnXvva1o3l7AAAAUiq/29uTH/+42fZ8aCj56ldLG/NLLinV3qtXl8fT2bAhmeaPbgAAAABzylG1QJ+sVqtly5Yt2bJlS2q12rF6WwAAgHmtXi8tzZ/3vOSUU5qzvMfGkne+sxmK9/aWFueNgPxQ1Wqye/fM7BkAAACgVY6qAjxJvvWtb+WP/uiP8vd///c5cOBAkmThwoVZuXJlfv/3fz8vfOELj3qTAAAA802tlhw4kHzkI8nateVao+15o8X5PfckXV0lFN++vTkXfLoQvFIpawEAAADmsqMKwL/5zW/md37nd9LW1pbf+q3fyjnnnJMk2bJlSz7/+c/n7W9/ez71qU/lF37hF47JZgEAAOaDej0ZHS1h98hI83qj7XmSfPnLydVXJ489lgwMlJD80IB8soGBZGLC/G8AAABgbjuqAPz666/PmWeemb/4i79IT0/PlHuXXXZZ3va2t+X666/Pf/kv/+WoNgkAADBf/OQnyR//cfLXf528+c3Nau4VK8oc75NPTt7//hJ+b9iQXHddcvvtpVq88TgpAXq1Wiq/BwaS4eGko6NFXwoAAABghhzVDPBvfvObectb3nJY+J0k3d3defOb35x/+Id/OJqPAAAAmDfq9aStrVR9T25pvmJFsnlz8o1vJM96Vnl83XWl6vvee5NVq5KVK5M77ihB+OBgsmNHmQm+Y0d5LvwGAAAA5oOjqgBfuHBhHnvssSPeP3DgQBYuPKqMHQAAYF6o1ZKbbkpe//pm1Xejpfkv/mIJxdetS7q7k9Wrk0suab52bCzp6yv3zj03ufPO0uq88W+VtT0HAAAA5oujSqdf/OIX5+abb85DDz102L2HH344f/EXf5Hzzz//aD4CAABgXmhrS66/vln1nZSZ35dfnrzudaWleZL09pbK7kZIPtn4ePL1rye7d8/UrgEAAABml6OqAH//+9+fiy++OL/+67+e1772tVm2bFmSZOvWrfnv//2/Z+HChbnyyiuPxT4BAADmtGo12bKlWfW9bl2p7H7nO5P/6/9qBt6TW6NPF4JXKklX14xtGwAAAGBWOaoA/PnPf37+6q/+Ktdff33uuOOO7Nu3L0myePHivPrVr05/f39OP/30Y7JRAACAuaxSKcfQUJnxnZSq73vuKYF2I/AeH58akh9qYCCZmND2HAAAAJifjioAT5LnPve5+ZM/+ZMcOHAgP/rRj5IkZ5xxRhYuXJg/+7M/y8jISP7pn/7pqDcKAAAwV9Xryfe+1wy1V61K1q9Ptm1Ldu1KDhwowfbatWX9oSF5tVoC8oGBZHg46eho1TcBAAAAaK2jDsAbFi5cmO7u7mP1dgAAAHNerVbC7Y98JLnllqmhdl9fcs45yfvel7z97SXcPngwufHG0hr9wguTj30s+YM/KDO/u7pK5bfwGwAAAJjPFrZ6AwAAAPNRvV6C7oULk5GREmqvWpWsXFkqv7duTb75zeR3fif53d9NXvWq5PWvT3bsSHbuTL761RKQt7cnPT3l3NnZ6m8FAAAA0FoCcAAAgBlWq5UW55/+dAmzq9VyfWysVH4vW5ZcdFE5/+hHyQMPlHs33yzwBgAAAHg8x6wFOgAAAE9OW1up+l60KFmypMzvboTgSTI+Xo5KpYTd27eX66ZOAQAAADy+pxyAf/vb337Sa3fu3PlU3x4AAGBOe+SRZNeuZuC9aVPS35+sW3f42oGB5CtfKWF4UsJwAAAAAI7sKQfgv/mbv5kFCxY8qbUHDx580msBAADmulqtzP3u729WfQ8NJZs3l/ujo+VapVLC79///eQlL2m+XgU4AAAAwON7ygH4+vXrj8c+AAAA5ry2tmTDhmTFimbV99hYsmpVmQm+bVsJwHt6komJZOvWcr9BAA4AAADw+J5yAN7X13c89gEAADDnVavTV32PjSXveEe5/p73JO3t5ejqmvp6ATgAAADA41vY6g0AAADMF5VKORpV3ytXlqrvrVvL+T/8h+QDH2iuP3TmtxngAAAAAI9PAA4AADAD6vXku98trc+TEoL39SXLliUXXZSMjCR33JHs2tV8TUdHctppzefPeMaMbhkAAADghPOUW6ADAADw1NRqycaNyS23TG19Xq0m+/cnb3xjcumlpSr8l3956mvPPLO0Q3/uc8sZAAAAgCMTgAMAABxnbW2lwrtaLSH3+vWl5fmuXaWt+WOPJb/2a6Uq/OKLp772Yx9LfvEXy9pHH00mJpLOzpZ8DQAAAIBZTwt0AACA46xaLUdyeOvzZcuSH/0o2bOn3O/tbb6uXk/++39Pli5NnvOcUg2+cWO5DgAAAMDhVIADAAAcR7VaUqmUoxGCJ8n4eDkqlVIFvn17ud4IwBtt0z/0oeZrqtVk7dryeHBQJTgAAADAoVSAAwAAHCf1egmxv/KVpL9/+jUDA+X++Hh5ftZZ5dxomz6dkZFyHwAAAICpVIADAAAcB40K7rVrkxUrks2by/XR0VLJXamU8Ht4OHnNa5Lu7lL9/cxnlnWT26YfqlpNdu8uleMAAAAANAnAAQAAjoPJFdxjY8mqVcn69cm2bcmuXSXonphIOjqSdeuSl7882bkzecYzjtw2vaFSSbq6Zu67AAAAAJwotEAHAAA4hvbuLa3Pd+6cGl6PjSV9fcmyZclFFyU/+UmZ4V2vJ3ffnSxdmixfnpx5Zqkcf+SRUiE+nYGBEp4DAAAAMJUKcAAAgGOkXk+2bEluv72E1NNVcI+PJ/v3J6ed1myT/qEPNe9Xq6Vtend3MjRUro2MHN42vaNjRr4SAAAAwAlFBTgAAMAxUKslN9xQqrg3bkw2bUr6+6dfe9llpYJ7cpv0Q61Zk5x0UjI4mOzYUSrKd+woz4XfAAAAANNTAQ4AAHAMtLUlt92WvPnNpVp7aCjZvLncGx2dWsF95ZWl/fmhbdInq1aT3buTnp7yvHFubz+uXwMAAADghKYCHAAA4BioVpP770+WLClB99hYsmpVsnJlsm1bsnVrOV95ZXLrreU1lUo5plOpJF1dM7J1AAAAgDlDAA4AAHCUarXk1FPLbO/Jrc/HxpK+vmTZsuSii0q78zvuSBb+7z+JTUyUivDpDAyU+wAAAAA8ebMqAP/BD36QNWvW5A1veEOe//zn58ILL5x23V/91V/lV3/1V/PCF74wv/Ebv5E777xzhncKAADQ1NZWgu3+/tL6fGAgufbaZnX3/v3JG9+YvOtdySc+UQLxpLRBHx4u874bayuV8nx4uNwHAAAA4MmbVTPAv/e97+Xuu+/OeeedlwMHDuTgwYOHrfn85z+fP/iDP8h73vOevPzlL88XvvCF9Pf35+abb86LXvSimd80AAAw71WryVVXNWd+X3hhCbC3bSv3enqS8fFk8eLkhhuSZz6zVI13diYdHcngYHLNNWXmd1dXqfzu6GjlNwIAAAA4Mc2qCvBf/uVfzt13352RkZH8H//H/zHtmpGRkfybf/NvcsUVV+TlL3951q5dmxe+8IX5kz/5kxneLQAAQFKvl/bn27c3Z37fcUfS+Pe5Dz2UPPZY8md/lixdmixfnvT2Jhs3ltcmJQhvby9BeXu7ym8AAACAp2tWBeALFz7+dv7lX/4lDz74YH791399yvXXv/71ueeee/Loo48ez+0BAABMUasl69cnX/5yaX9+6MzvZctK1feGDcm6daUaPCnntWvLa2u11u0fAAAAYK6ZVS3Qn8gDDzyQJHnOc54z5fo555yTiYmJ/Mu//EvOOeecp/XeBw8ezN69e496j0y1b9++KWcAjj2/tQDH35F+a9vaFmdkZEF6e5vtz0dHS7vz/fuToaGD+df/OhkZWTDt+46MJNdcczB79/oNB0j8ty3ATPBbC3D8+a099g4ePJgFC6b/+5VDnVAB+O7du5Mkp5122pTrjeeN+0/HxMRE/umf/unpb47H9eCDD7Z6CwBznt9agONv8m9tV1dXTj31nFSrpaJ71apS0b1tW7JrV2lnvn9/8qMfHUy1Ov0f0KrV5Mc/PpDx8a2pN/qhA+C/bQFmgN9agOPPb+2x1d7e/qTWnVAB+PHU1taW5z73ua3expyzb9++PPjgg1m2bFkWL17c6u0AzEl+awGOv0N/a8u/OO5IklQqJchutD/v7i4zvvftS77znWTx4gU/W3OoSiU5/fSFOfXU5xx+E2Ae8t+2AMef31qA489v7bH3/e9//0mvPaEC8K6uriTJnj170tPT87PrP/nJT6bcfzoWLFiQk08++eg2yBEtXrzY/74Ax5nfWoDj7+STT86BA4vz2GPJRz6SnH9+mf29bl1zzfh4OdasSSYmSuX3wECZ+X2ogYGyprPT7zfAZP7bFuD481sLcPz5rT12nmz78+QEC8CXL1+epMwCbzxuPG9ra8uzn/3sVm0NAACY4zo6OpJ05C/+Ivnd3y3zuw+d/V2tlqrugYFkeDjpKEXiGR4u55GRI68BAAAA4OgtbPUGnopnP/vZWbZsWb74xS9Ouf6FL3whr3jFK55033cAAICn6owznp0NG5Jzzikzvhttz1etSlauLLO/t24t5yuumBpsd3Qkg4PJjh3Jzp3lPDgo/AYAAAA41mZVBfi+ffty9913J0keeuih/PSnP/1Z2P3Sl740Z5xxRi677LJcddVVOfvss/Oyl70sX/jCF/Ktb30rn/70p1u5dQAAYI4744xTcvPNCzI4WJ4/0ezvQ3V2lnNjmpN/vwsAAABw7M2qAPyHP/xhLr/88inXGs//63/9r3nZy16WCy+8MPv27cvHPvax/Of//J/znOc8J6Ojo3nxi1/cii0DAADzwIIFC/LjHx/M4sWlgvtb33qi2d8CbgAAAIBWmFUB+NKlS/Pd7373Cde96U1vypve9KYZ2BEAAEBy8ODBnH76guzblyxZklx3XXL77eXe5Nnf/f3J0FCyeHErdwsAAAAwf51QM8ABAABa5Uc/+mkuvvhgNm1KXve66Wd//9ZvJQcPtnqnAAAAAPPXrKoABwAAmI0WLFiQRYsWZmgo+fM/TwYGyvV3vCNZtCg599zkN34jufzypKOjtXsFAAAAmM9UgAMAADyhjoyOduaCCxbk2c9OOjuT970v2bEj+fa3kzvvLO3Phd8AAAAAraUCHAAA4HHUasnGjcnatQuSJH19SXd30tubvOtdyaWXJu3t5QAAAACgtVSAAwAAPI62tmRkZMGUa+PjyX33JX/4h6UFOgAAAACzgwAcAADgCPbuTarVckynWk12757BDQEAAADwuATgAAAA06jXkxtvLPO+K5Xp11QqSVfXTO4KAAAAgMcjAAcAADhErZasX5/cdFOya1cyMDD9uoGBZGJiRrcGAAAAwOMwrQ4AAOAQbW3JF7+YbN6cfOYzSX9/cuBAMjpa2p5XKiX8Hh5OOjpavVsAAAAAGlSAAwAAHKJaTa6+OhkZSS6/PFm1Klm5Mtm2Ldm6NXn44eTKK4XfAAAAALONCnAAAIBDVCrJ6tXJJZeU52NjSV9f0t2d9PYm+/Yl3/lOK3cIAAAAwHQE4AAAAP9brVbanz/ySLJ7d6kEn2x8vBxJud/TM+NbBAAAAOBxaIEOAACQpF5PPvGJEmz/5V+WcLtSmX5tpZJ0dc3k7gAAAAB4MgTgAADAvFerJevXJ2efXeZ+v/vdyebNycDA9OsHBpKJiZndIwAAAABPTAt0AABg3mtrS26+ORkcbM79HhgoIfiBA8noaGmHXqmU68PDSUdHCzcMAAAAwLQE4AAAwLxXrSaLFyc7dzbnfo+NJatWlcrwbduSXbuSs846mP37Fwi/AQAAAGYpLdABAIB5r1JJ9u1LliyZOvd7bCzp60uWLUsuvjhZsCDp7GzRJgEAAAB4QgJwAABg3puYKAH3pk1Jf//h98fHk9WrD6ZePzDzmwMAAADgSROAAwAA89reveU8NJT88z+XGd/XXtusBK9UkjVrDmZw8EDa2h5t1TYBAAAAeBIE4AAAwLz1yCOl+nvDhuSCC5JnP7u0OH/f+5IdO5rHlVceyIMPjuXgwYOt3jIAAAAAj2NRqzcAAADQCrVa8sADyS23JOvWlWt9fUl3d9Lbm7zrXcmllybt7cn+/Y+kXq+3dsMAAAAAPCEV4AAAwLzU1pYsX56Mjk69Pj6e3Hdf8od/mCzyT4YBAAAATigCcAAAYF6p1Urb8z17kp07k2p1+nXVarJ790zuDAAAAICjJQAHAADmjXo92bgxed7zko6OZMmSpFKZfm2lknR1zeTuAAAAADhaAnAAAGBeqNWS9euTtWuTLVuSTZvKub9/+vUDA6VSHAAAAIATh4l2AADAvNDWloyMlMcrViSnnJKce25y1VXl2uhoaXteqZTwe3i4VIkDAAAAcOJQAQ4AAMwL1Wo5VqxINm9O7ror+aVfSv7u75Lf//1k+/YyE3zHjmRwUPgNAAAAcCISgAMAAHNerVYquyuVZMOGUgm+bl1y773Ja1+bnHNOOd92W2l73tnZ6h0DAAAA8HRogQ4AAMxp9XqycWPy0peWyu7Vq5NLLpm6Znw8+epXk3/8x+R3fqcl2wQAAADgGFABDgAAzFm1WrJ+fXLLLaWl+fvel/z4x6UV+nSq1WT37pncIQAAAADHkgAcAACYs9raki9+sTnz+w1vSLq7Syv06VQqSVfXDG4QAAAAgGNKAA4AAMxZ1Wpy9dXNmd9f/nLypS8l/f3Trx8YKDPAAQAAADgxmQEOAADMWZXK4TO/h4ZKRXiSjI6WkLxSKeH38HBplQ4AAADAiUkADgAAzFn79x8+83tsLFm1qswG37atzAmvVErlt/AbAAAA4MSmBToAADDn1GrJo4+WAHzJksNnfo+NJX19yXnnJaefnrS3J52dLdkqAAAAAMeQABwAAJhT6vXkE59Idu9O/uiPkq99rbQ3n87FF5egHAAAAIC5QQt0AABgzqjVko0bk/PPT0ZGknXrks9+tsz8PnDAzG8AAACAuU4ADgAAzBltbcnNNyeDg8kll5Rrh8783rUreeYzzfwGAAAAmIsE4AAAwJywd2/y058mixcnO3eWSu+Gxszv7u6ktze5887yGAAAAIC5xQxwAADghFevJzfemHR2Jvv2JUuWlDbnhxofL1Xgp50241sEAAAAYAYIwAEAgBNarVbamw8NJZs2JW97Wzn390+/fmCgtD8HAAAAYO7RAh0AADgh1Wpl5veiRcnISLk2NJRs3px85jMl6E6S0dHSDr1SKdeGh83+BgAAAJirVIADAAAnnHo92bgxec1rkocfbs77HhtLVq1Kzj67tEO//PJkx47mMTgo/AYAAACYy1SAAwAAJ5RarYTfa9cm3d3Ned+TQ/C+vnLv3HOTO+8sa5Kkvb1VuwYAAABgJqgABwAATihtbc2W5+Pjh8/77u5OXvCC8nj1avO+AQAAAOYTFeAAAMAJY+/e5Ec/alZ7J8253z09yTnnJBdckOzcmfT2Jvv3l1boAAAAAMwPKsABAIATxqJFyemnl5bnDWNjySWXJO98Z3LvvcnSpcny5ckzn5l85CNlXjgAAAAA84MAHAAAOGFUq4e3PE+Sd787+fCHy1zwRnV4tVqer19f5oYDAAAAMPcJwAEAgBNGpZJcd10yMJBce2153t1dZn2Pjk7/mpGRMjccAAAAgLlPAA4AAJww6vXk134tWbUqWbky2bYt+Yd/KBXek+eCT1atJrt3z+AmAQAAAGiZRa3eAAAAwJNRryef/GRpf37gQPKOd5SZ4C99afLZz5Zq8OlC8Eol6eqa4c0CAAAA0BIqwAEAgFmvViuzvAcGplZ//93flfC7Vjt8LnjDwEAyMTGz+wUAAACgNVSAAwAAs15bW5nlnSRjY0lfX5n93dub7NuXfOtbyeWXJwsWJDfeWCrBK5USfg8PJx0drdw9AAAAADNFAA4AAMxatVrS3p78+MeHtzcfHy9HkuzcWcLuq69Orr22zPzu6iqV38JvAAAAgPlDC3QAAGBWqteTjRuT5z0vOeWUUtE9nUol6elJ7rknueWWEpj39JRzZ+dM7hgAAACAVhOAAwAAs05j5vfatcmWLcmmTUee8X3ZZcndd5d26OecM7P7BAAAAGB20QIdAACYdSbP/E6SoaFk8+byeHS0OeN77drk3/275KSTkttuS846q4TnKr8BAAAA5icV4AAAwKyyb18JuCfP/B4bS1atSlauTLZtS3bsKMellyZ//MfJ0qXJ8uUlAN+4sbRPBwAAAGD+EYADAACzRr2ebN2anHrq4TO/x8aSvr7kvPOSrq5kYiLZsKFUgTfC8mq1PF+/vlSCAwAAADC/CMABAIBZoVZLbrgh+fmfT+6448gzvy++uATdh7ZJn2xkpNwHAAAAYH4RgAMAALNCW1uZ471zZ3LVVcnAQHLttc1K8EqlPP/AB8raQ9ukT1atJrt3z8i2AQAAAJhFBOAAAMCsUK0m99+fLFmSbN8+deb31q3l/NKXlrWNFumHtklvqFRKm3QAAAAA5hcBOAAA0HL1egm19+9PNm0q7c8bM7+XLUsuuqic7703ueuuUgE+MVGqxKczMFDuAwAAADC/LGr1BgAAgPmtVks2bkzOP78E30NDyebN5d7oaDI+XoLxgYFy/93vTl7/+qSzMxkeLutGRkoFeaVS1g0PJx0drfpGAAAAALSKABwAAGiptrYSYPf2NoPvCy8sIfa2bSXY7u5O7r67tEVftqz52o6OZHAwueaaMvO7q6tUfgu/AQAAAOYnLdABAICWqlbLMTbWnPt9xx3Ji15U7p9ySvLnf5786q+WNWedNfX1nZ1Je3vS01POnZ0z/AUAAAAAmDVUgAMAAC1VqZSjEYL39ZWK797eZN++5DvfaVaB9/Ym55zT2v0CAAAAMHupAAcAAFpqYqLM7Z5sfDy5777k4ovL/O83vSl58MHkttuSK68sc8MBAAAA4FACcAAAoKUWLEiGhpI1a0oleFLOa9YkV1+dLFyYfPrTydKlyfLlpQX6xo1Jvd7KXQMAAAAwG2mBDgAAtEy9nnz4w8kXv5gMDyfbtpV25z09pTJ8//4Sdn/oQ83XVKvJ2rXl8eCgmd8AAAAANKkABwAAWqJWS9avL2H2vfeW2d/LliW/9mvJ9deXNW1tycjI9K8fGSn3AQAAAKBBAA4AALTEdOF2Y/b3hg1Je3up9q5Wp399tZrs3n2cNwkAAADACUUADgAAzLi9e59cuF2pNOeCH6pSSbq6jsfuAAAAADhRCcABAIAZVa8nN95YZnc/Xrh92mllDvjAwPRrBgbKfQAAAABoEIADAAAzpjH3e2go2bQp6e+ffl0j3O7sTIaHkzVrmmF5pVKeDw+X+wAAAADQsKjVGwAAAOaPyXO/h4aSzZvL49HR0va8Uinh9/Bw0tFR7nV0JIODyTXXlLboXV0lHG/cBwAAAIAGATgAADAjHnkk2bWrOfd7bCxZtapUhG/bVu6ddVayf//h4Xaj0runp5zb22ds2wAAAACcQLRABwAAjrtaLfnoR5PTT58693tsLOnrS5YtSy6+OFmwQFtzAAAAAJ4+ATgAAHDctbUlGzYcee73+Hjyy79cWpsDAAAAwNOlBToAAHBcTW59/nhzv6+4QvU3AAAAAEdHBTgAAHDcHNr6vDH3e+XKMvd769ZyvvLK5LrrWr1bAAAAAE50KsABAIDjptH6fMWK0vp83brm3O/u7qS3N/nt306e97zkhz9s9W4BAAAAONEJwAEAgOOmWj1y6/P9+5M3vjF597uTV70qecMbWrhRAAAAAOYELdABAIDjplJ54tbnF11U7vf0tHq3AAAAAJzoBOAAAMBxMzGRDAyUx43W58uWldB7dDS5447knnvK/SVLWrZNAAAAAOYILdABAIDjprMzGR4uj0dGprY+v+yyUhHeoAIcAAAAgKMlAAcAAI6rAweSq65Krr462b27tESfmEgWLixV4Q0CcAAAAACOlhboAADAcVOvJx/+cHL22cnSpaUF+vXXJyedlHR0JGec0VyrBToAAAAAR0sFOAAAcFzUasnGjcnatc1r4+PJ17+e7N2bDA4mz3xmqQTv7VUBDgAAAMDRUwEOAAAcF21tZe73dEZGyv2PfSx58MHktttKEF6rzegWAQAAAJhjBOAAAMBxUa2WYzq9vcljjyVf+EJpjb58eXLmmaVivF6fyV0CAAAAMJdogQ4AABxTtVqp7u7qSiqV6UPwj3wk2bAh+dCHmteq1Wa79MHBpLNzBjYLAAAAwJyiAhwAADhm6vVSxX3mmckXv5j09x++prs7+eVffuL26AAAAADwVKkABwAAjolarYTfjSruoaFk8+byeHS0VHhXKskHP5js2XPk9ujVarJ7d9LTc9y3DAAAAMAcIwAHAACOiba2qVXdY2PJqlXJ+vXJtm0lIK9UkomJsvZI7dErldI+HQAAAACeKi3QAQCAY6JaPTzQHhtL+vqSZcuSRYuS9vYy23tiIhkYmP59BgbKfQAAAAB4qlSAAwAAx0SlcuSq7v37k1NOaT7v7EyGh8vjkZFme/SBgXK9o+O4bxcAAACAOUgFOAAAcExMV9Xd3Z284AXJBz5weFV3R0cyOJjs2JHs3FnOg4PCbwAAAACePhXgAADAMdGo6j54MPnSl5Krr05Wr05+/ONkyZLp25p3dpZzT085t7fP3H4BAAAAmHtUgAMAAMdErZYsXJi8//3JXXclf//3ydKlybOfnZx5ZrJxY1Kvt3qXAAAAAMxlAnAAAOCo1esl4D7zzOTuu5PrrkvWrm3OA69Wy/P160tQDgAAAADHgwAcAAA4KrVaCbbXrk0WLSptz0dHp187MpK0tc3s/gAAAACYPwTgAADAUWlrK8F2kvT2Jjt3Niu/D1WtJrt3z9TOAAAAAJhvBOAAAMBRqVabgff27cmSJUmlMv3aSiXp6pqZfQEAAAAw/wjAAQCAp61WS049tRl4j48nmzYl/f3Trx8YSCYmZmx7AAAAAMwzAnAAAOBpa2tL7rhjauA9NFSC7muvbQbjlUqyZk0yPJx0drZipwAAAADMB4tavQEAAODEVa0mV12VbN5cno+OJmNjyYUXJv/5Pyd/8Adl5ndXV6n87uho6XYBAAAAmONUgAMAAE/Z3r3Jvn2l/fn27cmqVcnKlcm2bcnWraUq/J//uazt6Una21V+AwAAAHD8CcABAICnpF5PtmxJrrsu+fKXS/vzsbGkry9Ztiy56KJy/sY3zPsGAAAAYGZpgQ4AADxptVppc97fX869vVPbn4+PJ/v3lxngw8NangMAAAAwswTgAADAk9bWltx2W/LmN5f539VqaX++fn1pf75rV2l5/uijwm8AAAAAZp4AHAAAeNKq1eT++5MlS5JKpTxvtD/v7i4V4fv2Jd/5Tos3CgAAAMC8ZAY4AADwpOzZk5x6amlxvmlTaYM+2fh4ct99ycUXm/0NAAAAQGuoAAcAAJ5QvZ4sWpTccUcJvoeGps7+rlZLRbjZ3wAAAAC0kgAcAAB4XLVactNNyetfn1x1VTP4vvDCEnZv21YC8J6eUvkt/AYAAACgVbRABwAAHldbW3L99WXu9/btyapVycqVpRr8RS8qa773vXLu7GzZNgEAAABAAA4AADy+ajXZsqU593tsLOnrS5YtSy66qJzvvNPcbwAAAABaTwt0AADgiOr15NRTy3zvQ+d+j48n+/eb+w0AAADA7KECHAAAmFatlqxfn3z5y83K70b7823bkq1bk4cfTq68UvgNAAAAwOygAhwAAJhWW1syMpL09k6t/O7rS845J3nf+5Lf+73ktNNau08AAAAAaBCAAwAAh3nkkWTXrjL/u1otld/r15fK7127kp6e5NFHVX4DAAAAMLtogQ4AAPxMrZbs2ZN89KPJ6aeX2d9JaX/e15csW5ZcdFFy3nlJZ2cLNwoAAAAA0xCAAwAASZJ6PfmLv0ja25MNG5JNm8rs78nGx5P77ksuvjiZmGjNPgEAAADgSLRABwAAUqslGzcmv/RLzdbnQ0NTZ39Xq6UifGAgGR7W/hwAAACA2UcADgAApK0tufnmZHCwPK9UStvz6WZ/Hzgg/AYAAABgdtICHQAASLWaLF6c7Nw5tfX5obO/R0eThf4UAQAAAMAspQIcAADmuXo9OfXUZN++ZMmS5LrrkttvL/carc/370/+7b8t7c8XL27lbgEAAADgyATgAAAwj/3kJ8kf/3Fy/vnJ295Wqr9f97rpW59///vJwYOt3jEAAAAAHJkAHAAA5ql6vcz+HhlJenuTzZuTz3ymVHknyTvekSxalJx7bvIbv5FcfrnZ3wAAAADMbqb3AQDAPFSrJR//eLJ9e2lxPjZWqr7PPjvp7Cxh944dybe/ndx5Z5kJLvwGAAAAYLYTgAMAwDzU1pZcf32Z+V2plGtjY0lfX/LzP5+85jXJq16VnH560t5eQnEAAAAAmO0E4AAAMA9Vq8mWLWXmd3//1Hvj48l99yW//uvJo4+2ZHsAAAAA8LSYAQ4AAPNQpVKOoaEy+ztJRkdLMF6plDngw8PangMAAABwYlEBDgAA88xPfpJ897ul8rsx+3vlymTbtmTr1uThh5MrrxR+AwAAAHDiEYADAMA8Uq+X+d+XXlqqvK+9Ntm+vcz+Pu+85Pbby7rTTmvtPgEAAADg6RCAAwDAPFGrJR//eAm877338Mrvb34zedazkn37Wr1TAAAAAHh6zAAHAIB5oq0tuf765JJLypzvsbFS+d3dnfT2lmB8//5kx45W7xQAAAAAnh4V4AAAME9Uq8mWLcmmTWX+d8P/z969h2lZlnvj/44wIzgKj8YuI0L0VbLcpCbLTEojy9SSlplmmZus1YrGLCNGyVrEK0Qbc5z26tK3jWnWaqPVSjSjzEqXbXQZWbiLip06gAMjg8zvj+s3DANjuRnmYYbP5zie457nvq/n4bo56o74cp7nihXJ3XeXY0ND0t5etS0CAAAAwLOiAhwAALYTlUp5zZiRLFhQzjU3l2C8Uinhd2NjMmRI9fYIAAAAAM+GCnAAANhOtLWVkHvhwi3nf//tb8kHPiD8BgAAAKB/E4ADAMAAt2ZNsnZtctVVpfX5zJll3vfUqckBByQ33JDU1CTDhlV7pwAAAADw7GiBDgAAA9jatWXu97XXJrNnJ5/7XDJnTqn8Xr48GT26zPxW+Q0AAADAQKACHAAABqjVq5NLLkkmTCizvpPS/nzq1GT8+OT445P99hN+AwAAADBwCMABAGAAam1NBg9Ovv/9ZNmypKWl+/UVK5K77y7V4StXVmWLAAAAANDrBOAAADAA1dWVOd/33puMGpVUKj2vq1SS4cP7cmcAAAAAsPUIwAEAYABqaSnB9/r1yfz5ybRpPa9raCgzwAEAAABgIBhc7Q0AAAC9q60t2Xnn5OabS/A9Y0ayYEG51txcwvFKpYTfjY1mgAMAAAAwcAjAAQBgAGltTebNSw46KHnwwRJyJ8lxx5Wwe/HiEoCPHFkqv4XfAAAAAAwkAnAAABhAamuTpqZkzJhS9X311cmkSaUKfPnyZIcdSvV3XV15AQAAAMBAYgY4AAAMIC0t5bVwYTJ5cjJuXHLkkcnDD5eZ4I8/ntTXV3uXAAAAALB1qAAHAIABoq0t2WWXUuHdGYJPnZqMGFEqwteuTe65p9q7BAAAAICtRwU4AAAMAK2tyZw5yY9/nEyb1v3aihXJ3Xcnp55a5n4DAAAAwEClAhwAAAaAzWd/J0lzc6kEr1SShoaksTEZMqSauwQAAACArUsADgAAA0Dn7O+WljL7e86cZPHiZPnyZOTIZN064TcAAAAAA58AHAAA+rnW1lLlbfY3AAAAANs7M8ABAKAfa2tL5s1LbrzR7G8AAAAAUAEOAAD91KpVyac+lcyalUycaPY3AAAAAAjAAQCgH2prS2prk6amrnO//33ywQ8mM2cmK1eWALy9XfgNAAAAwPZDC3QAAOhnWluTyy9Pliwpld6d1d+33JK84AXJ2LFlBvjFFyeDBlV7twAAAADQd1SAAwBAP9DaWiq+H3ss2XnnEm6ffnqp8p47t1SCz57dtX7FiuQXv0jWrEmmT0/q66u1cwAAAADoOyrAAQBgG9fWlsyblxxxRHm/bFmyaFEyf34Jt6dMKXO/e9LUVIJzAAAAANgeCMABAGAb1tqazJmTzJqVnH9+8sUvJrvuWiq/Z8xI3vWu5NFHSyv0nrS0lHngAAAAALA9EIADAMA2rLa2VHGPGFEqvefNK5Xf06YlCxcmxx1XrlUqPX++UkmGD+/LHQMAAABA9QjAAQBgG9bSUl5jxiTLl5efZ8xIGhqSmTOTP/wh+e//LoF4Txoakvb2PtwwAAAAAFTR4GpvAAAA6FlbW7LLLqWKe5ddkuc+t/y8cGEyeXJpjb54cQnFjz462WGHUi3e0lLWNTQkjY3JkCFVvQ0AAAAA6DMCcAAA2Aa1tpZ25wcdVKq7Dzkk+eMfy8+zZ5cQfOrU0v58zJjknHOSD3wgueCCMvN7+PBS+S38BgAAAGB7IgAHAIBtUOfs7zFjkp/9LKmvT175yuT668v15uZS6b1+fXLCCcmppyZDh5ZrI0eWY11dFTYOAAAAAFVkBjgAAGxjHn88WbasBNwLFyZnnpk8+mjy61+X1ucHH1xan99/fzkedFCydm21dw0AAAAA1acCHAAAtiGtraW6e9q0Mse7pSW57bbS0rxz/vemrc+XLClV4EuXVnnjAAAAALANUAEOAADbkNraZO7cZP78EoInyYoV3d93nrv77nJsaCjzvgEAAABge6cCHAAAtiEtLeU1Y0ayYEE519zc9b6mJrn00rKmUinhd2NjMmRI9fYMAAAAANsKATgAAGwjVq9Odtmlq9X55MnJnDllzvfy5Ul9ffKBDyQzZyYrV5a26O3twm8AAAAA6KQFOgAAbAPa2pLBg5Obb+5qdd4573v8+OT440vl9+DBSV1dMnJkOdbXV3XbAAAAALBNUQEOAABVtmpV8pWvJK97XXLeed1bn7e0JOvXJyecUNqdDx1azZ0CAAAAwLZNBTgAAFRRW1tSW5tcfHEyalSyZElpfX7wwaX1+f33l+OhhyaDBlV7twAAAACwbROAAwBAlbS2JpdfXkLvRYuS+fNL+/PNW5+PH5/ccUeZ9w0AAAAAPDkt0AEAoEo6K79PPz2pVJIZM7q3P1+xorQ/b2hIGhuTIUOquVsAAAAA2PapAAcAgCpoa0uWLduy8nvz9ud/+1vygQ8IvwEAAADgqRCAAwBAH1u9OvnMZ5Jdd+2q/G5oSGbOLO3Qp05NDjggueGGpKYmGTas2jsGAAAAgP5BAA4AAH1o9epk8ODk4x//x5Xfd92VvO1tKr8BAAAA4OkQgAMAQB9paytzv5csSVpaVH4DAAAAQG8bXO0NAADA9qC1NbnyyuS445JRo0rr887K7zlzSuX38uXJyJHJhg0qvwEAAADgmVABDgAAfaC2Nrn44mTEiOSWW0rr86SE4FOnJuPHJ8cfnzQ3Jzv4UzoAAAAAPCMqwAEAYCt7/PFS3b1oUZn7/eCDpfV5UgLvlpZk/foShDc0JEOHVnW7AAAAANBvqS0BAICtqLU1+cxnkl13LW3PZ8xITjklufrqZNKk0vr8/vuTv/0t+eAHhd8AAAAA8GwIwAEAYCuqrU3mzi2V39Omdc39HjcuOfLI5OGHkzFjSgX4LrtUe7cAAAAA0L8JwAEAYCtqaSmvGTNKe/OZM5MlS0q78wMOSL7//bJO+A0AAAAAz54Z4AAAsJWsXVuC7Uqlq/J7zpzS9nz58mTkyGTDhmTIkGrvFAAAAAAGBhXgAADQy9asSVavLmH3j39cWp8nJQSfOjUZPz45/vikuTnZwZ/IAQAAAKDXqAAHAIBe0Npa5n0/8UTywAPJHnskl15a5nsvWFDWNDeXdujr1ydvfGNyzjmqvwEAAACgNwnAAQDgWWprS+bNSw49NPn970uwvWRJ1/zvnlqfr1sn/AYAAACA3qbhIgAAPAutrSXc/tznkle+Mrn++hJyjxpVZn8nW7Y+P+CApL6+ipsGAAAAgAFKAA4AAM9CbW3S1FRanS9bltx7bzJiRHLLLV2zvzutWJHcfXdy6qlJe3tVtgsAAAAAA5oW6AAA8Cx0tjl/4QuT5z63zPeePz958MGkoaGs6Zz9Xakk731vMmNGMnRo9fYMAAAAAAOVABwAAJ6FSqXM/v7ud0v197RpJeBesCC5+upk0qTyfvnyZPToEpALvwEAAABg6xCAAwDAM7BmTVJTk/z5z8mXv1zaoF93XQm+k+S445LGxuTII5NVq5LnPS9Zty7ZZZfq7hsAAAAABjIzwAEA4Glqa0sWLUouuih517uSffYpbc4XLkwmT04OPji5+ebkwAPL+mHDyqzw+vqqbhsAAAAABjwBOAAAPA2trckllyQTJpTQe/Xq5O9/LzO+kxKCT52ajB+fHH98Oa5ZU8UNAwAAAMB2RAAOAABPQ21t8r3vJcuWldB7yZJk5MgyC3xTK1Ykd99dZn4PH16NnQIAAADA9kcADgAAT6K1tcztfvTRcly5soTe996bjBpVQu8VK5L585Np03r+joaGpL29L3cNAAAAANsvATgAAPSgrS254ooSen/lK+X4uc+VOd7r13cPvWfMKEH3zJldleCVSnLhhUljo9nfAAAAANBXBld7AwAAsK1pbU3mzUsOOihpakoOOaQcZ89O9t23BN8zZiQLFpT1zc3J5MnJJz6RnH9+mQteqZTK7yFDqnorAAAAALBdEYADAMBmamuTr30tmT49+cAHSth9+unJxInJzjuXkHuHHZLjjisV3osXl9boI0eW0HvUqPI9dXXVvAsAAAAA2P5ogQ4AAJtpaUmGDk2WLes6jhlTKr5vuSV55SuTl7wkufnm5KUvLYH5rruWwFu7cwAAAACoHhXgAACwmUolWbu2VHJ3Hj/5ya426EkydWoyYkQJxt/61q554AAAAABA9agABwCAzbS3P/X2NwABAABJREFUJ6eemsyfn5xySqn8PuqoMut7UytWJHffncydW6rAAQAAAIDqEoADAMBm6uvLbO8HH0waGpJHHklWrSqt0XvS0pKsXNmXOwQAAAAAeqIFOgAAJGltLVXcra0lAF+zJjnjjDLX+9hjk512Kq3RewrBK5Vk+PA+3jAAAAAAsAUV4AAAbPfa2pIrrihV3F/5SnLjjcmOO5YW5xs2lGC8vb1Ug/ekoaFcBwAAAACqSwAOAMB2bdWqZM6cZNy45Oqry8zvX/4yGTs2mTAh2X335OMfTwYNKm3RL7ywVHwn5XjhheV8fX017wIAAAAASATgAABsx9raSnX3176WTJmS7Lln0tSUzJ7d1eq8pSX52MdKSL5hQzJ9erJ0abJsWTlOn54MGVLNuwAAAAAAOgnAAQDYLrW2JpdfnixZkgwdWtqdv/KVSXNzz+ubmpLBg0uld11dMnJkOar8BgAAAIBthwAcAIDtUm1tcvHFyahRydq1JdBetqyr8ntzLS1lRjgAAAAAsO0SgAMAsF1qaUkWLUrmzy9zv3/602TMmK753purVJLhw/twgwAAAADA0yYABwBgu1SplNeMGUlDQ/KnP5WZ4O99b8/rGxqS9va+3CEAAAAA8HQJwAEA2O6sWVPC7oaGZOHCZPLkZNy4ZMiQpLExufDCrkrwSqW8b2w07xsAAAAAtnWDq70BAADoS48/Xiq5r7oqmTYt2bAhaW5Opk5N9twzmTUrOe+85IILyszv4cPL+iFDqr1zAAAAAOCfEYADADDgtbYmtbXJ2rXJQw8l116bzJ6dfP7zyZw5yeLFyfLlyejRJezeZZfyuZEjy7Gurnp7BwAAAACeOi3QAQAY0NrakiuuSFatKkH2hAml4jsp7c+nTk3Gj0+OPz7Zbz+V3gAAAADQnwnAAQAYsFpbS4X3uHHJN76RPPxwsmxZ0tLSfd2KFcnddyeLFpW25wAAAABA/yQABwBgwKqtTb72tWTKlOQzn0l23TUZNSqpVHpeX6mUmd8AAAAAQP8kAAcAYMBqaUmGDi1V34sWJfPnl+O0aT2vb2goM8ABAAAAgP5pcLU3AAAAW0ulkqxd21X1PWNGcuutyXnnlevNzSUkr1RK+N3YaAY4AAAAAPRnKsABABiQ2tqSP/4xOeWUUvk9bVqycGFy+OHJ7bcnH/xgsmRJsnRpeU2fLvwGAAAAgP5OAA4AwIDS2pqsXp3MmZOcdFKp7H7wwXKcObOE3q9+dXLQQclXv1pmftfVJfX11d45AAAAAPBsCcABABgQ1qwp7c6//vUSaDc1lYrvyZOTceNKwH3OOV0V3/fck5x8crLjjtXeOQAAAADQW8wABwCg32trSxYtSq69NjnyyGT58jLbOykh+NSpyYgRyZgxSXt78otflJC8rq6q2wYAAAAAepkKcAAA+rXW1uSSS5IJE5Krr04mTUp23TWpVLqvW7EiufvuUv29885V2SoAAAAAsJX1uwD829/+dvbZZ58tXp/85CervTUAAKqgtjb53veSZcuSoUPLcf78ZNq0ntc3NJQqcAAAAABg4Om3LdAvu+yy7LLLLhvfjx49uoq7AQCgWlpaknvvTUaNKjPAR41KLroouf76cr25uaypVEooPmNGCcoBAAAAgIGn3wbgL3rRi7LbbrtVexsAAFRZpZKsX1+qvk85pRyPPjqZPDmZMydZvLjMBB85Mvnzn5OOjmrvGAAAAADYWvptAA4AAElpZ97QUCq7Fywoc8AbGsq1M85IBg9O9t47ef3rk3POSYYMqe5+AQAAAICtp98G4Mcdd1weffTR7L777jnppJPyjne8I4MGDXrG39fR0ZE1a9b04g5JkrVr13Y7AtD7PGvZXtXU1GT9+rrsuOMOmTGjnDvuuJo0Nib19cm553bkwx/uan/e1rYhHR3rsmaNEnCePs9agL7heQuw9XnWAmx9nrW9r6OjIzU1NU9pbU1HR/9qAvmzn/0sv/vd73LAAQekpqYmN998c66++uqccsopufDCC5/Rd951111Zt25dL+8UAICtYciQIRk16gXZeef6XHZZcvLJNbn66mTChOTII0u789GjO/LEE8mKFQ+mtbU169evz/r166u9dQAAAADgGaqrq8t+++33T9f1uwC8Jx//+Mdz1VVX5ZZbbsmoUaOe9ufvuuuudHR0ZK+99toKu9u+rV27Ng888EDGjx+foUOHVns7AAOSZy3bk/KvPIfkz39Orr22JoccktxxRzJ7drk+YkQyZkyyZEny7//ekQ98YEMGD368qntmYPCsBegbnrcAW59nLcDW51nb+/785z+npqbmKQXg/bYF+qaOOeaYXHHFFfnDH/7wjALwpPxl6k477dTLO6PT0KFD/f4CbGWetWwPWluT5uZk2rQy63vGjOT007uur1hRXknS1FSTCy4YlLo6/72g93jWAvQNz1uArc+zFmDr86ztPU+1/XmS7LAV9wEAAL2qtjb53veSZcuSoUPLsaWl57UtLcnKlX25OwAAAACg2gZEAP6DH/wggwYNyr777lvtrQAA0EtaW5N160rIvW5d0tZWfr733mTUqGTt2nKsVHr+fKWSDB/elzsGAAAAAKqt37VAP+usszJp0qTss88+SZKbbrop1157bU477bSMHDmyyrsDAKA3tLUl8+YlTU2lkvvmm5Nf/7q0Pl+/Ppk/PznllHKcNq1rBvimGhqS9vakrq7Ptw8AAAAAVEm/C8D32GOPfOtb38qSJUuyYcOGjB8/Pueff37e9ra3VXtrAAD0gtbWEn7PmlXeH3ZY8rKXJW98YzJxYgm8Z8xIFiwoc8AbGsq65uYSllcq5VxjYzJkSLXuAgAAAACohn4XgM+cObPaWwAAYCuqrS2V351mz06WLy/hdmfwnSTHHVdC7vr65Nxzkw9/uCsAb28XfgMAAADA9mhAzAAHAGDgaGkpryQZMSKZNCnZddcSbC9cmEyenBx8cGmLfuCBSUdHsuOOpdX5qFHlWF9fvf0DAAAAANUjAAcAYJtSqZRXkowZkyxb1jXrOykh+NSpyfjxyfHHl9bnAAAAAACJABwAgG1Me3vXXO8lS0pV90UXlXMzZ3aF4+vXJyecUM6r+AYAAAAAkn44AxwAgIGtvr7M9u7oSC69tFR/H310aX0+Z06yeHGZCT5yZPLnP5d1AAAAAACJABwAgG1Aa2tSW1tmf1cqyYYNyetfn8yYkaxbVwLwHXZIzjgjGTw42Xvvcv2cc5IhQ6q9ewAAAABgW6EFOgAAVdXWlsybl4wenbziFcmNNyY1NWXGd01NcvHFyStfmbzkJaX6+7e/TX7ykzITXPgNAAAAAGxKAA4AQNWsWlXams+alYwZkyxYkPzyl8nuuye33lpmf8+alfz618nUqSUUf+1rSygOAAAAALA5ATgAAFXR1lbanjc1lfdz55afZ88ubc6nTEmam7t/ZsWK5O67y9ra2r7fMwAAAACwbROAAwDQ51pbk8svT5YsKXO/R4zoHniPGZMsW1au9aSlJVm5so82CwAAAAD0GwJwAAD6VGtrqd6++OJk1KikUtky8F6ypOtaTyqVZPjwvtkvAAAAANB/CMABAOgzbW3JlVcmf/1rsmhRMn9+Mm3aloH3ihVd13rS0JC0t/fVrgEAAACA/mJwtTcAAMD2obU1mTcv+dznktNPL2H3jBnJggXJyJHJunXJe9+bfOxjZX3ntaS0Rm9pKZ9paEgaG5MhQ6pzHwAAAADAtksFOAAAfaK2Nmlq6l7dvXBhMnly8vrXJ1/9agnAZ84sQffChclxxyVvelOydGlpkb50aTJ9uvAbAAAAAOiZABwAgD7R0tI143vGjFLJPXNmaWU+cmRy4YUlDD/44GTx4uT++5Obb04efLB8ZuTIpK4uqa+v1h0AAAAAANs6ATgAAH2iUuma8b1wYfKGNySveEXy+98njz1WwvGFC5OpU5Px45Pjjy/H178+WbmyatsGAAAAAPoRATgAAFtFa2uZ671sWTm2tZWq74kTk+98J7nxxmTPPZOOju7heFLapN99dzlWKsnw4dW5BwAAAACgfxGAAwDQ69raknnzktGju15XXZU0Nia//GVyxx3J2LHJhAnleO+9JRzvSUNDaZMOAAAAAPDPDK72BgAAGFhaW0v4PWtW17mWlhJkv+Y1yde+lsye3f3aSSeVYDxJmprKuUqlfKaxMRkypO/2DwAAAAD0XwJwAAB6VW1tCbE3NXFi8ulPl5nem19Lyuzvo49O5s9PLrigzPwePrxUfgu/AQAAAICnSgt0AAB6VUtLeXWaODFZsCBZtCj561+7X9vUr39dWqfX1SUjR5ZjfX0fbBgAAAAAGDAE4AAA9KpKpbw6zZ1bqr7/4z+SUaO6X9v8c8OHb/39AQAAAAADlwAcAIBe1d5eZncnyYgRyZQpSXNzsmJFaXE+bVrPn2toKJ8FAAAAAHimzAAHAOBZa20ts79bWkol94wZ5fz8+cmyZV1tz2fMKO3QkxKKd65vaEgaG837BgAAAACeHRXgAAA8K21tybx5yejR5XXEEclf/pKcd15y883J7rt3tT1fuDCZPDk5+OBk8eLk/vuTpUuT6dOF3wAAAADAsycABwDgGWttTebMSWbNKtXcEycm11+ffOUrybhxydixyW23Je99b9dnFi5Mpk5Nxo9PbrihtD2vr6/WHQAAAAAAA4kW6AAAPCNr1pS2501NXefmzi3vZ8/uOvfud3e1Pb/00q625//+78lZZ6n8BgAAAAB6jwAcAICnra2ttDDfaaeu+d4jRiRTpiSnn959bWfb8098orQ7f+SRZLfdSuW38BsAAAAA6E1aoAMA8LS0tiaXXJI873nJrrt2zfceMyZZtqwrEN/UihVJY2O5vssuSV2dtucAAAAAQO8TgAMA8LTU1ibf+14Js+fPT6ZNK+eXLElGjeoKxJMyE/w730keeKB8ZtSoKmwYAAAAANhuCMABAHhaWlqSe+8tYfZFFyUNDcnMmcn69d0D8YkTy+zvO+5Ixo5NJkxIRo9O5s0rLdQBAAAAAHqbGeAAADwtlUpX2H300WW+95w5ZSZ4S0s5t8MOyaGHJk1NyezZXZ9taUlmzSo/T5+uDToAAAAA0LtUgAMA8LS0tZWq7xkzyvHEE5Mzzkhe/erkT39KamqSc88t75ube/6OpqbSSh0AAAAAoDcJwAEAeMra2pKrriptzk88MTnuuOTgg5O//rVUhP/kJ8lzn5sccUTyt7+Viu+etLQkK1f25c4BAAAAgO2BABwAgKdk1arS6ryhobQ9P/jg5OabkwMPTJ54Ivn4x0t785aWZMmSZOTI0i69J5VKMnx43+0dAAAAANg+CMABAPin2tpKy/KmpvJ+4cJk6tRk/PjktNPKzO/Oa0myYkWpCJ82refva2hI2tu3+rYBAAAAgO3M4GpvAACAbVtra3LllcnrXrdlS/MVK5JHH02WLdvy2owZyYIF5efm5nK9Uinhd2NjMmTIVt86AAAAALCdUQEOAMA/VFubXHxxMmpUzy3Nlyzp+drChaVV+qRJydKlJSRfujSZPl34DQAAAABsHQJwAAD+oZaWZNGiJ29pvmJFct99pbJ7cwsXJrffXtqdjxyZ1NUl9fVbfcsAAAAAwHZKC3QAAP6hSqW8/lFL8733Lm3NkzILXLtzAAAAAKAaBOAAAGyhtbW0Pm9pSYYOLUH2rFmlpfmcOcnixcny5cno0aW6e8cdy+emT08uuCBZuTIZPrxcE34DAAAAAH1FC3QAALppa0vmzSvh9ujRybnnJh/6UHLhhWXe99SpyQEHJDfckNTUJMOGdX22vr60OdfuHAAAAACoBhXgAABs1Npawu9Zs5KJE5Prrkte9rLkssuSl760e+X3+vWquwEAAACAbYsAHACAjWprywzviRPLvO+//z256KJk9uxyfcSIZMyYUgn+7/9eWp6r8gYAAAAAthVaoAMAsFFLS3nNnVuqvvfcM2lu7rq+YkVy993l2NRUAnMAAAAAgG2FABwAgI0qlRJ6T5mSXH99smxZCcR70tKSrFzZh5sDAAAAAPgnBOAAAGzU3p6ce24Jvu+9Nxk1qoTiPalUkuHD+3J3AAAAAAD/mAAcAICN6uuTs84qc77Xr0/mz0+mTet5bUNDCcwBAAAAALYVg6u9AQAAti1DhiR//WsJuGfMSBYsKOebm0vb80qlXGtsLGsBAAAAALYVAnAAgO1ca2tSW9sVbre3Jx/9aDJnTrl+3HEl7F68uKwZObKsEX4DAAAAANsaLdABALZjbW3JvHnJ6NHlte++yTe+Uc69973JOeckP/tZcvjhJSTfddekrq60SgcAAAAA2NaoAAcA2E61tpage9asZOLE5Lrrkpe+NNlxx2TlyuTKK8sc8Lq6UvWdlJ8BAAAAALZVKsABALZTtbVJU1MJv2+9NTnkkOQTn0jGjCnV4GPGlIC8ra3aOwUAAAAAeGpUgAMAbKdaWsrryivLfO9vfjOZPbv79Vmzys/Tp2t7DgAAAABs+1SAAwBspyqVZM89kylTyrG5ued1TU2lWhwAAAAAYFsnAAcA2E61tyfnnps8+miybFmp+O5JS0uZCQ4AAAAAsK3TAh0AYDtVU5OceWb5ecOGUhHeUwheqSTDh/fhxgAAAAAAniEV4AAA26G2tuTjH09e+cpk6dLkvvuSadN6XtvQUKrFAQAAAAC2dSrAAQC2M62tybx5yaxZ5f0xxyS/+EVy3nnlfXNzqQSvVEr43diYDBlSrd0CAAAAADx1KsABALYztbVJU1PX+4ULk5e9LLn99uSDH0yWLCkzwZcuTaZPF34DAAAAAP2HABwAYDvT0rLlrO+FC5NXvzrZc89kzZpk5Mikri6pr6/GDgEAAAAAnhkBOADAdqZSKa+erF8v9AYAAAAA+i8BOADAdqa9vcz27klDQ7kOAAAAANAfDa72BgAA6Fv19UljY9LRkVx6aWmHXqmU8Lux0cxvAAAAAKD/EoADAGyHhgxJXv/65EMfSlauTEaMKJXfwm8AAAAAoD8TgAMAbGdaW5Pa2hJ6J+XnurryAgAAAADoz8wABwDYjrS1JfPmJaNHJ3vskYwdmzQ3l/MAAAAAAP2dCnAAgAGqs9K7c8Z3W1vyqU8ls2Z1rWlp6Xo/fXqZDw4AAAAA0F+pAAcAGIA2rfQePTrZd98Shjc19by+qalcBwAAAADozwTgAAADTGtrMmdOqexuaUkmTky+9KVk9eryvictLcnKlX24SQAAAACArUAADgAwwGxa6T1xYrJgQbJoUTJ8eGmF3pNKpVwHAAAAAOjPBOAAAANMS0tXpffcuSUMHzUq+eMfk2nTev5MQ0PS3t5XOwQAAAAA2DoGV3sDAAD0rkqlvAYPTqZMST7wgWTGjOSoo5Lrry9rmptLSF6plFB8xoxk6NDq7RkAAAAAoDeoAAcAGGDa20tF92GHJatWlWB72bLk179OJk9ODj44Wbw4uf/+cjzooGTt2mrvGgAAAADg2VMBDgAwQLS2lvnfbW1JY2Oybl15v3ZtaYFeqSQLFyZTpyYjRiRjxiRLliTr1ydLl1Z79wAAAAAAz54KcACAAaCtLZk3Lxk9uoTbt92WfOpTyY9/nJxySjJ/fvf53ytWJHffXY7mfwMAAAAAA4UKcACAfq61tYTfs2aV9yNGJIcemrzxjaXKe8GC5OqrS9CddJ//3dBQqsWHDKnW7gEAAAAAeo8KcACAfq62Nmlq6no/ZkyZ+d3SUlqeT56cjBuX1Ncn55xT2p13vqZPF34DAAAAAAOHCnAAgH6upaW8Oi1Z0jXzuzME33Tu99Chya23luC8rq46ewYAAAAA2BpUgAMA9HOVSnl1WrFiy5nfnefvvjs55phk3bq+3CEAAAAAQN8QgAMA9GNtbckf/7hl2D1jRpnv/eEPd4XjlUpy4YVl5nd9fV/vFAAAAABg69MCHQCgn2ptTebNS669NlmwoJxrbi5tz9vbkxtvTM47L5k5M1m5Mhk+vJw38xsAAAAAGKgE4AAA/VRtbdLUVALvyZOTOXOSv/41Wbs22WWX5NFHkx13LKH3yJHlM2Z+AwAAAAADmRboAAD9VEtLeSXJwoWltfmaNclnPpOMHp2MGVNe8+aVVukAAAAAAAOdCnAAgH6otbVUeVcqXSH43LnJJZcks2d3rWtpSWbNKj9Pn272NwAAAAAwsKkABwDoh2prk5tvTqZNK+9HjEimTCkzwHvS1FQ+AwAAAAAwkKkABwDoh1pakvPOSxYsKO9vvjlZtqyrGryn9StXds0CBwAAAAAYiFSAAwD0Q5VKsmRJMnlycvDByY9/nOy+ezn/ZOuHD+/DDQIAAAAAVIEAHACgH2prSxoakoULk6lTk/Hjk5//vKsl+uYaGpL29j7dIgAAAABAn9MCHQCgn2lrS666qoTdGzaUud8rViTnn18qwXfYocz8bmkpld8NDUljYzJkSLV3DgAAAACwdakABwDoR1pbkzlzSqjd2f588eLk/vuTW25JBg1Kpk9Pli4tM8GXLi3vhd8AAAAAwPZABTgAQD9SW1uqu5Ou9ucjRiRjxiRr1yb33JPU1ZXrI0eWY+d7AAAAAICBTgAOANCPtLSU16ZWrCivJFm5siv4BgAAAADY3miBDgDQj1Qq5fVk14YP78PNAAAAAABsYwTgAAD9SHt7mf/dk4aGch0AAAAAYHulBToAQD9SX580NiYdHcmll5Z26JVKCb8bG5MhQ6q9QwAAAACA6hGAAwD0MzvumBx6aLJ4cfLYY8muu5bKb+E3AAAAALC90wIdAKAfaG1N1q1Lli0rx/Xrk6OPLjO/6+pKZTgAAAAAwPZOAA4AsI1ra0vmzUtGjy6vF72oVH9/73uqvgEAAAAANiUABwDYhrW2JnPmJLNmJWPGJN/5TvK73yXHHpvstFOyalW1dwgAAAAAsO0QgAMAbMNqa5OmpmTixGTBguSOO5KxY5MJE5Ldd08+9alSIQ4AAAAAQDK42hsAAKBna9YkjzyStLQkV15ZgvDZs7uut7SUyvAkmT7dHHAAAAAAABXgAADbqMGDk113TfbcM5kyJWlu7nldU1OpFAcAAAAA2N4JwAEAtlEtLcn8+cn73pcsW1beP9m6lSv7bl8AAAAAANsqLdABALZRlUpy0UXJDTeU9uaVSs8heKWSDB/et3sDAAAAANgWqQAHANhGtbcnr31tcsQRydKlSUNDz+saGspaAAAAAIDtnQpwAIBtVE1NMmNG+fnNby6V4B0dyaWXlkrwSqWE342NyZAh1dwpAAAAAMC2QQU4AMA2qK0t+fjHk1e+MnnJS5Kbb06eeCL54AdLNfiyZeU4fbrwGwAAAACgkwpwAIBtTGtrMm9eMmtWeT91ajJiRDJmTPLWtybTpiUjR5ZrdXXV2ycAAAAAwLZGBTgAwDamtjZpaup+bsWK5O67k7lzy3UAAAAAALYkAAcA2Ma0tJTXk11bubIPNwMAAAAA0I8IwAEAtjGVSnk92bXhw/twMwAAAAAA/YgAHABgG9PenjQ09HytoaFcBwAAAABgS4OrvQEAALqrr08aG5OOjuTSS0vb80qlhN+NjcmQIdXeIQAAAADAtkkADgCwDRoyJHn5y5MPfShZvTrZbbdS+S38BgAAAAB4clqgAwBsA1pbk3XrkmXLynH16uTcc5Px45NBg5K6ulIZDgAAAADAkxOAAwBUWVtbMm9eMnp01+tzn0t+9rPkwAOTESOqvUMAAAAAgP5BC3QAgCpqbS3h96xZ5f3EicncucmUKcmjjybf/36yZo3qbwAAAACAp0IFOABAFdXWJk1N5eeJE5MFC5I77kjGjk2e//zkuc8tAXlbW3X3CQAAAADQH6gABwCoopaW8kpK5XdTUzJ7dvfrndXh06erBAcAAAAA+EdUgAMAVFGlUl4jRpS2583NPa9rairV4gAAAAAAPDkBOABAFbW3Jw0NyZgxybJlXdXgm2tpSVau7MudAQAAAAD0P1qgAwBUUU1NMmNGstNOyahRpRq8pxC8UkmGD+/jzQEAAAAA9DMqwAEAqqStLfn4x5NXvjLZZ5/kiSdKNXhPGhpKtTgAAAAAAE9OBTgAQBW0tibz5iWzZpX3U6cmhx2WXH99ed/UVCrBK5USfjc2JkOGVGu3AAAAAAD9gwAcAKAKamtLyN1p4sTkQx8qIfd735tccEFXAN7eLvwGAAAAAHgqtEAHAKiClpauWd8TJyYLFiR33JE873nJyJHJvvsm3/xmsmFDUl9fzZ0CAAAAAPQfAnAAgCqoVMorSebOLdXgs2d3heKLFiXTpiVz5pR26QAAAAAA/HMCcACAKmhrK7O9R4xIpkxJmpt7XtfUVNqlAwAAAADwz5kBDgDQx9rakquuKhXeI0cmy5Z1VX5vrqUlWbmyrAMAAAAA4B9TAQ4A0IdaW0tb84aGZPLkZMKEMve7sx365iqVZPjwvtwhAAAAAED/JQAHAOhDtbWlrXmSLFyYHHtsMn9+qQbvSUND0t7ed/sDAAAAAOjPtEAHAOhDLS1btjv/wAeSBQvKz83N5XqlUsLvxsZkyJC+3SMAAAAAQH+lAhwAoA9VKlu2O1+4sLRDnzQpWbq0zARfujSZPl34DQAAAADwdAjAAQD6SFtb8sc/9tzufOHC5PbbS7vzkSOTurqkvr7v9wgAAAAA0J9pgQ4A0AdaW5N585Jrr9XuHAAAAABgaxGAAwD0gdrapKmpBN6TJydz5iSLFyfLl5eK7w0bhN8AAAAAAM+WABwAoA+0tJRXUtqdT52ajBiRjBmTLFmS3HNPsssu1dwhAAAAAED/ZwY4AEAfqFTKa1MrViR3352sX58MH16NXQEAAAAADCwCcACAPtDeXuZ896ShoVwHAAAAAODZ0QIdAKAP1NcnjY3l585Z4JVKCb8bG83/BgAAAADoDQJwAIA+MmRIcu65yfTpyfLlye67l8pv4TcAAAAAQO/QAh0AoA898EAyfnxy+ulJXV2pDAcAAAAAoHcIwAEA+tBDDyUrViStrdXeCQAAAADAwCMABwDoQw89VI7jxlV3HwAAAAAAA5EAHACgDwnAAQAAAAC2HgE4AEAfEoADAAAAAGw9g6u9AQCAgWjNmuSJJ5K6umTlyqRSSdrbk2HDkhe/ONlrr2rvEAAAAABg4FEBDgDQyx5/vITd8+YlY8Yko0cnRxyR3Hdfcumlyfe/nxx9dNLaWu2dAgAAAAAMLAJwAIBetGpVcu+9ySc/mcyenbS0JBMnJtdfn1x7bQnE99ijHOfNS9raqr1jAAAAAICBQwt0AIBe0taW1NYmEyYkzc1d5+fOTZqaSiDeqaUlmTWr/Dx9elJf36dbBQAAAAAYkFSAAwD0gtbW5PLLk4cfTpYtKwF3kowYkUyZ0j0Q31RTUwnNAQAAAAB49gTgAAC9oLY2ufjiZNddk1GjkkqlnB8zpnsgvrmWlmTlyj7aJAAAAADAACcABwDoBS0tyaJFyfz55ThtWjm/yy7Jc5/bFYhvrlJJhg/vo00CAAAAAAxwAnAAgF5QqZTXjBnJ2LHJeecll1ySfPe7yb33dgXim2toSNrb+3KnAAAAAAADlwAcAKAXtLeXMHvhwuTww5Pbb0/e/vYy+/vNby7XZs7sqgSvVJILL0waG5P6+mruHAAAAABg4Bhc7Q0AAAwE9fUlzO7oSC69NDnllOSBB5KmptIeffLkZM6cZPHiZPnyZOTIZMOGZMiQau8cAAAAAGDgUAEOANBLdtwxmTSphNwLFyatrSX8Tsr7qVOT8eOT448vx7a2Km4WAAAAAGAAEoADAPSSFSuS445L9tgj2Wmnrrngm6+5++5k/fpk+PBq7BIAAAAAYOASgAMAPAutrcm6dcmyZcmwYclNNyVHHJEMHdo1F7wnDQ3lOgAAAAAAvUcADgDwDLW1JfPmlcD7tttKVffeeydf/3oJxjvngl94YVcleKVS3jc2lusAAAAAAPQeATgAwDPQ2prMmZNce21y/fXJHXckY8cmz39+MmZMCcbb2pIhQ5Lp05OlS0uV+NKl5f2QIdW+AwAAAACAgWdwtTcAANAf1dYmTU3JlVeW4+zZXddaWpJZs8rP06d3VXqPHFmOdXV9uVMAAAAAgO2HCnAAgGegpSUZPDiZMiVpbu55TVNTCcoBAAAAAOgbAnAAgGegUinzvpctK2F4T1pakpUr+3BTAAAAAADbOQE4AMDT1Nqa/PGPyXHHJaNGlTC8J5VKMnx4X+4MAAAAAGD7JgAHAHiaamuTd7yjvBYtSqZN63ldQ0PS3t63ewMAAAAA2J4JwAEAnqaWluTXv04mTy4t0M8/P7nwwq5K8EqlvG9sTOrrq7hRAAAAAIDtjAAcAOBpqlTKa+HC5NWvTl71quSII5LFi5P770+WLk2mT0+GDKn2TgEAAAAAti8CcACAp6m9vbQ373TbbSUIHz8+ueGGcl3lNwAAAABA3xtc7Q0AAPQ39fWlvfmGDUlzc2mJXqkk//7vyVlnqfwGAAAAAKgWATgAwDPQ1pYcfHBpe97aWgLw9nbhNwAAAABANWmBDgDwDNx7bzJ1avIv/5KMGpXU1Wl7DgAAAABQbQJwAIB/oLU1WbcuWbasHFtby/lFi8pxt92qtzcAAAAAALoTgAMAPIm2tmTevGT06K7XvHnlfEtLWbPXXlXdIgAAAAAAmzADHACgB62tJeyeNau8HzEiGTMm+dznyvs3vCF58YuT/far3h4BAAAAAOhOAA4A0IPa2qSpKZk4MZk7N5kypbRBHz06Wb48ee5zk+9/v4Tira3mfwMAAAAAbAu0QAcA2Mzjj5ewe8yYZMGC5I47krFjk9e9roTdl19egvA99ihBeGdbdAAAAAAAqksADgCwidbW5DOfSXbdNfnkJ0sV+OzZZeb33Lnl/cc+1jUDvKWltEmfM6d8FgAAAACA6hGAAwBsora2BN0LFiRHHZU0N5fzI0aUNuid7zfX1FQ+CwAAAABA9QjAAQA20dJSXp//fLJqVVel92GHdX/f0+dWruyTLQIAAAAA8CQGV3sDAADbkkqlvG67Ldl55/LzmDHJFVck9fXlfU8heKWSDB/elzsFAAAAAGBzKsABADbR3p40NCQrViTz5yfTppWW6Jdckvz4x+V9TxoaymcBAAAAAKgeFeAAwHZvzZrkiSeSurqkrS2ZMaOcv+ii5IYbSuX36aeXSvAFC8q15uZSCV6plPC7sTEZMqRKNwAAAAAAQBIV4ADAdu7xx0vl9rx5JeAeMSJ55SuTf/3XEnbvsEPX7O+FC5PJk5ODD04WL07uv78c3/c+4TcAAAAAwLZAAA4AbLdaW5N7700++clk9uyu2d6//nVywAHJZz6TdHQku+5aKr2TEoJPnZqMH58cf3xZV19fnf0DAAAAANCdABwA2G7V1iYTJpR25j2ZOzfZZZeuueCbWrEiufvu5NRTzf4GAAAAANhWmAEOAGy3Vq/uam++qREjSjv0JUuSlSuTkSPLjO8kaWoy+xsAAAAAYFslAAcAthutraXquzPA3mWXEl5XKuXcxIml6nvKlGTZsmTUqGTDhvLZIUOS6dOTCy4oofjw4aXyW/gNAAAAALDt0AIdANgutLUl8+Ylo0d3vf74x+S++5Jp00r4vWBBcscdydixpTX62LFlPnhbW/mO+vqkrq5UhNfVmf0NAAAAALCtEYADAANea2syZ04ya1ZXu/PBg0s19x57JOedl1xzTWlvPnt215qWlvKZOXPKdwAAAAAAsG0TgAMAA15tbQm3k1Lp/Z3vJA88kFxySbLDDsmgQeV8c3PPn29qKt8BAAAAAMC2TQAOAAx4LS1dM743b3P+vOclV12VPPpoV+V3T59fubLv9gsAAAAAwDMjAAcABrxKpbzmzu25zflHP5oMG1bWPNnnhw/vg40CAAAAAPCsCMABgAGvvT2ZMSOZMmXLNucjRiRjxiQ/+1nS0NDz5xsayncAAAAAALBtG1ztDQAA9JbW1jKre+3aMtu7rq60Lq9UknPOSVas6Kr8njixVIRPmZIsW5aMHp0ccUS51tRU1lUqJfxubEyGDKnOPQEAAAAA8NSpAAcABoS2tuSKK5LHHivv580rld2veEUJwb/ylVLtXak8+Szwyy5LzjsvWbq0hOJLlybTpwu/AQAAAAD6CxXgAEC/19paAu+DDkoWL06++c0y5ztJrryya+73qFHJtGnJIYd0nevU0lKqvVesKKH3yJHlfF1dX98NAAAAAADPlApwAKDfq61Nvva10s58zz275nyPGNF97veMGaUV+tFHbzkLvFNTU/k+AAAAAAD6HwE4ANCvPf54aVc+dGjy6KPl584532PGdH+/cGFy5pllXee5zbW0lJbpAAAAAAD0PwJwAKDfam1NPvOZZNddk7Vry3HUqDLnO0mWLOn+Pkluuy0ZPrz7uU1VKuU6AAAAAAD9jwAcAOgXWluTdetKRfe6deV9bW0yd24yf35yyinluGhRmfOdlBboy5eX2d6dVqwo6zrXbK6hIWlv3/r3AwAAAABA7xOAAwDbvLa2ZN68ZPTorteVV5Z25S0tZbZ3Q0Py4IPJ2LHJeecll1ySLFiQfO97JeyeObOr6vuii8qaCy/sOleplPeNjUl9fTXuEgAAAACAZ0sADgBs01atSubMSWbN6prb3dKSfO1rXa3MFy5MJk9Oxo1LhgxJNmxITj89aW5OzjmnXDv44GTx4uT++5NbbkkGDUqmT0+WLi1V5UuXlvdDhlTtVgEAAAAAeJYE4ADANqutrbQ5b2rqfn7ixOS7303++MeuVuYLFyZTpyYveEHyhjckdXVdn+u8Nn58cvzxyX77lev19eU4cmTXewAAAAAA+q/B1d4AAEBPWltLm/PXva6r8rtTU1Ny6aXJN79Z2pwnpdq7pSVZvz5585tL5fjmn1uxorySZOXKEnwDAAAAADBwqAAHALZJtbXJxRcno0Z1zelOksMOKy3NL720q/X5pu3NFy9O3v728plNP7epSqW0TwcAAAAAYGARgAMAVdXamqxblyxfnrS3l/dJqd5etCiZP7+rzfnEiaXqe/nyruruzdubjx+frF1bvquhoedfs6GhXAcAAAAAYGDRAh0AqJq2tuSKK5IJE5JXvrJUb48Zkzz2WFcF94wZpc35yJHJW95S5nRv2FCubdrivLO9eWd1d11d0thYrjU1lbWVSgm/GxuTIUP69l4BAAAAANj6VIADAFWxalXy5S8nJ5+c/PKXydixJQjfb7/ku99NHn88ee97u9qcv/71yTXXJEuWdK8K39ym1d1DhiTTpydLlybLlpXj9OnCbwAAAACAgUoFOADQ59rayozvCRNKdfbs2aW9+ZVXJlOmlBbmV15ZAvCOjuTqq0sF+MUXJ6efnlx0UXL99eW7mpu7qrunTSsV40OHdv1a9fXlOHJkOdbV9dltAgAAAADQx1SAAwB9qrU1ufzyUpH9yleWAHvixNLm/I47kgMOKAH2hReWyu+DD05+//vSFr1zJvjRR3ddW7w4uf/+cjzxxBKYAwAAAACwfVIBDgD0qdraUsl9xhmlJXlLS6n27qwEP/740h69paW8pk5N9tkn+Z//6T4TPCnfMXhwsvfepUX6Oedobw4AAAAAsD1TAQ4A9LrW1mTduuTRR8tx2bJyXLu2hNqLFiU//WkyZkyy556l7XlnJfgVV5Sgu1Lp+r4//rFr7nfnTPDO6u/bby/X3v1u4TcAAAAAwPZOAA4A9IrO0Hvt2hJir1yZfPrTyRFHJLfdlrS3l8ruznD7/e8vs8Df974SkLe0JHPnJpdckvz4xyXs3tSMGUlDQ/LhDydLlpTK8AMOSG64IampSYYN6/t7BgAAAABg2yIABwCetba2ZN685NZbkzlzknHjSkvz665Lrr++zPY+6qhk0KBSwd1Zyf3Wtyann75lJXhn2D1zZlcl+JIlyTe+kXzwg6V1+rJlyT33lM+r/AYAAAAAIDEDHAB4llatSj71qeTGG5Pzz0/OPjv50IdKMN052/u665If/jC59NLk2mu7Zng3NycvfWny3/+dnHtuVyV4S0tpcz5nTmlzvnx5MnJkqTDfZZfy2ZEjy7Guru/vGQAAAACAbZMAHAB4Uq2tSW1tOdbXl2C6UknWr0922qlUftfWlpD7W98qQfXQoSXIHjy4VHRfdFHys5+Vzzc19RxujxiRnHlm+TUrlbJm4cLS5nzEiFIhvnZtqfgGAAAAAIAnowU6ANCjtrYnn+X9yCPJ6tXJ5ZeX1uSDByeTJiW77lqC6lGjkr33LkH4+eeX1uVLlpRgO+kKt8ePT44/PnnBC8rn2ttL6/NNrViR3H13cuqp5ToAAAAAADwZATgAsIXW1ief5T12bHLSSaXy++KLu4fd8+cnp5xSjscdV65NmZJ85jPl58553p06w+3165Odd06GDUsaG5MLL+xaW6mU942NpYocAAAAAACejAAcANhCbW3yta+V8Lq5OZk7twThs2cnL3tZCbhXrEgWLeoedl90UangfvDB5B3vSP761+TRR7vWTZvW86/X0NBV3T1kSDJ9erJ0aQnVly4t74cM6bv7BwAAAACgfzIDHAC2A52zvDtneD/+eLLjjqXt+A47JHV1pdV557WVK7ec5X366cnEiclXv1rC8Pe+t6yfMSNZsKCE3Ecf3TXfu76+/Jo77dR9XVJC9c69NDSU6u5NA+7OSu+RI8uxrq5vfp8AAAAAAOjfVIADwADT2pqsW1cqr9etKyH3vHllhvdddyVPPJFceWXy2GNl/bx5yZgxySteUeZ6Dx7c8yzvlpYyC3zIkOTjH++q6F64sITenfO+TzopOeOMMtf72GOThx4qIXfnuoMPThYvTu6/P/nb35IPfEB1NwAAAAAAvUMADgD9UGfIXSq0h2bcuL2zww47pK0tueKKUsH96U8nt95aqrGvvbbM8B45srQpHzeuhNCf/GRpaz5mTPLDHybf+EayZEnPs7z33LOE5EuWlDB8xowSbM+cWc69+tXJCSeUzy1dmtxzT/KTnyS7794113vJkmTq1OSAA5IbbkhqasrcbwAAAAAA6A0CcADYBnUG3MuXl9nYra1d1zat6L7ttmT9+poMGrRz2tt3zJw5JdxuakpuuqnM67700jLD+7LLSoh99dWlpfmee5ZW5BMnJj/7WTJ6dHLxxT3P8l60KHnf+8p+Ro0qrct7quj+9reT5z2vtCwfObIcd9ppy7ne99xTWqqr/AYAAAAAoDcJwAGgijZvV/7wwyXgvuKK5MYbyyzslSvLLO2VK0uL8k0ruh98sHzHt75Vk7q6mnztayXcbm4uld3Ll3fN8L7++hI+Dx1afr3OtuZz53ZVfi9aVKq+O2d5jxtX9vD85ydnnVXC71tuKa3PkxKCT52ajB+fHH98+XV3eJI/XdTXdw/GO+d8AwAAAABAbxGAA0Av2jzQXrasHFtbS3i9+WzuTduVH3FEWXPZZcnJJ5cwurU1+frXy5rFi0uY3VnR3dRU5mw3NZVq7uXLS7hd2qInkyaVWd6dM7zvvbcE2GvXlvOdbc2nTEk+85muyu7O1uYnntg1y/u445LrrivV6Pfd19X6vFIp971+fQnCGxoE2wAAAAAAVI8AHAD+f5vO1e4MqDcPtDursDc9948C7bvuSjZsKAHxf/5nufaVr5Tv3bRd+XXXlZnYo0cnEyZ0D7df8IISiu+xR6nS7qzo7mxlfvXVXWH32rUlyO4MvTed4b1+fffZ3p1tzZct66r8njZty9bmt99err3hDWVe99lnl4rxSZO6Wp//7W/JBz9YAngAAAAAAKiWfhmAL1q0KGeccUYOPPDAHH744Zk3b17WrVtX7W3xJIYY8Ao8Q08nfH661zY9t3Jl97nad91VziddgfZXvpI89lg5t2mQ/Y8C7euvL+2+7703+eQnuwLt//N/yuzrr3+9q115ZxvyZcuSV76ye7jd2b68c/52Z7jdWe3dedw03O4MvS+6qGuG97RpXdXdDz6YjB2bnHZaMmZM98rvmTNL0D51anLAASWYr6kp4XdS9n7mmaVN+tq1Zeb3hg3JLrtU4z8lAAAAAADQpd8F4CtXrszb3/72tLe359JLL825556ba6+9NnPnzq321thMa2syePDQjB27b554YugzCqWebcDVG8HYQN/D9nSv9tB/7rW1tXs19T8Ln5/utc3PPfhgCYqvvTb54Q+TQw4pbb4vuqgr0H7BC0q186ZB9j8LtC+7rLQY33PP7oH2K15RAuZN25V3tiEfObLncPvee5MRI8r87c5wu7Pau/N40UVd4XZn6N05y3vZsuT885OTTiqfHzeu7HnDhtLWvKFhy8rv++8v/xjgbW8razfVOc97xIgyn1zbcwAAAAAAtgX9LgD/xje+kdbW1jQ3N+eII47IiSeemA9+8IP5xje+kaVLl1Z7e/z/2tpKcPXYYzVJavKf/1nzjEKpZxNw9VYwNpD3sD3dqz30r3tdtOiph8/P5Nqm5zpD6s7QevHi5HOf2zK0njKl53P/KNC+/voS7G8eaHdWcm/ernzRouSnPy0V2ZuH253tyxct6gq3O6u9O4+dYfe4cSWQfv7zu0LvN70pedWrkiOPLCH6y15W9llXVyq7GxuTCy/8x5XfAAAAAACwravp6OjoqPYmno5TTz01w4cPz+c+97mN51atWpVDDz00F110Ud74xjc+7e+86667kiT77bdfr+1ze9baWtoIH3RQmVf7zW+Waso77uh+7Ona012/Na5tT3vYnu7VHvrPvd5wQ6lGPuCA5He/6zomW557ptc2PXf00clXv5ocemjywAPlWue5178++d73yvGHPyyV0pue6zxef32phP5H37n5Hm65JfnlL8t933NPueexY0v4/YtfJBdfXCqxN/09uu66ZMGCErrvv3+ZwX3ZZcnJJ5dzp5xSQv7m5hJu77138m//lpxwQrLjjuUfGgwfXu6jp4rt1tZSzf3P1gFsj9asWZM//OEPeeELX5iddtqp2tsBGLA8bwG2Ps9agK3Ps7b3PZ08t98F4Icddlj+9V//Needd16380cccUTe8IY3bHH+qbjrrrvS0dGRvfbaq7e2uV0bPHho9t235lmHUtW6tj3tYXu6V3voP/f6dMPnp3tt83OdIfXRRyfXXFOudZ57Ont/skD7yit7Dv0ffLAE1p3B9d//XtbMnp287nVlL//5n1uG2z/+canWfvWrkzVrOjJ0aKnibm0tYXXnsaWlzPVua9uQ2tp16ejoSE1NTZ7K/+w/1XUA25O1a9fmgQceyPjx4zN06NBqbwdgwPK8Bdj6PGsBtj7P2t735z//OTU1NU8pAB/cB/vpVatWrcqwHnqxDh8+PCtXrnzG39ve3p4//OEPz2ZrJBkyZEie85yJGTp0UB59tARJm8+xHTo0PV57uuu3xrXtaQ/b073aQ/+613vv3XK+9a67lmfM5m3Bn8m1zc91thY/7riua53nNm8xvsceW57rPD74YAm9r766q0X5tGnJjBnJrbcm552XXHVV15rO8HvChE3blXdkhx2SpqaavPSlZSb4sGEdedvbypr3vz/58IdLuD14cJI8lsWL/5rHH388gwcP7hZy19TU5O9/X5/169f33f8IAGwHHuhs7QHAVuV5C7D1edYCbH2etb2rrq7uKa3rdwH41lJbW6sCvJcMHrxDr4RS1bi2Pe1he7pXe+hf9/pMwuene23Tc50h9YIFJbROup+7+uqu0PqII7YMsp9KoH3MMTX5v/83Of30ZMiQrkC787hyZanWXrduQ847L7nggh02VnA//viGDB3anvXrN2TIkJqsX9+RXXapyRNPdKS+flDq68f514QAfcCzFqBveN4CbH2etQBbn2dt7/vzn//8lNf2yxboJ554Yj7wgQ90O/9sW6AnZoD3FjPA+88etqd7tYf+da+bzrruDJbf+tZk0KASPm/aOvyZXNv8XGdr8f/7f5OXvzx5/PHkk5/sajd+9NFJW1uy884lrN9hhzJTe/O2453HzkC7vT3p6Og+V3v9+qS3R76YJwOw9XnWAvQNz1uArc+zFmDr86ztfU8nz91ha2+mt02YMCH33Xdft3OrV6/O8uXLM2HChCrtik3V15fA6KGHyvzb887rag286bGna093/da4tj3tYXu6V3voX/d64omlJfm4ceWZ8ta3lvA5Sc44owTJb3tb1/HpXtv83Pvfn/zsZ8n++5dwe9CgZPr0EsIffnhpNz5kSJm1PXx4sssu5eddd+35OHJkCb132qnsv/NcXV3vh98AAAAAAECXflcB/sUvfjFf+MIX8tOf/nTjLPBvfvOb+chHPpKf/OQnGT169NP+ThXgW0dra1Jb27FJtWTNFlWSPVVS/qPqyr66tj3tYXu6V3voX/daV5esWrX1qqYHEv+aEGDr86wF6BuetwBbn2ctwNbnWdv7nk6eO3hrb6a3nXzyyfnKV76S97znPXnXu96VpUuXZt68eTn55JOfUfjN1lNfn6xZszaLF9+fPfbYI3V1O6VzNv3mx6d6rq+vbU976Ktfxx62nT301a/zbPYwcuSW5wAAAAAAAJ5Mv2uBPnz48Fx11VUZNGhQ3vOe9+RTn/pUTjzxxMyYMaPaW+NJtLW1VXsLAAAAAAAAwHag31WAJ8mee+6ZK6+8strbAAAAAAAAAGAb0u8qwAEAAAAAAACgJwJwAAAAAAAAAAYEATgAAAAAAAAAA4IAHAAAAAAAAIABQQAOAAAAAAAAwIAgAAcAAAAAAABgQBCAAwAAAAAAADAgCMABAAAAAAAAGBAE4AAAAAAAAAAMCAJwAAAAAAAAAAYEATgAAAAAAAAAA4IAHAAAAAAAAIABQQAOAAAAAAAAwIAgAAcAAAAAAABgQBCAAwAAAAAAADAgCMABAAAAAAAAGBAE4AAAAAAAAAAMCAJwAAAAAAAAAAYEATgAAAAAAAAAA4IAHAAAAAAAAIABQQAOAAAAAAAAwIAgAAcAAAAAAABgQBCAAwAAAAAAADAgCMABAAAAAAAAGBAE4AAAAAAAAAAMCAJwAAAAAAAAAAYEATgAAAAAAAAAA4IAHAAAAAAAAIABQQAOAAAAAAAAwIAgAAcAAAAAAABgQBCAAwAAAAAAADAgCMABAAAAAAAAGBAE4AAAAAAAAAAMCAJwAAAAAAAAAAYEATgAAAAAAAAAA4IAHAAAAAAAAIABQQAOAAAAAAAAwIAgAAcAAAAAAABgQBCAAwAAAAAAADAgCMABAAAAAAAAGBAE4AAAAAAAAAAMCAJwAAAAAAAAAAYEATgAAAAAAAAAA0JNR0dHR7U3UW133nlnOjo6UldXV+2tDDgdHR1pb29PbW1tampqqr0dgAHJsxZg6/OsBegbnrcAW59nLcDW51nb+9atW5eampocdNBB/3Tt4D7YzzbPf/C2npqaGv+wAGAr86wF2Po8awH6huctwNbnWQuw9XnW9r6ampqnnOmqAAcAAAAAAABgQDADHAAAAAAAAIABQQAOAAAAAAAAwIAgAAcAAAAAAABgQBCAAwAAAAAAADAgCMABAAAAAAAAGBAE4AAAAAAAAAAMCAJwAAAAAAAAAAYEATgAAAAAAAAAA4IAHAAAAAAAAIABQQAOAAAAAAAAwIAgAAcAAAAAAABgQBCAAwAAAAAAADAgCMDZKhYtWpQzzjgjBx54YA4//PDMmzcv69atq/a2APqFH/7wh3n3u9+dyZMn58ADD8wb3vCGXHfddeno6Oi27pvf/GZe85rXZL/99svrX//6/OQnP9niu1avXp3zzz8/hx56aF7ykpekoaEhy5Yt66tbAeg3WltbM3ny5Oyzzz656667ul3zvAV4dv7rv/4rJ5xwQvbbb79MmjQp73jHO9LW1rbx+s0335zXv/712W+//fKa17wm3/rWt7b4jnXr1uXjH/94Dj/88Bx44IE544wzct999/XlbQBss2666aa86U1vykte8pK8/OUvzznnnJO//OUvW6zz51qAp+bBBx/MhRdemDe84Q3Zd999c9xxx/W4rjefq3feeWfe/OY3Z//998+RRx6ZL33pS1v8fTBPnQCcXrdy5cq8/e1vT3t7ey699NKce+65ufbaazN37txqbw2gX7jyyiszdOjQzJgxI5///OczefLkfPjDH85nP/vZjWtuuOGGfPjDH84xxxyTL3/5yznwwAMzbdq0/Pa3v+32Xe973/ty66235qMf/Wg++clP5v7778/ZZ5+d9evX9/FdAWzbPve5z+WJJ57Y4rznLcCz8/nPfz4f+9jH8rrXvS6XX355Zs2albFjx2585t5xxx2ZNm1aDjzwwHz5y1/OMccckwsuuCA/+tGPun3P7Nmz881vfjPnnntuLr300qxbty6nn356Vq9eXY3bAthm/OpXv8q0adOy11575bOf/WzOP//8LFy4MGeeeWa3f2zkz7UAT92f/vSn/PSnP80LXvCC7Lnnnj2u6c3n6oMPPpizzjorI0eOzBe/+MW8/e1vT1NTU6644oqteZsDWwf0si984QsdBx54YMejjz668dw3vvGNjhe+8IUdS5Ysqd7GAPqJhx9+eItzM2fO7DjooIM6nnjiiY6Ojo6Oo48+uuP9739/tzVvfvObO97xjndsfH/nnXd27L333h0/+9nPNp5btGhRxz777NNxww03bKXdA/Q/f/7znzsOPPDAjquvvrpj77337vj973+/8ZrnLcAzt2jRoo59992345ZbbnnSNWeeeWbHm9/85m7n3v/+93ccc8wxG9///e9/73jhC1/Y8Y1vfGPjuUcffbTjwAMP7PjSl77U+xsH6Ec+/OEPdxx11FEdGzZs2Hjutttu69h77707br/99o3n/LkW4Knr/DvYjo6Ojg996EMdxx577BZrevO5+uEPf7jjyCOP7Hj88cc3nvvUpz7Vccghh3Q7x1OnApxet2DBghx22GGpVCobzx1zzDHZsGFDbr311uptDKCf2G233bY498IXvjCPPfZY1qxZk7/85S954IEHcswxx3Rb87rXvS633XbbxpETCxYsyLBhw3L44YdvXDNhwoS88IUvzIIFC7buTQD0I7Nnz87JJ5+cPfbYo9t5z1uAZ+fb3/52xo4dm1e84hU9Xl+3bl1+9atf5bWvfW2386973euyaNGiLF68OEny85//PBs2bOi2rlKp5PDDD/ecBbZ769evT319fWpqajae22WXXZJkY+tcf64FeHp22OEfx6e9/VxdsGBBXvWqV6Wurq7bd61atSq/+c1veuOWtjsCcHrdfffdlwkTJnQ7N2zYsIwcOdJ8LoBn6H/+538yevTo7LzzzhufpZsHNXvuuWfa29s3zvm67777sscee3T7P8FJ+UOW5zFA8aMf/Sj33ntv3vOe92xxzfMW4Nn53e9+l7333juf+9zncthhh+XFL35xTj755Pzud79Lkjz00ENpb2/f4u8QOttMdj5D77vvvjznOc/J8OHDt1jnOQts7974xjdm0aJF+drXvpbVq1fnL3/5Sz796U9n3333zUEHHZTEn2sBeltvPlfXrFmTv//971v8mXjChAmpqanx/H2GBOD0ulWrVmXYsGFbnB8+fHhWrlxZhR0B9G933HFHfvCDH+TMM89Mko3P0s2ftZ3vO6+vWrVq47/63pTnMUCxdu3azJ07N+eee2523nnnLa573gI8O8uXL8/Pf/7zfPe7381HPvKRfPazn01NTU3OPPPMPPzww8/6OTts2DDPWWC7d8ghh6S5uTmf+tSncsghh2TKlCl5+OGH8+UvfzmDBg1K4s+1AL2tN5+rq1ev7vG76urqMnToUM/fZ0gADgDbsCVLluTcc8/NpEmTctppp1V7OwADyuc///k85znPyb/+679WeysAA1JHR0fWrFmTSy65JK997Wvzile8Ip///OfT0dGRr371q9XeHsCAcOedd2b69Ok56aSTctVVV+WSSy7Jhg0b8s53vjNtbW3V3h4AVIUAnF43bNiwjf9iZVMrV67col0ZAE9u1apVOfvss1OpVHLppZdunD3T+Szd/Fm7atWqbteHDRuWxx57bIvv9TwGSP7617/miiuuSENDQ1avXp1Vq1ZlzZo1SUr7sdbWVs9bgGdp2LBhqVQqmThx4sZzlUol++67b/785z8/6+fsqlWrPGeB7d7s2bPzL//yL5kxY0b+5V/+Ja997WvzpS99Kffcc0+++93vJvH3CAC9rTefq50V4pt/17p167J27VrP32dIAE6v62kmzOrVq7N8+fItZhgA0LO2tra8613vyurVq3PZZZd1a5XT+Szd/Fl73333pba2Ns9//vM3rrv//vvT0dHRbd3999/veQxs9xYvXpz29va8853vzEtf+tK89KUvzb/9278lSU477bScccYZnrcAz9Jee+31pNcef/zxjBs3LrW1tT0+Z5OuP/dOmDAhK1as2KL943333ec5C2z3Fi1a1O0fGiXJmDFjsuuuu+ahhx5K4u8RAHpbbz5Xd9pppzz3uc/d4rs6P+f5+8wIwOl1kydPzi9+8YuN/9IlSX70ox9lhx12yOGHH17FnQH0D+vXr8/73ve+3HfffbnssssyevTobtef//znZ/z48fnRj37U7fwPfvCDHHbYYamrq0tSnscrV67MbbfdtnHN/fffn3vuuSeTJ0/e+jcCsA174QtfmP/3//5ft1djY2OS5D/+4z/ykY98xPMW4Fk68sgj09LSkj/84Q8bzz366KP53//937zoRS9KXV1dJk2alP/+7//u9rkf/OAH2XPPPTN27Ngkyctf/vLssMMO+fGPf7xxzcqVK/Pzn//ccxbY7u2+++655557up3761//mkcffTTPe97zkvh7BIDe1tvP1cmTJ+emm25Ke3t7t+8aNmxYXvKSl2zluxmYBld7Aww8J598cr7yla/kPe95T971rndl6dKlmTdvXk4++eQtQhwAtvQf//Ef+clPfpIZM2bksccey29/+9uN1/bdd9/U1dXlve99b84777yMGzcukyZNyg9+8IP8/ve/7zZL8SUveUle/vKX5/zzz8+HPvSh7Ljjjrn44ouzzz775Oijj67CnQFsO4YNG5ZJkyb1eO1FL3pRXvSiFyWJ5y3AszBlypTst99+aWhoyLnnnpsdd9wxX/rSl1JXV5e3vOUtSZJ3v/vdOe200/LRj340xxxzTH71q1/l+uuvz8UXX7zxe8aMGZMTTzwx8+bNyw477JDRo0fni1/8YnbZZZecfPLJ1bo9gG3CySefnIsuuiizZ8/OUUcdlZaWlnz+85/Pc57znBxzzDEb1/lzLcBTt3bt2vz0pz9NUv5R0WOPPbYx7D700EOz22679epz9ayzzsr3v//9fOADH8gpp5ySe++9N5dffnnOPffcjWE6T09Nx+Z199ALFi1alI997GP5zW9+k/r6+rzhDW/wX1SAp+ioo47KX//61x6v3XTTTRsrYb75zW/my1/+cv72t79ljz32yPvf//4ceeSR3davXr06c+bMyY033pj169fn5S9/eWbOnOkfJAH04Fe/+lVOO+20XHfdddlvv/02nve8BXjmHnnkkcyZMyc/+clP0t7enkMOOSSNjY3d2qPfdNNN+cxnPpP7778/u+++e975znfmxBNP7PY969aty8UXX5zvfve7aW1tzUEHHZSZM2dmzz337OtbAtimdHR05Bvf+Eauvvrq/OUvf0l9fX0OPPDAnHvuuVs8I/25FuCpWbx4cV71qlf1eO3//b//t/Ef1Pfmc/XOO+/M3Llz84c//CG77bZbTj311Jx99tmpqanZOjc5wAnAAQAAAAAAABgQzAAHAAAAAAAAYEAQgAMAAAAAAAAwIAjAAQAAAAAAABgQBOAAAAAAAAAADAgCcAAAAAAAAAAGBAE4AAAAAAAAAAOCABwAAAAAAACAAUEADgAAAAAAAMCAIAAHAAAAAAAAYEAYXO0NAAAAAN398Y9/zGc/+9ncddddWbFiRSqVSvbaa68cddRRedvb3pYk+cIXvpC99torU6ZMqfJuAQAAYNtR09HR0VHtTQAAAADFnXfemdNOOy277757TjjhhIwcOTJ///vf87vf/S4PPfRQbrzxxiTJS17ykrzmNa/J3Llzq7xjAAAA2HaoAAcAAIBtyBe+8IXssssuue666zJs2LBu1x5++OEq7QoAAAD6BzPAAQAAYBvy0EMPZa+99toi/E6S5zznOUmSffbZJ2vWrMl//dd/ZZ999sk+++yTGTNmbFy3dOnSNDY25mUve1le/OIX59hjj811113X7bt+9atfZZ999skPfvCDfPrTn87hhx+eAw88MP/2b/+Wv//971v3JgEAAGArUQEOAAAA25DnPe95+c1vfpN77703e++9d49r5s2bl5kzZ2b//ffPSSedlCQZN25ckmTFihU56aSTUlNTk1NPPTW77bZbFixYkAsuuCCPPfZYTj/99G7f9fnPfz41NTU5++yz8/DDD+eqq67K6aefnu9+97sZMmTIVr1XAAAA6G1mgAMAAMA25NZbb83ZZ5+dJNl///1z8MEH57DDDsukSZNSW1u7cd2TzQC/4IIL8tOf/jTf//73s+uuu248//73vz8LFizIz3/+8wwZMiS/+tWvctppp2X06NH5wQ9+kJ133jlJ8sMf/jDve9/7csEFF+S0007rgzsGAACA3qMFOgAAAGxDDj/88HzjG9/IUUcdlYULF+ayyy7LWWedlcmTJ+emm276h5/t6OjIj3/84xx11FHp6OjII488svH18pe/PKtXr87//u//dvvMCSecsDH8TpLXvva1GTlyZH76059ulfsDAACArUkLdAAAANjG7L///mlubs66deuycOHCzJ8/P1deeWXOOeecfOc738lee+3V4+ceeeSRrFq1Ktdcc02uueaaJ12zqRe84AXd3tfU1OQFL3hB/vrXv/bOzQAAAEAfEoADAADANqquri77779/9t9//4wfPz6NjY350Y9+lGnTpvW4fsOGDUmS17/+9Zk6dWqPa/bZZ5+ttl8AAACoNgE4AAAA9AMvfvGLkyTLli170jW77bZb6uvrs2HDhrzsZS97St/74IMPdnvf0dGRBx98UFAOAABAv2QGOAAAAGxDfvnLX6ajo2OL850zuSdMmJAk2WmnnbJq1apuawYNGpTXvOY1+e///u/ce++9W3zH5u3Pk+Q73/lOHnvssY3vf/SjH2X58uWZPHnys7oPAAAAqIaajp7+XzUAAABQFccdd1zWrl2bV7/61ZkwYULa29tz55135oc//GHGjBmT73znOxk2bFje+c535vbbb09DQ0NGjRqVsWPH5oADDsiKFSty0kkn5ZFHHsmb3vSm7LXXXlm5cmX+93//N7fddlt+/etfJ0l+9atf5bTTTsvee++dmpqavPGNb8zDDz+cq666KmPGjMl3v/vdDB06tMq/GwAAAPD0CMABAABgG7JgwYL86Ec/ym9+85ssWbIk7e3t2X333TN58uS8+93vznOe85wkyX333ZcLL7wwd911V9ra2jJ16tTMnTs3SfLwww/ns5/9bG6++easWLEilUole+21V173utflpJNOStIVgH/605/OH//4x1x33XVpbW3Nv/zLv+QjH/lIdt9996r9HgAAAMAzJQAHAACA7VBnAH7JJZfkta99bbW3AwAAAL3CDHAAAAAAAAAABgQBOAAAAAAAAAADggAcAAAAAAAAgAHBDHAAAAAAAAAABgQV4AAAAAAAAAAMCAJwAAAAAAAAAAYEATgAAAAAAAAAA4IAHAAAAAAAAIABQQAOAAAAAAAAwIAgAAcAAAAAAABgQBCAAwAAAAAAADAgCMABAAAAAAAAGBAE4AAAAAAAAAAMCAJwAAAAAAAAAAYEATgAAAAAAAAAA4IAHAAAAAAAAIABQQAOAAAAAAAAwIAgAAcAAAAAAABgQBCAAwAAAAAAADAgCMABAAAAAAAAGBAE4AAAAAAAAAAMCAJwAAAAAAAAAAaEwdXeAAAAAGwLvv3tb6exsTHXXXdd9ttvv2pv5x+644478oUvfCF//OMf09LSkuc85zmZOHFijj322Bx//PFJkrVr1+ayyy7LoYcemkmTJlV5x0/ubW97W379618nSWpqarLTTjtl5MiR2X///XPCCSfk8MMPr/IOAQAA6E8E4AAAANCP/PCHP8y5556bF77whTnttNMyfPjwLF68OLfffnuuvfbabgF4c3Nzpk2btk0H4EkyZsyYvP/9709S9v3ggw/mxhtvzPe+970cc8wx+cQnPpHa2toq7xIAAID+QAAOAAAA/Uhzc3P22muvXHPNNamrq+t27eGHH67Srp6dXXbZJW94wxu6nTvvvPMye/bsfP3rX8/znve8fPCDH6zS7gAAAOhPzAAHAACAp+Gee+7JO97xjhx00EF5yUtekre//e357W9/221Ne3t7mpubc/TRR2e//fbLpEmTcsopp+TWW2/duGb58uVpbGzM5MmT8+IXvzgvf/nL8+53vzuLFy/+h7/+Qw89lP3222+L8DtJnvOc5yRJFi9enMMOOyxJCcz32Wef7LPPPrn00ks3rl20aFEaGhpy6KGHZr/99ssb3/jG3HTTTd2+79vf/nb22Wef3H777bnwwgszadKkHHTQQZk+fXpWrlzZbe1dd92Vs846K5MmTcr++++fo446Ko2Njf/8N/RJDBo0KDNnzsxee+2Vr33ta1m9evUz/i4AAAC2HyrAAQAA4Cn605/+lFNPPTX19fV5xzvekcGDB+eaa67J2972tnz1q1/NAQcckKSEzl/84hfzpje9Kfvvv38ee+yx3H333fnf//3fjTOt3/ve9+bPf/5z3vrWt+Z5z3teHnnkkdx66635+9//nrFjxz7pHnbffffcdtttWbJkScaMGdPjmt122y0f/ehH89GPfjSvfvWr8+pXvzpJss8++2y8j1NOOSWjR4/O2WefnZ122ik//OEP8573vCeXXnrpxvWdZs2alWHDhmXatGm5//77c/XVV+dvf/tbvvKVr6SmpiYPP/xwzjrrrOy666555zvfmWHDhmXx4sW58cYbn9Xv96BBg3Lsscfmkksuyf/8z//kla985bP6PgAAAAY+ATgAAAA8RZ/5zGfS3t6eq6++Os9//vOTJCeccEJe+9rX5hOf+ES++tWvJkluueWWvOIVr8jHPvaxHr9n1apV+c1vfpPp06fnrLPO2nj+Xe961z/dw9lnn50LLrggU6ZMyUEHHZSDDz44hx9+eA466KDssENp9LbTTjvlNa95TT760Y9mn3322aK9+P/9v/83z33uc/Otb31rYyX5W97ylpxyyin55Cc/uUUAXltbmyuvvHLjHO7dd989n/jEJ3LzzTfnVa96VX7zm99k5cqVufzyy7Pffvtt/Ny55577T+/nn9l7772TlMp3AAAA+Ge0QAcAAICn4Iknnsitt96aKVOmbAy/k2TUqFE57rjj8j//8z957LHHkiTDhg3Ln/70pzzwwAM9fteQIUNSW1ubX//611u0Ev9nTjzxxFx22WWZNGlS7rzzznzuc5/LqaeemqOPPjp33nnnP/18S0tLfvnLX+aYY47JY489lkceeSSPPPJIHn300bz85S/PAw88kKVLl3b7zJvf/OaN4XeSnHLKKRk8eHB++tOfJikzvJMS/Le3tz+t+/lndtpppyRJa2trr34vAAAAA5MAHAAAAJ6CRx55JGvXrs0ee+yxxbU999wzGzZsyN///vckSUNDQ1avXp3XvOY1Of744/Pxj388Cxcu3Li+rq4u5513XhYsWJDDDz88p556ar785S9n+fLlT2kvRxxxRC6//PLcfvvt+drXvpZTTz01f/vb3/Jv//Zvefjhh//hZx966KF0dHTkkksuyWGHHdbt1TkjfPPveMELXtDtfX19fUaOHJm//vWvSZJDDz00r3nNa9Lc3Jx/+Zd/ybvf/e5861vfyrp1657S/fwja9as2fhrAgAAwD+jBToAAAD0spe+9KW58cYbc9NNN+XWW2/Nddddl6uuuir/8R//kTe96U1JktNPPz1HHXVU5s+fn5///Oe55JJL8qUvfSlXXXVV9t1336f06wwdOjSHHHJIDjnkkOy6665pbm7OggULMnXq1Cf9zIYNG5IkZ555Zo444oge14wbN+5p3W9NTU2ampry29/+Nj/5yU/ys5/9LOeff37+8z//M9dcc82zCq/vvffeJFuG8AAAANATFeAAAADwFOy2224ZOnRo7r///i2u3Xfffdlhhx3y3Oc+d+O5SqWSf/3Xf82nP/3p3HLLLdlnn302Vlh3GjduXM4888xcccUVuf7669Pe3p4rrrjiGe3vxS9+cZJsrCKvqanpcV1n+/ba2tq87GUv6/G18847d/vMgw8+2O19a2trli9fnuc973ndzh944IE599xz8+1vfzuf/OQn86c//Sk/+MEPntH9JKXt/PXXX5+hQ4fm4IMPfsbfAwAAwPZDAA4AAABPwaBBg3L44YfnpptuyuLFizeeX7FiRa6//vocfPDBG4PjRx99tNtn6+vrM27cuI0twdeuXZvHH3+825px48alvr7+n7YNv+2223o83zmPu7NF+9ChQ5Mkq1at6rbuOc95Tg499NBcc801WbZs2Rbf88gjj2xx7pprruk22/vqq6/O+vXrM3ny5CTJypUr09HR0e0zL3zhC5PkGbdBf+KJJzJ79uwsWrQob3vb27YI5QEAAKAnWqADAADAJr71rW/lZz/72RbnTzvttLzvfe/LL37xi7zlLW/JW97ylgwaNCjXXHNN1q1blw9+8IMb1x577LE59NBD86IXvSiVSiV33XVX/vu//ztvfetbkyQPPPBATj/99Lz2ta/NXnvtlUGDBmX+/PlZsWJFjj322H+4v3//93/P2LFjc+SRR+b5z39+1q5dm1/84hf5yU9+kv322y9HHnlkkmTIkCHZa6+98sMf/jDjx49PpVLJ//k//yd77713PvKRj+Qtb3lLjj/++Jx00kl5/vOfnxUrVuS3v/1tlixZku9973vdfs329vacfvrpOeaYY3L//ffn61//eg4++OC86lWvSpL813/9V66++upMmTIl48aNS2tra6699trsvPPOG0Pyf2T16tX57ne/myRpa2vLgw8+mBtvvDEPPfRQjj322Jxzzjn/9DsAAAAgSWo6Nv8n2gAAALAd+va3v53GxsYnvf7Tn/40Y8aMyT333JNPfepTufPOO9PR0ZH9998/5557bl7ykpdsXPv5z38+N998cx544IGsW7cuu+++e97whjfkrLPOSm1tbR599NFceumlue2227JkyZIMGjQoEyZMyBlnnJFjjjnmH+7zhhtuyE033ZS77rory5YtS0dHR57//OdnypQpOfvss7tVSv/mN7/Jxz72sdx7771pb2/PtGnT8t73vjdJ8pe//CXNzc259dZb09LSkt122y377rtvpk6dmte85jXdfk+++tWv5vvf/35+9KMfpb29Pa961asyc+bMVCqVJMk999yTyy+/PHfeeWdWrFiRXXbZJfvvv3+mTZu2sTX7k3nb296WX//61xvf77TTThk1alT233//nHDCCTn88MP/4ecBAABgUwJwAAAAoEedAfh1112X/fbbr9rbAQAAgH/q/2Pv/qMkq+s74b976B5rKKRLnKFBGhxoFPwxBMxmUU90dyOaqBk9iUt+68k+xl8JVjQmxZQes7GHJ5qOOWohYVfEn1HzxORE88RRNxvNL9cnLm5Uhgg6g21QmAHUaoZimq6Z6eePKwwD3TAz3V23uvr1Oueeuvd+aqj3GeH+8/b7vd4BDgAAAAAAAMBAUIADAAAAAAAAMBAU4AAAAAAAAAAMBO8ABwAAAAAAAGAg9O0K8E6nk2c/+9k577zzcv311x8x+/jHP56f/MmfzJYtW/KiF70on//850tKCQAAAAAAAEC/GC47wGL++I//OAcPHnzI/U996lN585vfnFe/+tV5+tOfnh07duSyyy7LRz7ykVx44YXH9Vv/8i//kvn5+YyMjCwxNQAAAAAAAADLqdvtZmhoKBdddNEjfrcvV4Dv3r07H/3oR/Pa1772IbNWq5UXvvCFed3rXpenP/3pmZyczJYtW3LVVVcd9+/Nz8/HTvArY35+PnNzc/5+AVaQZy3AyvOsBegNz1uAledZC7DyPGuX37H0uX25AvyKK67IL/zCL+Tss88+4v4tt9yS6enp/M7v/M4R91/wghdkamoqc3NzWb9+/TH/3n0rv7ds2XL8oVnQPffck69//es599xzc+KJJ5YdB2AgedYCrDzPWoDe8LwFWHmetQArz7N2+T34ldkPp+8K8M985jP5xje+kSuvvDI33HDDEbObb745SR5SjE9MTKTb7eaWW27JxMTEcf3u/Px87rnnnuMLzaL2799/xCcAy8+zFmDledYC9IbnLcDK86wFWHmetctvfn4+Q0NDR/XdvirA9+/fn7e97W15/etfn5NOOukh85mZmSTJySeffMT9+67vmx+Pbrebr3/968f953l409PTZUcAGHietQArz7MWoDc8bwFWnmctwMrzrF1eR7sTeF8V4FdffXUe+9jH5iUveUnPf3tkZCTnnntuz3930O3fvz/T09PZvHlzNmzYUHYcgIHkWQuw8jxrAXrD8xZg5XnWAqw8z9rlt2vXrqP+bt8U4N/97nfzvve9L1dddVX27duXJPdvSX7PPfek0+lkdHQ0SbJv375s2rTp/j971113Jcn98+MxNDRkD/4VtGHDBn+/ACvMsxZg5XnWAvSG5y3AyvOsBVh5nrXL52i3P0/6qAD/zne+k263m1e+8pUPmb3sZS/Lj/zIj+SP/uiPkhTvAj/nnHPun998880ZGRnJmWee2bO8AAAAAAAAAPSXvinAn/SkJ+VDH/rQEfe+/vWv561vfWve8pa3ZMuWLTnzzDOzefPmfOYzn8kll1xy//d27NiRZzzjGUe97zsAAAAAAAAAg6dvCvCTTz45F1988YKzpzzlKXnKU56SJHnta1+b3/7t385ZZ52Viy++ODt27MjXvva1/Mmf/Ekv4wIAAAAAAADQZ/qmAD9aP/3TP539+/fnmmuuyXve856cffbZefe7352LLrqo7GgAAAAAAAAAlKivC/CLL744N91000PuX3rppbn00ktLSAQAAAAAAABAv1pXdgAAAAAAAAAAWA4KcAAAAAAAAAAGggIcAAAAAAAAgIGgAAcAAAAAAABgICjAAQAAAAAAABgICnAAAAAAAAAABoICHAAAAAAAAICBoAAHAAAAAAAAYCAowAEAAAAAAAAYCApwAAAAAAAAAAaCAhwAAAAAAACAgaAABwAAAAAAAGAgKMABAAAAAAAAGAgKcAAAAAAAAAAGggIcAAAAAAAAgIGgAAcAAAAAAABgICjAAQAAAAAAABgICnAAAAAAAAAABoICHAAAAAAAAICBoAAHAAAAAAAAYCAowAEAAAAAAAAYCApwAAAAAAAAAAaCAhwAAAAAAACAgaAABwAAAAAAAGAgKMABAAAAAAAAGAgKcAAAAAAAAAAGggIcAAAAAAAAgIGgAAcAAAAAAABgICjAAQAAAAAAABgICnAAAAAAAAAABoICHAAAAAAAAICBoAAHAAAAAAAAYCAowAEAAAAAAAAYCApwAAAAAAAAAAaCAhwAAAAAAACAgaAABwAAAAAAAGAgKMABAAAAAAAAGAgKcFZcpVIpOwIAAAAAAACwBijAWTmdTjYMD+f8xz42G4aHk06n7EQAAAAAAADAAFOAszJmZ5OpqQyNjeWE00/P0NhYMjVV3AcAAAAAAABYAcNlB2AAdTpF2T05efheu334utFIqtVSogEAAAAAAACDywpwlt/ISNJqLTxrtYo5AAAAAAAAwDJTgLP82u3iWGw2M9PDMAAAAAAAAMBaoQBn+dVqxbHYbHS0h2EAAAAAAACAtUIBzvLrdpN6feFZvV7MAQAAAAAAAJbZcNkBGEDVatJsFuetVrHtea1WlN/NZlKplJkOAAAAAAAAGFAKcFZGpZI0GplvNjO0Z0/mTz01Q/fdBwAAAAAAAFgBtkBn5VSrufeb30x2707m55O7707m5pJOp+xkAAAAAAAAwABSgLOiHnXuuZn/u7/L0Ph4MjZWHFNTyexs2dEAAAAAAACAAWMLdFZOp5NMTWXoiisO32u3k8nJ4rzRKN4XDgAAAAAAALAMrABn5YyMZKjVWnjWaiUjI73NAwAAAAAAAAw0BTgrp90ujsVmMzM9DAMAAAAAAAAMOgU4K6dWK47FZqOjPQwDAAAAAAAADDoFOCun2818vb7wrF5Put3e5gEAAAAAAAAG2nDZARhg1WqybVvmk+Jd4O12sfK7Xk+azaRSKTkgAAAAAAAAMEgU4Kyo2SRzr351Tr788gzdfnty2mnJwYPKbwAAAAAAAGDZ2QKdFTU/P59dt96ag697XbJ1a3L11cXKcAAAAAAAAIBlpgCnJw6ddVayZ09y221lRwEAAAAAAAAGlC3QWXGVSiUn/PIvJ69/fXLnncncXNLtWgkOAAAAAAAALCsrwFlRQ0NDOX/z5qz7wAeS8fFk8+ZkbCyZmkpmZ8uOBwAAAAAAAAwQK8BZUesPHMi6t789Q5OTh2+228l9142GleAAAAAAAADAsrACnBW17lGPylCrtfCw1UpGRnobCAAAAAAAABhYCnBWVrtdHIvNZmZ6GAYAAAAAAAAYZApwVlatVhyLzUZHexgGAAAAAAAAGGQKcFbUoXvvzXy9vvCwXk+63d4GAgAAAAAAAAbWcNkBGGxzw8NZ32hkXVK8C7zdLlZ+1+tJs5lUKiUnBAAAAAAAAAaFFeCsqPn5+dw4PZ1Db3hDsndv8q1vJd/5TvIbv6H8BgAAAAAAAJaVApwVNzs7m3uHh5P165PLL09+9EeTnTvLjgUAAAAAAAAMGFug01tveEPylKcUW6HPzRXvAK9Wy04FAAAAAAAADAArwOmJoaGhZHY2+eu/TsbHi2NsLJmaKu4DAAAAAAAALJEV4PTE+gMHkre/Pdm+/fDNdjuZnCzOGw0rwQEAAAAAAIAlsQKcFTc8PJx1j3pU0mot/IVWKxkZ6W0oAAAAAAAAYOAowFlxw8PDxWrvdnvhL7TbycxMDxMBAAAAAAAAg0gBzoo7cOBAUqsVx0JqtWR0tIeJAAAAAAAAgEGkAGfFHThwIIfuvTep1xf+Qr2edLu9DQUAAAAAAAAMnOGyA7A2zA0PZ0OzWVy0WsW257VaUX43m0mlUmY8AAAAAAAAYAAowOmJ+fn5ZMOGpNEoCu89e5JTT03m55XfAAAAAAAAwLKwBTq9Va0mX/lKsnVr8qxnFdcAAAAAAAAAy8AKcHrv7LOTnTuToaFkdtYKcAAAAAAAAGBZWAFO7516ajI6mjz2scktt5SdBgAAAAAAABgQCnB6b2go+cu/TKaniy3Q5+aSTqfsVAAAAAAAAMAqpwCn92Znk89/PhkfT844IxkbS6amivsAAAAAAAAAx8k7wOmtTqcou7dvP3yv3U4mJ4vzRqNYFQ4AAAAAAABwjKwAp7dGRpJWa+FZq1XMAQAAAAAAAI6DApzeareLY7HZzEwPwwAAAAAAAACDRAFOb9VqxbHYbHS0h2EAAAAAAACAQaIAp7e63aReX3hWrxdzAAAAAAAAgOMwXHYA1phqNWk2i/NWq9j2vFYryu9mM6lUykwHAAAAAAAArGIKcHqvUkkajWTbtmTv3mRsLDl0SPkNAAAAAAAALIkt0ClHtZp86EPJ1q3Jb/xGcQ0AAAAAAACwBFaAU56zzkp27kxOOaXsJAAAAAAAAMAAsAKc8jzlKcknPpHs2JHcfnsyN5d0OmWnAgAAAAAAAFYpBTjlOfXU5LrrkvHx4j3gY2PJ1FQyO1t2MgAAAAAAAGAVsgU65eh0irL7iisO32u3k8nJ4rzR8F5wAAAAAAAA4JhYAU45RkaSVmvhWatVzAEAAAAAAACOgQKccrTbxbHYbGamh2EAAAAAAACAQaAApxy1WnEsNhsd7WEYAAAAAAAAYBAowClHt5vU6wvP6vViDgAAAAAAAHAMhssOwBpVrSbNZnHeahXbntdqRfndbCaVSpnpAAAAAAAAgFVIAU55KpWk0SiOO+5ITj89OXBA+Q0AAAAAAAAcF1ugU65qNfnZn022bk3+x/8orgEAAAAAAACOgwKc8o2NJTt3FgcAAAAAAADAcVKAU74nPjHZuDHpdMpOAgAAAAAAAKxiCnDK90u/lExPJ694RTI3pwgHAAAAAAAAjosCnHLNziYf/GAyPp5s3lxshz41VdwHAAAAAAAAOAbDZQdgDet0irJ7cvLwvXb78HWjkVSrpUQDAAAAAAAAVh8rwCnPyEjSai08a7WKOQAAAAAAAMBRUoBTnna7OBabzcz0MAwAAAAAAACw2inAKU+tVhyLzUZHexgGAAAAAAAAWO0U4JSn203q9YVn9XoxBwAAAAAAADhKw2UHYA2rVpNmszhvtYptz2u1ovxuNpNKpcx0AAAAAAAAwCqjAKdclUrSaCRvfGNy223Jpk3J/LzyGwAAAAAAADhmtkCnfNVqMjyc/OzPJps3J3feWXYiAAAAAAAAYBVSgNMfTjghuffeovy+6aay0wAAAAAAAACrkAKc/vHEJyYbNybf+17ZSQAAAAAAAIBVyDvA6R+/93vJuecm3/9+MjeXdLvF9ugAAAAAAAAAR8EKcPrD7GzyF3+RjI8nZ52VjI0lU1PFfQAAAAAAAICjYAU45et0irJ7+/bD99rtZHKyOG80rAQHAAAAAAAAHpEV4JRvZCRptRaetVrFHAAAAAAAAOARKMApX7tdHIvNZmZ6GAYAAAAAAABYrRTglK9WK47FZqOjPQwDAAAAAAAArFYKcMrX7Sb1+sKzer2YAwAAAAAAADyC4bIDQKrVpNkszlutYtvzWq0ov5vNpFIpMx0AAAAAAACwSijA6Q+VStJoFIX3nj3Jqacm8/PKbwAAAAAAAOCo9VUB/vd///e55pprsmvXrtx9990ZGxvLJZdckssuuyyPfvSjkyTbtm3LX/7lXz7kz15zzTV59rOf3evILKdqNbnhhuQXfiG5997kG98oOxEAAAAAAACwivRVAd5ut3PBBRfkpS99aWq1Wr75zW/myiuvzDe/+c28733vu/97Z555Zt7+9rcf8WcnJiZ6HZeVcPbZyc6dycaNyQ9+kDzmMWUnAgAAAAAAAFaJvirAX/ziFx9xffHFF2f9+vV585vfnL1792ZsbCxJUqlUcuGFF5aQkBV34onJZz6T/PiPJ3fdlczNJd1usTocAAAAAAAA4GGsKzvAI6nVakmSbrdbbhB6Y3Y2+cIXkvHx5HGPS8bGkqmp4j4AAAAAAADAw+irFeD3OXjwYA4cOJBdu3blqquuyk/8xE9kfHz8/vm3v/3t/OiP/mjuvffePPGJT8yv//qv55JLLlnSb87Pz+eee+5ZanQeZP/+/Ud8PpxHHTyYdW9/e4a2bz98s91OJiczn+TQG96Qe4f78l9ZgFIdy7MWgOPjWQvQG563ACvPsxZg5XnWLr/5+fkMDQ0d1XeH5ufn51c4zzF79rOfnb179yZJnvWsZ6XVauXEE09Mknzwgx/M8PBwzj333Ozbty8f+9jH8k//9E9517velZ/6qZ86rt+7/vrrMzc3t2z5OXbDw8PZcv75WXfaaUXp/WC1Wg7t2ZPrb7wxBw4c6Hk+AAAAAAAAoDzr16/Pli1bHvF7fVmA33jjjdm/f3927dqVq6++OuPj43n/+9+fE0444SHfPXToUH7hF34hd999d3bs2HFcv3f99ddnfn4+55577lKj8yD79+/P9PR0Nm/enA0bNiz6vaGhoVT27cvQD9/zvpD5vXsz++hHpw//lQUo1dE+awE4fp61AL3heQuw8jxrAVaeZ+3y27VrV4aGho6qAO/L/aTPP//8JMlFF12ULVu25MUvfnH+5m/+ZsEV3uvWrcvznve8/OEf/mFmZ2dTqVSO6zeHhobuX2XO8tuwYcMj//2ecEJSqy26AnyoVsuG9etXIh7AQDiqZy0AS+JZC9AbnrcAK8+zFmDledYun6Pd/jxJ1q1gjmVx3nnnZWRkJP/2b/9WdhRWWreb1OsLz+r1Yg4AAAAAAACwiL5cAf5AX/3qV9PtdjM+Pr7g/NChQ/nMZz6TJzzhCce9+ps+Ua0mzWZx3moVK8FrtaL8bjYT//sCAAAAAAAAD6OvCvDLLrssT33qU3PeeeelUqnkxhtvzLXXXpvzzjsvl1xySb773e9m27ZteeELX5jHP/7xmZmZycc+9rHs3LkzV155ZdnxWQ6VStJoJJdfntx+e3LaacnBg8pvAAAAAAAA4BH1VQF+wQUXZMeOHXnPe96T+fn5nHHGGbn00kvz8pe/POvXr0+1Ws1JJ52Uq6++Ot/73vcyMjKSpz71qbnmmmvyrGc9q+z4LJdqNdm+PfmzP0te+MLkbW8rOxEAAAAAAACwCvRVAf7KV74yr3zlKxed12q1XH311T1MRGnGx5OdO4sV4AAAAAAAAABHYV3ZAWBBT3xisnFjMjRUdhIAAAAAAABgleirFeBwv6c8JZmeLt4DPjeXdLvF1ugAAAAAAAAAi7ACnP4zO5u84x3FNujnnJOMjSVTU8V9AAAAAAAAgEVYAU5/6XSKsnty8vC9dvvwdaNhJTgAAAAAAACwICvA6S8jI0mrtfCs1SrmAAAAAAAAAAtQgNNf2u3iWGw2M9PDMAAAAAAAAMBqogCnv9RqxbHYbHS0h2EAAAAAAACA1UQBTn/pdpN6feFZvV7MAQAAAAAAABYwXHYAOEK1mjSbxXmrVWx7XqsV5XezmVQqZaYDAAAAAAAA+pgCnP5TqSSNRvKmNyW33ZZs3FgU4cpvAAAAAAAA4GHYAp3+VK0m69cn//f/nWzenHz2s2UnAgAAAAAAAPqcApz+tn598TkzU24OAAAAAAAAoO8pwOlvl12WTE8nL3lJMjeXdDplJwIAAAAAAAD6lAKc/jU7m3z0o8n4ePL4xydjY8nUVHEfAAAAAAAA4EGGyw4AC+p0irJ7+/bD99rtZHKyOG80iveEAwAAAAAAAPyQFeD0p5GRpNVaeNZqFXMAAAAAAACAB1CA05/a7eJYbDYz08MwAAAAAAAAwGqgAKc/1WrFsdhsdLSHYQAAAAAAAIDVQAFOf+p2k3p94Vm9XswBAAAAAAAAHmC47ACwoGo1aTaL81ar2Pa8VivK72YzqVTKTAcAAAAAAAD0IQU4/atSSRqN5I1vTG67Ldm06fB9AAAAAAAAgAexBTr9rVpNOp1k69Zk8+ZkaKjsRAAAAAAAAECfUoDT/045Jbn11uTOO5NvfrPsNAAAAAAAAECfUoCzOjzxicnGjcmePWUnAQAAAAAAAPqUd4CzOrzrXclTnpK028ncXNLtFtujAwAAAAAAAPyQFeD0v9nZ5K//OhkfL46xsWRqqrgPAAAAAAAA8ENWgNPfOp2i7N6+/fC9djuZnCzOGw0rwQEAAAAAAIAkVoDT70ZGklZr4VmrVcwBAAAAAAAAogCn37XbxbHYbGamh2EAAAAAAACAfqYAp7/VasWx2Gx0tIdhAAAAAAAAgH6mAKe/dbtJvb7wrF4v5gAAAAAAAABJhssOAA+rWk2azeK81Sq2Pa/VivK72UwqlTLTAQAAAAAAAH1EAU7/q1SSRqMovPfsSU49NZmfV34DAAAAAAAAR7AFOqtDtZp88YvJ1q3Jc59bXAMAAAAAAAA8gBXgrB5nn53s3Jmcfnpy8GBywgllJwIAAAAAAAD6iBXgrB5nnZX81V8l3/xmcuedydxc0umUnQoAAAAAAADoEwpwVo+5ueRLX0rGx5PTTkvGxpKpqWR2tuxkAAAAAAAAQB+wBTqrQ6dTlN1XXHH4XrudTE4W542G94IDAAAAAADAGmcFOKvDyEjSai08a7WKOQAAAAAAALCmKcBZHdrt4lhsNjPTwzAAAAAAAABAP1KAszrUasWx2Gx0tIdhAAAAAAAAgH6kAGd16HaTen3hWb1ezAEAAAAAAIA1bbjsAHBUqtWk2SzOW61i2/NarSi/m82kUikzHQAAAAAAANAHFOCsHpVK0mgkl1+e3H57ctppycGDym8AAAAAAAAgiS3QWW2q1WTbtmTr1uSP/7i4BgAAAAAAAIgCnNXo9NOTnTuTf/mXspMAAAAAAAAAfUQBzurzxCcmGzcW258DAAAAAAAA/JB3gLP6/PiPJ9PTxXvA5+aSbtdW6AAAAAAAAIAV4Kwys7PFu7/Hx5NzzknGxpKpqeI+AAAAAAAAsKZZAc7q0ekUZffk5OF77fbh60bDSnAAAAAAAABYw6wAZ/UYGUlarYVnrVYxBwAAAAAAANYsBTirR7tdHIvNZmZ6GAYAAAAAAADoNwpwVo9arTgWm42O9jAMAAAAAAAA0G8U4Kwe3W5Sry88q9eLOQAAAAAAALBmDZcdAI5atZo0m8V5q1Vse16rFeV3s5lUKmWmAwAAAAAAAEqmAGd1qVSSRiN505uSW29NNm1K5uaU3wAAAAAAAIAt0FmFqtVk/frkFa9INm9Obrqp7EQAAAAAAABAH1CAs3p1u8mddya7dpWdBAAAAAAAAOgDCnBWr3PPLT537y43BwAAAAAAANAXFOCsXhMTycaNyb33lp0EAAAAAAAA6AMKcFavn//5ZHo6edWrkrm5pNMpOxEAAAAAAABQIgU4q9PsbPLBDybj48nmzcnYWDI1VdwHAAAAAAAA1qThsgPAMet0irJ7cvLwvXb78HWjkVSrpUQDAAAAAAAAymMFOKvPyEjSai08a7WKOQAAAAAAALDmKMBZfdrt4lhsNjPTwzAAAAAAAABAv1CAs/rUasWx2Gx0tIdhAAAAAAAAgH6hAGf16XaTen3hWb1ezAEAAAAAAIA1Z7jsAHDMqtWk2SzOW61i2/NarSi/m82kUikzHQAAAAAAAFASBTirU6WSNBrJG9+Y3HZbsmnT4fsAAAAAAADAmmQLdFavarVY/b11a3LOOcnISNmJAAAAAAAAgBIpwFndTj01+da3kttvT6any04DAAAAAAAAlEgBzuo2NJRMTCQbNyZ795adBgAAAAAAACiRApzV77/9t2L19znnJHNzSadTdiIAAAAAAACgBApwVrfZ2eTTn07Gx4tjbCyZmiruAwAAAAAAAGvKcNkB4Lh1OkXZvX374XvtdjI5WZw3Gkm1Wko0AAAAAAAAoPesAGf1GhlJWq2FZ61WMQcAAAAAAADWDAU4q1e7XRyLzWZmehgGAAAAAAAAKJsCnNWrViuOxWajoz0MAwAAAAAAAJRNAc7q1e0m9frCs3q9mAMAAAAAAABrxnDZAeC4VatJs1mct1rFtue1WlF+N5tJpVJmOgAAAAAAAKDHFOCsbpVK0mgk27Yle/cmY2PJoUPKbwAAAAAAAFiDbIHO6letJh/6ULJ1a3LZZcU1AAAAAAAAsOZYAc5geNzjkp07k5GRspMAAAAAAAAAJbECnMEwMVF87t6dzM+XmwUAAAAAAAAohQKcwXDOOcnQULJ+ffKDH5SdBgAAAAAAACiBApzBUKkkO3Yk09PJ3FxxdDplpwIAAAAAAAB6SAHOYJidTb7whWR8PDn99GRsLJmaKu4DAAAAAAAAa8Jw2QFgyTqdouy+4orD99rtZHKyOG80kmq1lGgAAAAAAABA71gBzuo3MpK0WgvPWq1iDgAAAAAAAAw8BTirX7tdHIvNZmZ6GAYAAAAAAAAoiwKc1a9WK47FZqOjPQwDAAAAAAAAlEUBzurX7Sb1+sKzer2YAwAAAAAAAANvuOwAsGTVatJsFuetVrHtea1WlN/NZlKplJkOAAAAAAAA6BEFOIOhUkkajeTyy5Pbb09OOy05eFD5DQAAAAAAAGuILdAZHNVq8lu/lWzdmlxzTXENAAAAAAAArBkKcAbLqacmO3cWBwAAAAAAALCmKMAZLBMTycaNyaFDZScBAAAAAAAAesw7wBksl1ySTE8nd9yRzM0l3a6t0AEAAAAAAGCNsAKcwTE7m7znPcn4eHL22cnYWDI1VdwHAAAAAAAABp4V4AyGTqcouycnD99rtw9fNxpWggMAAAAAAMCAswKcwTAykrRaC89arWIOAAAAAAAADDQFOIOh3S6OxWYzMz0MAwAAAAAAAJRBAc5gqNWKY7HZ6GgPwwAAAAAAAABlUIAzGLrdpF5feFavF3MAAAAAAABgoA2XHQCWRbWaNJvFeatVbHteqxXld7OZVCplpgMAAAAAAAB6QAHO4KhUkkYjeeMbk9tuSzZtSubnld8AAAAAAACwRtgCncFSrSYnnJD8zM8kmzcnP/hB2YkAAAAAAACAHlGAM3iGh5NOJ7nzzmTXrrLTAAAAAAAAAD2iAGcwTUwUn7t3l5sDAAAAAAAA6BkFOINpYiLZuDG5++6ykwAAAAAAAAA9ogBnMP3GbyTT08nP/mwyN1dsiQ4AAAAAAAAMNAU4g2d2NvnYx5Lx8eTxj0/GxpKpqeI+AAAAAAAAMLCGyw4Ay6rTKcru7dsP32u3k8nJ4rzRSKrVUqIBAAAAAAAAK8sKcAbLyEjSai08a7WKOQAAAAAAADCQFOAMlna7OBabzcz0MAwAAAAAAADQSwpwBkutVhyLzUZHexgGAAAAAAAA6CUFOIOl203q9YVn9XoxBwAAAAAAAAbScNkBYFlVq0mzWZy3WsW257VaUX43m0mlUmY6AAAAAAAAYAUpwBk8lUrSaBSF9549yamnJvPzym8AAAAAAAAYcLZAZzBVq8lNNyVbtyY/9mPFNQAAAAAAADDQrABncD3+8cnOncX53XcnJ51Ubh4AAAAAAABgRVkBzuCq1ZLHPrY437271CgAAAAAAADAyrMCnME2MZEMDSV33FF2EgAAAAAAAGCF9dUK8L//+7/Pr/zKr+TpT396nvrUp+Y5z3lO3vrWt2bfvn1HfO9zn/tcXvSiF2XLli35yZ/8yfzFX/xFSYnpe61WMj2dPOlJydxc0umUnQgAAAAAAABYIX1VgLfb7VxwwQV5y1vekmuvvTb/5b/8l3ziE5/Ib/7mb97/neuuuy6XXXZZLrzwwlxzzTV5/vOfnze96U35zGc+U2Jy+tLsbPKpTyXj48UxNpZMTRX3AQAAAAAAgIHTV1ugv/jFLz7i+uKLL8769evz5je/OXv37s3Y2FiuvvrqXHDBBZmcnEySPP3pT88tt9ySVquVn/qpnyojNv2o0ynK7u3bD99rt5Mf/nuTRiOpVkuJBgAAAAAAAKyMvloBvpBarZYk6Xa7mZubyz//8z8/pOh+wQtekN27d+c73/lOCQnpSyMjxfbnC2m1ijkAAAAAAAAwUPpqBfh9Dh48mAMHDmTXrl256qqr8hM/8RMZHx/Prl270u12c8455xzx/YmJiSTJzTffnPHx8eP6zfn5+dxzzz1Lzs6R9u/ff8RnLwwNDaWyb1+G2u2Fv9BuZ77dzuyjH535+fme5QJYKWU8awHWGs9agN7wvAVYeZ61ACvPs3b5zc/PZ2ho6Ki+25cF+H/6T/8pe/fuTZI861nPyh/90R8lSWZmZpIkJ5988hHfv+/6vvnx6Ha7+frXv37cf56HNz093bPfGh4ezpbzz89QrVZse/5gtVrmR0fzjRtvzIEDB3qWC2Cl9fJZC7BWedYC9IbnLcDK86wFWHmetctr/fr1R/W9vizA3/Oe92T//v3ZtWtXrr766rz61a/O+9///hX9zZGRkZx77rkr+htr0f79+zM9PZ3Nmzdnw4YNPfvd+bm5zNfrGbrvnd8PnNXrmb/33jzhCU/oWR6AlVTWsxZgLfGsBegNz1uAledZC7DyPGuX365du476u31ZgJ9//vlJkosuuihbtmzJi1/84vzN3/zN/QX1vn37jvj+XXfdlSQZHR097t8cGhrKiSeeeNx/noe3YcOG3v/9NpvFZ6tVrASv1ZJ6PUPNZk6oVOJ/bWDQlPKsBVhjPGsBesPzFmDledYCrDzP2uVztNufJ8m6FcyxLM4777yMjIzk3/7t33LWWWdlZGQkN9988xHfue/6we8GZ42rVJJGI7n11uRb3yo+G43iPgAAAAAAADBw+r4A/+pXv5put5vx8fGsX78+F198cT772c8e8Z0dO3ZkYmIi4+PjJaWkb1Wryf/z/yRbtya/9mvFNQAAAAAAADCQ+moL9MsuuyxPfepTc95556VSqeTGG2/Mtddem/POOy+XXHJJkuQ1r3lNXvayl+X3fu/38vznPz///M//nL/+67/OO97xjpLT07fGx5OdO5ODB8tOAgAAAAAAAKygvirAL7jgguzYsSPvec97Mj8/nzPOOCOXXnppXv7yl2f9+vVJkn/37/5drrzyyrzzne/Mn//5n+dxj3tcrrjiijz/+c8vOT19a2Ki+LzrruTQoWRd3298AAAAAAAAAByHvirAX/nKV+aVr3zlI37vOc95Tp7znOf0IBED4cwzk09+MnnOc5I77kge85ik27UdOgAAAAAAAAwYS2EZfAcOJP/7fxdboZ92WjI2lkxNJbOzZScDAAAAAAAAllFfrQCHZdfpFGX3FVccvtduJ5OTxXmjYSU4AAAAAAAADAgrwBlsIyNJq7XwrNUq5gAAAAAAAMBAUIAz2Nrt4lhsNjPTwzAAAAAAAADASlKAM9hqteJYbDY62sMwAAAAAAAAwEpSgDPYut2kXl94Vq8XcwAAAAAAAGAgDJcdAFZUtZo0m8V5q1Vse16rFeV3s5lUKmWmAwAAAAAAAJaRApzBV6kkjUZx3HFHcvrpyYEDym8AAAAAAAAYMLZAZ22oVpNf/MVk69bkr/+6uAYAAAAAAAAGigKctWPjxmTnzuRf/7XsJAAAAAAAAMAKUICzdkxMFJ+7d5ebAwAAAAAAAFgRCnDWjomJYhX4/HzZSQAAAAAAAIAVMFx2AOiZ//Afkunp5I47krm5pNv1LnAAAAAAAAAYIFaAszbMzib/7b8l4+PJ2WcnY2PJ1FRxHwAAAAAAABgIVoAz+DqdouyenDx8r90+fN1oWAkOAAAAAAAAA8AKcAbfyEjSai08a7WKOQAAAAAAALDqKcAZfO12cSw2m5npYRgAAAAAAABgpSjAGXy1WnEsNhsd7WEYAAAAAAAAYKUowBl83W5Sry88q9eLOQAAAAAAALDqDZcdAFZctZo0m8V5q1Vse16rFeV3s5lUKmWmAwAAAAAAAJaJApy1oVJJGo3kjW9Mbrst2bQpmZ9XfgMAAAAAAMAAsQU6a0e1moyMJC95SbJ5c3L77WUnAgAAAAAAAJaRApy1Zd26ZP/+5M47k927y04DAAAAAAAALCMFOGvPxESycWPygx+UnQQAAAAAAABYRt4BztozOZk88YlFAT43l3S7xfboAAAAAAAAwKpmBThry+xs8pd/mYyPJ2eemYyNJVNTxX0AAAAAAABgVbMCnLWj0ynK7u3bD99rt4sV4UnSaFgJDgAAAAAAAKuYFeCsHSMjSau18KzVKuYAAAAAAADAqqUAZ+1ot4tjsdnMTA/DAAAAAAAAAMtNAc7aUasVx2Kz0dEehgEAAAAAAACWmwKctaPbTer1hWf1ejEHAAAAAAAAVq3hsgNAz1SrSbNZnLdaxbbntVpRfjebSaVSZjoAAAAAAABgiRTgrC2VStJoJNu2JXv3JqeemszPK78BAAAAAABgANgCnbWnWk3+5m+SrVuTl7ykuAYAAAAAAABWPSvAWZvOPDPZubNYBQ4AAAAAAAAMBCvAWZsmJorPO+5I9u0rNwsAAAAAAACwLBTgrE0nn5xs2pRs3Jh897tlpwEAAAAAAACWgQKctevP/iyZni7K8Lm5pNMpOxEAAAAAAACwBApw1qbZ2eRzn0vGx5MzzkjGxpKpqeI+AAAAAAAAsCoNlx0Aeq7TKcru7dsP32u3k8nJ4rzRSKrVUqIBAAAAAAAAx88KcNaekZGk1Vp41moVcwAAAAAAAGDVUYCz9rTbxbHYbGamh2EAAAAAAACA5aIAZ+2p1YpjsdnoaA/DAAAAAAAAAMtFAc7a0+0m9frCs3q9mAMAAAAAAACrznDZAaDnqtWk2SzOW61i2/NarSi/m82kUikzHQAAAAAAAHCcFOCsTZVK0mgkl1+e3H57ctppycGDym8AAAAAAABYxWyBztpVrSZTU8nWrcn27cU1AAAAAAAAsGpZAc7adtppyc6dyebNZScBAAAAAAAAlsgKcNa2iYni8/vfLzcHAAAAAAAAsGQKcNa2889PPvGJ5H/8j+Jd4HNzSadTdioAAAAAAADgOCjAWds2bkyuuy4ZH0/GxopjaiqZnS07GQAAAAAAAHCMvAOctavTKcruK644fK/dTiYni/NGI6lWS4kGAAAAAAAAHDsrwFm7RkaSVmvhWatVzAEAAAAAAIBVQwHO2tVuF8dis5mZHoYBAAAAAAAAlkoBztpVqxXHYrPR0R6GAQAAAAAAAJZKAc7a1e0m9frCs3q9mAMAAAAAAACrxnDZAaA01WrSbBbnrVax7XmtVpTfzWZSqZSZDgAAAAAAADhGCnDWtkolecMbkkYjueOO5PTTkwMHlN8AAAAAAACwCtkCHU4+Obn44mTr1uRf/7VYGQ4AAAAAAACsOgpwSJLHPCbZuTO56aaykwAAAAAAAADHSQEOSTIxUXzu3l1uDgAAAAAAAOC4KcAhKQrwjRuT2dmykwAAAAAAAADHSQEOSfJLv5RMTye/9mvJ3FzS6ZSdCAAAAAAAADhGCnCYnU0+9KFkfDzZvDkZG0umpqwGBwAAAAAAgFVmuOwAUKpOpyi7JycP32u3D183Gkm1Wko0AAAAAAAA4NhYAc7aNjKStFoLz1qtYg4AAAAAAACsCgpw1rZ2uzgWm83M9DAMAAAAAAAAsBQKcNa2Wq04FpuNjvYwDAAAAAAAALAUCnDWtm43qdcXntXrxRwAAAAAAABYFYbLDgClqlaTZrM4b7WKbc9rtaL8bjaTSqXMdAAAAAAAAMAxUIBDpZI0GkXhvWdPsmnT4fsAAAAAAADAqmELdEiKleB33pls3Zo84QnKbwAAAAAAAFiFFOBwn9NPT77xjeS225Jbbik7DQAAAAAAAHCMFOBwnxNOSM4+O9m4sdgKHQAAAAAAAFhVFODwQNdem0xPJ2edlczNJZ1O2YkAAAAAAACAo6QAh/vMziaf/WwyPp6ccUYyNpZMTRX3AQAAAAAAgL43XHYA6AudTlF2b99++F67nUxOFueNRlKtlhINAAAAAAAAODpWgEOSjIwkrdbCs1armAMAAAAAAAB9TQEOSbHau91efDYz08MwAAAAAAAAwPFQgEOS1GrFsdhsdLSHYQAAAAAAAIDjoQCHJOl2k3p94Vm9XswBAAAAAACAvjZcdgDoC9Vq0mwW561Wse15rVaU381mUqmUmQ4AAAAAAAA4CgpwuE+lkjQaybZtyd69ydhYcuiQ8hsAAAAAAABWCVugwwNVq8l//+/J1q3Jb/92cQ0AAAAAAACsClaAw4OdcUayc2dy8sllJwEAAAAAAACOgRXg8GATE8Xnrl3l5gAAAAAAAACOiQIcHuy+AvzQoaTTKTcLAAAAAAAAcNQU4PBgo6PJpz6VTE8n+/Ylc3OKcAAAAAAAAFgFFODwYLOzyRe/mIyPJ6efnoyNJVNTxX0AAAAAAACgbw2XHQD6SqdTlN1XXHH4XrudTE4W541GUq2WEg0AAAAAAAB4eFaAwwONjCSt1sKzVquYAwAAAAAAAH1JAQ4P1G4Xx2KzmZkehgEAAAAAAACOhQIcHqhWK47FZqOjPQwDAAAAAAAAHAsFODxQt5vU6wvP6vViDgAAAAAAAPSl4bIDQF+pVpNmszhvtYptz2u1ovxuNpNKpcx0AAAAAAAAwMNQgMODVSpJo1Ecd9yRnHZacvCg8hsAAAAAAAD6nC3QYSHVavKa1yRbtyYf+UhxDQAAAAAAAPQ1BTgsZtOmZOfO5IYbyk4CAAAAAAAAHAUFOCxmYqL43L273BwAAAAAAADAUVGAw2ImJpKNG5N1/jMBAAAAAACA1WC47ADQt572tGR6Orn99mRuLul2vQscAAAAAAAA+pilrbCQ2dnk3e9OxseTc85JxsaSqaniPgAAAAAAANCXrACHB+t0irJ7cvLwvXb78HWjYSU4AAAAAAAA9CErwOHBRkaSVmvhWatVzAEAAAAAAIC+owCHB2u3i2Ox2cxMD8MAAAAAAAAAR0sBDg9WqxXHYrPR0R6GAQAAAAAAAI6WAhwerNtN6vWFZ/V6MQcAAAAAAAD6znDZAaDvVKtJs1mct1rFtue1WlF+N5tJpVJmOgAAAAAAAGARCnBYSKWSNBrJm96U3HprsmlTcuCA8hsAAAAAAAD6mC3QYTHVarJ+ffKrv5ps3pzs2lV2IgAAAAAAAOBhKMDhkXS7yZ13Jrt3l50EAAAAAAAAeBgKcHgkExPFpwIcAAAAAAAA+poCHB7JxESycWMyO1t2EgAAAAAAAOBhKMDhkfzKryTT08nLX57MzSWdTtmJAAAAAAAAgAUowOHhzM4mH/pQMj6ePP7xydhYMjVlNTgAAAAAAAD0oeGyA0Df6nSKsnty8vC9dvvwdaORVKulRAMAAAAAAAAeygpwWMzISNJqLTxrtYo5AAAAAAAA0DcU4LCYdrs4FpvNzPQwDAAAAAAAAPBIFOCwmFqtOBabjY72MAwAAAAAAADwSBTgsJhuN6nXF57V68UcAAAAAAAA6BvDZQeAvlWtJs1mcd5qFdue12pF+d1sJpVKmekAAAAAAACAB1GAw8OpVJJGoyi89+xJNm06fB8AAAAAAADoK7ZAh0dSrSa33ZZs3Zqcf35y4ollJwIAAAAAAAAWoACHozE+nnz968l3vlOU4QAAAAAAAEDf6ast0D/96U/nr/7qr3LDDTfkrrvuyuMf//i89KUvzUte8pIMDQ0lSV760pfmS1/60kP+7I4dOzIxMdHryKwVIyPJWWcl3/pWcvPNyeMeV3YiAAAAAAAA4EH6qgD/wAc+kDPOOCPbtm3LYx7zmPyv//W/8uY3vzl79uzJZZdddv/3nva0p+Xyyy8/4s+Oj4/3Oi5rzTnnJPv2Jd//ftlJAAAAAAAAgAX0VQF+9dVX55RTTrn/+hnPeEba7Xbe//7359d//dezbl2xY/vJJ5+cCy+8sKSUrFlve1vypCclP/hBMjeXdLvF+8EBAAAAAACAvtBX7wB/YPl9nyc96Um5++67c88995SQCH5odjb5q78q3gV+5pnJ2FgyNVXcBwAAAAAAAPpCX60AX8iXv/zljI2N5aSTTrr/3pe+9KVceOGFOXjwYH7kR34kv/mbv5kf+7EfW9LvzM/PK9lXwP79+4/4XI0edfBg1r397Rnavv3wzXY7mZzMfJJDb3hD7h3u+/+UgAE2CM9agH7nWQvQG563ACvPsxZg5XnWLr/5+fkMDQ0d1XeH5ufn51c4z3G77rrr8tKXvjSXX355fvVXfzVJ0mq18rjHPS6bN2/O7bffnmuvvTY33XRTPvzhD+eiiy46rt+5/vrrMzc3t4zJGRTDw8PZcv75WXfaaUXp/WC1Wg7t2ZPrb7wxBw4c6Hk+AAAAAAAAWAvWr1+fLVu2POL3+rYA37NnTy699NJMTEzkfe973/3v/36we+65Jz/90z+diYmJXHPNNcf1W9dff33m5+dz7rnnLiUyC9i/f3+mp6ezefPmbNiwoew4x2xoaCiVffsyNDa26Hfm9+7N7KMfnT79TwlYA1b7sxZgNfCsBegNz1uAledZC7DyPGuX365duzI0NHRUBXhf7tt811135RWveEVqtVquvPLKRcvvJDnxxBPzH/7Df8hnP/vZJf3m0NBQTjzxxCX9M1jchg0bVu/f7wknJLXaoivAh2q1bFi/vtepAB5iVT9rAVYJz1qA3vC8BVh5nrUAK8+zdvkc7fbnSbJ4s1yS2dnZvOpVr8q+ffvy3ve+N49+9KPLjsRa1+0m9frCs3q9mAMAAAAAAACl66sV4AcOHMjrXve63HzzzfnIRz6SsYfZdvo+99xzT/7u7/7uqJa7w3GpVpNmszhvtYqV4LVaUX43m0mlUmY6AAAAAAAA4If6qgB/y1veks9//vPZtm1b7r777nzlK1+5f/bkJz85X/va1/Le9743z33uc3PGGWfk9ttvz/vf//7ccccdede73lVecAZfpZI0Gsm2bcnevcnYWHLokPIbAAAAAAAA+khfFeBf+MIXkiRve9vbHjL727/922zatCndbjfveMc70m63s2HDhlx00UV5y1vekgsuuKDXcVlrqtXkT/4k+YM/SJ7ylORP/7TsRAAAAAAAAMAD9FUB/rnPfe4Rv3Pttdf2IAks4owzkp07k7m5spMAAAAAAAAAD7Ku7ACwqpxzTvE5PZ0cPFhqFAAAAAAAAOBICnA4FuPjychIcvLJyR13lJ0GAAAAAAAAeAAFOByLE05IPvWpYgX4/HyxFXqnU3YqAAAAAAAAIApwODazs8k//mOxEvxxj0vGxpKpqeI+AAAAAAAAUKrhsgPAqtHpFGX39u2H77XbyeRkcd5oJNVqKdEAAAAAAAAAK8Dh6I2MJK3WwrNWq5gDAAAAAAAApVGAw9Fqt4tjsdnMTA/DAAAAAAAAAA+mAIejVasVx2Kz0dEehgEAAAAAAAAeTAEOR6vbTer1hWf1ejEHAAAAAAAASjNcdgBYNarVpNkszlutYtvzWq0ov5vNpFIpMx0AAAAAAACseQpwOBaVStJoJJdfntx+e3LaacnBg8pvAAAAAAAA6AO2QIdjVa0mv/VbydatyXvfW1wDAAAAAAAApVOAw/E49dRk587iAAAAAAAAAPqCAhyOxznnFJ8331xuDgAAAAAAAOB+CnA4Hueck2zcmAwPl50EAAAAAAAA+CHtHRyPpz41mZ5Obr89mZtLul3vAgcAAAAAAICSWQEOx2p2NnnXu5Lx8WIl+NhYMjVV3AcAAAAAAABKYwU4HItOpyi7JycP32u3D183GlaCAwAAAAAAQEmsAIdjMTKStFoLz1qtYg4AAAAAAACUQgEOx6LdLo7FZjMzPQwDAAAAAAAAPJACHI5FrVYci81GR3sYBgAAAAAAAHggBTgci243qdcXntXrxRwAAAAAAAAoxXDZAWBVqVaTZrM4b7WKbc9rtaL8bjaTSqXMdAAAAAAAALCmKcDhWFUqSaORvOlNya23Jps2FSu/ld8AAAAAAABQKlugw/GoVpP165P/6/9KNm9Odu0qOxEAAAAAAACseQpwWIpuN7nzzmT37rKTAAAAAAAAwJqnAIelOOec4vPmm8vNAQAAAAAAACjAYUkmJpKNG5PZ2bKTAAAAAAAAwJqnAIel+KVfSqank5e/PJmbSzqdshMBAAAAAADAmqUAh+M1O5t8+MPJ+Hjy+McnY2PJ1JTV4AAAAAAAAFCS4bIDwKrU6RRl9+Tk4Xvt9uHrRiOpVkuJBgAAAAAAAGuVFeBwPEZGklZr4VmrVcwBAAAAAACAnlKAw/Fot4tjsdnMTA/DAAAAAAAAAIkCHI5PrVYci81GR3sYBgAAAAAAAEgU4HB8ut2kXl94Vq8XcwAAAAAAAKCnhssOAKtStZo0m8V5q1Vse16rFeV3s5lUKmWmAwAAAAAAgDVJAQ7Hq1JJGo2i8N6zJzn11GR+XvkNAAAAAAAAJbEFOixFtZp85zvJ1q3Jk5+cnHhi2YkAAAAAAABgzbICHJbqzDOTG24oVn/fcUexEhwAAAAAAADoOSvAYake9ahkfLw4v/nmcrMAAAAAAADAGmYFOCyHiYlk//7ke98rOwkAAAAAAACsWQpwWA5TU8U7wNvtZG4u6XaL94MDAAAAAAAAPWMLdFiq2dnk//1/i23Qx8eTsbGiEJ+dLTsZAAAAAAAArClWgMNSdDpF2b19++F77XYyOVmcNxpWggMAAAAAAECPWAEOSzEykrRaC89arWIOAAAAAAAA9IQCHJai3S6OxWYzMz0MAwAAAAAAAGubAhyWolYrjsVmo6M9DAMAAAAAAABrmwIclqLbTer1hWf1ejEHAAAAAAAAemK47ACwqlWrSbNZnLdaxbbntVpRfjebSaVSZjoAAAAAAABYUxTgsFSVStJoJNu2JXv3JmNjyaFDym8AAAAAAADoMVugw3KoVpM/+7Nk69bk136tuAYAAAAAAAB6ygpwWC6Pe1yyc2ex+hsAAAAAAADoOSvAYbmcc07xefPNyfx8uVkAAAAAAABgDVKAw3I566zkhBOSk05K7rij7DQAAAAAAACw5ijAYbmMjCQ7diTT08nBg8ncXNLplJ0KAAAAAAAA1gwFOCyX2dnkn/4pGR8v3gc+NpZMTRX3AQAAAAAAgBU3XHYAGAidTlF2b99++F67nUxOFueNRlKtlhINAAAAAAAA1gorwGE5jIwkrdbCs1armAMAAAAAAAArSgEOy6HdLo7FZjMzPQwDAAAAAAAAa5MCHJZDrVYci81GR3sYBgAAAAAAANYmBTgsh243qdcXntXrxRwAAAAAAABYUcNlB4CBUK0mzWZx3moV257XakX53WwmlUqZ6QAAAAAAAGBNUIDDcqlUkkYjufzy5Pbbk9NOSw4eVH4DAAAAAABAj9gCHZZTtZq84Q3J1q3Je95TXAMAAAAAAAA9oQCH5XbqqcnOncUBAAAAAAAA9IwCHJbbxESycWNy6FDZSQAAAAAAAGBN8Q5wWG6XXJJMTyd33JHMzSXdrq3QAQAAAAAAoAesAIflNDub/Pf/noyPJ2efnYyNJVNTxX0AAAAAAABgRVkBDsul0ynK7snJw/fa7cPXjYaV4AAAAAAAALCCrACH5TIykrRaC89arWIOAAAAAAAArBgFOCyXdrs4FpvNzPQwDAAAAAAAAKw9CnBYLrVacSw2Gx3tYRgAAAAAAABYexTgsFy63aReX3hWrxdzAAAAAAAAYMUMlx0ABka1mjSbxXmrVWx7XqsV5XezmVQqZaYDAAAAAACAgacAh+VUqSSNRvLGNya33ZZs2pTMzyu/AQAAAAAAoAdsgQ7LrVpNTjgh+dmfTTZvTr7//bITAQAAAAAAwJqgAIeVMDyc3H13cuedye7dZacBAAAAAACANUEBDitlYqL4VIADAAAAAABATyjAYaVMTCQbNxYrwQEAAAAAAIAVpwCHlfIbv5FMTxfvAp+bSzqdshMBAAAAAADAQFOAw0qYnU0+9rFkfDx5/OOTsbFkaqq4DwAAAAAAAKyI4bIDwMDpdIqye/v2w/fa7WRysjhvNJJqtZRoAAAAAAAAMMisAIflNjKStFoLz1qtYg4AAAAAAAAsOwU4LLd2uzgWm83M9DAMAAAAAAAArB0KcFhutVpxLDYbHe1hGAAAAAAAAFg7FOCw3LrdpF5feFavF3MAAAAAAABg2Q2XHQAGTrWaNJvFeatVbHteqxXld7OZVCplpgMAAAAAAICBpQCHlVCpJI1GUXjv2ZNs2nT4PgAAAAAAALAibIEOK6VaTW67Ldm6NTnvvGTDhrITAQAAAAAAwEBTgMNKOuus5KabknvvLVaCAwAAAAAAACtGAQ4r6YQTkh07kunpZGgomZtLOp2yUwEAAAAAAMBAUoDDSpqdTf7hH5Lx8eRxj0vGxpKpqeI+AAAAAAAAsKyGyw4AA6vTKcru7dsP32u3k8nJ4rzRKN4TDgAAAAAAACwLK8BhpYyMJK3WwrNWq5gDAAAAAAAAy0YBDiul3S6OxWYzMz0MAwAAAAAAAINPAQ4rpVYrjsVmo6M9DAMAAAAAAACDTwEOK6XbTer1hWf1ejEHAAAAAAAAls1w2QFgYFWrSbNZnLdaxbbntVpRfjebSaVSZjoAAAAAAAAYOApwWEmVStJoJJdfntx+ezI2lhw6pPwGAAAAAACAFWALdFhp1Wpy9dXJ1q1FEV6tlp0IAAAAAAAABpIV4NAL4+PJzp3JySeXnQQAAAAAAAAGlhXg0AvnnVd8fu975eYAAAAAAACAAaYAh154whOST3wi+fKXk717k7m5pNMpOxUAAAAAAAAMFAU49MIJJyTXXVdshX7aacnYWDI1lczOlp0MAAAAAAAABoZ3gMNK63SKsvuKKw7fa7eTycnivNFIqtVSogEAAAAAAMAgsQIcVtrISNJqLTxrtYo5AAAAAAAAsGQKcFhp7XZxLDabmelhGAAAAAAAABhcCnBYabVacSw2Gx3tYRgAAAAAAAAYXApwWGndblKvLzyr14s5AAAAAAAAsGTDZQeAgVetJs1mcd5qFdue12pF+d1sJpVKmekAAAAAAABgYCjAoRcqlaTRSC6/PLnzzuTUU5ODB5XfAAAAAAAAsIxsgQ69Uq0m//ZvyWMfm9xxRzIyknQ6ZacCAAAAAACAgaEAh16ZnU0++tFkfDw566xkbCyZmiruAwAAAAAAAEtmC3TohU6nKLu3bz98r91OJieL80ajWCEOAAAAAAAAHDcrwKEXRkaSVmvhWatVzAEAAAAAAIAlUYBDL7TbxbHYbGamh2EAAAAAAABgMCnAoRdqteJYbDY62sMwAAAAAAAAMJgU4NAL3W5Sry88q9eLOQAAAAAAALAkw2UHgDWhWk2azeK81Sq2Pa/VivK72UwqlTLTAQAAAAAAwEBQgEOvVCpJo5G88Y3JbbclmzYlhw4pvwEAAAAAAGCZ2AIdeqlaTdavT37+55PNm5Nbbik7EQAAAAAAAAwMBTj02tBQcvBgcuedyTe+UXYaAAAAAAAAGBgKcCjDE5+YbNyYfO97ZScBAAAAAACAgaEAhzI0m8n0dPK85yVzc0mnU3YiAAAAAAAAWPX6qgD/9Kc/nde85jV59rOfnQsvvDAvfvGL8+d//ueZn58/4nsf//jH85M/+ZPZsmVLXvSiF+Xzn/98SYnhOMzOJh//eDI+npx1VjI2lkxNFfcBAAAAAACA49ZXBfgHPvCBbNiwIdu2bcvVV1+dZz/72Xnzm9+cq6666v7vfOpTn8qb3/zmPP/5z88111yTCy+8MJdddlm+8pWvlBccjlank7z1rcn27Um7Xdxrt5PJyeK+leAAAAAAAABw3IbLDvBAV199dU455ZT7r5/xjGek3W7n/e9/f379138969atS6vVygtf+MK87nWvS5I8/elPzze+8Y1cddVVueaaa0pKDkdpZCRptRaetVrJm97U2zwAAAAAAAAwQPpqBfgDy+/7POlJT8rdd9+de+65J7fcckump6fz/Oc//4jvvOAFL8gXv/jFzM3N9SoqHJ92+/DK74VmMzM9DAMAAAAAAACDpa9WgC/ky1/+csbGxnLSSSfly1/+cpLk7LPPPuI7ExMT6Xa7ueWWWzIxMXFcvzM/P5977rlnyXk50v79+4/4XOs21GoZqtUWLsFrtcyPjma/fw+BY+RZC7DyPGsBesPzFmDledYCrDzP2uU3Pz+foaGho/puXxfg1113XXbs2JHLL788STLzw9WxJ5988hHfu+96ZgmrZ7vdbr7+9a8f95/n4U1PT5cdoS884XGPy6Pr9QxNTj5kNl+v567vfS+7br21hGTAIPCsBVh5nrUAveF5C7DyPGsBVp5n7fJav379UX2vbwvwPXv25PWvf30uvvjivOxlL1vx3xsZGcm555674r+z1uzfvz/T09PZvHlzNmzYUHac0g0NDSXbtmU+yVCrVawEr9UyX68n27ZlfZInjY6WnBJYbTxrAVaeZy1Ab3jeAqw8z1qAledZu/x27dp11N/tywL8rrvuyite8YrUarVceeWVWbeueFX56A+LwX379mXTpk1HfP+B8+MxNDSUE088cQmpeTgbNmzw9/tAjUbSbCZ79iSbNmUoSTZsiEcgsBSetQArz7MWoDc8bwFWnmctwMrzrF0+R7v9eZKsW8Ecx2V2djavetWrsm/fvrz3ve/Nox/96Ptn55xzTpLk5ptvPuLP3HzzzRkZGcmZZ57Z06xw3KrV5NZbk61bk/PPT/y/fwAAAAAAAGDJ+qoAP3DgQF73utfl5ptvznvf+96MjY0dMT/zzDOzefPmfOYznzni/o4dO/KMZzzjqPd9h75w1lnJTTcls7PFSnAAAAAAAABgSfpqC/S3vOUt+fznP59t27bl7rvvzle+8pX7Z09+8pOzfv36vPa1r81v//Zv56yzzsrFF1+cHTt25Gtf+1r+5E/+pLzgcDyGh5NPfSp55jOTu+5K5uaSbrdYHQ4AAAAAAAAcs74qwL/whS8kSd72trc9ZPa3f/u3GR8fz0//9E9n//79ueaaa/Ke97wnZ599dt797nfnoosu6nVcWJrZ2eQf/zH5uZ9L2u2kVkvq9eLd4JVK2ekAAAAAAABg1emrAvxzn/vcUX3v0ksvzaWXXrrCaWAFdTrJ1FSyffvhe+12MjlZnDcaVoIDAAAAAADAMeqrd4DDmjEykrRaC89arWIOAAAAAAAAHBMFOJSh3S6OxWYzMz0MAwAAAAAAAINBAQ5lqNWKY7HZ6GgPwwAAAAAAAMBgWFIBfuutt+a666474t6NN96YRqOR173udfmf//N/LikcDKxuN6nXF57V68UcAAAAAAAAOCbDS/nDV1xxRe6555584AMfSJLceeedednLXpZut5tqtZrPfvazede73pXnPe95y5EVBke1mjSbxXmrVWx7XqsV5XezmVQqZaYDAAAAAACAVWlJK8C/9rWv5ZnPfOb915/4xCcyOzubT37yk/mHf/iHPOMZz8j73ve+JYeEgVSpJI1Gcuutybe+VXw2GspvAAAAAAAAOE5LKsBnZmby2Mc+9v7rv/u7v8uP/diP5ayzzsq6devy3Oc+NzfffPOSQ8LAqlaTD30o2bo1ec1rimsAAAAAAADguCypAD/llFNy6623JknuuuuufOUrX8mznvWs++cHDx7MgQMHlpYQBt3mzcnOnck3vlF2EgAAAAAAAFjVlvQO8Gc+85n58Ic/nJNOOin//M//nPn5+TznOc+5f75r166cfvrpSw4JA23LluQTn0guuSS5/fbiXeDdrtXgAAAAAAAAcIyWtAL8DW94Q84555z8wR/8Qb7whS+k0WjkzDPPTJLMzc3l05/+dJ7xjGcsS1AYWKecklx3XTI+noyNFcfUVDI7W3YyAAAAAAAAWFWWtAJ848aN+dM//dPs27cvj3rUo7J+/fr7Z4cOHcoHP/jBnHbaaUsOCQOr0ynK7iuuOHyv3U4mJ4vzRsNKcAAAAAAAADhKS1oBfp9HP/rRR5TfSVKpVHL++eenVqstx0/AYBoZSVqthWetVjEHAAAAAAAAjsqSCvAvfvGLee9733vEvT//8z/Pf/yP/zHPfOYz8/u///s5ePDgkgLCQGu3i2Ox2cxMD8MAAAAAAADA6rakAvzKK6/MjTfeeP/1TTfdlP/6X/9rTjnllPz7f//v8+EPfzjXXnvtkkPCwKrVimOx2ehoD8MAAAAAAADA6rakAnz37t156lOfev/1Jz/5yZx00kn5yEc+kne+85259NJL88lPfnLJIWFgdbtJvb7wrF4v5gAAAAAAAMBRWVIBvn///px00kn3X//jP/5jfvzHfzwbNmxIkmzZsiW33nrr0hLCIKtWk2Yz+d3fPbwSvFYrrpvNYg4AAAAAAAAclSUV4Keffnquv/76JMm3v/3tfPOb38yP//iP3z+fmZnJ+vXrl5YQBl2lkjQayXe/m3z728lttxXXlUrZyQAAAAAAAGBVGV7KH966dWuuuuqq7N27N7t27cro6Gie85zn3D+/4YYbsnnz5qVmhMFXrSY33ZSMjyd33JGMjSWdjhXgAAAAAAAAcAyWtAL81a9+dV75yldmz549Of3003PVVVfl5JNPTpK02+186Utfyk/8xE8sS1AYaLOzyUc/WhTgZ51VFOBTU8V9AAAAAAAA4KgsaQX48PBwXv/61+f1r3/9Q2a1Wi1f+MIXlvKPh7Wh0ynK7snJw/fa7cPXjYaV4AAAAAAAAHAUlrQC/IE6nU52796d3bt3p9PpLNc/FgbfyEjSai08a7WKOQAAAAAAAPCIlrQCPEm+9rWv5Q//8A/zf/7P/8mhQ4eSJOvWrcuP/uiP5nd+53eyZcuWJYeEgdZuF8dis5mZZNOmHgYCAAAAAACA1WlJBfhXv/rVvPSlL83IyEj+83/+z5mYmEiS7N69O5/61KfyK7/yK/nwhz+cCy64YFnCwkCq1YpjoRK8VktGR3ubBwAAAAAAAFapJRXg73jHOzI2NpaPfvSj2fSgFaqvfe1r84u/+It5xzvekfe///1LCgkDrdtN6vUj3wF+n3q9mK9f3/tcAAAAAAAAsMos6R3gX/3qV/PzP//zDym/k2Tjxo35uZ/7uXzlK19Zyk/A4KtWk2Yz+d3fLVZ8J8Xn7/5ucb9aLTMdAAAAAAAArBpLWgG+bt26HDx4cNH5oUOHsm7dkjp2WBsqlaTRSN70puTWW4t3fs/NFfcBAAAAAACAo7Kkdvqiiy7KRz7ykXz3u999yOzWW2/NRz/60TztaU9byk/A2lGtFludv+pVyebNyQ03lJ0IAAAAAAAAVpUlrQD/rd/6rfzyL/9ynv/85+e5z31uNm/enCT51re+lb/927/NunXr8oY3vGE5csLaMTRUfO7ZU24OAAAAAAAAWGWWVIA/+clPzsc//vG84x3vyOc+97ns378/SbJhw4Y861nPymWXXZbHPOYxyxIU1oy3vjV54hOTH/yg2Aa92/UecAAAAAAAADgKSyrAk+Tcc8/NVVddlUOHDuX73/9+kuSUU07JunXrcvXVV6fVauXrX//6koPCmjA7m/zlXyZXXpm020mtltTrSbPpfeAAAAAAAADwCJZcgN9n3bp12bhx43L942Dt6XSSqalk+/bD99rtZHKyOG80rAQHAAAAAACAh7Gu7ADAD42MJK3WwrNWq5gDAAAAAAAAi1KAQ79ot4tjsdnMTA/DAAAAAAAAwOqjAId+UasVx2Kz0dEehgEAAAAAAIDV55jfAX7DDTcc9Xdvv/32Y/3Hw9rV7Sb1+uF3fj9QvV7M16/vfS4AAAAAAABYJY65AH/JS16SoaGho/ru/Pz8UX8X1rxqNWk2i/NWq9j2vFYryu9mM6lUykwHAAAAAAAAfe+YC/C3vvWtK5EDSIqSu9EoCu89e5JNmw7fBwAAAAAAAB7WMRfgP/MzP7MSOYD7VKvJt7+dbN1aXP/Lv5SbBwAAAAAAAFaJdWUHABZw5pnJ7/9+8v/9f8mddyZzc0mnU3YqAAAAAAAA6GsKcOhHc3PJl76UjI8np5+ejI0lU1PJ7GzZyQAAAAAAAKBvHfMW6MAK63SKsvuKKw7fa7eTycnivNEotkkHAAAAAAAAjmAFOPSbkZGk1Vp41moVcwAAAAAAAOAhFODQb9rt4lhsNjPTwzAAAAAAAACweijAod/UasWx2Gx0tIdhAAAAAAAAYPVQgEO/6XaTen3hWb1ezAEAAAAAAICHGC47APAg1WrSbBbnrVax7XmtVpTfzWZSqZSZDgAAAAAAAPqWAhz6UaWSNBrJtm3J3r3J2Fhy6JDyGwAAAAAAAB6GLdChX1WryYc/nLzsZcn27cU1AAAAAAAAsCgrwKGfveAFyS//cnLHHcncXPH+b0U4AAAAAAAALMgKcOhXs7PJNdck4+PJ2WcX26BPTRX3AQAAAAAAgIewAhz6UadTlN2Tk4fvtduHrxsNK8EBAAAAAADgQawAh340MpK0WgvPWq1iDgAAAAAAABxBAQ79qN0ujsVmMzM9DAMAAAAAAACrgwIc+lGtVhyLzUZHexgGAAAAAAAAVgcFOPSjbjep1xee1evFHAAAAAAAADjCcNkBgAVUq0mzWZy3WsW25xMTyetfn7z85UmlUmo8AAAAAAAA6EdWgEO/qlSSRiPZuzf53veSf/3X5CUvSdatSzqdstMBAAAAAABA31GAQz+rVpNDh5J3vjMZG0tOP734nJpKZmfLTgcAAAAAAAB9xRbo0M86naLs3r798L12O5mcLM4bjaIkBwAAAAAAAKwAh742MlK8A/yBNm5MnvrU5CMfKeYAAAAAAABAEgU49Ld2uziS5Pzzk098IpmeTv7qr5KvftW7wAEAAAAAAOABFODQz2q14jj//OQf/iG57rpkfDw555zi853v9C5wAAAAAAAA+CHvAId+1u0m9XrytKcVW6FfccXhmXeBAwAAAAAAwBGsAId+Vq0mzWbyvOcl7373wt9ptbwLHAAAAAAAAKIAh/5XqST79h1+F/iDtdvJzEwvEwEAAAAAAEBfUoDDanDfu8AXm42O9jAMAAAAAAAA9CcFOKwG970LfCH1ejEHAAAAAACANW647ADAUbjvXeBJ8c7vdrtY+V2vF/crlTLTAQAAAAAAQF9QgMNqUakkjUaybVuyd29y6qnJ/LzyGwAAAAAAAH7IFuiwmlSryec/n2zdmrzoRcU1AAAAAAAAkMQKcFh9zjkn2bkz2bevWAE+NFR2IgAAAAAAAOgLVoDDajMxkXzyk8kNNyS3357MzSWdTtmpAAAAAAAAoHQKcFhtDh5M/vf/TsbHk9NOS8bGkqmpZHa27GQAAAAAAABQKlugw2rS6RRl9xVXHL7XbieTk8V5o+G94AAAAAAAAKxZVoDDajIykrRaC89arWIOAAAAAAAAa5QCHFaTdrs4FpvNzPQwDAAAAAAAAPQXW6DDalKrFcd9JfjGjcV7wPfsSQ4cSEZHSwwHAPz/7N1/mJ11fSf8d8JMmOQUctBJJtJxxcSFqeLSisVia1QaaVWkl32QSrXbCO3TfdwwwNYdM8Cy18Z5khjBwjBr7YqW/qBQtD4pmypqUAzbBXtl+xTpoxGFBn/UZIjmDDDJMBMyzx/fnZ6EDBggM2dmzut1Xee6z9z3fWY+N1vPevH28/kAAAAAAI2lAxxmk7GxpLs76epKNm9Odu5M7rijHL/61RKCAwAAAAAAQJMSgMNsUqkkV16Z3Hdfsn170tmZLF9ejn/1V8l8/5EGAAAAAACgeRmBDrPNgQPJRz+a9PXVz9Vqybp15X1PTwnKAQAAAAAAoMloF4XZprU16e+f/Fp/f7kOAAAAAAAATUgADrNNrVZez3RtaGgaiwEAAAAAAICZQwAOs021Wl7PdG3x4mksBgAAAAAAAGYOATjMNmNjSXf35Ne6u8t1AAAAAAAAaEItjS4AeI4qlaS3t7zv7y9jz6vVEn739iZtbY2sDgAAAAAAABpGAA6zUVtb0tOTXHVV8s//nCxZkgwPC78BAAAAAABoakagw2xVqSQLFiQf/GByyinJ//yfja4IAAAAAAAAGkoADrNdy/8e5PCjHzW2DgAAAAAAAGgwATjMdmvXJjt3Jueem4yOllHoAAAAAAAA0IQE4DCbjYwkn/500tmZ/Kt/lXR0JJs2lfMAAAAAAADQZFoaXQDwPA0Pl7D7Qx+qn2tpST772WTRomTNmrInHAAAAAAAAJqEABxmq9bWpL+/vO/qSjZuTFatSgYHk6VLk4MHG1sfAAAAAAAATDMj0GG2qtXKq6sr2bYt2b69jEJfvrwcr73WKHQAAAAAAACaig5wmK2q1fLauLF0gvf11a/Vasm6deV9T49R6AAAAAAAADQFHeAwW42NJWvXlrHnAwOT39PfX0alAwAAAAAAQBMQgMNsVakkl1+e7N1bOr4nU6slQ0PTWBQAAAAAAAA0jgAcZrPjj0+WLi2j0CdTrSaLF09nRQAAAAAAANAwAnCY7cbGku7uya91d5frAAAAAAAA0ARaGl0A8AJVKklvb3nf31/GnlerJfzu7U3a2hpZHQAAAAAAAEwbATjMBW1tSU9PCbx37Spj0SfOAwAAAAAAQJMwAh3mikol+d73koceSsbHkyeeSEZHk+HhRlcGAAAAAAAA00IADnNJZ2dy993l2NFRXps2JSMjja4MAAAAAAAAppwR6DBXDA+XsLuvr36uVkvWrSvve3pKlzgAAAAAAADMUTrAYa5obU36+ye/1t9frgMAAAAAAMAcJgCHuaJWK69nujY0NI3FAAAAAAAAwPQzAh3mimq1vCZC8Pb2ZNmyZNeu5MCBZPHiBhYHAAAAAAAAU08HOMwVY2NJd3fS1ZVs3pzs3JnccUc5fvWrJQQHAAAAAACAOUwADnNFpZJceWVy333J9u1JZ2eyfHk5/tVfJfP9xx0AAAAAAIC5zQh0mEsOHEg++tGkr69+rlZL1q0r73t6SlAOAAAAAAAAc5CWUJhLWluT/v7Jr/X3l+sAAAAAAAAwRwnAYS6p1crrma4NDU1jMQAAAAAAADC9BOAwl1Sr5fVM1xYvnsZiAAAAAAAAYHoJwGEuGRtLursnv9bdXa4DAAAAAADAHNXS6AKAY6hSSXp7y/v+/jL2vFot4Xdvb9LW1sjqAAAAAAAAYEoJwGGuaWtLenqSq65KfvjDpL297P4WfgMAAAAAADDHGYEOc1GlkixYkKxfn5x5ZvI//2ejKwIAAAAAAIApJwCHuez/+r+Sv//75M1vTkZHk8HBchwebnRlAAAAAAAAcMwJwGGuGhlJtm0rYff11ycdHfXXpk3lOgAAAAAAAMwhdoDDXDQ8XELu17wm6e9P+vrq12q1ZN268r6np4xLBwAAAAAAgDlABzjMRa2tyS23JKtWJQMDk9/T31/uAwAAAAAAgDlCAA5zUa2WLFxYdn7Xas98z9DQNBYFAAAAAAAAU8sIdJiLqtVk//5k6dLy/tAQvL09WbasXF+8uEEFAgAAAAAAwLGnAxzmorGx5D3vSbZuTdasKee6upLNm5OdO5M77kgeeCAZGWlklQAAAAAAAHBMCcBhLqpUkt7e5LvfTbq7kxtuSLZtS7ZvTzo7k+XLk5NPTq67TggOAAAAAADAnDGjRqA/8sgj+eQnP5n7778/3/72t7N8+fJs2bLlsHt+67d+K3/3d393xGc/97nPZcWKFdNVKsx8bW3JxRcnra3Jb/928tGPJn199eu1WrJuXXnf01NCcwAAAAAAAJjFZlQA/u1vfztf/epXc8YZZ+TgwYMZHx+f9L7XvOY1+eAHP3jYuc7OzukoEWaXQ0Pt/v7J7+nvT666anrqAQAAAAAAgCk0owLwc845J6tWrUqSrF27Nv/4j/846X0nnnhifvZnf3YaK4NZrlYrrwnt7cmyZcmuXcmePcnQULJkSaOqAwAAAAAAgGNiRgXg8+dbSQ5Tolotr2XLko0bk1WrksHBZOnS5CtfSRYvbnSFAAAAAAAA8ILNqAD8aP3d3/1dfvZnfzZPPfVUzjjjjFx22WX5+Z//+Rf0O8fHx7Nv375jVCET9u/ff9iRxjj+qacyf926zHv3u8vI89WrS0d4tZrxSy9NfvmXM7J//zOuHQBmNt+1AFPPdy3A9PB9CzD1fNcCTD3ftcfe+Ph45s2bd1T3zhufoYnXxAj0LVu2HHa+v78/J598ck455ZQMDg7mk5/8ZL71rW/lz/7sz/JzP/dzz+tvPfDAAxkdHT0WZcOM1NbWlle+9KXJtddm3oc+dMT18WuuyWP/7t/lO//8zw2oDgAAAAAAAJ7dggUL8upXv/on3jfrAvCn27dvX84777ysWLEin/jEJ57X33rggQcyPj6eV7ziFc/r8zyz/fv3Z+fOnTnllFOycOHCRpfT1Ba2tGReR8fhu8AnVKsZ3707+w8cmPa6gBfOdy3A1PNdCzA9fN8CTD3ftQBTz3ftsfed73wn8+bNO6oAfFaOQD/UokWL8sY3vjFf+MIXXtDvmTdvXhYtWnSMquLpFi5c6J9vow0OHhl+t7eXveC7dmXe0FAWLVnSkNKAY8N3LcDU810LMD183wJMPd+1AFPPd+2xc7Tjz5Nk/hTWAcwk1Wp5JUlXV7J5c7JzZ3LHHeXY1taw0gAAAAAAAOBYmPUB+L59+3L33XcfVbs7NLWxsaS7u4Tf27Yl27cnnZ3J8uXleO21ychIo6sEAAAAAACA521GjUDfv39/vvrVryZJfvCDH+SJJ57InXfemSQ566yz8vDDD+emm27KW97ylvz0T/90BgcH88d//Md59NFHc8MNNzSydJj5KpWktze54IKkvz/p66tfq9WSdevK+56eci8AAAAAAADMMjMqAP/Rj36Uyy677LBzEz//6Z/+aZYtW5axsbH8wR/8QWq1WhYuXJif+7mfy3/5L/8l/+bf/JtGlAyzS1tbctppycDA4ecndoHfckty1VWNqQ0AAAAAAABeoBkVgHd2duZb3/rWs97zyU9+cpqqgTmqViuvpIxD37gxWbUqGRxMli5NhoeTBQsaWSEAAAAAAAA8L7N+BzjwHFWr5fVMu8Cvv94ucAAAAAAAAGalGdUBDkyDsbGkuzt5zWvsAgcAAAAAAGBO0QEOzaZSSXp7k3PPPXIX+IT+/qS1dXrrAgAAAAAAgBdIAA7NqK0tefzx+i7wp6vVkqGh6awIAAAAAAAAXjABODSriV3gz3Rt8eJpLAYAAAAAAABeOAE4NKuJXeCT6e4u1wEAAAAAAGAWaWl0AUCDTOwCT8rO75aW5NRTk/PPTy67rIxJBwAAAAAAgFlEAA7NrK0t+eAHkw98IFmwoOz9rlaTAwcaXRkAAAAAAAA8Z0agQ7ObPz+59tpk5crk3nvL6PMf/zgZHU2GhxtdHQAAAAAAABw1ATg0s+HhZMOG5Pbbky1bku3bk87O5KUvTTo6kk2bkpGRRlcJAAAAAAAAR8UIdGhmra1l//fNN5djX1/9Wq2WrFtX3vf0lJ3hAAAAAAAAMIPpAIdmVqslLS3JqlXJwMDk9/T3l6AcAAAAAAAAZjgBODSzajU59dRkcLCE4ZOp1ZKhoWksCgAAAAAAAJ4fI9ChmY2NJeefnyxdWsLwQ0Pw9vZk2bJk//5k8eJGVQgAAAAAAABHTQc4NLNKJbnssuThh5M1a8q5rq5k8+Zk587kjjuSBx5IRkYaWSUAAAAAAAAcFQE4NLu2tmTFiuTKK8u+723bku3bk87OZPny5OSTk+uuE4IDAAAAAAAw4xmBDiSLFpXjb/92Cbv7+urXarVk3bryvqendI0DAAAAAADADKQDHKhraytd4E/X3p5s3Zq0tk5/TQAAAAAAAHCUBOBAXa1WXhMO3Qf+53+ejI8nw8ONqQ0AAAAAAAB+AgE4UFetlldSwu+n7wNftizZtMk+cAAAAAAAAGYkAThQNzaWdHeX9xs3lnHofX31rvCJfeAbNugEBwAAAAAAYMZpaXQBwAxSqSS9vcmiRcmqVcnq1ZPf19+fXHXVtJYGAAAAAAAAP4kOcOBwbW3JpZeWDu9D94EfqlZLhoamsyoAAAAAAAD4iQTgwJEWLTp8H/jTVavJ4sXTWBAAAAAAAAD8ZAJw4EjDw8m3vpWsWTP59e7usi8cAAAAAAAAZhA7wIEjtbYmv/M7yZYt5eeBgTL2vFotofjatcnChY2sEAAAAAAAAI6gAxw4Uq2W/N3fJStXJmeemXz/+8k//VM5vulNychIoysEAAAAAACAIwjAgSNN7P/esSN55zuTt7wleeihcm358qRSKWPSAQAAAAAAYAYRgANHGhsre76TpKsr+eu/Tu6+O+nsLAF4R0eyaZNOcAAAAAAAAGYUO8CBI1UqSW9veX/WWUl/f9LXV79eqyXr1pX3PT3lfgAAAAAAAGgwHeDA5NraSrj9lrckAwOT39Pfn7S2Tm9dAAAAAAAA8AwE4MAzq1RKt3etNvn1Wi0ZGprGggAAAAAAAOCZCcCBZ1etltczXVu8eBqLAQAAAAAAgGcmAAee3dhY0t09+bXu7nIdAAAAAAAAZoCWRhcAzHCVStLbW97395ex5ytWJFdckVxySdkVDgAAAAAAADOADnDgJ2trS3p6kt27kz17km98I7nggmT+/GR4uNHVAQAAAAAAQBIBOHC0KpXk4MHkhhuSjo5k2bLkla9Mbr45GRlpdHUAAAAAAAAgAAeO0vBwsmFD8qEPlfB78+bk/vuTt70tGR9PHnus0RUCAAAAAADQ5ATgwNFpbS07wLu6km3bku3bk87OZPny5OSTk+uu0wkOAAAAAABAQ7U0ugBglqjVyuvmm0sQ3td3+LV168r7np4yLh0AAAAAAACmmQ5w4OhUq8mKFcmqVcnAwOT39PeXTnEAAAAAAABoAAE4cHTGxpIrrkgGB0vH99O1t5eR6HaBAwAAAAAA0CACcODoVCrJJZcky5aVbvAJZ5+dbN2a7NyZ3HFHcuKJyfBwo6oEAAAAAACgiQnAgaPX1lY6wbu7k66uEnzfdVeybVvp/l6+POnoSDZtSkZGGl0tAAAAAAAATUYADjw3J56YXHllct99yZIlyfr1ybp19bHoLS3JZz+b3HCDTnAAAAAAAACmlQAceO4OHEg+/vFkxYpkYKCc6+pKNm+uj0JfsyY5eLCRVQIAAAAAANBkBODAc9faWkLuwcHS+d3VVcagb99eH4Xe2Zlce61R6AAAAAAAAEyblkYXAMxCtVry4IPJ0qVJtZps3Jj09yd9fYffs25ded/Tk1QqDSgUAAAAAACAZqIDHHjuqtUyBn3r1hJur1pVH4X+dP39pWMcAAAAAAAAppgOcOC5GxtLuruTtWuTv/3bZO/e0vF9qPb2ZNmyZNeuZGgoWbKkIaUCAAAAAADQPATgwHNXqSS9veX9RRclf/3XpSt8Yh/4xo2lK3xwsIxJP3iwkdUCAAAAAADQJIxAB56ftrYy/vy///dk//7SEd7VlWzblmzfnnR2JmedlZx7bvKxjyUjI42uGAAAAAAAgDlOBzjw/FUq5bhgQekIv+CCsvP7M59Jbr758C7wBx9MXvGKZNGihpYMAAAAAADA3KUDHDg22tqS005LvvjFw7vAly8vx09/Opk3r9FVAgAAAAAAMIfpAAeOnVotufLK0gXe13f4+b6+ZP78MjZ9onMcAAAAAAAAjiEd4MCxU62WsecDA5Nf7+9PWluntSQAAAAAAACahwAcOHYOHEj27i0d35Op1ZKhoemsCAAAAAAAgCZiBDpw7CxalLS0lE7wyULwajVZvHiaiwIAAAAAAKBZ6AAHjq2xsaS7e/Jr3d3lOgAAAAAAAEwBHeDAsVWpJL295X1/f+kEX7EiueKK5JJLkra2hpYHAAAAAADA3KUDHDj22tqSnp5k9+5kz57kG99I/o//I5k/PxkebnR1AAAAAAAAzFECcGBqVCrJwYPJDTckHR3JS15Sjps2JSMjja4OAAAAAACAOcgIdGBqDA+XsPtDH6qfq9WSdevK+56eEpIDAAAAAADAMaIDHJgara1lB/hk+vvLdQAAAAAAADiGBODA1KjVyuuZrg0NTWMxAAAAAAAANAMBODA1qtXymsyKFc98DQAAAAAAAJ4nATgwNcbGku7uw8+dfXaydWvywAPJ3r3J6GjZFQ4AAAAAAADHgAAcmBqVStLbm1xzTXLWWSX4vuuuZNu25OSTk46O8tq0KRkZaXS1AAAAAAAAzAECcGDqtLUlH/xgcvfdydKlyfr1ybp19d3gtVr5ecMGneAAAAAAAAC8YAJwYGqNjyf9/cny5cnAwOT39Pcnra3TWxcAAAAAAABzjgAcmFqtrckddySDg/XO76er1ZKhoemsCgAAAAAAgDlIAA5MrVotefDBMgK9Wp38nmo1Wbx4GosCAAAAAABgLhKAA1OrWk0OHEi2bk3WrJn8nu7uZGxsWssCAAAAAABg7mlpdAHAHDc2VgLutWuTbdvKuYGB0hlerZZrvb1JW1sjqwQAAAAAAGAOEIADU6tSKQF3kpx3Xnn//e8njz2WtLcno6PCbwAAAAAAAI4JI9CBqdfWlvT0JPfck6xcmRw8mJx0UjI4mLS2JsPDja4QAAAAAACAOUAADkyPSiVZsCBZtCj5yEeSl7wk6exMOjqSTZuSkZFGVwgAAAAAAMAsZwQ6MH2Gh0vY/aEP1c+1tCSf/WwJxtesKUE5AAAAAAAAPA8CcGD6tLYm/f3lfVdXsnFjsmpVGYW+dGkZjQ4AAAAAAADPkxHowPSp1cqrqyvZti3Zvr2MQV++vByvvdYodAAAAAAAAJ43HeDA9KlWy2vjxtIJ3tdXv1arJevWlfc9PUahAwAAAAAA8JzpAAemz9hYsnZtGXs+MHD4tfb25PTTk1tuKaPSAQAAAAAA4DkSgAPTp1JJLr882bu3dHwnZRz65s3Jzp3JHXck99+fDA83rkYAAAAAAABmLQE4ML2OPz5ZurSMQn+mXeDXX28XOAAAAAAAAM+ZHeDA9BsbS7q7k9e8xi5wAAAAAAAAjhkd4MD0q1SS3t7k3HMP3wU+sQe8vb0E43aBAwAAAAAA8BwIwIHGaGtLHn+8dHw/fQ/4zp3JH/9x8sQTja0RAAAAAACAWUUADjROtZqcddaRe8DPOCP5/veThQsbXSEAAAAAAACziAAcaJyxseSmm+p7wJctK53g99+fvP3t5Z7HHmtoiQAAAAAAAMweAnCgcSqV5LTTyh7wrq4jO8FPPjm57rpkZKTRlQIAAAAAADALtDS6AKDJ1WrldfPN9U7wQ6+tW1fe9/SUwBwAAAAAAACegQ5woLGq1WTFimTVqtIJPpn+/qS1dVrLAgAAAAAAYPYRgAONNTaWXHFFMjhYOr4n09KSPPHEtJYFAAAAAADA7CMABxqrUkkuuSRZtqx0gx+qqyvZvDnZuTM5cCAZHU2GhxtQJAAAAAAAALOBABxovLa20gne3V0/19WV3HNPsn170tmZdHSU16ZNychI42oFAAAAAABgxmppdAEASZITT0x6e8v7O+9M/vIvkxtvTPr66vfUasm6deV9T0/pHgcAAAAAAID/TQc4MHO0tSUf/GBy992l27u/f/L7+vuT1tZpLQ0AAAAAAICZTwAOzCzj48mnPpXs2lU6vidTqyVDQ9NZFQAAAAAAALOAAByYWVpbkz/4g2Tp0qRanfyeajVZvHg6qwIAAAAAAGAWEIADM0utljz0ULJ1a7JmzeT3dHcnY2PTWhYAAAAAAAAzX0ujCwA4TLVaXmvXJtu2lXMDAyUYnzh/+eXJ8cc3rEQAAAAAAABmJh3gwMwyNlY6vHfsSFauTM48M/n+95Mf/CD57neTK64o+79HR5Ph4UZXCwAAAAAAwAwiAAdmlkol6e1Nrrkm2bWrvP+Hf0hOOim59tqko6P+2rQpGRlpdMUAAAAAAADMEAJwYOZpa0t6epLdu5O///vkxBOT9euTdevKKPSkHNetSzZs0AkOAAAAAABAEgE4MFNVKmUcen9/snx52QN+qPb25PTTk1tuSVpbG1MjAAAAAAAAM4oAHJi5WluTO+5IBgfrnd9dXcnmzcnOneXa/ffrAAcAAAAAACCJAByYyWq15MEHk6VLk2q1hN/btiXbtyednaUzvLMzuf56u8ABAAAAAABIS6MLAHhG1Wpy4ECydWuyZk3y2teWkeh9ffV7JnaBJ2VveKXSiEoBAAAAAACYAQTgwMw1NpZ0dydr1yb33FPC7dWrJ7+3vz+56qppLQ8AAAAAAICZxQh0YOaqVJLe3uTCC0sQvndvfRf4hPb25B3vSG65JRkfL/vCR0ftBQcAAAAAAGhCAnBgZmtrK6PNb745efGLy1j0pOwD37w5eeSR5Lbbkq99LVm2LOnoKK9Nm+wFBwAAAAAAaDICcGDmq1SSBQvKPvDu7hJ+b9tWwu9HH00+/OGyB3yiO3xiL/iGDTrBAQAAAAAAmogAHJg9Jkai3357cuutyW/+ZrJ0adn/PZn+/qS1dXprBAAAAAAAoGEE4MDs0taWnHZasmJFGX2+a9eRe8En1GrJ0NB0VgcAAAAAAEADCcCB2WdoKHnTm5Lrry8d4BN7wZ+uWk0WL56+ugAAAAAAAGgoATgw+1SryeBg8tBDydatyZo1k9/X3Z2MjU1raQAAAAAAADROS6MLAHjORkeTZctKEL52bbJtWzk/MFDGnq9YkVxxRXLJJWVkOgAAAAAAAE1BBzgw+1QqyYEDyaWXJjt2JCtXJmeemfzgB8mePck3vpFccEEyf34yPNzoagEAAAAAAJgmAnBgdjrhhKS3N7nmmmTXrvJ+377khhuSjo7SId7RkWzalIyMNLpaAAAAAAAApoEAHJi9Fi5MenqS3buT//k/kxtvTD70oTIGPSnHdeuSDRt0ggMAAAAAADQBATgwu1UqyYIF5djff+T19vZk69aktXX6awMAAAAAAGBaCcCBuaFWq3d+J0lXV7J5c7JzZ/Lnf56Mj+sCBwAAAAAAmOME4MDcUK2WV1LC723bku3bk87OZPnyshPcPnAAAAAAAIA5TQAOzA1jY0l3d3m/cWMZh97XZx84AAAAAABAE5lRAfgjjzySa665Jr/2a7+WV77ylTnvvPMmve/Tn/50fuVXfiWvfvWrc/755+crX/nKNFcKzDiVStLbW8Lvt7wlGRiY/L7+fvvAAQAAAAAA5qgZFYB/+9vfzle/+tW87GUvy4oVKya952/+5m/yn/7Tf8pb3/rWfOITn8jP/uzPZs2aNfmHf/iH6S0WmHna2koX+BNPHL4P/FC1WjI0NJ1VAQAAAAAAME1aGl3Aoc4555ysWrUqSbJ27dr84z/+4xH39Pf35+1vf3suv/zyJMkv/MIv5MEHH8x//a//NZ/4xCems1xgJjp4MDnhhLIPfLIQvFpNFi+e5qIAAAAAAACYDjOqA3z+/Gcv53vf+1527tyZt771rYedf9vb3pZ77703o6OjU1keMBu0tiZf/nKyZs3h59vbk9NPT9auLfvCAQAAAAAAmHNmVAf4T/Lwww8nSV7+8pcfdn7FihUZGxvL9773vWccnf6TjI+PZ9++fS+4Rg63f//+w44wlebNm5e2xx/PvA98INm2rZz84heTK69MVq1K9u7N+NKlOfjkkxndvz/j4+ONLRiOEd+1AFPPdy3A9PB9CzD1fNcCTD3ftcfe+Ph45s2bd1T3zqoAfOh/7+098cQTDzs/8fPQC9jrOzY2lm9+85vPvzie1c6dOxtdAk2gpaUlr+7qyrxdu5KVK5Mbbyzh98aNyerVSa2WedVq5nd3Z0FPT3bs3JmRkZFGlw3HjO9agKnnuxZgevi+BZh6vmsBpp7v2mNrwYIFR3XfrArAp1Jra2te8YpXNLqMOWf//v3ZuXNnTjnllCxcuLDR5dAExkdHM97dnXnr1iXDw8n69UlfX/2GWi3z1q3L/CRdv//7ebLF1yCzn+9agKnnuxZgevi+BZh6vmsBpp7v2mPvO9/5zlHfO6uSn8WLFydJHn/88SxZsuRfzj/22GOHXX8+5s2bl0WLFr2wAnlGCxcu9M+X6dPbmyxaVMaer1496S3z+vtz3FVXZdFR/q+FYDbwXQsw9XzXAkwP37cAU893LcDU81177Bzt+PMkmT+FdRxzy5cvT1LfBT7h4YcfTmtra1760pc2oixgpmlrSy69tHSA12qT39PSkjzxxLSWBQAAAAAAwNSaVQH4S1/60pxyyim58847Dzv/uc99LmefffZRz30HmsCiRUm1Wl6HOvvsZOvWZOfO5MCBZHS0BOUAAAAAAADMejMqAN+/f3/uvPPO3HnnnfnBD36QJ5544l9+/vGPf5wkufTSS7Nly5b09/fna1/7Wv7zf/7P+frXv573v//9Da4emHHGxpLu7vK+q6sE33fdlWzblnR2Jh0d5bVpUzIy0thaAQAAAAAAeMFm1A7wH/3oR7nssssOOzfx85/+6Z/mda97Xc4777zs378/n/jEJ/Lf/tt/y8tf/vIMDAzk537u5xpRMjCTVSplH3h7e/Ludyc//GGyfn3S11e/p1ZL1q0r73t6ymcAAAAAAACYlWZUAN7Z2ZlvfetbP/G+d73rXXnXu941DRUBs15bW/Lbv5384R8ma9YkAwOT39ffn1x11fTWBgAAAAAAwDE1o0agA0yJtrbkjjuSwcHS8T2ZWi0ZGprOqgAAAAAAADjGBODA3FerJQ8+mCxdmlSrk99TrSaLF09jUQAAAAAAABxrAnBg7qtWkwMHkq1byxj0p2tvL3vBx8amvTQAAAAAAACOnRm1AxxgSoyNJd3dydq1ybZt5dzAQPIzP5OsX5+87nXJ448nra3J8HBSqTS2XgAAAAAAAJ4XHeDA3FepJL29yYUXJuedl7zxjcn3vpfcdVfyla8kJ5+cdHSU16ZNychIoysGAAAAAADgedABDjSHtrakpye56qrkqaeS73wnuf32Mvp8Qq2WrFtX3vf06AQHAAAAAACYZXSAA82jUinj0Pv7k+XLyxj0Q7W3J6efntxySxmHDgAAAAAAwKwiAAeaS2trcscdyeBg6fhOkq6uZPPmZOfOcu3++8sucAAAAAAAAGYVATjQXGq15MEHk6VLk2q1hN/btiXbtyednaUzvLMzuf56u8ABAAAAAABmGTvAgeZSrSYHDiRbtyZr1iSvfW0Zif70XeAf+1gJyVevtgscAAAAAABglhCAA81lbCzp7k7Wrk3uuaeE26tX16+ffXbyoQ8lv/ALZUx6a2uyb1+yaFHDSgYAAAAAAODoGIEONJdKJentTS68sAThe/eWju+urtIVftddZST6OeckX/96Ccx//ONkdNRecAAAAAAAgBlOAA40n7a2pKcnufnm5MUvTs46q4TeS5Yk69cnt9+ebNlS3wv+0pcmHR3Jpk32ggMAAAAAAMxgAnCgOVUqyYIFZR/4TTeV14oVycBAsnFjfS94rVbur9WSdeuSDRt0ggMAAAAAAMxQAnCguVUqyWmnlY7vwcGkpSVZtaoE4Ydqb09OPz255ZayFxwAAAAAAIAZRwAOUKslDz6YLF2anHpqCcInOr+7upLNm5OdO5M77kjuv18HOAAAAAAAwAwlAAeoVsso9K1bk/POK0F4tVrC723b6rvAly8vx+uvtwscAAAAAABgBmppdAEADTc2lnR3J2vXlsD7oYeSNWuS1762vgt8wsQu8CTp6Skj1AEAAAAAAJgRdIADVCpJb29y4YWlA3xwMLnyyuTcc4/cBT6hv98ucAAAAAAAgBlGAA6QJG1tpaP7nnuSM85IWlqSxx6r7wJ/ulotGRqazgoBAAAAAAD4CQTgABMqlWTBgmTJktLdfdJJZRf4ZKrVZPHi6awOAAAAAACAn0AADvBMJnaDT6a7u1wHAAAAAABgxmhpdAEAM9bEbvCk7PxuaUlOPTX5jd9Ifu/3kuOPb2x9AAAAAAAAHEYADvBs2tqSD34w+cAHSuA9PFyC8VqtjEEfGys/AwAAAAAA0HBGoAP8JPPnJzffnAwNJR/9aNLRUV6vfGU5PzLS6AoBAAAAAACIABzg2Q0PJxs2JP/qX5Ux6H19ybJlyebNyf33J297WzI+njz2WKMrBQAAAAAAaHoCcIBn09qa3HJLsmpVMjCQdHUl27Yl27cnnZ3J8uXJyScn112nExwAAAAAAKDB7AAHeDa1WrJwYTI4WN7ffHO9E/zQe9atK+97euwEBwAAAAAAaBAd4ADPplpN9u9Pli5NVqyod4JPpr+/dIwDAAAAAADQEAJwgGczNpa85z3J1q3J5ZfXO8EP1d6enH560tKSDA01okoAAAAAAAAiAAd4dpVK0tubfPe7ybvfnSxbVrrCk7IPfPPmZOfOZMuW5JFHyrh0AAAAAAAAGkIADvCTtLUlF1+cnHhiMjqadHeX8HvbthJ633136QLfvTs57rjk8ccbXTEAAAAAAEBTEoADHI1KJVmwIFm8uHSE3357cuutyUUXJffdl3R2JsuXJyefnHzkI2VvOAAAAAAAANOqpdEFAMw6bW3JaaeVsej9/UlfX/1arZZ86EPJvHlJT08JzgEAAAAAAJgWOsABno+hoeRNb0oGBg4/396enH568vnPl45xAAAAAAAApo0OcIDno1pNvv/90vGdlJ3gGzcmb3lLsm9f2Rf+xBOlA7xWK/ePjekIBwAAAAAAmEI6wAGej9HRZNmyEmx3dSXbtiWPPJIMDyd/8RelQ/yjH006OpJXvSp585tLt/jISKMrBwAAAAAAmLN0gAM8H5VK8vjjyaWXJmeeWXaBv/a1hx8/85nk5puTVauSwcFk6dLkwQeTV7wiWbSo0U8AAAAAAAAw5+gAB3i+Tjgh6e1Nzj03ufXWEnRPHL/4xdIVvn170tmZLF9ejp/+dDJvXqMrBwAAAAAAmJN0gAO8EAsXlu7upx+vvLJ0gff11e+t1crP8+cnPT32gQMAAAAAABxjOsABXqhqNdm/v4w4nziuWlV2fk+mvz9pbZ3WEgEAAAAAAJqBABzghRobS97znmTr1uSii5KvfS3Zu7d0fE+mVkuGhqazQgAAAAAAgKYgAAd4oSqVsgv8u99NuruThx5KliwpneGTqVaTxYuns0IAAAAAAICmIAAHOBba2pKLLy7B9gUXJKOjJQyfTHd36RoHAAAAAADgmGppdAEAc0alUo4LFpRjb2859veXsecrViRXXJFcckkJzAEAAAAAADimdIADTJW2tqSnJ9m9O9mzJ/nGN0p3+Pz5yfBwo6sDAAAAAACYcwTgAFOpUkkOHkxuuCHp6EiWLSvHT30qefzxMip9cLAcheIAAAAAAAAviAAcYCoNDycbNiQf+lAZg56UEPzd704+8pEShk+8Nm1KRkYaWi4AAAAAAMBsZgc4wFRqbS07wA913XXlXF9f/VytlqxbV9739NT3iQMAAAAAAHDUdIADTKVard753dWVfO5zyapVycDA5Pf395fQHAAAAAAAgOdMAA4wlarV8urqSrZtSx56KPnBD+qh+NPVasnQ0PTVBwAAAAAAMIcYgQ4wlcbGku7u5DWvKd3dH/948r73lVD80BC8vb3sBt+/P1m8uFHVAgAAAAAAzGo6wAGmUqWS9PYm555bxp63tyejo8mll5brXV3J5s3Jzp3JHXckDzyQjIw0smIAAAAAAIBZSwAOMNXa2pLHHy8d3xs3Jn/2ZyUAv+GGMhZ9+/akszNZvjw5+eTkuuuE4AAAAAAAAM+DEegA06FaTVasSFatKmH3H/5h8vnPl67wvr76fbVasm5ded/TUzrIAQAAAAAAOCo6wAGmw9hYcsUVyeBgCbn37EmWLCl7wSfT35+0tk5riQAAAAAAALOdDnCA6VCpJJdckoyPl27wZcvqYfhkarVkaKiE5AAAAAAAABwVHeAA06WtrXSCd3cnu3YlS5eWMHwy1WqyePF0VgcAAAAAADDrCcABptOJJya9vcn735/cfXeyZs3k93V3l7AcAAAAAACAo2YEOsB0a2tLenrKju9zzknmzy87v2u10vnd3V1C8ra2RlcKAAAAAAAwqwjAARqhUqm/7+lJrrwy+eEPy87v+fOTp55KRkfrofjY2OGfAQAAAAAA4AhGoAM0WqWSzJtX9oKPj5dzmzYlHR3l9cpXJjffnIyMNLRMAAAAAACAmU4HOMBMMD6e/MzPJA8/nHz600lfX9LVVYLvVauSwcFyz2OPlT3iAAAAAAAAHEEHOECjDQ8nDz6YfOxjyYoVycBACb+3bUu2b086O5Ply5OTT06uu04nOAAAAAAAwDMQgAM0WmtrCbi3bCmd3rVasnFj0t9fOsFrtXJfS0vy2c8mN9xQQnMAAAAAAAAOYwQ6QKM9/ngZbf7gg8nSpaULfNWqZPXqcr2rqwTiE6PQly5NDh5saMkAAAAAAAAzkQ5wgEY74YQSah84kGzdmlx+eb0TfLJR6J2dybXXGoUOAAAAAADwNDrAARptbCx5+OFkzZpk7drknnuSSiWpVg8fhT6hVkvWrSvve3rKvQAAAAAAAOgAB2i4SiU59dTkAx9ILrggefvbk927Sxi+alUyMDD55/r7y/5wAAAAAAAAkgjAAWaG448vYXZPTxl5fuKJyWWXJXv3lo7vydRqydBQeT88nIyOltHpo6PlZwAAAAAAgCYjAAeYKRYtKvvAjz8+edGLkra2shu8Wp38/mo1Wby47ALftCnp6Ki/Nm2yIxwAAAAAAGg6AnCAmWxsLOnurv/c3p6cfno5dneXkHvDhrITfKJTfGJH+IYNOsEBAAAAAICm0tLoAgB4FpVK0ttbAu/ly5M3vamMOV+2LDl4sIxN7++f/LP9/clVV01ruQAAAAAAAI2kAxxgpmtrS37nd5K/+7vknHOSr389GR9PDhwo3d5HsyMcAAAAAACgCQjAAWa64eFk48bk9tuTLVuSRx4p5z72sdIh/pN2hAMAAAAAADQJATjATDcx5nzjxnJ82cvK8cork61bkzVrJv9cd3fZIQ4AAAAAANAk7AAHmOlqtaSlJVm1Kvn930/Wrk1Wry7X1q5Ntm0r7wcGyr3Vagm/e3vL+HQAAAAAAIAmIQAHmOmq1eTUU5PBwWThwnKc2Pu9Y0eycmWyYUPy/e8njz6avOQlyWOPCb8BAAAAAICmYwQ6wEw3Npacf36ydGmyf385Hrr3e8eO5Hd/N/l3/66E4PPmJaOj5TU83LCyAQAAAAAAppsAHGCmq1SSyy5LHn44ueiiw/d+d3UlmzcnjzyS/NEfJV/6UtLRkXR2luOmTcnISEPLBwAAAAAAmC5GoAPMBm1tyYoVyZVXJjfdVHZ8L1lSAvH+/rIj/L77kr6++mdqtWTduvK+p6cE6QAAAAAAAHOYDnCA2WLRorID/OKLk8WLk9/+7WRgIPn4x5M3vam8n0x/f9LaOq2lAgAAAAAANIIAHGC2qVSSBQtKGN7fnyxblgwOlo7vp+vqSm6+ORkfL/fYCw4AAAAAAMxhAnCA2apWK69du5KlS5Nq9fDrXV3Jtm3J9u0lJO/osBccAAAAAACY0wTgALNVtVpee/YkW7cma9Ycfr2/P7nxxrIXfKI7fGIv+IYNOsEBAAAAAIA5RwAOMFuNjSXd3eX92rXl/dVXl1D87LOTlStLAH6o9vbk9NOTW26xFxwAAAAAAJhzBOAAs1WlkvT2JtdcU8agr1yZvO51yQ9/mHzhC8mjj9Y7v7u6ks2bk507kzvuSO6/P9m3r4HFAwAAAAAAHHsCcIDZrK0t6elJdu8u+77PPTcZH09aWpKTTird4IfuAu/sTN72tuTuu5Pjj08GB5PRUePQAQAAAACAOUEADjDbVSrJggXJkiVlLPqnPlU6wif2gm/cWPaB9/Uly5aVMPy++5KTT046Ospr06ZkZKTRTwIAAAAAAPCCtDS6AACOodbW5A/+IFm9Olm/PvmbvykB+erV5fqhYfiEWi1Zt6687+kp9wMAAAAAAMxCOsAB5pJaLXnoodL9fe65ycUXJ3v3lvPt7cmqVcnAwOSf7e8vAToAAAAAAMAsJQAHmEuq1fJauzbp7k7OPjtZvLicW7as7Pyu1Q7/THt7cvrpZW/40ND01wwAAAAAAHCMCMAB5pKxsRJ879iRrFyZ/MzPJE89Vc7t2pUsXVrC8CTp6ko2b0527kzuuKMc29oaVzsAAAAAAMALJAAHmEsqlaS3N7nmmhJ4v/Odybvelfz+7yfvf39y993JmjUl/N62Ldm+PensTJYvL8drr01GRhr9FAAAAAAAAM9LS6MLAOAYa2tLenqSq64qI80XL04OHCjnWluTc85JLryw7Pzu66t/rlZL1q0r73t6SpgOAAAAAAAwi+gAB5iLKpVkwYJkyZJyXLSonDt4MPnzP0/+9b9OBgYm/2x/fwnKAQAAAAAAZhkBOECzGB5ONmwoAfcPf1g6vidTq5XOcQAAAAAAgFnGCHSAZtHaWsLvlpZk6dKkWp08BK9Wy9h0AAAAAACAWUYHOECzqNXKa8+eZOvWZM2aye/r7k7GxqazMgAAAAAAgGNCBzhAs6hW613fa9cm27aV8wMD5dyKFckVVySXXJK0tTWuTgAAAAAAgOdJBzhAsxgbK93dSbJjR7JyZXLmmckPfpD86EfJN76R/MZvlPtGR5PBwXIcHm5s3QAAAAAAAEdJAA7QLCqVpLc3ueaa0gm+Y0eyYUNy4EByww3JG96QjI8nmzYlHR3116ZNychIo6sHAAAAAAD4iYxAB2gmbW1JT09y1VXJ0FD5+dprk3Xrki9+MbnxxqSvr35/rVauLVqUXHppOQIAAAAAAMxQAnCAZlOplOOSJWXEeX9/cvbZZST6hRcefm9XV7JxY7JqVfLEE0lLSxmRPvE7AAAAAAAAZhAj0AGaWa1WXn19yaOPlvcTurqSbduS7duTzk4j0QEAAAAAgBlPBzhAM6tWkxUrkte9rv7zRAi+cWPpDp9sJHpSRqnrBAcAAAAAAGYQHeAAzWxsLLniimRwMNm6NVmzppxvby9jzwcGDr+/vT05/fTklluS1tbprxcAAAAAAOBZ6AAHaGaVSnLJJcn4ePLudydbtpTzDz2UPPZYvRv80F3gg4PJ0qXJ8HCyYEHDSgcAAAAAAHg6HeAAza6trXSC/+qvJitXJm98Y/LJT5Zx6NXqkbvAly8vx//235Inn2x09QAAAAAAAP9CBzgAyYknJr295f2TTybr1yevfW0Zif7a1x6+C/zss5MPfSj5hV9IHn20dIOPjdkHDgAAAAAANJwAHICirS3p6Sm7vd/73mTZsuSee0qwvXp16QQfGEhe//oyDv2CC8qI9Go16e4uAXpbW4MfAgAAAAAAaGZGoANQV6mUULtWS3bsSC6+ONm7t4Th27YlS5aU7vB16+r7wWu15Pbbk29/OxkdLTvCR0fLjnAAAAAAAIBpJAAH4HATu7+T5N57k8WLk2uvTW66KVmxonSBH2piR/jttycdHfXXpk3JyMh0Vw8AAAAAADQxATgAhxsbKyPNk2TPnhJun3NOsmVL6e6e6Pye0N+f3Hhj2RE+ca2lJfnsZ5MbbtAJDgAAAAAATBs7wAE4XKVS9nknJdz+wz9MXvOa5MEHk6VLS3f4RNB99tnJypXJhReWn7u6yn7wVatKWL50aXLwYCOeAgAAAAAAaEI6wAE4Ultb0tOT7N6dfOpTyUknJQcOJFu3JmvW1O/r60sefbQE4hOj0LdvTzo7k+XLy/Haa41CBwAAAAAApoUOcAAmV6mUY3t7GWPe3Z2sXVtC7iT5u79LXv/65KmnSlf4xo2lY7yvr/47arVk3bryvqen/jsBAAAAAACmgA5wAH6yibHoF16YnHde8sY3Jn/912VH+NatJdxetSoZGJj88/39SWvr9NYMAAAAAAA0HQE4AEdnYiz6PfckZ56Z3HBDGY2+fn3ye7+X7N1b3w3+dLVaMjQ0ndUCAAAAAABNSAAOwNGrVJIFC8px48bS/X3uuaUrvL29jEKfTLWaLF48nZUCAAAAAABNSAAOwHNXq5XX2rVlN/gv/3Jy113JmjVH3tvVVd8bPjiYjI6WneIAAAAAAADHmAAcgOeuWi2vHTuSlSvLSPQ3v7nsCb/mmnon+LnnJl/7WvKZzyQdHfXXpk3JyEgDHwAAAAAAAJiLBOAAPHdjY6XzOykh+DvfmbzsZcm7351ceGGye3eyZ08Jvq+7Llm3rr4fvFYrP2/YoBMcAAAAAAA4pmZdAP7Zz342p5122hGva6+9ttGlATSPSuXIbu8DB5Kf+7nkFa9IDh5MPvvZsi+8v3/y39Hfn7S2TlvJAAAAAADA3NfS6AKer5tuuiknnHDCv/zc0dHRwGoAmlBbW9LTk1x1VTI0lCxeXDrDDxwoI87f+Mbk0Ufrnd8T2tuTZcuSXbvK55YsaUj5AAAAAADA3DNrA/BXvepVedGLXtToMgCaW6VSjhMh9oIFyehocsstJRxPSod4rZZ0dSUbNyarViWDg8nSpaVTHAAAAAAA4BiZdSPQAZjharVk4cIScm/dmqxZU8LvbduS7duTzs5k+fJyvPbaZGSk0RUDAAAAAABzxKztAD/vvPOyd+/enHzyybnwwgvzO7/zOznuuOOe9+8bHx/Pvn37jmGFJMn+/fsPOwJz38JqNfP27y8d3uvXJ1u2JO96V9n53ddXv7FWSz72sYwvXZqD//bf5skX8B3e7HzXAkw937UA08P3LcDU810LMPV81x574+PjmTdv3lHdO298fHx8ius5pu65557cf//9OeOMMzJv3rx8+ctfzq233pqLLroo11xzzfP6nQ888EBGR0ePcaUAzelfn3xyTvj4xzPvNa8pHd933ZV8+cvJS15S3wf+tHHo4z/903nixz/OD2q1PPXUUzlw4EAOHDjQ0OcAAAAAAABmjgULFuTVr371T7xv1gXgk/nwhz+cP/mTP8ndd9+dpUuXPufPP/DAAxkfH88rXvGKKaiuue3fvz87d+7MKaeckoULFza6HGAazJs3L21JctNNmffudyd/+ZfJ299exp4n9XHo/f3JwEAJxc86K+M33ZScdlr5uVrNwZGRjLa2Zg78f1NTznctwNTzXQswPXzfAkw937UAU8937bH3ne98J/PmzTuqAHzWjkA/1Fvf+tZ86lOfyje/+c3nFYAnJbBZtGjRMa6MCQsXLvTPF5rNxRcnra3Je9+bLFqUVKsl3O7vT268sT4Ovasr2bIl8w4NxFesyHFXXJGFl1xS9olzVHzXAkw937UA08P3LcDU810LMPV81x47Rzv+PJkjATgAM1ClUo4LFiTDw0l3d/KlLyUrVyYXXli/b+PG+n7wrq7k5pv/ZTR6xseTxx5LTjyxIY8AAAAAAADMLvMbXcCx8LnPfS7HHXdcXvnKVza6FAAmU6kkvb0l3H700fou8Pb2EnYPDNRHo2/fnnR2lpHpJ5+cXHddMjLSyOoBAAAAAIBZYtZ1gF9yySV53etel9NOOy1Jctddd+X222/Pv/23/zZLlixpcHUAPKO2tuSUU5Kxsfo49GXLSqd3rVbC8YlO8KSE48uWJR/7WPm5p6feVQ4AAAAAADCJWReAv/zlL89f/dVfZdeuXTl48GBOOeWUXHnllfmt3/qtRpcGwE9SqyX33pusWVOC7hNOSF7ykmTFitIJvnp16QTfuLE+Bn3p0uQrXyn7xAEAAAAAAJ7FrAvAr7766kaXAMDzVa0m69cnW7YkS5YkF12UPPhgcvnlJexetqyMQe/vL2F4rVY+s2ZN8su/3NDSAQAAAACAmW/WBeAAzGJjY8mv/mqycmXy+c+X3d+3357cc08Zb37ttYePQU9KCN7Xl8yfbww6AAAAAADwrOY3ugAAmkilkvT2lu7uJUtK2L1jR/KGNySPPpqcc04JxSfT328MOgAAAAAA8KwE4ABMr7a25NJLk+Hh0t2dlBB8zZrkscfq556uVkuGhqapSAAAAAAAYDYSgAMw/RYtKru9q9X6uXvvTX7qpw4/d6izzirh+eho2Rc+OlpCdAAAAAAAgP9NAA5AY4yNJd3d9Z/37Em2bi2d4Idqb0/e8Y7kS18qO8I7OuqvTZuSkZHprRsAAAAAAJixWhpdAABNamIfeFL2e9dqyfr1yRe/mMyfn9x5Z3LllcmqVcmTTybXXZesW1f/fK1W/7mnp/w+AAAAAACgqekAB6Bx2tpKeL17dxlrfs89SWtr8sEPJnffnfz93ye//utlZHp//+S/o7+/fAYAAAAAAGh6OsABaKyJzu0lS8pxwYKy2/vDH05uvz35278t49Frtck/X6slQ0P1zwMAAAAAAE1LBzgAM09ra+ns3rgx+aM/Sk46KalWJ7+3Wk0WL57O6gAAAAAAgBlKAA7AzFOrJS0tZf/3pk3J1q3JmjVH3nf22cm995b3g4PJ6GjpHgcAAAAAAJqSAByAmadaTU49tYTatVqydm3S3Z1cfXW5dvbZyde+ltx1V3LrrUlHR/21aVMyMtLgBwAAAAAAABpBAA7AzDM2lpx/frJ0aQm8d+xIVq5M3vjG5HvfK8H3woXJ+vXJunX1/eC1Wvl5wwad4AAAAAAA0IQE4ADMPJVKctllycMPHz76/Iwzyrnrr0+WL08GBo78bHt7GZne2jpt5QIAAAAAADNDS6MLAIBJtbUlK1YkV16ZzJ+fnHVWctNNZRT6li3Ju99d7/xOkq6uZOPGsjd8cDAZHy9d4JVKwx4BAAAAAACYXjrAAZi5Fi0qo857epK3vKUE34ODyYMP1sejJyX83rYt2b496ews3eGvelVy8832gQMAAAAAQBMRgAMw81Uqpdt7Ivg+cKCMOZ8Yj75xY9Lfn/T1JcuWJZs3J/ffn7ztbaUT/LHHGlk9AAAAAAAwTQTgAMwO1erhwffatWUc+vr1pTt8YGDyTvCTT06uu04nOAAAAAAANAE7wAGYHcbGSuC9dm0JuZPkvPOS669PnniidIjffHO9E3xCrZasW1fGqXd3JwcPJq2t5Xy1Wn6vPeEAAAAAADAn6AAHYHaoVJLe3uTCC0vwfeaZyZe/XLq+TzghWbEiWbWqdIIf6uyzS9f4pZeWnzdtSjo66q9Nm3SHAwAAAADAHKEDHIDZo60t6elJrroqGRoqndytrcmXvpRcfnkyOFg6u5MSjA8MJK9/fdkRPjKS3Hff4d3hLS3JZz9busPXrNEJDgAAAAAAs5wAHIDZZSKkXrKkHAcHkw98ILnnnnKtWk2WLStj0n/4w7Ij/OMfL8H5e99bPtPVVULxVavK55cuLaPRAQAAAACAWc0IdABmt2o12bUrecMbkt27y57vjRuTm24qY9EHBkogPtEd3tVVwvHt25POzmT58nK89lqj0AEAAAAAYJYTgAMwu42NldB7x47krW8t7889N9mypR5679pVuryr1RKO9/eXUei1WtLeXgLwj30s2bAhGR5u8AMBAAAAAADPlwAcgNmtUkl6e5NrrilB9/veV4LtBx+sh9579iRbt5Yx6KtWla7wrq5k8+Zk587kjjvK8ed/vuwUBwAAAAAAZiUBOACzX1tbCbd3704+9ankxS9ODhwoofeaNSXs/qmfSq64Itm7t74j/Olj0L/2teSppxr9NAAAAAAAwPMkAAdgbqhUkgULykjzibHoa9eW0Pu++5K7705+7dfK9WuvPXwMelKOfX1lRLox6AAAAAAAMCu1NLoAADjmJsaiJ8n99ydf+UoJt5Py/pxzkve+d/LP9vcnV101PXUCAAAAAADHlAAcgLlpYix6a2vy679eP/+Hf5i85jX1zu+nW7as3gFeq5Ud4mNjJVQHAAAAAABmNCPQAZi7KpUSYh8adt97b9kHXq0efm97e/KOdyT33JNcf33S0ZG86lXJm9+cDAwkIyPTVzcAAAAAAPC8CMABmNuq1cPD7j17kq1bkzVrys9dXcnmzcnOncmnPpXceGNy++3JzTeXc3/+5+XeBx9M9u2b7uoBAAAAAIDnQAAOwNw2NpZ0dx9+bu3acu6GG5Jt25Lt25MzzkgWLkzuvLN+rrMzWb68HD/96WTevMY8AwAAAAAAcFQE4ADMbZVK0tubXHNNvRN8167kttuS1avLePO+vuSVr0weeyy58sqkv7+cmxidXquVnzdurO8HBwAAAAAAZhwBOABzX1tb0tOT7N6dDA6W48UXl/P9/WUM+qc+VQLyVatKKD6Z/v6ktXVaSwcAAAAAAI6eAByA5lCpJAsWJEuWlGOlUjq7a7XS2X3DDcm99yZ799Y7v5+uVkuGhqavZgAAAAAA4DkRgAPQvKrVZMWKetf31VeXgHxiVPrTnXVW6RofHS2d5KOjRqIDAAAAAMAMIgAHoHmNjSVXXFHC7FqtdIBv25Z0dx9+X3t78o53JF/6UnLttUlHR/21aVMyMtKQ8gEAAAAAgMO1NLoAAGiYSiW55JJkfLx0fddqJfzeti05eDD54heTK69M3vKW0u193XXJunX1z9dqJRS/6KJk+fLyc7VagvVKpSGPBAAAAAAAzUwHOADNra2tBNYTXd87diQrVyZvfGNy993Jd7+b7NuXHH980t9f/1xXV7J1a3LXXcmtt+oKBwAAAACAGUAADgAnnpj09ibXXFM6uHfsSJ58MtmwIflX/yq57bZk167S4Z2U8HvbtrIvfP360hU+ca1WKz9v2GA/OAAAAAAATDMBOAAkpRO8pyfZvTt59NEy9vwv/iJZtSq5/vpk6dISjifJxo3JTTclK1YkAwOH/5729uT005NbbklaW6f7KQAAAAAAoKkJwAFgQqWSLFhQQuxaLVm4MBkcTB56qIw7X7MmOfvs5Fd+JdmypVw7tCt88+Zk587kjjuS++/XAQ4AAAAAANNMAA4Ak6lWk/37653fa9cmV1yRfOELyZ49yYMP1q9NjETfvj3p7EyWLy/H66+3CxwAAAAAAKaRABwAJjM2lrznPfXO7x07kv/1v5L/+l+Tk05KDhyoX9u4MenvT/r67AIHAAAAAIAGaml0AQAwI1UqSW9v8olPJN3dyaJFyetfn1x4YfIzP1OC77Vrk3vuKfeuXn3k7+jqSs46q+wCHxws3eJjY+V+AAAAAADgmNMBDgDPpK0tufjiZPHi5P3vL53ctVoJvru7kwsuKMe9e+ud3xMmxqLfd1/S0VF/bdpkLDoAAAAAAEwRATgAPJtKJVmwoITg1Wp57diRrFyZnHlm6RB/8YvL+Qnt7cnAwORj0W+/Pfn2t5PR0dIVPjpqRDoAAAAAABwjAnAAOFpjY6XjOykh+DvfmZxySvI//kdy6aWl63vz5uSRR5Jf+qUSgh/q7LPLvbffriscAAAAAACmgAAcAI7WxF7wa66pd3wfOJB84xvJlVeWceePPFLC7EcfrXd+TwTjd945eVf4unXJhg06wQEAAAAA4AUSgAPAc9HWlvT0JLt3lxHmu3eXPeFPPZV89KPJy16W/NEfJSedVELyiV3g3/hGctxxJQA/VHt7cvrpyS23JK2tDXkkAAAAAACYK1oaXQAAzDqVSjkuWVKOCxaUXd633FLC8dWrk1e+MlmzJnnta0vovXlz8u53H94VvnFjsmpVCdKXLi0d4AsWNOCBAAAAAABgbtABDgDHQq2WLFxYwuxaLVm7NrnssuTcc8su8F27Ssh9aFf49u1JZ2eyfHk5Xn+9XeAAAAAAAPAC6AAHgGOhWk3276+H3Dt2lNHoH/94vet769bDu8L7+uqfn9gFnpQu8okucwAAAAAA4KjpAAeAY2FsLHnPe+ohd5Lce2+yeHG96/unfiq58sp6V/jTdXUlZ51VdoEPDpax6sPD0/oYAAAAAAAwmwnAAeBYqFSS3t7ku99NuruTq69ODhwogfh/+S9l5PnddyeXXJLs3VvvCp8wMRb9vvuSjo7kjW9MvvSlZP78ehi+b18jngwAAAAAAGYNATgAHCttbWXs+eLFyX/4D8nu3ckv/VLyvvclN95YRp5/6Uv1rvAJ7e2lI3xiLPqyZfUw/E1vKp3kY2PJj3+sKxwAAAAAAJ6FABwAjqVKJVmwIDnppHJ88YuT448vAXiS7NlTH5Pe1ZVs3pzs3FmC8omx6Bs3ljD8M59JtmxJtm9POjuTl760dIdv2pSMjDTqCQEAAAAAYMYSgAPAVKvVDh95vnZtcsUVpcN7+/bkd3+3dHfXaqUbfNWqEoZPBOF9ffXP12rJunXJhg06wQEAAAAA4GlaGl0AAMx51Wp5TYTYO3Yk/+t/JffcU7q877mndI5Xq2X8+eBg0tJSgvDVqw//Xe3t5Z5bbkmuumpaHwMAAAAAAGY6HeAAMNXGxpLu7vrP7e3J619fxqJv3JjccEPyxS+Wsei7diVLlyannlqC8InQ/NBx6Xfckdx//2Ed4G1tbdP5RAAAAAAAMCPpAAeAqVapJL295X1//+Rd3suWJdu2lXvuvjs577wShE90hW/bVj67enUJxavVMgp99eosPP74dL34xZnf0pLs25csWtSIpwQAAAAAgIbTAQ4A06GtLenpSXbvTr7yleSnf/rwLu8dO5KVK5Mzz0zOOSe5/PLk4YdLV/hku8CXLUve/e7kIx/JvDe8Icd97WuZNzZWdomPjtoPDgAAAABAUxKAA8B0qVSSBQvKCPSxseT88+td3kkJwXt7S3j9iU8kP/pRcuWVybnnJgMDh/+uiVD8059OtmxJtm9POjuTl7406ehINm1KRkam/REBAAAAAKCRBOAA0AiVSnLZZfUu7wkTO8Evuyx585uT3/3d0vU90fmdlAB91aoSik/WHV6rlfHoGzboBAcAAAAAoKkIwAGgUdrakhUrSpf3NdeU9xPB9oQvfCE58cR6l3hSxp8/+mh9h/jTu8Pb25PTT09uuSVpbZ2WRwEAAAAAgJlAAA4AjbRoUbJwYdkP/s1vJk88cXi39549ydat9S7xrq4y3vzkkw/fIT5xbfPmZOfO5I47kvvv1wEOAAAAAEBTEYADwExQqZRu7ZNOOrzbO0nWrk26u8to9G3bkm9/O/nhDw/fId7VVa5N7AJfvrwcr7/eLnAAAAAAAJqGABwAZpKxsRJ2H2rHjmTlyuQ3fiO57bbkoouSv/7r5OKLk4ceKt3hz7QL/PbbS2A+Olq6xUdHdYUDAAAAADBnCcABYCapVJLe3rITfKITvFpNVq8u3eHLl5eg+7LLSig+OFh2iJ977pG7wCe6wm+/PenoqL82bdIVDgAAAADAnCQAB4CZpq2t7ATfvbsE3Lt3J5demgwNJW96Uz3o3rEjectbkt/93dLtfeju8OSZu8LXrUs2bDiyE3x4WKc4AAAAAACzmgAcAGaiSiVZsCBZsqQcFy0qneCDg0cG3V/4QnLiiYfvDj/ttGTVqiO7wif095ed4xNGRkpnuE5xAAAAAABmMQE4AMwWo6PJsmWHB91JsmdPsnVr2QXe1ZVs3pz8v/9v8sQTR4blE2q10lGelE7vDRtKZ/jRdIoDAAAAAMAMJQAHgNmiUkkOHCjj0J9u7dqM9/Qk992XPPJICa2f3hU+ob09ef3rk8WLy8+traUjfDJP7xQHAAAAAIAZTAAOALPJCSckvb3JNdfUw+1qNeMXXljef/SjycteltxwQ/LFL5au8Alnn106xXfuTP7yL8u5ffsm3x/e3p6cfnrS0lLvFAcAAAAAgBmupdEFAADP0cKFSU9PctVVydBQxhcvzuN79+aE449PbrmlXFu9uoxL37at7BE/44zkrLOSjRuTCy4ogfeKFcmHP5y84x0lTK/Vygj1jRvL/vDBwWTp0uTgwcY+LwAAAAAAHCUd4AAwG1UqyYIFyZIl2X/gQL63Z08JsBcuLMF1rZbs2FGC8IsvTl70omT9+rLXe9mysif8619P3vrW5Fvfqu8P37Yt2b496exMli8vx2uvTUZGGvu8AAAAAABwFHSAA8AccODAgdLFvX9/6dqe6Oj+P//P5MYbk+7uZGCgHnL395cd4V/5SvLpT5dz73pXOd/XV//FtVoJzZPSWV6pTP/DAQAAAADAUdIBDgBzwIEDB3LwySeT97yn7Ples6bs8V61Ktmypd4VvnFjCbnvuit5/etLOL5jR/Jrv5acemoJySfT35+0tk7rMwEAAAAAwHOlAxwA5ojRlpYs7O1NPvGJ0vHd0VGC7wcfLF3hK1aUQHz16uSv/ip59NESiifJ41tpKyYAACvYSURBVI8nP/xh/eenq9WSoaGyTxwAAAAAAGYoATgAzBHj4+NlB/jFF5du7fe+N1m0KDlwoHSFX355CcRbWpLXva58aGJU+q5dh49Of7pqNVm8uP7z8HD5G7VauTY2Zjw6AAAAAAANZwQ6AMw1lUqyYEE9mO7uTtauTd797mTZsjLqfHCwPio9SfbsOfznCe3tyemnl8+PjZVzIyPJpk2lw/xVr0re/OYyOn1kZFofEwAAAAAAnk4HOADMZZVK0ttb3r/97clf/mVy/vml23v9+rIfPCkB9tq1ybZtybx5yRe+kFx5ZRmZvndvuX9srHR+b9qU3H57cvPN5frgYLn+4IPJK15Rus4BAAAAAKABdIADwFzX1pb09CT33JOceGLpCH/44eTcc5OVK5Mzz0y+//3k858vgfmllyZ33538/d8nnZ3JS19aur3/4i/K2PM77yxB+fbt5fry5eX46U+X8BwAAAAAABpEBzgANIOJ/dwvelE5rlhROrznz0/e976yF/zUU0t3+Pvfn2zcmKxbV+5tb0/OPjt517tKt/eVVyb9/UlfX/3312rl5/nzS9huHzgAAAAAAA2gAxwAmtGiRcnChSWs3r07+cY3kq98pewAP/74EnB3dSWbNyePPJL86Z8mf/RHyUknlbHnAwOT/97+/tIlDgAAAAAADSAAB4BmVqkkCxYkS5aUY6VSurmXLStjzh95JNm3r4TiGzcmX/ta2Qleq03++2q1ZGhoGh8AAAAAAADqBOAAwOGq1eTaa0s398teltx2W7JrVwm3r766hOXV6uGfaW9PTj+9jFZfvLgBRQMAAAAAgAAcAHi6sbHknHOSW28t486vvz5ZurSE3vfeWzrDu7vLvRNj0nfuTO64I3nggWRkZPLfOzycjI6WPeKjo+VnAAAAAAA4hgTgAMDhKpXk8cfLjvDBweShh5KtW8t+8KSE32vWJDfcUMLw7duTzs5k+fLkTW8qY9MPDbr37Suh+KZNSUdH/bVp0zOH5QAAAAAA8Dy0NLoAAGAGqlaT/fvrnd9r15awO0kGBpKVK5M77yzv+/rK+a6u5G/+pgTjAwNlZHq1Wj73mc8k69bVf39LS/LZzyaLFpUwvVKZ3ucDAAAAAGBO0gEOABxpbCx5z3vqnd87dpTQ+8wzk+9/P/nCF5KXvKTsCU9K+P35zyc33lgC8VqtnG9pKXvBD73v0JHpa9YkBw824AEBAAAAAJiLBOAAwJEqlaS3N/nud8vI86uvTnbtSt75zuSMM5K//dsSctdqJdS+554y1nwi6E6Ss88uAfoTT9Tve/rI9M7O5NprjUIHAAAAAOCYEIADAJNra0suvjhZvDj5D/8h2b27vL7xjeT888t482o12bgxue22EpBPBN1btyZ33ZW0tiYnnFC/r7//8A7xWq2MRt+wIRkebtSTAgAAAAAwRwjAAYBnVqkkCxYkJ51UjkuXlmOlUsakr12brFqVXH99uXbWWaXLe8mScu5lL0u+/OWkp6fcNzAw+d/p7y9hOQAAAAAAvAACcADg+alUkssvT/buTR56qHR9f+ITyU03lb3fW7Ykg4PJBz6Q/N7vlfsmOr+frlZLhobqPw8PJ6Oj5fOjo7rDAQAAAAA4KgJwAOD5O/740vldrSYf/nBy2mn14PvBB8u1XbuS885L2tvLfYdqb09OP70E5osXJ/v2Jfv3J5s2lZ3iE69Nm+wJBwAAAADgJxKAAwAvzNhY0t2dPP548sMf1oPvAwdKV/iaNcm99yZf+EJ5n5Q94Zs3Jzt3JnfckfzjPyZPPVU6ydevL3vB7QkHAAAAAOA5aml0AQDALFepJL29yaJFZff3ocH32rVlJ3hSgu0tW8o9F11U9n6vXp0sW5bceWdy223lM0/fE97eXu655Zbkqqum/fEAAAAAAJg9dIADAC9cW1sJrw8eLN3ga9eW4wUXlPHnZ56ZfPnL5frq1SXk7usrwfY995SO8TvuKKPTJzq/n94lfv/9OsABAAAAAHhWAnAA4NioVJITTijd4BdeeHjw/fM/n7S2luttbaX7OynH224re8InRqdXqyX83rYt2b496exMli8vxz//8zJqfXS0hOWjo0JxAAAAAAD+hQAcADi22tqSnp7S2f2Lv1iC75NOShYsKGPSa7XyOvvsZOXK5Prrj9wZvnFjCcf7+uod4cuWJe9+d/KRjyQdHeX1ylcmN9+cjIw07HEBAAAAAJg57AAHAI69SqUclywpxwUL6teq1fLq60sefTR56KHDd4bfc0/5/OrV9c+0t5ex6ROheFdXCb5XrUr27Cn3PP546TAHAAAAAKBp6QAHAKbX2FgJul/3utIZXq0evjO8uzvZu7d0fh+6B/yXfqmE4BPj0R95JLn77hKO//CHSUtLCcEBAAAAAGhaAnAAYHpVKsnll5eQe6Lze8eOMg79zDOTT3wiefGLk7POqu8BP/fc5J//uYTiGzcmt96aXHRRct999R3hJ59cxqPv39/oJwQAAAAAoEEE4ADA9Dv++LL3e/360vF99dXJrl3JO9+ZnHFG8k//lNx0U33k+fh48pKXJCtWlLHnK1YcuSO8Vks+9KESkA8P1//W8HAyOpoMDpbjodcAAAAAAJhTBOAAQGOMjSW/+qv1zu/vf78E3/ffnxx3XHLaafWR53/918mDD5bO8T17kje9qVw7VHt7cvrpyS23JK2t5dzISLJpU9LRUX9t2lTOAwAAAAAw57Q0ugAAoElVKklvb3n/vveVHd6nnpqcf34Jumu18rr55tLt/ZnPJPfck/zUT5Wd3xOd311dpet71arS5b10aRmDPjZWwu516+p/s1ar/9zTU2oAAAAAAGDO0AEOADROW1sJonfvTr7xjeQrXyk7wY8/PqlW6yPPBwbKnvA3vKGE3MuWletdXfU94eeck3z96+X3joyULvD+/sP/3mRd4gAAAAAAzBkCcACgsSqVZMGCZMmScpzoyh4bS664ogTeE93eO3Ykv/IryZNPJpdeWjq/J7rDt2wpQfjFFycnnHD457q6ks2bk507kzvuKGPWR0bsBwcAAAAAmGME4ADAzFSpJJdcUu/2nrBjR/Ke95TO8XPPLd3hhwbhH/94eX/SSUd2iXd2Jm97W3nf2mo/OAAAAADAHCMABwBmrra20gne3X34+c99LvnN30yGhsru8Ikx6R/9aPnMhz+cbN1axqlPhON9fSVMv+eeEo6vX1/2gU90iU/sB9+wQSc4AAAAAMAs1dLoAgAAntWJJya9veV9f38JqqvV5Bd/MXnRi5JTTy0jzFtakje+Mdm1q9yzdm0JuyuVZPXq0gn++c8nn/xkCcYHBo78W11dyVlnle7wwcHyd8bG6mPZAQAAAACY0XSAAwAzX1tbGXm+e3cJpnfvLiH22Fhy/vnJ0qUlCH/00fK+Wi2j0i++ONm7t9753dFRdoAfuh98wtlnJ3/7t8l99xmLDgAAAAAwSwnAAYDZoVJJFixIliwpx0qlvC67LHn44eS885L29uTuu0s4niT33pssXpxce21y222lO/zBB+sheVK6vrduLd3hN9xQRqUbiw4AAAAAMCsJwAGA2a2tLVmxIrn88hKEP/RQ2Rl+9dXJgQPJtm3JOeck119fgu8DB+r7wbu6Stf3z/98CdX7+w//3e3tyemnJ7fcUsaiDw8no6Olg3x0NNm3rxFPDAAAAADAMxCAAwCz36JFycKFJQj/3d8t3d6ve13y/e8nr3lN8vjjJRifCL7Xri0h+V/+ZbnnT/+0vjs8KcH45s3Jzp2lM/wf/iF56qkyDv0Nbyid5WNjyY9/XIJw3eEAAAAAADOCABwAmDsmgvCLL07OPTfZvz950YvKuPNqtR58X3BB8t73lr3hK1bUu8Or1RJ+b9uWbN9efk+1mvzTPyXr1ye3355s2VKudXYmL32pPeEAAAAAADOIABwAmHsm9oW3t5fR5WNjJfjesSNZuTI588zk//l/Suf24ODh3eEbN5ZR6J/5TPLxjycDA8ny5eU4ce3QPeEtLclnP1v2h+sEBwAAAABoqJZGFwAAMOUqlaS3t7zv70/e+c6y93vbtrJDfKI7/J57yr2rVyd/8Rfl2n//78lv/EYJuletKteS0im+cWM5t2dPsmRJcvBggx4QAAAAAIBEBzgA0Cza2pKenmT37tL1/T/+R9nf/fDDpfN7x44y8nzv3hJ2v/GNZS/4gw+W8einnlo+V6vVx6Q/8khy990l/N63r3Sd795dfu++fY1+YgAAAACApiMABwCax8Ro9CVLyvHEE0uw/YEPJFdfnXzjG8nixeXco4+W4PvAgTIe/bzz6nvCN25Mbr01ueiiMj59eLiMQH/DG5L77isj13/84xKE799fro+OlnB9dLQE6aOjRqYDAAAAABxjAnAAoLkdf3zZE97Tk/x//18Jps8/v+wPv/vu0h2+dm3yO79Twu6enjL2fMWKMk79ZS+r7wzfsiXZvj0555zkW98qvytJPvWpZGgo+ehHk46O5FWvSt785uSTn0yefLKhjw8AAAAAMJfYAQ4AsGhR/f3xxyeXXZZ8+9sl8O7uLufPOy/5v//v5IorSnf3m95U7lu7tuwFv/nmehD+t3+bHHdcGa/+6U8nr31t/drNNydveUsZkX7iieV3vehFpdP80DoAAAAAAHjOdIADADxdW1vp8P7d301uuy153euSL385ecUryvUlS8oY84ULy7GlpXSFDwyU8ejf/37ysY+V33HrreXaF79Y3xt+6Mj0r32thN+jo8njj08+Kt0+cQAAAACAoyIABwCYzKJFJeC++OLk3HPLLu+f/unkqadKKL1sWTm3dGnZGX5oEL5iRRmHfmhIfuWVk49Mf+SRZGQkmTcv+eM/ro9Kf8MbknvvFY4DAAAAADwHszIAf+ihh/K+970vP/uzP5tf/MVfzKZNmzI6sWMTAOBYqlSSBQvKTvDW1vJzpVKC6d/8zWTr1jIefSII37u3hNMPPljOTYTkq1bVu8EnOsUnAvHvfz+59tpjF45PHIeGjrw22bmjuTZxfPTRZGysdLFDE5k3b15+6qd+qtFlADSFSqWSefPmNboMgDmtra2t0SUAwJSZdTvAh4aG8tu//ds55ZRTcuONN2b37t3ZuHFjRkZGcs011zS6PACgWZxwQtLbm9x0U/I7v1P2hZ93XnLSSeX6gQMlHL/oojLm/NRTjxyZ/vu/X3aIJyUcf/o+8de+toTjT98jvm1buX9kpOwa/+M/Ln/n1lvrx/e+98hrk507mmsTxxUrkje/uQTlJ55YQvFKpYT88+eX/enDw898brqvqWFu/p0G1dB2/PE5denS8p9f/7fffM+qhplTQzM9a5PWsHD+/Jy2fHk5d9xxc/pZ1eBZ1TCDamimZx0ezsJKJa/s7KxPOGvGfw7N9KxqaL5nVUNjn3XBgmRoKAur1ZzW2el/2Nkg8xtdwHN12223ZXh4OAMDA3nDG96QCy64IP/xP/7H3Hbbbdm9e3ejywMAmsnEiPTFi0v39uWXJw8/XMLwNWtKoN3dXX5esqT8F+FDR6YvXFjvGH96OD7RLX7oHvGf1Dl+6HGya8/1/onjRAj+0EPlv9T/xV+UEPzP/ix54onyz2KiQ/2Zzk33NTXMzb/TwBrmDQ1l3p/9WeY1+T+HpnxWNcycGprpWZu4hnmHfOfO9WdVQ5M/qxpmTg3N9Kz/+zjviScyL+U7tyn/OTTTs6qh+Z5VDY191k2bytrEjo7M6+hI5WMfi3kbjTFvfHx8vNFFPBfvec97snjx4nzsYx/7l3OPPfZYzjrrrKxfvz6//uu//px/5wMPPJAkefWrX33M6qTYt29fvvnNb+ZnfuZnsmjRokaXAzAn+a6dYfbtK/9rzyefLIHyF79YOsV/+ZfLz695TfKNb5Rg/IwzkvvvL5+beH/uucmf/3ly/vnJ5z9fxo2ff35yxx3JWWclO3dO/rmnHye79lzvnzjefXdy332lC3379sOPL395vUP92c5N9zU1zM2/o4aZU0MzPasaZk4NzfSsami+Z1VD8z2rGmZODc30rGpovmdVQ/M9qxoa+6x9fUf8q8Lxa67JvJ6e0iXOC/Kc8tzxWeYXfuEXxj/ykY8ccf6XfumXJj1/NL7+9a+P33///ePDw8Nex/i1Z8+e8e3bt4/v2bOn4bV4eXl5zdWX79qZ99q3b9/4gSeeGD/42GPjB0dGxg8ODo4ffPLJ8YP79o0f7O8fHx8cHB+///7x8auvHh/fvPnw9+vXj48/8cT4+IoV5Xjo+9e/fnz84YfHx08/fXz8e9+rv3/6cbJrz/X+iePOnYfX8PTj0Z6b7mtqmJt/Rw0zp4ZmelY1zJwamulZ1dB8z6qG5ntWNcycGprpWdXQfM+qhuZ7VjU07lmr1fHx5MhXtTp+8MknG/7vK+fC6/777x//+te/flTZb8sUh/HH3GOPPZYTTzzxiPOLFy/O0NDQ8/69Y2Nj+eY3v/lCSuNZ7Ny5s9ElAMx5vmtnnpaWlhx//PGZN29eRv75n9PS0pLOCy7ICYsXZ96CBcmVVyY33ZR5b3hD8oEPJH/yJ/V94hddVHaIv/zl9ffnnVdGqO/fX981PjFW/dDjZNee6/0TxyVLkt276yPaDz3u3Vs61H/Suem+poa5+XfUMHNqaKZnVcPMqaGZnlUNzfesami+Z1XDzKmhmZ5VDc33rGpovmdVQ2OftVab/F8Q1moZ37s3/7RnT0ZGRqb030U2gwULFhzVfbMuAJ8qra2tecUrXtHoMuac/fv3Z+fOnTnllFOycOHCRpcDMCf5rp19Rg4cKAF4kgWrV2f+8ceXsPl970uOPz55PuH404+TXXuu9190UfLVryZvetPUB+3H8poa5ubfUcPMqaGZnlUNM6eGZnpWNTTfs6qh+Z5VDTOnhmZ6VjU037OqofmeVQ2NfdZqdfIQvFrNvJNOystPOOHIazwn3/nOd4763lm3A/zss8/OBRdckN///d8/7Pwb3vCG/Nqv/Vo+8IEPPOffaQf41LGXFmDq+a6dw4aHk9bW8l+m588v4fj+/SUgv+mm5D3vSY47roTjF12U3Hpr/fje9x55bbJzR3Ptt34r+YM/SM48c2bvWVJD8+3WUoP/N1dDc9XQTM+qhuZ7VjU037OqYebU0EzPqobme1Y1NN+zqqGxz2oH+JSa0zvAf/M3f3P8/e9//2HnHnvssfHTTjtt/K/+6q+e1+/8+te/ftQz43luhoeHx7dv3z4+PDzc6FIA5izftU3qiSfGx598cny8Vhsff+yx8v7HPz78ONm153r/xHHfvvHxif3lN9xQP/7oR+VzP+ncdF9Tw9z8O2qYOTU007OqYebU0EzPqobme1Y1NN+zqmHm1NBMz6qG5ntWNTTfs6qhsc969dX1XeDV6vjBa64ZP7hvX6P/LeKc8Vzy3Hnj47OrA/yP/uiP8vGPfzxf/epX/2UX+Kc//en85//8n/OVr3wlHR0dz/l36gCfOroSAaae71qmzURH+vBw+V+tThwP7VB/tnPTfU0Nc/PvNKiG8UOuzWvifw5N+6xqmDk1NNOzNmkN44dcmzfHn1UNnlUNM6iGZnrW4eGMH3JtXrP+c2imZ1VD8z2rGhr7rAsWJI89lvHFizO8d2+OO/FEayuPkeeS5866AHxoaChvf/vb8/KXvzy/93u/l927d2fjxo15xzvekWuuueZ5/U4B+NQRygBMPd+1AFNv//79+e53v5uXvvSlvmsBptC+ffvy3e9+Ny972cv8i0KAKbJv37780z/9U17+8pf777YAU8S/sz32nkueO3+qiznWFi9enD/5kz/Jcccdl3//7/99rrvuulxwwQVZu3Zto0sDAADmqPHx8TzxxBONLgOgKQwPD2eW9WsAzDojIyONLgEApkxLowt4PlasWJGbb7650WUAAAAAAAAAMIPMug5wAAAAAAAAAJiMABwAAAAAAACAOUEADgAAAAAAAMCcIAAHAAAAAAAAYE4QgAMAAAAAAAAwJwjAAQAAAAAAAJgTBOAAAAAAAAAAzAkCcAAAAAAAAADmBAE4AAAAAAAAAHOCABwAAAAAAACAOUEADgAAAAAAAMCcIAAHAAAAAAAAYE4QgAMAAAAAAAAwJwjAAQAAAAAAAJgTBOAAAAAAAAAAzAkCcAAAAAAAAADmBAE4AAAAAAAAAHOCABwAAAAAAACAOUEADgAAAAAAAMCcIAAHAAAAAAAAYE4QgAMAAAAAAAAwJwjAAQAAAAAAAJgTBOAAAAAAAAAAzAkCcAAAAAAAAADmBAE4AAAAAAAAAHOCABwAAAAAAACAOUEADgAA/P/t3XmM1dXdP/D3aMF9RAjFIm4wgYJCB6oiQsaAVhaNWxVpTVFRUCNFoU0FAZdKnlJStVYRBTVq24iVVokVUYsWiqVUCy5xowIVpLiwyAybDHJ/fxjn6Tj4/KwODlxfr2T+uOece3LOBD75zn1/7/cAAAAAQFEQgAMAAAAAAABQFATgAAAAAAAAABQFATgAAAAAAAAARUEADgAAAAAAAEBREIADAAAAAAAAUBQE4AAAAAAAAAAUBQE4AAAAAAAAAEVBAA4AAAAAAABAURCAAwAAAAAAAFAUBOAAAAAAAAAAFAUBOAAAAAAAAABFQQAOAAAAAAAAQFEQgAMAAAAAAABQFATgAAAAAAAAABQFATgAAAAAAAAARUEADgAAAAAAAEBRKCkUCoWGXkRDW7BgQQqFQho3btzQSyk6hUIh1dXVadSoUUpKShp6OQBFSa0F2PHUWoAvh3oLsOOptQA7nlpb/7Zs2ZKSkpJ06dLl/zv2a1/CenZ6/uHtOCUlJW4sANjB1FqAHU+tBfhyqLcAO55aC7DjqbX1r6Sk5DNnur4BDgAAAAAAAEBRcAY4AAAAAAAAAEVBAA4AAAAAAABAURCAAwAAAAAAAFAUBOAAAAAAAAAAFAUBOAAAAAAAAABFQQAOAAAAAAAAQFEQgAMAAAAAAABQFATgAAAAAAAAABQFATgAAAAAAAAARUEADgAAAAAAAEBREIADAAAAAAAAUBQE4AAAAAAAAAAUBQE4O8TixYtzwQUXpLy8PN27d8+ECROyZcuWhl4WwC7hsccey6WXXpqKioqUl5fntNNOy7Rp01IoFGqNe/DBB9O7d+907Ngxp556ap5++uk6c1VVVeWqq67KMccck86dO2fYsGF59913v6ytAOwyNmzYkIqKirRr1y4vvfRSrT71FuCLeeihh3L66aenY8eO6dq1ay666KJs3ry5pv+pp57Kqaeemo4dO6Z37975/e9/X2eOLVu25Oc//3m6d++e8vLyXHDBBVmyZMmXuQ2AndasWbNy9tlnp3PnzunRo0cuv/zyLF++vM4417UAn82bb76Zq6++Oqeddlo6dOiQU045Zbvj6rOuLliwIOecc046deqUnj17ZvLkyXU+D+azE4BT79atW5fzzjsv1dXVueWWWzJ8+PD87ne/y/jx4xt6aQC7hHvuuSd77bVXRo4cmUmTJqWioiJjx47NxIkTa8Y8+uijGTt2bPr27ZspU6akvLw8Q4cOzfPPP19rriuuuCLPPPNMrr322vziF7/I0qVLM3jw4GzduvVL3hXAzu22227Lhx9+WKddvQX4YiZNmpTrr78+/fr1y1133ZWf/vSnadWqVU3Nfe655zJ06NCUl5dnypQp6du3b0aPHp2ZM2fWmmfcuHF58MEHM3z48Nxyyy3ZsmVLzj///FRVVTXEtgB2GvPnz8/QoUNTVlaWiRMn5qqrrsprr72WQYMG1brZyHUtwGf3z3/+M7Nnz86hhx6aNm3abHdMfdbVN998MxdeeGGaN2+eO+64I+edd15+9atf5e67796R2yxuBahnt99+e6G8vLywdu3amrapU6cW2rdvX3j77bcbbmEAu4jVq1fXaRszZkyhS5cuhQ8//LBQKBQKJ510UmHEiBG1xpxzzjmFiy66qOb1ggULCm3bti385S9/qWlbvHhxoV27doVHH310B60eYNfzxhtvFMrLywv3339/oW3btoUXX3yxpk+9Bfj8Fi9eXOjQoUPhz3/+86eOGTRoUOGcc86p1TZixIhC3759a16vXLmy0L59+8LUqVNr2tauXVsoLy8vTJ48uf4XDrALGTt2bKFXr16Fbdu21bTNmzev0LZt28Kzzz5b0+a6FuCz+/gz2EKhULjyyisLJ598cp0x9VlXx44dW+jZs2fhgw8+qGm74YYbCkcddVStNj473wCn3s2ZMyfdunVLkyZNatr69u2bbdu25Zlnnmm4hQHsIpo2bVqnrX379lm/fn02btyY5cuX51//+lf69u1ba0y/fv0yb968miMn5syZk9LS0nTv3r1mTOvWrdO+ffvMmTNnx24CYBcybty4DBgwIIcffnitdvUW4Iv5wx/+kFatWuX444/fbv+WLVsyf/789OnTp1Z7v379snjx4rz11ltJkrlz52bbtm21xjVp0iTdu3dXZ4GvvK1bt2afffZJSUlJTdt+++2XJDWPznVdC/Df2W23/zs+re+6OmfOnJxwwglp3LhxrbkqKyuzcOHC+tjSV44AnHq3ZMmStG7dulZbaWlpmjdv7nwugM/pH//4R1q0aJF99923ppZ+Mqhp06ZNqqura875WrJkSQ4//PBafwQnH11kqccAH5k5c2YWLVqUyy67rE6fegvwxbzwwgtp27ZtbrvttnTr1i1HHnlkBgwYkBdeeCFJsmzZslRXV9f5DOHjx0x+XEOXLFmSZs2aZf/9968zTp0FvurOPPPMLF68OL/97W9TVVWV5cuX58Ybb0yHDh3SpUuXJK5rAepbfdbVjRs3ZuXKlXWuiVu3bp2SkhL193MSgFPvKisrU1paWqd9//33z7p16xpgRQC7tueeey4zZszIoEGDkqSmln6y1n78+uP+ysrKmru+/5N6DPCRTZs2Zfz48Rk+fHj23XffOv3qLcAX895772Xu3LmZPn16rrnmmkycODElJSUZNGhQVq9e/YXrbGlpqToLfOUdddRRufXWW3PDDTfkqKOOyoknnpjVq1dnypQp2X333ZO4rgWob/VZV6uqqrY7V+PGjbPXXnupv5+TABwAdmJvv/12hg8fnq5du2bgwIENvRyAojJp0qQ0a9Ys3/3udxt6KQBFqVAoZOPGjbn55pvTp0+fHH/88Zk0aVIKhUJ+85vfNPTyAIrCggUL8pOf/CT9+/fPvffem5tvvjnbtm3LkCFDsnnz5oZeHgA0CAE49a60tLTmjpX/tG7dujqPKwPg01VWVmbw4MFp0qRJbrnllpqzZz6upZ+stZWVlbX6S0tLs379+jrzqscAyYoVK3L33Xdn2LBhqaqqSmVlZTZu3Jjko8ePbdiwQb0F+IJKS0vTpEmTfPOb36xpa9KkSTp06JA33njjC9fZyspKdRb4yhs3blyOPfbYjBw5Mscee2z69OmTyZMn55VXXsn06dOT+BwBoL7VZ139+Bvin5xry5Yt2bRpk/r7OQnAqXfbOxOmqqoq7733Xp0zDADYvs2bN+fiiy9OVVVV7rzzzlqPyvm4ln6y1i5ZsiSNGjXKwQcfXDNu6dKlKRQKtcYtXbpUPQa+8t56661UV1dnyJAhOfroo3P00UfnkksuSZIMHDgwF1xwgXoL8AWVlZV9at8HH3yQQw45JI0aNdpunU3+97q3devWWbVqVZ3HPy5ZskSdBb7yFi9eXOtGoyQ58MADc8ABB2TZsmVJfI4AUN/qs67uvffe+cY3vlFnro/fp/5+PgJw6l1FRUX++te/1tzpkiQzZ87Mbrvtlu7duzfgygB2DVu3bs0VV1yRJUuW5M4770yLFi1q9R988ME57LDDMnPmzFrtM2bMSLdu3dK4ceMkH9XjdevWZd68eTVjli5dmldeeSUVFRU7fiMAO7H27dvnvvvuq/UzatSoJMl1112Xa665Rr0F+IJ69uyZ999/P6+++mpN29q1a/Pyyy/niCOOSOPGjdO1a9c8/vjjtd43Y8aMtGnTJq1atUqS9OjRI7vttlueeOKJmjHr1q3L3Llz1VngK69ly5Z55ZVXarWtWLEia9euzUEHHZTE5wgA9a2+62pFRUVmzZqV6urqWnOVlpamc+fOO3g3xelrDb0Ais+AAQPy61//OpdddlkuvvjivPPOO5kwYUIGDBhQJ8QBoK7rrrsuTz/9dEaOHJn169fn+eefr+nr0KFDGjdunB/+8If58Y9/nEMOOSRdu3bNjBkz8uKLL9Y6S7Fz587p0aNHrrrqqlx55ZXZY489ctNNN6Vdu3Y56aSTGmBnADuP0tLSdO3adbt9RxxxRI444ogkUW8BvoATTzwxHTt2zLBhwzJ8+PDssccemTx5cho3bpzvf//7SZJLL700AwcOzLXXXpu+fftm/vz5+eMf/5ibbrqpZp4DDzwwZ511ViZMmJDddtstLVq0yB133JH99tsvAwYMaKjtAewUBgwYkP/5n//JuHHj0qtXr7z//vuZNGlSmjVrlr59+9aMc10L8Nlt2rQps2fPTvLRTUXr16+vCbuPOeaYNG3atF7r6oUXXphHHnkkP/rRj/K9730vixYtyl133ZXhw4fXhOn8d0oKn/zePdSDxYsX5/rrr8/ChQuzzz775LTTTvMfFeAz6tWrV1asWLHdvlmzZtV8E+bBBx/MlClT8u9//zuHH354RowYkZ49e9YaX1VVlZ/97Gd58skns3Xr1vTo0SNjxoxxQxLAdsyfPz8DBw7MtGnT0rFjx5p29Rbg81uzZk1+9rOf5emnn051dXWOOuqojBo1qtbj0WfNmpVf/vKXWbp0aVq2bJkhQ4bkrLPOqjXPli1bctNNN2X69OnZsGFDunTpkjFjxqRNmzZf9pYAdiqFQiFTp07N/fffn+XLl2efffZJeXl5hg8fXqdGuq4F+GzeeuutnHDCCdvtu++++2puqK/PurpgwYKMHz8+r776apo2bZpzzz03gwcPTklJyY7ZZJETgAMAAAAAAABQFJwBDgAAAAAAAEBREIADAAAAAAAAUBQE4AAAAAAAAAAUBQE4AAAAAAAAAEVBAA4AAAAAAABAURCAAwAAAAAAAFAUBOAAAAAAAAAAFAUBOAAAAAAAAABFQQAOAAAAAAAAQFH4WkMvAAAAAKjt9ddfz8SJE/PSSy9l1apVadKkScrKytKrV6/84Ac/SJLcfvvtKSsry4knntjAqwUAAICdR0mhUCg09CIAAACAjyxYsCADBw5My5Ytc/rpp6d58+ZZuXJlXnjhhSxbtixPPvlkkqRz587p3bt3xo8f38ArBgAAgJ2Hb4ADAADATuT222/Pfvvtl2nTpqW0tLRW3+rVqxtoVQAAALBrcAY4AAAA7ESWLVuWsrKyOuF3kjRr1ixJ0q5du2zcuDEPPfRQ2rVrl3bt2mXkyJE14955552MGjUqxx13XI488sicfPLJmTZtWq255s+fn3bt2mXGjBm58cYb071795SXl+eSSy7JypUrd+wmAQAAYAfxDXAAAADYiRx00EFZuHBhFi1alLZt2253zIQJEzJmzJh06tQp/fv3T5IccsghSZJVq1alf//+KSkpybnnnpumTZtmzpw5GT16dNavX5/zzz+/1lyTJk1KSUlJBg8enNWrV+fee+/N+eefn+nTp2fPPffcoXsFAACA+uYMcAAAANiJPPPMMxk8eHCSpFOnTvn2t7+dbt26pWvXrmnUqFHNuE87A3z06NGZPXt2HnnkkRxwwAE17SNGjMicOXMyd+7c7Lnnnpk/f34GDhyYFi1aZMaMGdl3332TJI899liuuOKKjB49OgMHDvwSdgwAAAD1xyPQAQAAYCfSvXv3TJ06Nb169cprr72WO++8MxdeeGEqKioya9as//O9hUIhTzzxRHr16pVCoZA1a9bU/PTo0SNVVVV5+eWXa73n9NNPrwm/k6RPnz5p3rx5Zs+evUP2BwAAADuSR6ADAADATqZTp0659dZbs2XLlrz22mv505/+lHvuuSeXX355Hn744ZSVlW33fWvWrEllZWUeeOCBPPDAA5865j8deuihtV6XlJTk0EMPzYoVK+pnMwAAAPAlEoADAADATqpx48bp1KlTOnXqlMMOOyyjRo3KzJkzM3To0O2O37ZtW5Lk1FNPzRlnnLHdMe3atdth6wUAAICGJgAHAACAXcCRRx6ZJHn33Xc/dUzTpk2zzz77ZNu2bTnuuOM+07xvvvlmrdeFQiFvvvmmoBwAAIBdkjPAAQAAYCfyt7/9LYVCoU77x2dyt27dOkmy9957p7KystaY3XffPb17987jjz+eRYsW1Znjk48/T5KHH34469evr3k9c+bMvPfee6moqPhC+wAAAICGUFLY3l/VAAAAQIM45ZRTsmnTpnznO99J69atU11dnQULFuSxxx7LgQcemIcffjilpaUZMmRInn322QwbNixf//rX06pVq3zrW9/KqlWr0r9//6xZsyZnn312ysrKsm7durz88suZN29e/v73vydJ5s+fn4EDB6Zt27YpKSnJmWeemdWrV+fee+/NgQcemOnTp2evvfZq4N8GAAAA/HcE4AAAALATmTNnTmbOnJmFCxfm7bffTnV1dVq2bJmKiopceumladasWZJkyZIlufrqq/PSSy9l8+bNOeOMMzJ+/PgkyerVqzNx4sQ89dRTWbVqVZo0aZKysrL069cv/fv3T/K/AfiNN96Y119/PdOmTcuGDRty7LHH5pprrknLli0b7HcAAAAAn5cAHAAAAL6CPg7Ab7755vTp06ehlwMAAAD1whngAAAAAAAAABQFATgAAAAAAAAARUEADgAAAAAAAEBRcAY4AAAAAAAAAEXBN8ABAAAAAAAAKAoCcAAAAAAAAACKggAcAAAAAAAAgKIgAAcAAAAAAACgKAjAAQAAAAAAACgKAnAAAAAAAAAAioIAHAAAAAAAAICiIAAHAAAAAAAAoCj8PwV/zZpQEtotAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "\n",
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    loss_steps_i = []\n",
        "    loss_steps_d = []\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print(loss)\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "\n",
        "        loss_steps_i.append(criterion(y_model, y.view(-1).long()).detach().item())\n",
        "        loss_steps_d.append(loss.detach().item())\n",
        "\n",
        "\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var < (0.999)) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var < 1.) * 1\n",
        "        grad4insertion = (gradients > 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #print(torch.abs(gradients).sum())\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(((x_next + perturbation * step_length)- torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "        #loss_steps_i.append(criterion(y_model, y.view(-1).long()).detach().item())\n",
        "        #loss_steps_d.append(loss.detach().item())\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next,loss_steps_i,loss_steps_d"
      ],
      "metadata": {
        "id": "ajnP6RALG7KM"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_next,loss_steps_i,loss_steps_d = pgd_min(mals[13:14].to(torch.float32).to(device), mals_y[13:14].to(device), model_AT_rFGSM, insertion_array, removal_array, k=1000, step_length=.002, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e4785b-30d5-4d4c-8e1d-fa10f2e27834",
        "id": "a-WJw2WQH3lH"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Convert the loss data to DataFrames for seaborn\n",
        "df_i = pd.DataFrame({\n",
        "    'Step': range(len(loss_steps_i)),\n",
        "    'Loss': loss_steps_i\n",
        "})\n",
        "\n",
        "df_d = pd.DataFrame({\n",
        "    'Step': range(len(loss_steps_d)),\n",
        "    'Loss': loss_steps_d\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 1, figsize=(20, 25))\n",
        "\n",
        "# Plot loss_steps_i\n",
        "sns.lineplot(data=df_i, x='Step', y='Loss', ax=axes[0], marker='o', color='blue')\n",
        "axes[0].set_title('Loss Steps I')\n",
        "axes[0].set_xlabel('Step')\n",
        "axes[0].set_ylabel('Loss')\n",
        "\n",
        "# Plot loss_steps_d\n",
        "sns.lineplot(data=df_d, x='Step', y='Loss', ax=axes[1], marker='o', color='red')\n",
        "axes[1].set_title('Loss Steps D')\n",
        "axes[1].set_xlabel('Step')\n",
        "axes[1].set_ylabel('Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "667a180e-ab56-4dc3-b450-e92e7596d29c",
        "id": "g-IJgHHsH3lI"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x2500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IlIuS8W7Ltgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYiecbU8LtSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    if (attack ==  gkde) or (attack ==  mimicry):\n",
        "      # Pre-select benign samples\n",
        "      benign_samples = []\n",
        "      for x_batch, y_batch in test_loader:\n",
        "        benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "      ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "      # Forward pass to get logits for benign samples\n",
        "      with torch.no_grad():  # No need for gradients\n",
        "          outputs = model(ben_x.to(torch.float32))\n",
        "\n",
        "      # Calculate softmax probabilities\n",
        "      probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "      # Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "      sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "      # Select the top 500 high confidence benign samples\n",
        "      top_500_high_confidence_benign_samples = ben_x[sorted_indices[:500]]\n",
        "\n",
        "      del benign_samples, outputs, probabilities, ben_x  # Free up memory\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            if attack == mimicry:\n",
        "                pertb_mal_x = mimicry(top_500_high_confidence_benign_samples, mal_x_batch, model, **kwargs)\n",
        "            elif attack == gkde:\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde(mal_x_batch, mal_y_batch, model, top_500_high_confidence_benign_samples, **kwargs)\n",
        "            else :\n",
        "                with torch.enable_grad():\n",
        "                    pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n",
        "\n",
        "\n",
        "\n",
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "\n",
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    loss_steps_i = []\n",
        "    loss_steps_d = []\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print(loss)\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "\n",
        "        loss_steps_i.append(criterion(y_model, y.view(-1).long()).mean().detach().item())\n",
        "        loss_steps_d.append(loss.mean().detach().item())\n",
        "\n",
        "\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var < (0.999)) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var < 1.) * 1\n",
        "        grad4insertion = (gradients > 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #print(torch.abs(gradients).sum())\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(((x_next + perturbation * step_length)- torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "        #loss_steps_i.append(criterion(y_model, y.view(-1).long()).detach().item())\n",
        "        #loss_steps_d.append(loss.detach().item())\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next,loss_steps_i,loss_steps_d\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B2S5U28YIGzk"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "    attack_success_rates = []\n",
        "\n",
        "    for x_test, y_test in test_loader:\n",
        "        x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "        mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "        n_samples += len(mal_y_batch)\n",
        "\n",
        "        outputs = model(mal_x_batch)\n",
        "        predicted = outputs.argmax(1)\n",
        "        cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            pertb_mal_x, _, _ = attack(mal_x_batch, mal_y_batch, model, insertion_array, removal_array, **kwargs)\n",
        "\n",
        "        outputs = model(pertb_mal_x)\n",
        "        y_pred = outputs.argmax(1)\n",
        "        cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "        attack_success_rate = 1 - (cor_ad_test / n_samples)\n",
        "        attack_success_rates.append(attack_success_rate)\n",
        "\n",
        "    return attack_success_rates\n",
        "\n",
        "# Helper function to run the PGD attack and collect data\n",
        "def run_pgd_attack(model, test_loader, device, iterations=50, step_size=0.02):\n",
        "    attack_success_rates = []\n",
        "    for k in range(50, iterations + 1):\n",
        "        print(f\"Running PGD with {k} iterations...\")\n",
        "        attack_rates = adv_predict(test_loader, model, pgd_min, device, k=k, step_length=step_size, norm='linf')\n",
        "        attack_success_rates.append(attack_rates[-1])\n",
        "\n",
        "    return attack_success_rates\n",
        "\n",
        "# Plot the results\n",
        "def plot_attack_success_rates(success_rates):\n",
        "    iterations = np.arange(1, len(success_rates) + 1)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.errorbar(iterations, success_rates, yerr=np.std(success_rates), fmt='-o', ecolor='r', capsize=5)\n",
        "    plt.title('PGD Attack Success Rate vs. Number of Iterations')\n",
        "    plt.xlabel('Number of Iterations')\n",
        "    plt.ylabel('Attack Success Rate')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Assuming you have model, test_loader, and device defined\n",
        "iterations = 50  # Number of iterations for PGD\n",
        "step_size = 0.02  # Step size for PGD\n",
        "attack_success_rates = run_pgd_attack(model_AT_rFGSM, test_loader, device, iterations=iterations, step_size=step_size)\n",
        "plot_attack_success_rates(attack_success_rates)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "vVKJIK0LLvHS",
        "outputId": "b8b02dd6-a786-4a62-cdb4-81d3edcaf75d"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running PGD with 50 iterations...\n",
            "PGD linf: Attack effectiveness 58.261%.\n",
            "PGD linf: Attack effectiveness 54.701%.\n",
            "PGD linf: Attack effectiveness 57.600%.\n",
            "PGD linf: Attack effectiveness 54.369%.\n",
            "PGD linf: Attack effectiveness 59.292%.\n",
            "PGD linf: Attack effectiveness 56.757%.\n",
            "PGD linf: Attack effectiveness 59.259%.\n",
            "PGD linf: Attack effectiveness 52.555%.\n",
            "PGD linf: Attack effectiveness 61.600%.\n",
            "PGD linf: Attack effectiveness 65.306%.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "    attack_success_rates = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            with torch.enable_grad():\n",
        "                pertb_mal_x, _, _ = attack(mal_x_batch, mal_y_batch, model, insertion_array, removal_array, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "    # Calculate success rate as 1 - (correct predictions under attack / total samples)\n",
        "    attack_success_rate = 1 - (cor_ad_test / n_samples)\n",
        "    return attack_success_rate\n",
        "\n",
        "def run_pgd_attack(model, test_loader, device, iterations_list, step_size=0.02):\n",
        "    attack_success_rates = []\n",
        "    for k in iterations_list:\n",
        "        print(f\"Running PGD with {k} iterations...\")\n",
        "        success_rate = adv_predict(test_loader, model, pgd_min, device, k=k, step_length=step_size, norm='linf')\n",
        "        attack_success_rates.append(success_rate)\n",
        "\n",
        "    return attack_success_rates\n",
        "\n",
        "def plot_attack_success_rates(success_rates, iterations_list):\n",
        "    mean_rates = np.mean(success_rates, axis=0)\n",
        "    std_rates = np.std(success_rates, axis=0)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.errorbar(iterations_list, mean_rates, yerr=std_rates, fmt='-o', ecolor='r', capsize=5)\n",
        "    plt.title('PGD Attack Success Rate vs. Number of Iterations')\n",
        "    plt.xlabel('Number of Iterations')\n",
        "    plt.ylabel('Attack Success Rate')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Assuming you have model, test_loader, and device defined\n",
        "iterations_list = list(range(50, 65, 5))  # Specific iteration counts from 50 to 100 with a step of 5\n",
        "step_size = 0.02  # Step size for PGD\n",
        "\n",
        "# Run PGD attack and plot the success rates\n",
        "attack_success_rates = run_pgd_attack(model_AT_rFGSM, test_loader, device, iterations_list, step_size=step_size)\n",
        "plot_attack_success_rates(attack_success_rates, iterations_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hO-wyRSfOgK4",
        "outputId": "93bf9dfd-195a-4548-efcd-235235d10133"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running PGD with 50 iterations...\n",
            "PGD linf: Attack effectiveness 58.261%.\n",
            "PGD linf: Attack effectiveness 54.701%.\n",
            "PGD linf: Attack effectiveness 57.600%.\n",
            "PGD linf: Attack effectiveness 54.369%.\n",
            "PGD linf: Attack effectiveness 59.292%.\n",
            "PGD linf: Attack effectiveness 56.757%.\n",
            "PGD linf: Attack effectiveness 59.259%.\n",
            "PGD linf: Attack effectiveness 52.555%.\n",
            "PGD linf: Attack effectiveness 61.600%.\n",
            "PGD linf: Attack effectiveness 65.306%.\n",
            "Running PGD with 55 iterations...\n",
            "PGD linf: Attack effectiveness 62.609%.\n",
            "PGD linf: Attack effectiveness 61.538%.\n",
            "PGD linf: Attack effectiveness 63.200%.\n",
            "PGD linf: Attack effectiveness 58.252%.\n",
            "PGD linf: Attack effectiveness 63.717%.\n",
            "PGD linf: Attack effectiveness 61.261%.\n",
            "PGD linf: Attack effectiveness 60.741%.\n",
            "PGD linf: Attack effectiveness 57.664%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 67.347%.\n",
            "Running PGD with 60 iterations...\n",
            "PGD linf: Attack effectiveness 62.609%.\n",
            "PGD linf: Attack effectiveness 61.538%.\n",
            "PGD linf: Attack effectiveness 61.600%.\n",
            "PGD linf: Attack effectiveness 59.223%.\n",
            "PGD linf: Attack effectiveness 62.832%.\n",
            "PGD linf: Attack effectiveness 60.360%.\n",
            "PGD linf: Attack effectiveness 60.741%.\n",
            "PGD linf: Attack effectiveness 56.934%.\n",
            "PGD linf: Attack effectiveness 64.800%.\n",
            "PGD linf: Attack effectiveness 67.347%.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "'x' and 'y' must have the same size",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-216-c8659fdea545>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Run PGD attack and plot the success rates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mattack_success_rates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pgd_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_AT_rFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mplot_attack_success_rates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattack_success_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-216-c8659fdea545>\u001b[0m in \u001b[0;36mplot_attack_success_rates\u001b[0;34m(success_rates, iterations_list)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PGD Attack Success Rate vs. Number of Iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36merrorbar\u001b[0;34m(x, y, yerr, xerr, fmt, ecolor, elinewidth, capsize, barsabove, lolims, uplims, xlolims, xuplims, errorevery, capthick, data, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m         \u001b[0muplims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlolims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxuplims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         capthick=None, *, data=None, **kwargs):\n\u001b[0;32m-> 2564\u001b[0;31m     return gca().errorbar(\n\u001b[0m\u001b[1;32m   2565\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m         \u001b[0melinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melinewidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcapsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbarsabove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbarsabove\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36merrorbar\u001b[0;34m(self, x, y, yerr, xerr, fmt, ecolor, elinewidth, capsize, barsabove, lolims, uplims, xlolims, xuplims, errorevery, capthick, **kwargs)\u001b[0m\n\u001b[1;32m   3535\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make sure all the args are iterable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3537\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'x' and 'y' must have the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3539\u001b[0m         \u001b[0meverymask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_errorevery_to_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorevery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'x' and 'y' must have the same size"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0wAAAG3CAYAAACHV9VEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjgUlEQVR4nO3df2zV9b348VcLKxTwHIIB/LW7riwwtgSRzWHt0nBNyNZpQuZQukluUaNdbqdecPfKFsOFoF5W53ZHdcNfBCR3EG/M7ox3EFluZgO9M1uc2y5/GKTULXIvMMTTQgot7fn+sdivXfcGTuk5gjweCcn69rw/fR/zGubZ8zmnZfl8Ph8AAAAMU/5BHwAAAOB8JZgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQUHAwvfXWW7Fq1apYtGhRfOpTn4qbbrrprPbl8/l46qmnYsGCBTFnzpxYsmRJvP7664V+ewAAgJIpOJj27t0br7zySnzsYx+LGTNmnPW+p59+OtavXx/Lli2LJ598MqZOnRp33HFH/PGPfyz0CAAAACVRls/n84VsGBgYiPLyP3fWypUr43/+53/ipZdeOu2ekydPxvXXXx+33XZbrFixIiIient744tf/GLU1dXF6tWrR3Z6AACAIir4Fab3YqkQr732Whw7dizq6+sH1yoqKmLhwoXR1tZW8PUAAABKoSQf+tDR0REREdXV1UPWZ8yYEQcOHIgTJ06U4hgAAAAFGVuKb9LV1RUVFRUxbty4IeuZTCby+XzkcrkYP358Qdf8zW9+E/l8Pj7ykY+M5lEBAIALTF9fX5SVlcU111wz6tcuSTAVQz6fj3w+H729vR/0UQAAgA+pkgRTJpOJ3t7eOHny5JBXmbq6uqKsrCyy2WzB1/zIRz4Svb29UVVVFZWVlaN5XBiip6cnOjs7zRpFZ9YoFbNGqZg1SmXv3r0j+qyFs1GSYHrvvUv79++PT37yk4PrHR0dccUVVxR8O977VVZWxoQJE875jHAmZo1SMWuUilmjVMwaxVZWVla0a5fkQx/mzZsXkyZNiu3btw+u9fX1xcsvvxx1dXWlOAIAAEDBCn6FqaenJ1555ZWIiHj77bfj2LFjsWPHjoiI+NznPhdTpkyJxsbGOHDgQOzcuTMiIsaNGxdNTU3R2toaU6ZMiZkzZ8bWrVvj3XffjTvvvHMUnw4AAMDoKTiYjhw5Evfdd9+Qtfe+fu6552L+/PkxMDAQ/f39Qx5z1113RT6fj40bN8Y777wTs2fPjmeffTY++tGPnsPxAQAAiqfgYLrqqqvijTfeOO1jtmzZMmytrKwsmpqaoqmpqdBvCQAA8IEoyXuYAAAALkSCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgoOJj27dsXt99+e8ydOzdqa2ujpaUlent7z7jv6NGjsWrVqliwYEHMnTs3brrppti6deuIDg0AAFAKYwt5cC6Xi8bGxqiqqorW1tY4ePBgrFu3Lk6cOBGrVq067d777rsvOjo6YsWKFXH55ZdHW1tbrF69OsaMGRO33nrrOT0JAACAYigomLZt2xbHjx+Pxx9/PCZPnhwREf39/bFmzZpoamqK6dOn/9V9hw8fjldffTX+5V/+JW6++eaIiKipqYnf//738Z//+Z+CCQAAOC8VdEteW1tb1NTUDMZSRER9fX0MDAzE7t27k/tOnToVERGXXHLJkPVJkyZFPp8v5AgAAAAlU1AwdXR0RHV19ZC1TCYTU6dOjY6OjuS+yy+/PD7/+c/Hhg0b4s0334xjx47Fz372s9i9e3fcdtttIzs5AABAkRV0S15XV1dkMplh69lsNnK53Gn3tra2xvLly+PGG2+MiIgxY8bEgw8+GF/4whcKOcIwPT0957QfzuS9GTNrFJtZo1TMGqVi1iiVfD4fZWVlRbl2QcE0Uvl8Pr71rW9FZ2dnPPbYYzF16tRob2+PRx55JLLZ7GBEjURnZ+foHRROw6xRKmaNUjFrlIpZoxQqKiqKct2CgimTyUR3d/ew9VwuF9lsNrnvF7/4RezYsSNefPHFmDVrVkREzJ8/P44cORLr1q07p2CqqqqKysrKEe+HM+np6YnOzk6zRtGZNUrFrFEqZo1S2bt3b9GuXVAwVVdXD3uvUnd3dxw+fHjYe5ve780334wxY8bEzJkzh6zPnj07/v3f/z16enpG/H+iysrKmDBhwoj2QiHMGqVi1igVs0apmDWKrVi340UU+KEPdXV10d7eHl1dXYNrO3bsiPLy8qitrU3uu/LKK6O/vz/eeOONIet79uyJSy+91E8cAACA81JBwdTQ0BATJ06M5ubm2LVrV7zwwgvR0tISDQ0NQ34HU2NjYyxcuHDw67q6urjiiivi3nvvjZ/+9Kfx3//93/Hoo4/GT37yk1i6dOnoPRsAAIBRVNAtedlsNjZv3hxr166N5ubmmDhxYixevDiWL18+5HEDAwPR398/+PWkSZNi06ZN8f3vfz+++93vRnd3d1x11VWxcuVKwQQAAJy3Cv6UvBkzZsSmTZtO+5gtW7YMW/vYxz4W//qv/1rotwMAAPjAFHRLHgAAwMVEMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJBQfTvn374vbbb4+5c+dGbW1ttLS0RG9v71ntPXjwYDzwwANx3XXXxZw5c6K+vj5efPHFgg8NAABQCmMLeXAul4vGxsaoqqqK1tbWOHjwYKxbty5OnDgRq1atOu3eQ4cOxZIlS+LjH/94rF27NiZNmhR79+4969gCAAAotYKCadu2bXH8+PF4/PHHY/LkyRER0d/fH2vWrImmpqaYPn16cu+jjz4al112WTzzzDMxZsyYiIioqakZ+ckBAACKrKBb8tra2qKmpmYwliIi6uvrY2BgIHbv3p3cd+zYsdi+fXt87WtfG4wlAACA811BwdTR0RHV1dVD1jKZTEydOjU6OjqS+/bs2RN9fX0xduzYWLp0aXz605+O2traePTRR6Ovr29kJwcAACiygm7J6+rqikwmM2w9m81GLpdL7vvTn/4UEREPPvhg3HrrrfGNb3wjfve738X69eujvLw87r///gKP/f/19PSMeC+cjfdmzKxRbGaNUjFrlIpZo1Ty+XyUlZUV5doFBdNIDQwMRETE9ddfHytXroyIiOuuuy6OHz8eGzdujObm5hg/fvyIrt3Z2Tlax4TTMmuUilmjVMwapWLWKIWKioqiXLegYMpkMtHd3T1sPZfLRTabPe2+iD9H0vvV1NTEhg0b4q233opZs2YVcpRBVVVVUVlZOaK9cDZ6enqis7PTrFF0Zo1SMWuUilmjVPbu3Vu0axcUTNXV1cPeq9Td3R2HDx8e9t6m9/vEJz5x2uuePHmykGMMUVlZGRMmTBjxfjhbZo1SMWuUilmjVMwaxVas2/EiCvzQh7q6umhvb4+urq7BtR07dkR5eXnU1tYm91155ZUxc+bMaG9vH7Le3t4e48ePP2NQAQAAfBAKCqaGhoaYOHFiNDc3x65du+KFF16IlpaWaGhoGPI7mBobG2PhwoVD9i5fvjz+67/+Kx5++OHYvXt3bNiwITZu3BjLli3zEwcAAOC8VNAtedlsNjZv3hxr166N5ubmmDhxYixevDiWL18+5HEDAwPR398/ZO2GG26I733ve/HDH/4wtm7dGtOmTYt77rkn7r777nN/FgAAAEVQ8KfkzZgxIzZt2nTax2zZsuWvrn/pS1+KL33pS4V+SwAAgA9EQbfkAQAAXEwEEwAAQIJgAgAASBBMAAAACYIJAAAgQTABAAAkCCYAAIAEwQQAAJAgmAAAABIEEwAAQIJgAgAASBBMAAAACYIJAAAgQTABAAAkCCYAAIAEwQQAAJAgmAAAABIEEwAAQIJgAgAASBBMAAAACYIJAAAgQTABAAAkCCYAAIAEwQQAAJAgmAAAABIEEwAAQIJgAgAASBBMAAAACYIJAAAgQTABAAAkCCYAAIAEwQQAAJAgmAAAABIEEwAAQIJgAgAASBBMAAAACYIJAAAgQTABAAAkCCYAAIAEwQQAAJAgmAAAABIEEwAAQIJgAgAASBBMAAAACYIJAAAgQTABAAAkCCYAAIAEwQQAAJAgmAAAABIEEwAAQIJgAgAASBBMAAAACYIJAAAgQTABAAAkCCYAAIAEwQQAAJAgmAAAABIEEwAAQIJgAgAASBBMAAAACYIJAAAgQTABAAAkCCYAAIAEwQQAAJBQcDDt27cvbr/99pg7d27U1tZGS0tL9Pb2FnSNTZs2xaxZs6KpqanQbw8AAFAyYwt5cC6Xi8bGxqiqqorW1tY4ePBgrFu3Lk6cOBGrVq06q2scPnw4nnjiibj00ktHdGAAAIBSKSiYtm3bFsePH4/HH388Jk+eHBER/f39sWbNmmhqaorp06ef8RqPPvpo3HDDDXHgwIERHRgAAKBUCrolr62tLWpqagZjKSKivr4+BgYGYvfu3Wfc/+tf/zp+/vOfx/3331/wQQEAAEqtoGDq6OiI6urqIWuZTCamTp0aHR0dp93b398fa9euja9//esxbdq0wk8KAABQYgXdktfV1RWZTGbYejabjVwud9q9P/7xj6OnpyeWLVtW0AHPpKenZ1SvB3/pvRkzaxSbWaNUzBqlYtYolXw+H2VlZUW5dkHBNFJHjhyJ9evXx3e+852oqKgY1Wt3dnaO6vUgxaxRKmaNUjFrlIpZoxRGuzPeU1AwZTKZ6O7uHraey+Uim80m9/3gBz+IWbNmxWc/+9no6uqKiIhTp07FqVOnoqurKyZMmBBjx46s3aqqqqKysnJEe+Fs9PT0RGdnp1mj6MwapWLWKBWzRqns3bu3aNcuqFKqq6uHvVepu7s7Dh8+POy9Te+3f//++NWvfhXXXnvtsH927bXXxtNPPx11dXWFHGVQZWVlTJgwYUR7oRBmjVIxa5SKWaNUzBrFVqzb8SIKDKa6urrYsGHDkPcy7dixI8rLy6O2tja579vf/vbgK0vveeSRR2L8+PGxYsWKmDVr1giODgAAUFwFBVNDQ0Ns2bIlmpubo6mpKQ4ePBgtLS3R0NAw5HcwNTY2xoEDB2Lnzp0RETF79uxh18pkMjFhwoSYP3/+OT4FAACA4ijoY8Wz2Wxs3rw5xowZE83NzfHYY4/F4sWLY+XKlUMeNzAwEP39/aN6UAAAgFIr+JMWZsyYEZs2bTrtY7Zs2XLG65zNYwAAAD5IBb3CBAAAcDERTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAECCYAIAAEgQTAAAAAmCCQAAIEEwAQAAJAgmAACABMEEAACQIJgAAAASBBMAAEDC2EI37Nu3Lx566KH4zW9+ExMnToxFixbFP/zDP0RFRUVyz6FDh2LTpk2xe/fu+MMf/hCXXHJJXHvttbFixYq48sorz+kJAAAAFEtBwZTL5aKxsTGqqqqitbU1Dh48GOvWrYsTJ07EqlWrkvv27NkTO3fujK985Stx9dVXx9GjR+NHP/pR3HLLLfHSSy/FlClTzvmJAAAAjLaCgmnbtm1x/PjxePzxx2Py5MkREdHf3x9r1qyJpqammD59+l/d95nPfCa2b98eY8f+/283b968WLBgQfzHf/xH3HHHHSN/BgAAAEVS0HuY2traoqamZjCWIiLq6+tjYGAgdu/endyXyWSGxFJExGWXXRZTpkyJQ4cOFXZiAACAEikomDo6OqK6unrIWiaTialTp0ZHR0dB33j//v1x5MiRmDFjRkH7AAAASqWgW/K6uroik8kMW89ms5HL5c76Ovl8Ph566KGYNm1a3HjjjYUcYZienp5z2g9n8t6MmTWKzaxRKmaNUjFrlEo+n4+ysrKiXLvgT8kbDa2trfHLX/4ynnnmmZgwYcI5Xauzs3N0DgVnYNYoFbNGqZg1SsWsUQqn+9Tuc1FQMGUymeju7h62nsvlIpvNntU1nn/++XjiiSfi4YcfjpqamkK+/V9VVVUVlZWV53wdSOnp6YnOzk6zRtGZNUrFrFEqZo1S2bt3b9GuXVAwVVdXD3uvUnd3dxw+fHjYe5v+mp07d8bq1avj3nvvjcWLFxd20oTKyspzfpUKzoZZo1TMGqVi1igVs0axFet2vIgCP/Shrq4u2tvbo6ura3Btx44dUV5eHrW1tafd++qrr8aKFSvilltuiebm5pGdFgAAoIQKCqaGhoaYOHFiNDc3x65du+KFF16IlpaWaGhoGPI7mBobG2PhwoWDX+/bty+am5ujqqoqFi1aFK+//vrgnz/84Q+j92wAAABGUUG35GWz2di8eXOsXbs2mpubY+LEibF48eJYvnz5kMcNDAxEf3//4Ne//e1vo7u7O7q7u+OrX/3qkMd++ctfjnXr1p3DUwAAACiOgj8lb8aMGbFp06bTPmbLli1Dvr755pvj5ptvLvRbAQAAfKAKuiUPAADgYiKYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgATBBAAAkCCYAAAAEgQTAABAgmACAABIEEwAAAAJggkAACBBMAEAACQIJgAAgISCg2nfvn1x++23x9y5c6O2tjZaWlqit7f3jPvy+Xw89dRTsWDBgpgzZ04sWbIkXn/99ZGcGQAAoCQKCqZcLheNjY3R19cXra2tsXz58nj++edj3bp1Z9z79NNPx/r162PZsmXx5JNPxtSpU+OOO+6IP/7xjyM+PAAAQDGNLeTB27Zti+PHj8fjjz8ekydPjoiI/v7+WLNmTTQ1NcX06dP/6r6TJ0/Gk08+GXfccUcsW7YsIiI+85nPxBe/+MV49tlnY/Xq1efyHAAAAIqioFeY2traoqamZjCWIiLq6+tjYGAgdu/endz32muvxbFjx6K+vn5wraKiIhYuXBhtbW2FnxoAAKAECgqmjo6OqK6uHrKWyWRi6tSp0dHRcdp9ETFs74wZM+LAgQNx4sSJQo4BAABQEgXdktfV1RWZTGbYejabjVwud9p9FRUVMW7cuCHrmUwm8vl85HK5GD9+fCFHib6+voiI2Lt3b5SVlRW0FwqRz+cjwqxRfGaNUjFrlIpZo1T6+vqKNmMFBdP55L1/IeXlPhmd4iorK4uKiooP+hhcBMwapWLWKBWzRqmUlZWdH8GUyWSiu7t72Houl4tsNnvafb29vXHy5MkhrzJ1dXVFWVnZafemXHPNNQXvAQAAKERBL89UV1cPe69Sd3d3HD58eNj7k/5yX0TE/v37h6x3dHTEFVdcUfDteAAAAKVQUDDV1dVFe3t7dHV1Da7t2LEjysvLo7a2Nrlv3rx5MWnSpNi+ffvgWl9fX7z88stRV1c3gmMDAAAUX0G35DU0NMSWLVuiubk5mpqa4uDBg9HS0hINDQ1DfgdTY2NjHDhwIHbu3BkREePGjYumpqZobW2NKVOmxMyZM2Pr1q3x7rvvxp133jm6zwgAAGCUFBRM2Ww2Nm/eHGvXro3m5uaYOHFiLF68OJYvXz7kcQMDA9Hf3z9k7a677op8Ph8bN26Md955J2bPnh3PPvtsfPSjHz33ZwEAAFAEZfn3Pu8RAACAIXwmNwAAQIJgAgAASBBMAAAACYIJAAAgQTABAAAkCCYAAICE8zKY9u3bF7fffnvMnTs3amtro6WlJXp7e8+4L5/Px1NPPRULFiyIOXPmxJIlS+L1118v/oG5YI1k1g4dOhQtLS2xaNGiuOaaa6Kuri7uv//+ePvtt0t0ai5EI/177f02bdoUs2bNiqampiKdkg+Dc5m1gwcPxgMPPBDXXXddzJkzJ+rr6+PFF18s8om5UI101o4ePRqrVq2KBQsWxNy5c+Omm26KrVu3luDEXKjeeuutWLVqVSxatCg+9alPxU033XRW+0arDQr6xbWlkMvlorGxMaqqqqK1tTUOHjwY69atixMnTsSqVatOu/fpp5+O9evXxze/+c2YNWtW/Nu//Vvccccd8dOf/tQvyGWYkc7anj17YufOnfGVr3wlrr766jh69Gj86Ec/iltuuSVeeumlmDJlSgmfBReCc/l77T2HDx+OJ554Ii699NIin5YL2bnM2qFDh2LJkiXx8Y9/PNauXRuTJk2KvXv3Fhz2XBzOZdbuu+++6OjoiBUrVsTll18ebW1tsXr16hgzZkzceuutJXoGXEj27t0br7zySlx99dUxMDAQZ/trZEetDfLnmQ0bNuTnzp2bP3r06ODatm3b8rNnz87/3//9X3LfiRMn8vPmzcs/9thjg2snT57M/+3f/m3+n//5n4t4Yi5UI521XC6X7+vrG7L2v//7v/lZs2bln3322WIdlwvYSGft/f7xH/8x/0//9E/5pUuX5u++++4inZQL3bnM2je/+c38kiVL8qdOnSryKfkwGOmsHTp0KD9z5sz8Cy+8MGT9tttuy//d3/1dsY7LBa6/v3/wfz/wwAP5G2+88Yx7RrMNzrtb8tra2qKmpiYmT548uFZfXx8DAwOxe/fu5L7XXnstjh07FvX19YNrFRUVsXDhwmhrayvmkblAjXTWMplMjB079MXZyy67LKZMmRKHDh0q1nG5gI101t7z61//On7+85/H/fffX8RT8mEw0lk7duxYbN++Pb72ta/FmDFjSnBSLnQjnbVTp05FRMQll1wyZH3SpEln/aoBF5/y8sKTZTTb4LwLpo6Ojqiurh6ylslkYurUqdHR0XHafRExbO+MGTPiwIEDceLEidE/LBe0kc7aX7N///44cuRIzJgxYzSPyIfEucxaf39/rF27Nr7+9a/HtGnTinlMPgRGOmt79uyJvr6+GDt2bCxdujQ+/elPR21tbTz66KPR19dX7GNzARrprF1++eXx+c9/PjZs2BBvvvlmHDt2LH72s5/F7t2747bbbiv2sbmIjGYbnHfvYerq6opMJjNsPZvNRi6XO+2+ioqKGDdu3JD1TCYT+Xw+crlcjB8/ftTPy4VrpLP2l/L5fDz00EMxbdq0uPHGG0fziHxInMus/fjHP46enp5YtmxZkU7Hh8lIZ+1Pf/pTREQ8+OCDceutt8Y3vvGN+N3vfhfr16+P8vJyr24yzLn8vdba2hrLly8f/G/mmDFj4sEHH4wvfOELRTkrF6fRbIPzLpjgQtPa2hq//OUv45lnnokJEyZ80MfhQ+TIkSOxfv36+M53vhMVFRUf9HH4EBsYGIiIiOuvvz5WrlwZERHXXXddHD9+PDZu3BjNzc1+6MioyOfz8a1vfSs6Ozvjsccei6lTp0Z7e3s88sgjkc1m/eCR89J5F0yZTCa6u7uHredyuchms6fd19vbGydPnhxSkl1dXVFWVnbavVycRjpr7/f888/HE088EQ8//HDU1NSM9hH5kBjprP3gBz+IWbNmxWc/+9no6uqKiD/f/3/q1Kno6uqKCRMmDHs/HRe3c/lvaMSfI+n9ampqYsOGDfHWW2/FrFmzRvewXNBGOmu/+MUvYseOHfHiiy8OztT8+fPjyJEjsW7dOsHEqBnNNjjv3sNUXV097N7X7u7uOHz48LB7EP9yX8Sf30vyfh0dHXHFFVf4yRjDjHTW3rNz585YvXp13HvvvbF48eJiHZMPgZHO2v79++NXv/pVXHvttYN/Xnvttdi1a1dce+210d7eXuyjc4EZ6ax94hOfOO11T548OSrn48NjpLP25ptvxpgxY2LmzJlD1mfPnh2HDh2Knp6eopyXi89otsF5F0x1dXXR3t4++NPUiIgdO3ZEeXl51NbWJvfNmzcvJk2aFNu3bx9c6+vri5dffjnq6uqKemYuTCOdtYiIV199NVasWBG33HJLNDc3F/uoXOBGOmvf/va347nnnhvy55Of/GTMnTs3nnvuuZgzZ04pjs8FZKSzduWVV8bMmTOHRXh7e3uMHz/+jEHFxedcZq2/vz/eeOONIet79uyJSy+9NCorK4t2Zi4uo9kG5929HA0NDbFly5Zobm6OpqamOHjwYLS0tERDQ0NMnz598HGNjY1x4MCB2LlzZ0REjBs3LpqamqK1tTWmTJkSM2fOjK1bt8a7774bd9555wf1dDiPjXTW9u3bF83NzVFVVRWLFi0a8hujp0yZEn/zN39T6qfCeW6kszZ79uxh18pkMjFhwoSYP39+yc7PhWOksxYRsXz58vj7v//7ePjhh2PBggXx+9//PjZu3Bh33nmn92cyzEhnra6uLq644oq49957o7m5OaZNmxa7du2Kn/zkJ3HPPfd8UE+H81xPT0+88sorERHx9ttvx7Fjx2LHjh0REfG5z30upkyZUtQ2OO+CKZvNxubNm2Pt2rXR3NwcEydOjMWLF8fy5cuHPG5gYCD6+/uHrN11112Rz+dj48aN8c4778Ts2bPj2WefLew3+XLRGOms/fa3v43u7u7o7u6Or371q0Me++UvfznWrVtXkvNz4TiXv9egEOcyazfccEN873vfix/+8IexdevWmDZtWtxzzz1x9913l/IpcIEY6axNmjQpNm3aFN///vfju9/9bnR3d8dVV10VK1eujKVLl5b6aXCBOHLkSNx3331D1t77+rnnnov58+cXtQ3K8n5LGAAAwF913r2HCQAA4HwhmAAAABIEEwAAQIJgAgAASBBMAAAACYIJAAAgQTABAAAkCCYAAIAEwQQAAJAgmAAAABIEEwAAQIJgAgAASPh/iwhyhvSm+EgAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            with torch.enable_grad():\n",
        "                pertb_mal_x, _, _ = attack(mal_x_batch, mal_y_batch, model, insertion_array, removal_array, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "    # Calculate success rate as 1 - (correct predictions under attack / total samples)\n",
        "    attack_success_rate = 1 - (cor_ad_test / n_samples)\n",
        "    return attack_success_rate\n",
        "\n",
        "def run_pgd_attack(model, test_loader, device, iterations_list, step_size=0.02):\n",
        "    attack_success_rates = []\n",
        "    for k in iterations_list:\n",
        "        print(f\"Running PGD with {k} iterations...\")\n",
        "        success_rate = adv_predict(test_loader, model, pgd_min, device, k=k, step_length=step_size, norm='linf')\n",
        "        attack_success_rates.append(success_rate)\n",
        "\n",
        "    return attack_success_rates\n",
        "\n",
        "def plot_attack_success_rates(success_rates, iterations_list):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(iterations_list, success_rates, '-o', color='blue')\n",
        "    plt.title('PGD Attack Success Rate vs. Number of Iterations')\n",
        "    plt.xlabel('Number of Iterations')\n",
        "    plt.ylabel('Attack Success Rate')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Assuming you have model, test_loader, and device defined\n",
        "iterations_list = list(range(50, 105, 5))  # Specific iteration counts from 50 to 100 with a step of 5\n",
        "step_size = 0.02  # Step size for PGD\n",
        "\n",
        "# Run PGD attack and plot the success rates\n",
        "attack_success_rates = run_pgd_attack(model_AT_rFGSM, test_loader, device, iterations_list, step_size=step_size)\n",
        "plot_attack_success_rates(attack_success_rates, iterations_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2idEW3lRmzl",
        "outputId": "4207a1af-e8e1-4815-eed4-6210c4f69901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running PGD with 50 iterations...\n",
            "PGD linf: Attack effectiveness 58.261%.\n",
            "PGD linf: Attack effectiveness 54.701%.\n",
            "PGD linf: Attack effectiveness 57.600%.\n",
            "PGD linf: Attack effectiveness 54.369%.\n",
            "PGD linf: Attack effectiveness 59.292%.\n",
            "PGD linf: Attack effectiveness 56.757%.\n",
            "PGD linf: Attack effectiveness 59.259%.\n",
            "PGD linf: Attack effectiveness 52.555%.\n",
            "PGD linf: Attack effectiveness 61.600%.\n",
            "PGD linf: Attack effectiveness 65.306%.\n",
            "Running PGD with 55 iterations...\n",
            "PGD linf: Attack effectiveness 62.609%.\n",
            "PGD linf: Attack effectiveness 61.538%.\n",
            "PGD linf: Attack effectiveness 63.200%.\n",
            "PGD linf: Attack effectiveness 58.252%.\n",
            "PGD linf: Attack effectiveness 63.717%.\n",
            "PGD linf: Attack effectiveness 61.261%.\n",
            "PGD linf: Attack effectiveness 60.741%.\n",
            "PGD linf: Attack effectiveness 57.664%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 67.347%.\n",
            "Running PGD with 60 iterations...\n",
            "PGD linf: Attack effectiveness 62.609%.\n",
            "PGD linf: Attack effectiveness 61.538%.\n",
            "PGD linf: Attack effectiveness 61.600%.\n",
            "PGD linf: Attack effectiveness 59.223%.\n",
            "PGD linf: Attack effectiveness 62.832%.\n",
            "PGD linf: Attack effectiveness 60.360%.\n",
            "PGD linf: Attack effectiveness 60.741%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4pf_wwVRxMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# validating updated insertion and removal array\n",
        "wirh considering insertion array on the run\n",
        "\n",
        "\n",
        "```\n",
        "pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "E5Dpa5hpuLu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(torch.abs(x_next - torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "bJasPS-vuzPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = mals[13:14].to(torch.float32).to(device)\n",
        "y = mals_y[13:14].to(device)\n",
        "adv = pgd_min(x, y, model_AT_rFGSM, insertion_array, removal_array, k=1000, step_length=.01, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e869a9d3-a196-4e72-d54e-8389a4f4c3d0",
        "id": "QI_tKs-1oNBs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 100.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand insertion_array and removal_array to match the batch size\n",
        "expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "# Update insertion and removal arrays based on input x\n",
        "insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))"
      ],
      "metadata": {
        "id": "5xTLkqYbrAHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.sum())\n",
        "print(adv.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j59hRfeaoo1W",
        "outputId": "a5940c9d-97b8-4c28-b67d-6e2992750db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(14., device='cuda:0')\n",
            "tensor(42., device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "insert = ((adv-x) > 0)\n",
        "remove =  ((adv-x) < 0)\n",
        "same = (adv != x)\n",
        "print(insert.sum())\n",
        "print(remove.sum())\n",
        "print(same.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8HRqvvFoysS",
        "outputId": "ac618f87-64de-4d4c-dfe5-5c8672bf1698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(31, device='cuda:0')\n",
            "tensor(3, device='cuda:0')\n",
            "tensor(34, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(insert * insertion_array_updated).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deR6NC6jqzGu",
        "outputId": "c411dd1f-6c8e-43c3-da11-826cd3045c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(31, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(remove * removal_array_updated).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Am53CKBrKg8",
        "outputId": "291fe368-cd9b-4c91-f6b8-32f0c8111e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OhpF6QCUsn0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2K2YsavZtVAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = mals[3:4].to(torch.float32).to(device)\n",
        "y = mals_y[3:4].to(device)\n",
        "adv = pgd_min(x, y, model_AT_rFGSM, insertion_array, removal_array, k=1000, step_length=.01, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7044388-9d7c-4431-b1f9-d009ddaef69c",
        "id": "CfU7y5qitYVh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 0.000%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand insertion_array and removal_array to match the batch size\n",
        "expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "# Update insertion and removal arrays based on input x\n",
        "insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))"
      ],
      "metadata": {
        "id": "4dhqP9iJtYWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.sum())\n",
        "print(adv.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83877cad-2f9c-462b-b560-71f9042992e6",
        "id": "QdkfvZxetYWA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(128., device='cuda:0')\n",
            "tensor(93., device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "insert = ((adv-x) > 0)\n",
        "remove =  ((adv-x) < 0)\n",
        "same = (adv != x)\n",
        "print(insert.sum())\n",
        "print(remove.sum())\n",
        "print(same.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d750612-1eda-479f-f66d-16327a48e08d",
        "id": "xO-SInkGtYWA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(29, device='cuda:0')\n",
            "tensor(64, device='cuda:0')\n",
            "tensor(93, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(insert * insertion_array_updated).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c18f1f-eb60-47f7-e52a-43570fd77360",
        "id": "FM7EBlUXtYWA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(29, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(remove * removal_array_updated).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f68c97-725c-4bbe-d947-b970a29da642",
        "id": "cQCN8JCztYWB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(64, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 1000, 'step_length': 0.01, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu4Q10JJtoPD",
        "outputId": "52015930-78c6-44cf-c5e3-ae380ce9ed80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 53.846%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 76.471%.\n",
            "PGD linf: Attack effectiveness 69.697%.\n",
            "PGD linf: Attack effectiveness 73.810%.\n",
            "PGD linf: Attack effectiveness 65.517%.\n",
            "PGD linf: Attack effectiveness 90.000%.\n",
            "PGD linf: Attack effectiveness 67.647%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 85.185%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 80.645%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 84.000%.\n",
            "PGD linf: Attack effectiveness 77.419%.\n",
            "PGD linf: Attack effectiveness 77.778%.\n",
            "PGD linf: Attack effectiveness 76.667%.\n",
            "PGD linf: Attack effectiveness 90.000%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 77.273%.\n",
            "PGD linf: Attack effectiveness 65.385%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 75.676%.\n",
            "PGD linf: Attack effectiveness 62.500%.\n",
            "PGD linf: Attack effectiveness 71.875%.\n",
            "PGD linf: Attack effectiveness 77.778%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 81.579%.\n",
            "PGD linf: Attack effectiveness 77.419%.\n",
            "PGD linf: Attack effectiveness 78.571%.\n",
            "PGD linf: Attack effectiveness 82.143%.\n",
            "PGD linf: Attack effectiveness 82.353%.\n",
            "PGD linf: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.84%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlcFrE-At2Es",
        "outputId": "7e575a64-f991-4b96-d0f5-11f540ee7b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 68.966%.\n",
            "PGD linf: Attack effectiveness 52.000%.\n",
            "PGD linf: Attack effectiveness 38.462%.\n",
            "PGD linf: Attack effectiveness 68.571%.\n",
            "PGD linf: Attack effectiveness 56.000%.\n",
            "PGD linf: Attack effectiveness 48.000%.\n",
            "PGD linf: Attack effectiveness 55.882%.\n",
            "PGD linf: Attack effectiveness 57.576%.\n",
            "PGD linf: Attack effectiveness 57.143%.\n",
            "PGD linf: Attack effectiveness 58.621%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 35.000%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 48.000%.\n",
            "PGD linf: Attack effectiveness 58.065%.\n",
            "PGD linf: Attack effectiveness 53.333%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 54.839%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 63.333%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 36.364%.\n",
            "PGD linf: Attack effectiveness 51.724%.\n",
            "PGD linf: Attack effectiveness 54.545%.\n",
            "PGD linf: Attack effectiveness 53.846%.\n",
            "PGD linf: Attack effectiveness 71.429%.\n",
            "PGD linf: Attack effectiveness 59.459%.\n",
            "PGD linf: Attack effectiveness 37.500%.\n",
            "PGD linf: Attack effectiveness 53.125%.\n",
            "PGD linf: Attack effectiveness 63.889%.\n",
            "PGD linf: Attack effectiveness 54.054%.\n",
            "PGD linf: Attack effectiveness 65.789%.\n",
            "PGD linf: Attack effectiveness 51.613%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 53.571%.\n",
            "PGD linf: Attack effectiveness 79.412%.\n",
            "PGD linf: Attack effectiveness 33.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.48%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11GKRtxxvDS7",
        "outputId": "6ba75f1a-a08d-44c4-bfc0-82609ca77e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l2: Attack effectiveness 72.414%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 53.846%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 68.000%.\n",
            "PGD l2: Attack effectiveness 76.471%.\n",
            "PGD l2: Attack effectiveness 72.727%.\n",
            "PGD l2: Attack effectiveness 71.429%.\n",
            "PGD l2: Attack effectiveness 68.966%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 64.706%.\n",
            "PGD l2: Attack effectiveness 65.000%.\n",
            "PGD l2: Attack effectiveness 81.481%.\n",
            "PGD l2: Attack effectiveness 68.000%.\n",
            "PGD l2: Attack effectiveness 83.871%.\n",
            "PGD l2: Attack effectiveness 70.000%.\n",
            "PGD l2: Attack effectiveness 92.000%.\n",
            "PGD l2: Attack effectiveness 77.419%.\n",
            "PGD l2: Attack effectiveness 77.778%.\n",
            "PGD l2: Attack effectiveness 80.000%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 50.000%.\n",
            "PGD l2: Attack effectiveness 68.966%.\n",
            "PGD l2: Attack effectiveness 72.727%.\n",
            "PGD l2: Attack effectiveness 65.385%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 75.676%.\n",
            "PGD l2: Attack effectiveness 62.500%.\n",
            "PGD l2: Attack effectiveness 75.000%.\n",
            "PGD l2: Attack effectiveness 80.556%.\n",
            "PGD l2: Attack effectiveness 72.973%.\n",
            "PGD l2: Attack effectiveness 81.579%.\n",
            "PGD l2: Attack effectiveness 74.194%.\n",
            "PGD l2: Attack effectiveness 78.571%.\n",
            "PGD l2: Attack effectiveness 82.143%.\n",
            "PGD l2: Attack effectiveness 82.353%.\n",
            "PGD l2: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.4%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQlv0R8cy8ei",
        "outputId": "3b586d1b-bb6f-4ca6-e86b-68e7b14c180d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 46.154%.\n",
            "PGD l1: Attack effectiveness 77.143%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 70.588%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 85.000%.\n",
            "PGD l1: Attack effectiveness 61.765%.\n",
            "PGD l1: Attack effectiveness 55.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 64.516%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 76.667%.\n",
            "PGD l1: Attack effectiveness 45.455%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 63.636%.\n",
            "PGD l1: Attack effectiveness 61.538%.\n",
            "PGD l1: Attack effectiveness 82.143%.\n",
            "PGD l1: Attack effectiveness 70.270%.\n",
            "PGD l1: Attack effectiveness 56.250%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 72.222%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 78.947%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "conclusion"
      ],
      "metadata": {
        "id": "Ul_o4rkOlGbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        #pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "QlY-uEEslagY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd2(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    #insertion_array_updated = torch.bitwise_or(insertion_array.to(torch.uint8), x.squeeze().to(torch.uint8) )\n",
        "    #removal_array_updated = torch.bitwise_or(removal_array.to(torch.uint8), (1 - x.squeeze().to(torch.uint8)) )\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, y.view(-1).long())\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "            #perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            #perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            #perturbation[torch.isnan(perturbation)] = 0.\n",
        "            #perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), y.view(-1).long()).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        outputs = model(x_next)\n",
        "        _, predicted = torch.topk(outputs, k=1)\n",
        "        done = (predicted != y).squeeze()\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "SpW_1QIilagZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "\n",
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(torch.abs(x_next - torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "NBZMVbXDlJHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWM9Rk7rlFGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a1360c-0121-4b4a-af74-29cbfcf2e145",
        "id": "BALrCnWXopyU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03975ba4-44f7-44dc-9378-190d4f61177c",
        "id": "44nIZ-BOopyV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3482cec-ca56-4635-e3a6-40df6cdcd869",
        "id": "_LtUf4DpopyV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDOzv4PEoxCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910cc077-94db-40d8-f9c7-5089fe7bdb6c",
        "id": "TZXatuR9opyW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 80.18%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3a4611-2a25-42f2-cf5c-1659a16d4a68",
        "id": "DPKydw6TopyW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 69.12%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb97212-bdf4-4f26-bfc6-8e75f1caec39",
        "id": "nSywqfscopyW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.4%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MHc85REZo0wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9e502c-6dc1-4574-f730-823817f99c5a",
        "id": "ynb613QSopyX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 78.58%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3788fbe-aba1-4625-f0e5-fa1bf9848105",
        "id": "8sYbWremopyX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4087e04b-bcd7-413b-8b0f-a3a353adacb5",
        "id": "_Skn51BBopyY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.48%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hEGhljhiopKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db551d4d-5554-4691-ab91-ee604468179b",
        "id": "jNJ8MZZ9l44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PbugNsKmK4E",
        "outputId": "ac165f7d-6fcb-49fa-bcc0-7fa07b3104ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cak-U6IRmKGe",
        "outputId": "4fab09b7-8e2e-4fd2-f694-cc48a5a5daa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JaqDhY32mRVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc77889-61c3-45e2-9c3e-1c3a6579bc54",
        "id": "ZQT0WTE4l44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 86.46%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5IFBSEtmPtm",
        "outputId": "0fb02373-d479-469a-8c4e-325c53dc0301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 68.32%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpBKzLPNmQPy",
        "outputId": "ba7c0bfb-f8c8-4c0f-ad3a-f9d7f4218b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_hA66l_mYj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9f2fa6-79d4-48e9-dc56-84c65b36deb1",
        "id": "AfSDAvhYl44L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd2, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvZZq6OPmaEg",
        "outputId": "c25277f0-f675-4ec0-f2ef-387d5de4af5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.75%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2pkuMd_marG",
        "outputId": "040366b5-5500-4231-eb0d-b49d5e904266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ytAbwnAmfA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Study GKDE gradients"
      ],
      "metadata": {
        "id": "F5OlODkKX4jK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I8VdyzW2Xerd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde(adv_x,y,model,benigns, bandwidth, penalty_factor):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y.view(-1).long())\n",
        "    #print('ce: ', ce)\n",
        "    kde = KDE(adv_x, benigns, bandwidth)\n",
        "    #print('kde : ', kde)\n",
        "    loss_no_reduction = ce + penalty_factor * kde\n",
        "    _, predicted = torch.topk(outputs, k=1)\n",
        "    done = (predicted != y).squeeze()\n",
        "\n",
        "    return loss_no_reduction, done\n",
        "\n",
        "\n",
        "def gkde(x, y, model,bens, bandwidth, penalty_factor, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural, _ = get_loss_kde(x,y,model,bens, bandwidth, penalty_factor)\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        print('************** t ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        #y_model = model(x_var)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        outputs = model(x_var)\n",
        "        ce_loss = criterion(outputs, y.view(-1).long())\n",
        "        print('ce_loss: ', ce_loss)\n",
        "        kde_loss = KDE(x_var, bens, bandwidth)\n",
        "        print('kde_loss: ', kde_loss)\n",
        "        ce_grad = torch.autograd.grad(ce_loss.mean(), x_var, retain_graph=True)[0].data\n",
        "        kde_grad = torch.autograd.grad(kde_loss.mean(), x_var)[0].data\n",
        "        print('ce_grad ',torch.abs(ce_grad).sum(dim=-1).detach())\n",
        "        print('kde_grad ',torch.abs(kde_grad).sum(dim=-1).detach())\n",
        "        penalty_factor = torch.abs(ce_grad).sum(dim=-1).detach()/(torch.abs(kde_grad).sum(dim=-1).detach()+ 1e-20)\n",
        "        print('penalty_factor ',penalty_factor)\n",
        "\n",
        "        if t > 5:\n",
        "          decayed_penalty_factor = penalty_factor * (1 - t / k)\n",
        "        else:\n",
        "          decayed_penalty_factor = penalty_factor\n",
        "\n",
        "        # Compute loss\n",
        "        loss, _ = get_loss_kde(x_var,y,model,bens, bandwidth, decayed_penalty_factor)\n",
        "        #loss,_ = get_loss_kde(x_var,y,model,bens, bandwidth, penalty_factor)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "        pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        #pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x_next - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "\n",
        "            val, _ = torch.topk(torch.abs(gradients), 1)\n",
        "            perturbation = (torch.abs(gradients) >= val.expand_as(gradients)).float() * torch.sign(gradients).float()\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            _, done = get_loss_kde(x_next,y,model,bens, bandwidth, penalty_factor)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(x_next)\n",
        "    loss_adv = criterion(outputs, y.view(-1).long())\n",
        "    _, predicted = torch.topk(outputs, k=1)\n",
        "    done = (predicted != y).squeeze()\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "uV0R8nbaXfAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-select benign samples\n",
        "benign_samples = []\n",
        "\n",
        "for x_batch, y_batch in test_loader:\n",
        "  benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "# Forward pass to get logits for benign samples\n",
        "with torch.no_grad():  # No need for gradients\n",
        "    outputs = model_AT_rFGSM(ben_x.to(torch.float32))\n",
        "\n",
        "# Calculate softmax probabilities\n",
        "probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "# Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "# Select the top 500 high confidence benign samples\n",
        "top_500_high_confidence_benign_samples = ben_x[sorted_indices[:500]]\n",
        "\n",
        "del benign_samples, outputs, probabilities, ben_x  # Free up memory"
      ],
      "metadata": {
        "id": "U-eZopKFjIGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv = gkde(mals.to(torch.float32).to(device), mals_y.to(device), model_AT_rFGSM, top_500_high_confidence_benign_samples,0.6,1., insertion_array, removal_array, k=100, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krs1slxHjjin",
        "outputId": "3cde2f43-6cfb-463c-8502-43b750dcde8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************** t  0\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 9.2741e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1325e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.3344e-05, 2.2177e-10, 1.1448e-15, 0.0000e+00, 2.2177e-10, 3.4137e-18,\n",
            "        1.3935e-26, 6.6971e-06, 1.2336e-39, 1.8229e-36, 1.3344e-05, 5.0246e-11,\n",
            "        2.2177e-10, 5.7728e-08, 3.5037e-15, 4.1197e-12, 1.2445e-38, 3.1590e-40,\n",
            "        8.5677e-20, 2.3542e-38, 7.7047e-27, 2.7702e-10, 2.2177e-10, 4.0407e-39,\n",
            "        1.8901e-17, 1.1674e-15, 2.3014e-07, 4.1197e-12, 7.4781e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.0327e-15, 1.5589e-35, 3.6246e-23, 0.0000e+00, 1.5589e-35, 3.7404e-14,\n",
            "        0.0000e+00, 8.7472e-02, 0.0000e+00, 0.0000e+00, 1.0327e-15, 5.3577e+02,\n",
            "        1.5589e-35, 1.6707e-14, 1.1631e-37, 1.0270e-30, 0.0000e+00, 2.1986e-10,\n",
            "        1.8128e-25, 0.0000e+00, 0.0000e+00, 1.4061e-06, 1.5589e-35, 1.8835e-19,\n",
            "        3.6079e-13, 8.1566e-07, 1.8215e-31, 1.0270e-30, 1.0897e-14])\n",
            "kde_grad  tensor([1.1016e-05, 3.5169e-10, 2.7524e-15, 0.0000e+00, 3.5169e-10, 9.5880e-18,\n",
            "        5.7000e-26, 5.5288e-06, 7.5966e-39, 1.0206e-35, 1.1016e-05, 7.7543e-11,\n",
            "        3.5169e-10, 6.9480e-08, 7.9209e-15, 7.6794e-12, 7.4443e-38, 1.9407e-39,\n",
            "        2.6343e-19, 1.4114e-37, 3.1644e-26, 4.2795e-10, 3.5169e-10, 2.4811e-38,\n",
            "        5.0840e-17, 2.8093e-15, 2.5477e-07, 7.6794e-12, 2.9783e-03])\n",
            "penalty_factor  tensor([9.3744e-11, 4.4326e-26, 1.3169e-08, 0.0000e+00, 4.4326e-26, 3.8971e+03,\n",
            "        0.0000e+00, 1.5821e+04, 0.0000e+00, 0.0000e+00, 9.3744e-11, 6.9094e+12,\n",
            "        4.4326e-26, 2.4046e-07, 1.4684e-23, 1.3374e-19, 0.0000e+00, 2.1986e+10,\n",
            "        6.6298e-07, 0.0000e+00, 0.0000e+00, 3.2857e+03, 4.4326e-26, 1.8835e+01,\n",
            "        7.0952e+03, 2.9034e+08, 7.1495e-25, 1.3374e-19, 3.6588e-12])\n",
            "************** t  1\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 9.6215e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1673e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.7364e-05, 3.5889e-10, 2.0438e-15, 0.0000e+00, 3.5889e-10, 7.1692e-18,\n",
            "        1.3935e-26, 6.9177e-06, 1.2336e-39, 1.8229e-36, 1.7364e-05, 9.2329e-11,\n",
            "        3.5889e-10, 7.3458e-08, 5.6621e-15, 6.4538e-12, 1.2445e-38, 1.0376e-39,\n",
            "        2.0702e-19, 2.3542e-38, 7.7047e-27, 4.3162e-10, 3.5889e-10, 9.6569e-39,\n",
            "        3.9067e-17, 2.5587e-15, 2.8502e-07, 6.4538e-12, 7.6893e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([3.5289e-15, 1.4416e-33, 7.3212e-22, 0.0000e+00, 1.4416e-33, 1.7363e-11,\n",
            "        0.0000e+00, 8.1916e-01, 0.0000e+00, 0.0000e+00, 3.5289e-15, 6.2337e+02,\n",
            "        1.4416e-33, 2.8014e-13, 2.0506e-35, 8.6054e-29, 0.0000e+00, 6.6531e-09,\n",
            "        9.6210e-25, 0.0000e+00, 0.0000e+00, 4.9014e-06, 1.4416e-33, 2.6390e-18,\n",
            "        1.0897e-12, 5.1903e-06, 1.0666e-28, 8.6054e-29, 1.5972e-13])\n",
            "kde_grad  tensor([1.8160e-05, 7.0789e-10, 5.7878e-15, 0.0000e+00, 7.0789e-10, 2.4803e-17,\n",
            "        5.7000e-26, 7.5852e-06, 7.5966e-39, 1.0206e-35, 1.8160e-05, 1.5668e-10,\n",
            "        7.0789e-10, 1.0875e-07, 1.4909e-14, 1.4090e-11, 7.4443e-38, 6.7503e-39,\n",
            "        6.9992e-19, 1.4114e-37, 3.1644e-26, 7.4803e-10, 7.0789e-10, 6.1113e-38,\n",
            "        1.0927e-16, 6.8721e-15, 4.7459e-07, 1.4090e-11, 5.2694e-03])\n",
            "penalty_factor  tensor([1.9432e-10, 2.0365e-24, 1.2649e-07, 0.0000e+00, 2.0365e-24, 6.9973e+05,\n",
            "        0.0000e+00, 1.0799e+05, 0.0000e+00, 0.0000e+00, 1.9432e-10, 3.9787e+12,\n",
            "        2.0365e-24, 2.5760e-06, 1.3754e-21, 6.1073e-18, 0.0000e+00, 6.6531e+11,\n",
            "        1.3552e-06, 0.0000e+00, 0.0000e+00, 6.5524e+03, 2.0365e-24, 2.6390e+02,\n",
            "        9.9721e+03, 7.5526e+08, 2.2474e-22, 6.1073e-18, 3.0310e-11])\n",
            "************** t  2\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.3584e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1650e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.3439e-05, 6.2128e-10, 4.0406e-15, 0.0000e+00, 6.2128e-10, 1.7692e-17,\n",
            "        1.3935e-26, 7.5188e-06, 1.2336e-39, 1.8229e-36, 2.3439e-05, 1.7606e-10,\n",
            "        6.2128e-10, 9.8817e-08, 1.0016e-14, 1.0751e-11, 1.2445e-38, 2.4089e-39,\n",
            "        5.3579e-19, 2.3542e-38, 7.7047e-27, 7.0119e-10, 6.2128e-10, 2.1177e-38,\n",
            "        8.2252e-17, 5.9727e-15, 4.0464e-07, 1.0751e-11, 8.2887e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([7.4656e-15, 1.1246e-32, 1.7981e-21, 0.0000e+00, 1.1246e-32, 3.3273e-11,\n",
            "        0.0000e+00, 1.0796e+00, 0.0000e+00, 0.0000e+00, 7.4656e-15, 6.2854e+02,\n",
            "        1.1246e-32, 9.4188e-13, 8.2866e-35, 3.4244e-28, 0.0000e+00, 1.0506e-07,\n",
            "        1.2217e-24, 0.0000e+00, 0.0000e+00, 1.1980e-05, 1.1246e-32, 2.9213e-17,\n",
            "        1.8892e-12, 1.1184e-05, 5.3788e-28, 3.4244e-28, 5.5975e-13])\n",
            "kde_grad  tensor([2.6128e-05, 1.2949e-09, 1.1551e-14, 0.0000e+00, 1.2949e-09, 6.1039e-17,\n",
            "        5.7000e-26, 8.8794e-06, 7.5966e-39, 1.0206e-35, 2.6128e-05, 2.9445e-10,\n",
            "        1.2949e-09, 1.5242e-07, 2.6504e-14, 2.4106e-11, 7.4443e-38, 1.6271e-38,\n",
            "        1.8109e-18, 1.4114e-37, 3.1644e-26, 1.2027e-09, 1.2949e-09, 1.3819e-37,\n",
            "        2.2895e-16, 1.6048e-14, 6.9808e-07, 2.4106e-11, 6.6194e-03])\n",
            "penalty_factor  tensor([2.8574e-10, 8.6852e-24, 1.5568e-07, 0.0000e+00, 8.6852e-24, 5.4501e+05,\n",
            "        0.0000e+00, 1.2159e+05, 0.0000e+00, 0.0000e+00, 2.8574e-10, 2.1346e+12,\n",
            "        8.6852e-24, 6.1794e-06, 3.1266e-21, 1.4205e-17, 0.0000e+00, 1.0506e+13,\n",
            "        6.7096e-07, 0.0000e+00, 0.0000e+00, 9.9610e+03, 8.6852e-24, 2.9213e+03,\n",
            "        8.2511e+03, 6.9690e+08, 7.7051e-22, 1.4205e-17, 8.4562e-11])\n",
            "************** t  3\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.7025e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1608e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.2010e-05, 1.0892e-09, 7.9941e-15, 0.0000e+00, 1.0892e-09, 4.2879e-17,\n",
            "        1.3935e-26, 8.1964e-06, 1.2336e-39, 1.8229e-36, 3.2010e-05, 3.3198e-10,\n",
            "        1.0892e-09, 1.3365e-07, 1.7660e-14, 1.7798e-11, 1.2445e-38, 4.9348e-39,\n",
            "        1.3681e-18, 2.3542e-38, 7.7047e-27, 1.1198e-09, 1.0892e-09, 4.7752e-38,\n",
            "        1.7110e-16, 1.3825e-14, 5.8241e-07, 1.7798e-11, 9.0112e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.1756e-14, 6.8905e-32, 4.0493e-21, 0.0000e+00, 6.8905e-32, 6.2882e-11,\n",
            "        0.0000e+00, 1.3587e+00, 0.0000e+00, 0.0000e+00, 2.1756e-14, 6.8419e+02,\n",
            "        6.8905e-32, 2.9358e-12, 3.3108e-34, 1.3261e-27, 0.0000e+00, 1.2461e-06,\n",
            "        1.5574e-24, 0.0000e+00, 0.0000e+00, 2.3855e-05, 6.8905e-32, 2.6250e-16,\n",
            "        3.2374e-12, 2.3583e-05, 2.6036e-27, 1.3261e-27, 1.5457e-12])\n",
            "kde_grad  tensor([3.5867e-05, 2.2996e-09, 2.2653e-14, 0.0000e+00, 2.2996e-09, 1.4590e-16,\n",
            "        5.7000e-26, 1.0199e-05, 7.5966e-39, 1.0206e-35, 3.5867e-05, 5.4258e-10,\n",
            "        2.2996e-09, 2.0900e-07, 4.6321e-14, 4.0487e-11, 7.4443e-38, 3.4378e-38,\n",
            "        4.5750e-18, 1.4114e-37, 3.1644e-26, 1.9005e-09, 2.2996e-09, 3.1902e-37,\n",
            "        4.7136e-16, 3.6733e-14, 9.9710e-07, 4.0487e-11, 7.8553e-03])\n",
            "penalty_factor  tensor([6.0657e-10, 2.9964e-23, 1.7875e-07, 0.0000e+00, 2.9964e-23, 4.3097e+05,\n",
            "        0.0000e+00, 1.3322e+05, 0.0000e+00, 0.0000e+00, 6.0657e-10, 1.2610e+12,\n",
            "        2.9964e-23, 1.4047e-05, 7.1475e-21, 3.2754e-17, 0.0000e+00, 1.2461e+14,\n",
            "        3.3967e-07, 0.0000e+00, 0.0000e+00, 1.2552e+04, 2.9964e-23, 2.6250e+04,\n",
            "        6.8682e+03, 6.4203e+08, 2.6112e-21, 3.2754e-17, 1.9678e-10])\n",
            "************** t  4\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.2715e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1565e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-07, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.3643e-05, 1.8847e-09, 1.5592e-14, 0.0000e+00, 1.8847e-09, 1.0099e-16,\n",
            "        1.3935e-26, 8.9815e-06, 1.2336e-39, 1.8229e-36, 4.3643e-05, 6.1583e-10,\n",
            "        1.8847e-09, 1.8032e-07, 3.0713e-14, 2.8450e-11, 1.2445e-38, 1.0078e-38,\n",
            "        3.4176e-18, 2.3542e-38, 7.7047e-27, 1.7539e-09, 1.8847e-09, 9.7802e-38,\n",
            "        3.5012e-16, 3.1429e-14, 8.3200e-07, 2.8450e-11, 9.9633e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([6.2167e-14, 4.3156e-31, 9.1189e-21, 0.0000e+00, 4.3156e-31, 1.1911e-10,\n",
            "        0.0000e+00, 1.5993e+00, 0.0000e+00, 0.0000e+00, 6.2167e-14, 6.7873e+02,\n",
            "        4.3156e-31, 8.9621e-12, 1.3228e-33, 3.3705e-27, 0.0000e+00, 1.2298e-05,\n",
            "        2.3477e-24, 0.0000e+00, 0.0000e+00, 1.1821e-04, 4.3156e-31, 1.9823e-15,\n",
            "        5.5479e-12, 1.0407e-04, 1.2603e-26, 3.3705e-27, 3.9053e-12])\n",
            "kde_grad  tensor([4.8400e-05, 4.0373e-09, 4.3796e-14, 0.0000e+00, 4.0373e-09, 3.3865e-16,\n",
            "        5.7000e-26, 1.1508e-05, 7.5966e-39, 1.0206e-35, 4.8400e-05, 9.8416e-10,\n",
            "        4.0373e-09, 2.8340e-07, 7.9848e-14, 6.7716e-11, 7.4443e-38, 7.1722e-38,\n",
            "        1.1324e-17, 1.4114e-37, 3.1644e-26, 2.9657e-09, 4.0373e-09, 6.7114e-37,\n",
            "        9.5516e-16, 8.2624e-14, 1.4134e-06, 6.7716e-11, 8.7465e-03])\n",
            "penalty_factor  tensor([1.2844e-09, 1.0689e-22, 2.0821e-07, 0.0000e+00, 1.0689e-22, 3.5173e+05,\n",
            "        0.0000e+00, 1.3897e+05, 0.0000e+00, 0.0000e+00, 1.2844e-09, 6.8965e+11,\n",
            "        1.0689e-22, 3.1624e-05, 1.6567e-20, 4.9773e-17, 0.0000e+00, 1.2298e+15,\n",
            "        2.0714e-07, 0.0000e+00, 0.0000e+00, 3.9858e+04, 1.0689e-22, 1.9823e+05,\n",
            "        5.8083e+03, 1.2595e+09, 8.9162e-21, 4.9773e-17, 4.4650e-10])\n",
            "************** t  5\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.7973e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1572e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-07, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.3842e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.9109e-05, 3.2439e-09, 2.9981e-14, 0.0000e+00, 3.2439e-09, 2.3105e-16,\n",
            "        1.3935e-26, 9.9041e-06, 1.2336e-39, 1.8229e-36, 5.9109e-05, 1.1235e-09,\n",
            "        3.2439e-09, 2.4199e-07, 5.2683e-14, 4.6266e-11, 1.2445e-38, 1.8472e-38,\n",
            "        8.3749e-18, 2.3542e-38, 7.7047e-27, 2.7205e-09, 3.2439e-09, 1.8805e-37,\n",
            "        7.0558e-16, 7.0214e-14, 1.1796e-06, 4.6266e-11, 1.1023e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.7764e-13, 2.0958e-30, 2.0493e-20, 0.0000e+00, 2.0958e-30, 2.2579e-10,\n",
            "        0.0000e+00, 2.1647e+00, 0.0000e+00, 0.0000e+00, 1.7764e-13, 6.8874e+02,\n",
            "        2.0958e-30, 2.7023e-11, 5.2853e-33, 1.2417e-26, 0.0000e+00, 7.0598e-05,\n",
            "        3.8284e-24, 0.0000e+00, 0.0000e+00, 2.6931e-04, 2.0958e-30, 1.2279e-14,\n",
            "        9.8367e-12, 2.1428e-04, 6.1003e-26, 1.2417e-26, 9.3877e-12])\n",
            "kde_grad  tensor([6.4872e-05, 6.9745e-09, 8.3465e-14, 0.0000e+00, 6.9745e-09, 7.6330e-16,\n",
            "        5.7000e-26, 1.2766e-05, 7.5966e-39, 1.0206e-35, 6.4872e-05, 1.7547e-09,\n",
            "        6.9745e-09, 3.8130e-07, 1.3575e-13, 1.1018e-10, 7.4443e-38, 1.3453e-37,\n",
            "        2.7499e-17, 1.4114e-37, 3.1644e-26, 4.5359e-09, 6.9745e-09, 1.3167e-36,\n",
            "        1.9064e-15, 1.8260e-13, 1.9885e-06, 1.1018e-10, 9.6349e-03])\n",
            "penalty_factor  tensor([2.7384e-09, 3.0049e-22, 2.4553e-07, 0.0000e+00, 3.0049e-22, 2.9581e+05,\n",
            "        0.0000e+00, 1.6957e+05, 0.0000e+00, 0.0000e+00, 2.7384e-09, 3.9252e+11,\n",
            "        3.0049e-22, 7.0872e-05, 3.8933e-20, 1.1270e-16, 0.0000e+00, 7.0598e+15,\n",
            "        1.3917e-07, 0.0000e+00, 0.0000e+00, 5.9372e+04, 3.0049e-22, 1.2279e+06,\n",
            "        5.1598e+03, 1.1735e+09, 3.0678e-20, 1.1270e-16, 9.7434e-10])\n",
            "************** t  6\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 3.4756e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1589e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.3842e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 4.7684e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.9524e-05, 5.5630e-09, 5.6800e-14, 0.0000e+00, 5.5630e-09, 5.1339e-16,\n",
            "        1.3935e-26, 1.0927e-05, 1.2336e-39, 1.8229e-36, 7.9524e-05, 2.0154e-09,\n",
            "        5.5630e-09, 3.2410e-07, 8.9097e-14, 7.4817e-11, 1.2445e-38, 4.2564e-38,\n",
            "        2.0134e-17, 2.3542e-38, 7.7047e-27, 4.1390e-09, 5.5630e-09, 3.2551e-37,\n",
            "        1.4004e-15, 1.5422e-13, 1.6600e-06, 7.4817e-11, 1.2156e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([5.0761e-13, 1.1923e-29, 4.5796e-20, 0.0000e+00, 1.1923e-29, 4.2856e-10,\n",
            "        0.0000e+00, 2.6888e+00, 0.0000e+00, 0.0000e+00, 5.0761e-13, 6.8874e+02,\n",
            "        1.1923e-29, 8.1428e-11, 1.9879e-32, 4.5203e-26, 0.0000e+00, 1.7382e-04,\n",
            "        6.2429e-24, 0.0000e+00, 0.0000e+00, 5.6683e-04, 1.1923e-29, 6.8372e-14,\n",
            "        1.7545e-11, 4.3569e-04, 2.9529e-25, 4.5203e-26, 2.2721e-11])\n",
            "kde_grad  tensor([8.6362e-05, 1.1845e-08, 1.5682e-13, 0.0000e+00, 1.1845e-08, 1.6707e-15,\n",
            "        5.7000e-26, 1.4064e-05, 7.5966e-39, 1.0206e-35, 8.6362e-05, 3.0702e-09,\n",
            "        1.1845e-08, 5.0702e-07, 2.2767e-13, 1.7659e-10, 7.4443e-38, 3.1597e-37,\n",
            "        6.5492e-17, 1.4114e-37, 3.1644e-26, 6.8824e-09, 1.1845e-08, 2.3220e-36,\n",
            "        3.7472e-15, 3.9641e-13, 2.7763e-06, 1.7659e-10, 1.0625e-02])\n",
            "penalty_factor  tensor([5.8777e-09, 1.0066e-21, 2.9203e-07, 0.0000e+00, 1.0066e-21, 2.5652e+05,\n",
            "        0.0000e+00, 1.9118e+05, 0.0000e+00, 0.0000e+00, 5.8777e-09, 2.2433e+11,\n",
            "        1.0066e-21, 1.6060e-04, 8.7313e-20, 2.5598e-16, 0.0000e+00, 1.7382e+16,\n",
            "        9.5309e-08, 0.0000e+00, 0.0000e+00, 8.2359e+04, 1.0066e-21, 6.8372e+06,\n",
            "        4.6821e+03, 1.0991e+09, 1.0636e-19, 2.5598e-16, 2.1385e-09])\n",
            "************** t  7\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 4.2848e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1601e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4305e-06, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.0729e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.0628e-04, 9.4263e-09, 1.0608e-13, 0.0000e+00, 9.4263e-09, 1.1080e-15,\n",
            "        1.3935e-26, 1.2037e-05, 1.2336e-39, 1.8229e-36, 1.0628e-04, 3.5552e-09,\n",
            "        9.4263e-09, 4.3129e-07, 1.4868e-13, 1.1955e-10, 1.2445e-38, 1.0787e-37,\n",
            "        4.7480e-17, 2.3542e-38, 7.7047e-27, 6.2459e-09, 9.4263e-09, 4.9218e-37,\n",
            "        2.7346e-15, 3.3286e-13, 2.3183e-06, 1.1955e-10, 1.3389e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.4505e-12, 6.8402e-29, 1.0251e-19, 0.0000e+00, 6.8402e-29, 8.1343e-10,\n",
            "        0.0000e+00, 3.2077e+00, 0.0000e+00, 0.0000e+00, 1.4505e-12, 6.8874e+02,\n",
            "        6.8402e-29, 2.4536e-10, 7.9423e-32, 1.7351e-25, 0.0000e+00, 8.2211e-04,\n",
            "        1.0180e-23, 0.0000e+00, 0.0000e+00, 1.1842e-03, 6.8402e-29, 3.2599e-13,\n",
            "        3.1381e-11, 9.5006e-04, 1.4367e-24, 1.7351e-25, 5.5322e-11])\n",
            "kde_grad  tensor([1.1419e-04, 1.9874e-08, 2.9045e-13, 0.0000e+00, 1.9874e-08, 3.5508e-15,\n",
            "        5.7000e-26, 1.5445e-05, 7.5966e-39, 1.0206e-35, 1.1419e-04, 5.3065e-09,\n",
            "        1.9874e-08, 6.6984e-07, 3.7653e-13, 2.8007e-10, 7.4443e-38, 8.1377e-37,\n",
            "        1.5296e-16, 1.4114e-37, 3.1644e-26, 1.0238e-08, 1.9874e-08, 3.5794e-36,\n",
            "        7.2556e-15, 8.4546e-13, 3.8467e-06, 2.8007e-10, 1.1652e-02])\n",
            "penalty_factor  tensor([1.2702e-08, 3.4418e-21, 3.5292e-07, 0.0000e+00, 3.4418e-21, 2.2908e+05,\n",
            "        0.0000e+00, 2.0768e+05, 0.0000e+00, 0.0000e+00, 1.2702e-08, 1.2979e+11,\n",
            "        3.4418e-21, 3.6630e-04, 2.1093e-19, 6.1953e-16, 0.0000e+00, 8.2211e+16,\n",
            "        6.6551e-08, 0.0000e+00, 0.0000e+00, 1.1567e+05, 3.4418e-21, 3.2599e+07,\n",
            "        4.3251e+03, 1.1237e+09, 3.7348e-19, 6.1953e-16, 4.7480e-09])\n",
            "************** t  8\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 5.2738e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1634e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.3842e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.2186e-06, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.3842e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.4109e-04, 1.5773e-08, 1.9543e-13, 0.0000e+00, 1.5773e-08, 2.3227e-15,\n",
            "        1.3935e-26, 1.3229e-05, 1.2336e-39, 1.8229e-36, 1.4109e-04, 6.2076e-09,\n",
            "        1.5773e-08, 5.7024e-07, 2.4473e-13, 1.8863e-10, 1.2445e-38, 2.5987e-37,\n",
            "        1.0982e-16, 2.3542e-38, 7.7047e-27, 9.3026e-09, 1.5773e-08, 6.8872e-37,\n",
            "        5.2354e-15, 7.0592e-13, 3.2080e-06, 1.8863e-10, 1.4707e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([4.2057e-12, 3.9267e-28, 2.2939e-19, 0.0000e+00, 3.9267e-28, 1.5439e-09,\n",
            "        0.0000e+00, 3.9058e+00, 0.0000e+00, 0.0000e+00, 4.2057e-12, 7.2351e+02,\n",
            "        3.9267e-28, 7.4628e-10, 3.1733e-31, 6.6947e-25, 0.0000e+00, 1.4471e-03,\n",
            "        1.6601e-23, 0.0000e+00, 0.0000e+00, 2.6792e-03, 3.9267e-28, 1.6372e-12,\n",
            "        5.7497e-11, 2.0282e-03, 6.9817e-24, 6.6947e-25, 1.3459e-10])\n",
            "kde_grad  tensor([1.4998e-04, 3.2956e-08, 5.3021e-13, 0.0000e+00, 3.2956e-08, 7.3285e-15,\n",
            "        5.7000e-26, 1.6925e-05, 7.5966e-39, 1.0206e-35, 1.4998e-04, 9.1385e-09,\n",
            "        3.2956e-08, 8.7919e-07, 6.1417e-13, 4.4027e-10, 7.4443e-38, 1.9921e-36,\n",
            "        3.5031e-16, 1.4114e-37, 3.1644e-26, 1.5011e-08, 3.2956e-08, 5.0994e-36,\n",
            "        1.3854e-14, 1.7715e-12, 5.2990e-06, 4.4027e-10, 1.2770e-02])\n",
            "penalty_factor  tensor([2.8042e-08, 1.1915e-20, 4.3265e-07, 0.0000e+00, 1.1915e-20, 2.1067e+05,\n",
            "        0.0000e+00, 2.3078e+05, 0.0000e+00, 0.0000e+00, 2.8042e-08, 7.9171e+10,\n",
            "        1.1915e-20, 8.4883e-04, 5.1668e-19, 1.5206e-15, 0.0000e+00, 1.4471e+17,\n",
            "        4.7387e-08, 0.0000e+00, 0.0000e+00, 1.7849e+05, 1.1915e-20, 1.6372e+08,\n",
            "        4.1502e+03, 1.1449e+09, 1.3176e-18, 1.5206e-15, 1.0539e-08])\n",
            "************** t  9\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.4359e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1659e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.0068e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1525e-06, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 5.0068e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.8606e-04, 2.6093e-08, 3.5491e-13, 0.0000e+00, 2.6093e-08, 4.7292e-15,\n",
            "        1.3935e-26, 1.4507e-05, 1.2336e-39, 1.8229e-36, 1.8606e-04, 1.0723e-08,\n",
            "        2.6093e-08, 7.4910e-07, 3.9731e-13, 2.9515e-10, 1.2445e-38, 5.7132e-37,\n",
            "        2.4915e-16, 2.3542e-38, 7.7047e-27, 1.3669e-08, 2.6093e-08, 8.5414e-37,\n",
            "        9.9268e-15, 1.4694e-12, 4.4129e-06, 2.9515e-10, 1.6128e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.2110e-11, 2.2532e-27, 5.1334e-19, 0.0000e+00, 2.2532e-27, 2.9304e-09,\n",
            "        0.0000e+00, 4.4885e+00, 0.0000e+00, 0.0000e+00, 1.2110e-11, 8.0757e+02,\n",
            "        2.2532e-27, 2.2748e-09, 1.2919e-30, 2.5709e-24, 0.0000e+00, 2.9371e-03,\n",
            "        2.7070e-23, 0.0000e+00, 0.0000e+00, 5.9480e-03, 2.2532e-27, 8.4685e-12,\n",
            "        1.0324e-10, 4.2652e-03, 3.3973e-23, 2.5709e-24, 3.2731e-10])\n",
            "kde_grad  tensor([1.9564e-04, 5.3972e-08, 9.5406e-13, 0.0000e+00, 5.3972e-08, 1.4687e-14,\n",
            "        5.7000e-26, 1.8504e-05, 7.5966e-39, 1.0206e-35, 1.9564e-04, 1.5559e-08,\n",
            "        5.3972e-08, 1.1465e-06, 9.8803e-13, 6.8409e-10, 7.4443e-38, 4.4199e-36,\n",
            "        7.8671e-16, 1.4114e-37, 3.1644e-26, 2.1708e-08, 5.3972e-08, 6.4365e-36,\n",
            "        2.6007e-14, 3.6484e-12, 7.2310e-06, 6.8409e-10, 1.3942e-02])\n",
            "penalty_factor  tensor([6.1900e-08, 4.1748e-20, 5.3806e-07, 0.0000e+00, 4.1748e-20, 1.9952e+05,\n",
            "        0.0000e+00, 2.4257e+05, 0.0000e+00, 0.0000e+00, 6.1900e-08, 5.1902e+10,\n",
            "        4.1748e-20, 1.9842e-03, 1.3075e-18, 3.7581e-15, 0.0000e+00, 2.9371e+17,\n",
            "        3.4409e-08, 0.0000e+00, 0.0000e+00, 2.7400e+05, 4.1748e-20, 8.4685e+08,\n",
            "        3.9698e+03, 1.1691e+09, 4.6982e-18, 3.7581e-15, 2.3476e-08])\n",
            "************** t  10\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 7.8545e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1676e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0967e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.6451e-05, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.0490e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.4373e-04, 4.2647e-08, 6.3539e-13, 0.0000e+00, 4.2647e-08, 9.3527e-15,\n",
            "        1.3935e-26, 1.5873e-05, 1.2336e-39, 1.8229e-36, 2.4373e-04, 1.8265e-08,\n",
            "        4.2647e-08, 9.7771e-07, 6.3489e-13, 4.5699e-10, 1.2445e-38, 1.4313e-36,\n",
            "        5.5432e-16, 2.3542e-38, 7.7047e-27, 1.9819e-08, 4.2647e-08, 9.2592e-37,\n",
            "        1.8536e-14, 3.0086e-12, 6.0246e-06, 4.5699e-10, 1.7647e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([3.4871e-11, 1.2930e-26, 1.1488e-18, 0.0000e+00, 1.2930e-26, 5.5621e-09,\n",
            "        0.0000e+00, 5.6970e+00, 0.0000e+00, 0.0000e+00, 3.4871e-11, 8.0761e+02,\n",
            "        1.2930e-26, 6.9140e-09, 4.8563e-30, 9.8724e-24, 0.0000e+00, 6.4069e-03,\n",
            "        4.4143e-23, 0.0000e+00, 0.0000e+00, 1.4238e-02, 1.2930e-26, 4.3973e-11,\n",
            "        1.8508e-10, 8.7869e-03, 1.6531e-22, 9.8724e-24, 7.9603e-10])\n",
            "kde_grad  tensor([2.5348e-04, 8.7325e-08, 1.6922e-12, 0.0000e+00, 8.7325e-08, 2.8583e-14,\n",
            "        5.7000e-26, 2.0186e-05, 7.5966e-39, 1.0206e-35, 2.5348e-04, 2.7023e-08,\n",
            "        8.7325e-08, 1.4853e-06, 1.5689e-12, 1.0517e-09, 7.4443e-38, 1.1163e-35,\n",
            "        1.7324e-15, 1.4114e-37, 3.1644e-26, 3.0969e-08, 8.7325e-08, 7.1079e-36,\n",
            "        4.8076e-14, 7.3783e-12, 9.7922e-06, 1.0517e-09, 1.5188e-02])\n",
            "penalty_factor  tensor([1.3757e-07, 1.4807e-19, 6.7886e-07, 0.0000e+00, 1.4807e-19, 1.9459e+05,\n",
            "        0.0000e+00, 2.8223e+05, 0.0000e+00, 0.0000e+00, 1.3757e-07, 2.9886e+10,\n",
            "        1.4807e-19, 4.6550e-03, 3.0954e-18, 9.3873e-15, 0.0000e+00, 6.4069e+17,\n",
            "        2.5480e-08, 0.0000e+00, 0.0000e+00, 4.5976e+05, 1.4807e-19, 4.3973e+09,\n",
            "        3.8498e+03, 1.1909e+09, 1.6882e-17, 9.3873e-15, 5.2413e-08])\n",
            "************** t  11\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 9.6846e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1653e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.3484e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.8742e-05, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.2053e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.1715e-04, 6.8872e-08, 1.1214e-12, 0.0000e+00, 6.8872e-08, 1.7965e-14,\n",
            "        1.3935e-26, 1.7329e-05, 1.2336e-39, 1.8229e-36, 3.1715e-04, 3.1652e-08,\n",
            "        6.8872e-08, 1.2678e-06, 1.0027e-12, 7.0012e-10, 1.2445e-38, 3.2395e-36,\n",
            "        1.2095e-15, 2.3542e-38, 7.7047e-27, 2.8324e-08, 6.8872e-08, 8.8475e-37,\n",
            "        3.4088e-14, 6.0391e-12, 8.1628e-06, 7.0012e-10, 1.9266e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.0041e-10, 7.4198e-26, 2.5707e-18, 0.0000e+00, 7.4198e-26, 1.0557e-08,\n",
            "        0.0000e+00, 7.0794e+00, 0.0000e+00, 0.0000e+00, 1.0041e-10, 8.6126e+02,\n",
            "        7.4198e-26, 2.1014e-08, 1.9657e-29, 3.7912e-23, 0.0000e+00, 1.3550e-02,\n",
            "        7.1983e-23, 0.0000e+00, 0.0000e+00, 3.3975e-02, 7.4198e-26, 1.0527e-10,\n",
            "        3.3274e-10, 1.8490e-02, 8.0440e-22, 3.7912e-23, 1.9798e-09])\n",
            "kde_grad  tensor([3.2620e-04, 1.3958e-07, 2.9585e-12, 0.0000e+00, 1.3958e-07, 5.4014e-14,\n",
            "        5.7000e-26, 2.1971e-05, 7.5966e-39, 1.0206e-35, 3.2620e-04, 4.8205e-08,\n",
            "        1.3958e-07, 1.9117e-06, 2.4576e-12, 1.5996e-09, 7.4443e-38, 2.5468e-35,\n",
            "        3.7406e-15, 1.4114e-37, 3.1644e-26, 4.3647e-08, 1.3958e-07, 6.9165e-36,\n",
            "        8.7515e-14, 1.4673e-11, 1.3159e-05, 1.5996e-09, 1.6507e-02])\n",
            "penalty_factor  tensor([3.0782e-07, 5.3157e-19, 8.6891e-07, 0.0000e+00, 5.3157e-19, 1.9545e+05,\n",
            "        0.0000e+00, 3.2221e+05, 0.0000e+00, 0.0000e+00, 3.0782e-07, 1.7866e+10,\n",
            "        5.3157e-19, 1.0992e-02, 7.9986e-18, 2.3700e-14, 0.0000e+00, 1.3550e+18,\n",
            "        1.9243e-08, 0.0000e+00, 0.0000e+00, 7.7841e+05, 5.3157e-19, 1.0527e+10,\n",
            "        3.8021e+03, 1.2601e+09, 6.1127e-17, 2.3700e-14, 1.1994e-07])\n",
            "************** t  12\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.1958e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1680e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.2914e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.3098e-05, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 4.5895e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.0996e-04, 1.0989e-07, 1.9509e-12, 0.0000e+00, 1.0989e-07, 3.3518e-14,\n",
            "        1.3935e-26, 1.8867e-05, 1.2336e-39, 1.8229e-36, 4.0996e-04, 5.5146e-08,\n",
            "        1.0989e-07, 1.6334e-06, 1.5654e-12, 1.0613e-09, 1.2445e-38, 6.6719e-36,\n",
            "        2.5880e-15, 2.3542e-38, 7.7047e-27, 3.9996e-08, 1.0989e-07, 9.2804e-37,\n",
            "        6.1737e-14, 1.1931e-11, 1.0976e-05, 1.0613e-09, 2.0964e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.9738e-10, 4.2564e-25, 5.7382e-18, 0.0000e+00, 4.2564e-25, 2.0299e-08,\n",
            "        0.0000e+00, 8.9003e+00, 0.0000e+00, 0.0000e+00, 2.9738e-10, 8.5971e+02,\n",
            "        4.2564e-25, 6.3868e-08, 8.5612e-29, 1.4894e-22, 0.0000e+00, 3.1191e-02,\n",
            "        1.1738e-22, 0.0000e+00, 0.0000e+00, 8.6752e-02, 4.2564e-25, 3.4541e-10,\n",
            "        5.9819e-10, 3.8384e-02, 3.9142e-21, 1.4894e-22, 4.8686e-09])\n",
            "kde_grad  tensor([4.1694e-04, 2.2042e-07, 5.0985e-12, 0.0000e+00, 2.2042e-07, 9.9114e-14,\n",
            "        5.7000e-26, 2.3884e-05, 7.5966e-39, 1.0206e-35, 4.1694e-04, 8.5672e-08,\n",
            "        2.2042e-07, 2.4444e-06, 3.8124e-12, 2.4073e-09, 7.4443e-38, 5.2857e-35,\n",
            "        7.9194e-15, 1.4114e-37, 3.1644e-26, 6.0617e-08, 2.2042e-07, 7.3480e-36,\n",
            "        1.5687e-13, 2.8646e-11, 1.7549e-05, 2.4073e-09, 1.7962e-02])\n",
            "penalty_factor  tensor([7.1325e-07, 1.9311e-18, 1.1255e-06, 0.0000e+00, 1.9311e-18, 2.0481e+05,\n",
            "        0.0000e+00, 3.7264e+05, 0.0000e+00, 0.0000e+00, 7.1325e-07, 1.0035e+10,\n",
            "        1.9311e-18, 2.6129e-02, 2.2456e-17, 6.1869e-14, 0.0000e+00, 3.1191e+18,\n",
            "        1.4822e-08, 0.0000e+00, 0.0000e+00, 1.4312e+06, 1.9311e-18, 3.4541e+10,\n",
            "        3.8133e+03, 1.3400e+09, 2.2304e-16, 6.1869e-14, 2.7105e-07])\n",
            "************** t  13\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.4786e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1731e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.9007e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.3100e-04, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 9.5720e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.2610e-04, 1.7324e-07, 3.3347e-12, 0.0000e+00, 1.7324e-07, 6.0738e-14,\n",
            "        1.3935e-26, 2.0506e-05, 1.2336e-39, 1.8229e-36, 5.2610e-04, 9.6354e-08,\n",
            "        1.7324e-07, 2.0908e-06, 2.4173e-12, 1.5892e-09, 1.2445e-38, 1.0904e-35,\n",
            "        5.4303e-15, 2.3542e-38, 7.7047e-27, 5.5748e-08, 1.7324e-07, 8.9664e-37,\n",
            "        1.1011e-13, 2.3171e-11, 1.4647e-05, 1.5892e-09, 2.2786e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([8.6840e-10, 2.4426e-24, 1.2986e-17, 0.0000e+00, 2.4426e-24, 3.8968e-08,\n",
            "        0.0000e+00, 1.0989e+01, 0.0000e+00, 0.0000e+00, 8.6840e-10, 8.4578e+02,\n",
            "        2.4426e-24, 1.9190e-07, 3.7744e-28, 5.8492e-22, 0.0000e+00, 3.0702e-02,\n",
            "        1.9163e-22, 0.0000e+00, 0.0000e+00, 2.1778e-01, 2.4426e-24, 1.1206e-09,\n",
            "        1.0748e-09, 8.0050e-02, 1.9046e-20, 5.8492e-22, 1.2201e-08])\n",
            "kde_grad  tensor([5.3002e-04, 3.4386e-07, 8.6703e-12, 0.0000e+00, 3.4386e-07, 1.7660e-13,\n",
            "        5.7000e-26, 2.5882e-05, 7.5966e-39, 1.0206e-35, 5.3002e-04, 1.5007e-07,\n",
            "        3.4386e-07, 3.1051e-06, 5.8486e-12, 3.5873e-09, 7.4443e-38, 8.7043e-35,\n",
            "        1.6439e-14, 1.4114e-37, 3.1644e-26, 8.3073e-08, 3.4386e-07, 7.1876e-36,\n",
            "        2.7688e-13, 5.4927e-11, 2.3224e-05, 3.5873e-09, 1.9435e-02])\n",
            "penalty_factor  tensor([1.6384e-06, 7.1036e-18, 1.4977e-06, 0.0000e+00, 7.1036e-18, 2.2066e+05,\n",
            "        0.0000e+00, 4.2460e+05, 0.0000e+00, 0.0000e+00, 1.6384e-06, 5.6360e+09,\n",
            "        7.1036e-18, 6.1800e-02, 6.4536e-17, 1.6305e-13, 0.0000e+00, 3.0702e+18,\n",
            "        1.1657e-08, 0.0000e+00, 0.0000e+00, 2.6216e+06, 7.1036e-18, 1.1206e+11,\n",
            "        3.8818e+03, 1.4574e+09, 8.2010e-16, 1.6305e-13, 6.2779e-07])\n",
            "************** t  14\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.8276e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1802e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0514e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.8717e-04, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.9954e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([6.7104e-04, 2.6983e-07, 5.6377e-12, 0.0000e+00, 2.6983e-07, 1.0691e-13,\n",
            "        1.3935e-26, 2.2239e-05, 1.2336e-39, 1.8229e-36, 6.7104e-04, 1.6729e-07,\n",
            "        2.6983e-07, 2.6575e-06, 3.6856e-12, 2.3585e-09, 1.2445e-38, 1.9248e-35,\n",
            "        1.1130e-14, 2.3542e-38, 7.7047e-27, 7.6707e-08, 2.6983e-07, 7.8965e-37,\n",
            "        1.9308e-13, 4.4215e-11, 1.9397e-05, 2.3585e-09, 2.4711e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.3759e-09, 1.3978e-23, 2.9203e-17, 0.0000e+00, 1.3978e-23, 7.4804e-08,\n",
            "        0.0000e+00, 1.3560e+01, 0.0000e+00, 0.0000e+00, 2.3759e-09, 8.4578e+02,\n",
            "        1.3978e-23, 5.8047e-07, 1.6764e-27, 2.2809e-21, 0.0000e+00, 4.8473e-02,\n",
            "        3.1503e-22, 0.0000e+00, 0.0000e+00, 5.7562e-01, 1.3978e-23, 3.6952e-09,\n",
            "        1.9412e-09, 1.6687e-01, 9.2678e-20, 2.2809e-21, 3.0250e-08])\n",
            "kde_grad  tensor([6.6832e-04, 5.2993e-07, 1.4518e-11, 0.0000e+00, 5.2993e-07, 3.0553e-13,\n",
            "        5.7000e-26, 2.7983e-05, 7.5966e-39, 1.0206e-35, 6.6832e-04, 2.5965e-07,\n",
            "        5.2993e-07, 3.9215e-06, 8.8778e-12, 5.2843e-09, 7.4443e-38, 1.5471e-34,\n",
            "        3.3477e-14, 1.4114e-37, 3.1644e-26, 1.1236e-07, 5.2993e-07, 6.4075e-36,\n",
            "        4.8153e-13, 1.0346e-10, 3.0499e-05, 5.2843e-09, 2.0983e-02])\n",
            "penalty_factor  tensor([3.5550e-06, 2.6376e-17, 2.0115e-06, 0.0000e+00, 2.6376e-17, 2.4483e+05,\n",
            "        0.0000e+00, 4.8457e+05, 0.0000e+00, 0.0000e+00, 3.5550e-06, 3.2573e+09,\n",
            "        2.6376e-17, 1.4802e-01, 1.8883e-16, 4.3164e-13, 0.0000e+00, 4.8473e+18,\n",
            "        9.4103e-09, 0.0000e+00, 0.0000e+00, 5.1229e+06, 2.6376e-17, 3.6952e+11,\n",
            "        4.0313e+03, 1.6129e+09, 3.0387e-15, 4.3164e-13, 1.4416e-06])\n",
            "************** t  15\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.2574e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1892e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7773e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5096e-03, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 4.1619e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([8.4833e-04, 4.1523e-07, 9.3956e-12, 0.0000e+00, 4.1523e-07, 1.8276e-13,\n",
            "        1.3935e-26, 2.4064e-05, 1.2336e-39, 1.8229e-36, 8.4833e-04, 2.8799e-07,\n",
            "        4.1523e-07, 3.3576e-06, 5.5626e-12, 3.4632e-09, 1.2445e-38, 3.9479e-35,\n",
            "        2.2457e-14, 2.3542e-38, 7.7047e-27, 1.0421e-07, 4.1523e-07, 6.2834e-37,\n",
            "        3.3342e-13, 8.2898e-11, 2.5493e-05, 3.4632e-09, 2.6739e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([7.1261e-09, 8.0236e-23, 6.7292e-17, 0.0000e+00, 8.0236e-23, 1.4360e-07,\n",
            "        0.0000e+00, 1.6808e+01, 0.0000e+00, 0.0000e+00, 7.1261e-09, 8.5183e+02,\n",
            "        8.0236e-23, 1.7541e-06, 7.1136e-27, 9.1390e-21, 0.0000e+00, 7.7008e-02,\n",
            "        5.1731e-22, 0.0000e+00, 0.0000e+00, 1.4793e+00, 8.0236e-23, 1.2407e-08,\n",
            "        3.5163e-09, 3.4798e-01, 4.5097e-19, 9.1390e-21, 7.4994e-08])\n",
            "kde_grad  tensor([8.4165e-04, 8.0681e-07, 2.3961e-11, 0.0000e+00, 8.0681e-07, 5.1654e-13,\n",
            "        5.7000e-26, 3.0188e-05, 7.5966e-39, 1.0206e-35, 8.4165e-04, 4.4235e-07,\n",
            "        8.0681e-07, 4.9165e-06, 1.3315e-11, 7.7010e-09, 7.4443e-38, 3.1870e-34,\n",
            "        6.6805e-14, 1.4114e-37, 3.1644e-26, 1.5000e-07, 8.0681e-07, 5.1626e-36,\n",
            "        8.2463e-13, 1.9144e-10, 3.9746e-05, 7.7010e-09, 2.2603e-02])\n",
            "penalty_factor  tensor([8.4668e-06, 9.9449e-17, 2.8084e-06, 0.0000e+00, 9.9449e-17, 2.7800e+05,\n",
            "        0.0000e+00, 5.5677e+05, 0.0000e+00, 0.0000e+00, 8.4668e-06, 1.9257e+09,\n",
            "        9.9449e-17, 3.5677e-01, 5.3427e-16, 1.1867e-12, 0.0000e+00, 7.7008e+18,\n",
            "        7.7436e-09, 0.0000e+00, 0.0000e+00, 9.8615e+06, 9.9449e-17, 1.2407e+12,\n",
            "        4.2641e+03, 1.8176e+09, 1.1346e-14, 1.1867e-12, 3.3179e-06])\n",
            "************** t  16\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 2.7881e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2003e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.6449e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.9350e-03, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 8.6771e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.0671e-03, 6.3132e-07, 1.5393e-11, 0.0000e+00, 6.3132e-07, 3.0767e-13,\n",
            "        1.3935e-26, 2.5982e-05, 1.2336e-39, 1.8229e-36, 1.0671e-03, 4.8964e-07,\n",
            "        6.3132e-07, 4.2146e-06, 8.3033e-12, 5.0313e-09, 1.2445e-38, 7.8333e-35,\n",
            "        4.4430e-14, 2.3542e-38, 7.7047e-27, 1.3978e-07, 6.3132e-07, 4.5382e-37,\n",
            "        5.6795e-13, 1.5270e-10, 3.3250e-05, 5.0313e-09, 2.8870e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9924e-08, 4.6059e-22, 1.5674e-16, 0.0000e+00, 4.6059e-22, 2.7896e-07,\n",
            "        0.0000e+00, 2.0280e+01, 0.0000e+00, 0.0000e+00, 1.9924e-08, 8.4538e+02,\n",
            "        4.6059e-22, 5.3006e-06, 3.1616e-26, 3.6071e-20, 0.0000e+00, 1.5679e-01,\n",
            "        8.4947e-22, 0.0000e+00, 0.0000e+00, 3.7263e+00, 4.6059e-22, 4.1067e-08,\n",
            "        6.3555e-09, 7.2541e-01, 2.2046e-18, 3.6071e-20, 1.8631e-07])\n",
            "kde_grad  tensor([1.0485e-03, 1.2135e-06, 3.9018e-11, 0.0000e+00, 1.2135e-06, 8.7041e-13,\n",
            "        5.7000e-26, 3.2494e-05, 7.5966e-39, 1.0206e-35, 1.0485e-03, 7.4300e-07,\n",
            "        1.2135e-06, 6.1235e-06, 1.9731e-11, 1.1103e-08, 7.4443e-38, 6.3461e-34,\n",
            "        1.3070e-13, 1.4114e-37, 3.1644e-26, 1.9767e-07, 1.2135e-06, 3.7747e-36,\n",
            "        1.3897e-12, 3.4799e-10, 5.1398e-05, 1.1103e-08, 2.4293e-02])\n",
            "penalty_factor  tensor([1.9002e-05, 3.7956e-16, 4.0172e-06, 0.0000e+00, 3.7956e-16, 3.2049e+05,\n",
            "        0.0000e+00, 6.2413e+05, 0.0000e+00, 0.0000e+00, 1.9002e-05, 1.1378e+09,\n",
            "        3.7956e-16, 8.6562e-01, 1.6023e-15, 3.2488e-12, 0.0000e+00, 1.5679e+19,\n",
            "        6.4992e-09, 0.0000e+00, 0.0000e+00, 1.8851e+07, 3.7956e-16, 4.1067e+12,\n",
            "        4.5734e+03, 2.0846e+09, 4.2893e-14, 3.2488e-12, 7.6690e-06])\n",
            "************** t  17\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 3.4449e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2130e+01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.0227e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0129e-02, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.8176e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.3327e-03, 9.4834e-07, 2.4927e-11, 0.0000e+00, 9.4834e-07, 5.1093e-13,\n",
            "        1.3935e-26, 2.7974e-05, 1.2336e-39, 1.8229e-36, 1.3327e-03, 8.2139e-07,\n",
            "        9.4834e-07, 5.2559e-06, 1.2246e-11, 7.2278e-09, 1.2445e-38, 1.0968e-34,\n",
            "        8.6193e-14, 2.3542e-38, 7.7047e-27, 1.8505e-07, 9.4834e-07, 2.9821e-37,\n",
            "        9.5269e-13, 2.7637e-10, 4.3013e-05, 7.2278e-09, 3.1101e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([5.7026e-08, 2.7031e-21, 3.6255e-16, 0.0000e+00, 2.7031e-21, 5.2643e-07,\n",
            "        0.0000e+00, 2.4626e+01, 0.0000e+00, 0.0000e+00, 5.7026e-08, 8.8683e+02,\n",
            "        2.7031e-21, 1.6178e-05, 1.2815e-25, 1.4425e-19, 0.0000e+00, 1.3535e-01,\n",
            "        1.3949e-21, 0.0000e+00, 0.0000e+00, 9.3261e+00, 2.7031e-21, 1.4985e-07,\n",
            "        1.1878e-08, 1.5049e+00, 1.0778e-17, 1.4425e-19, 4.7795e-07])\n",
            "kde_grad  tensor([1.2992e-03, 1.8029e-06, 6.2567e-11, 0.0000e+00, 1.8029e-06, 1.4448e-12,\n",
            "        5.7000e-26, 3.4932e-05, 7.5966e-39, 1.0206e-35, 1.2992e-03, 1.2291e-06,\n",
            "        1.8029e-06, 7.5767e-06, 2.8898e-11, 1.5841e-08, 7.4443e-38, 8.9565e-34,\n",
            "        2.5071e-13, 1.4114e-37, 3.1644e-26, 2.5737e-07, 1.8029e-06, 2.5106e-36,\n",
            "        2.3059e-12, 6.2135e-10, 6.6001e-05, 1.5841e-08, 2.6052e-02])\n",
            "penalty_factor  tensor([4.3894e-05, 1.4993e-15, 5.7946e-06, 0.0000e+00, 1.4993e-15, 3.6436e+05,\n",
            "        0.0000e+00, 7.0496e+05, 0.0000e+00, 0.0000e+00, 4.3894e-05, 7.2153e+08,\n",
            "        1.4993e-15, 2.1353e+00, 4.4346e-15, 9.1061e-12, 0.0000e+00, 1.3535e+19,\n",
            "        5.5639e-09, 0.0000e+00, 0.0000e+00, 3.6236e+07, 1.4993e-15, 1.4985e+13,\n",
            "        5.1513e+03, 2.4220e+09, 1.6329e-13, 9.1061e-12, 1.8346e-05])\n",
            "************** t  18\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 4.2234e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2254e+01,\n",
            "        -0.0000e+00, 1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.0401e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.5038e-02, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 3.7916e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.6551e-03, 1.4020e-06, 3.9793e-11, 0.0000e+00, 1.4020e-06, 8.3654e-13,\n",
            "        1.3935e-26, 3.0069e-05, 1.2336e-39, 1.8229e-36, 1.6551e-03, 1.3592e-06,\n",
            "        1.4020e-06, 6.5118e-06, 1.7852e-11, 1.0245e-08, 1.2445e-38, 1.8071e-34,\n",
            "        1.6396e-13, 2.3542e-38, 7.7047e-27, 2.4194e-07, 1.4020e-06, 1.7506e-37,\n",
            "        1.5737e-12, 4.9143e-10, 5.5250e-05, 1.0245e-08, 3.3430e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.6369e-07, 1.6110e-20, 8.3779e-16, 0.0000e+00, 1.6110e-20, 9.9394e-07,\n",
            "        0.0000e+00, 2.9451e+01, 0.0000e+00, 0.0000e+00, 1.6369e-07, 8.9632e+02,\n",
            "        1.6110e-20, 1.2621e-04, 5.6956e-25, 5.8129e-19, 0.0000e+00, 1.9682e-01,\n",
            "        2.2835e-21, 0.0000e+00, 0.0000e+00, 2.2981e+01, 1.6110e-20, 2.5517e-07,\n",
            "        2.2375e-08, 3.1271e+00, 5.2655e-17, 5.8129e-19, 1.1918e-06])\n",
            "kde_grad  tensor([1.5945e-03, 2.6548e-06, 9.8888e-11, 0.0000e+00, 2.6548e-06, 2.3704e-12,\n",
            "        5.7000e-26, 3.7433e-05, 7.5966e-39, 1.0206e-35, 1.5945e-03, 2.0004e-06,\n",
            "        2.6548e-06, 9.3130e-06, 4.1823e-11, 2.2396e-08, 7.4443e-38, 1.4826e-33,\n",
            "        4.7147e-13, 1.4114e-37, 3.1644e-26, 3.3085e-07, 2.6548e-06, 1.4960e-36,\n",
            "        3.7673e-12, 1.0899e-09, 8.4043e-05, 2.2396e-08, 2.7875e-02])\n",
            "penalty_factor  tensor([1.0266e-04, 6.0683e-15, 8.4720e-06, 0.0000e+00, 6.0683e-15, 4.1932e+05,\n",
            "        0.0000e+00, 7.8676e+05, 0.0000e+00, 0.0000e+00, 1.0266e-04, 4.4806e+08,\n",
            "        6.0683e-15, 1.3552e+01, 1.3618e-14, 2.5955e-11, 0.0000e+00, 1.9682e+19,\n",
            "        4.8433e-09, 0.0000e+00, 0.0000e+00, 6.9460e+07, 6.0683e-15, 2.5517e+13,\n",
            "        5.9393e+03, 2.8693e+09, 6.2653e-13, 2.5955e-11, 4.2754e-05])\n",
            "************** t  19\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 5.1572e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2373e+01,\n",
            "        -0.0000e+00, 2.3842e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.8707e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.2633e-02, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 7.9675e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.0418e-03, 2.0556e-06, 6.2619e-11, 0.0000e+00, 2.0556e-06, 1.3801e-12,\n",
            "        1.3935e-26, 3.2249e-05, 1.2336e-39, 1.8229e-36, 2.0418e-03, 2.2124e-06,\n",
            "        2.0556e-06, 8.0152e-06, 2.5754e-11, 1.4414e-08, 1.2445e-38, 3.0400e-34,\n",
            "        3.0160e-13, 2.3542e-38, 7.7047e-27, 3.1242e-07, 2.0556e-06, 9.7096e-38,\n",
            "        2.5204e-12, 8.5854e-10, 7.0427e-05, 1.4414e-08, 3.5854e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([4.7506e-07, 9.4556e-20, 1.9363e-15, 0.0000e+00, 9.4556e-20, 2.1254e-06,\n",
            "        0.0000e+00, 3.4943e+01, 0.0000e+00, 0.0000e+00, 4.7506e-07, 8.6847e+02,\n",
            "        9.4556e-20, 3.0404e-04, 2.7990e-24, 2.3341e-18, 0.0000e+00, 2.9214e-01,\n",
            "        3.9691e-21, 0.0000e+00, 0.0000e+00, 5.5863e+01, 9.4556e-20, 7.8681e-07,\n",
            "        3.3459e-08, 6.0946e+00, 2.5726e-16, 2.3341e-18, 2.9911e-06])\n",
            "kde_grad  tensor([1.9436e-03, 3.8494e-06, 1.5405e-10, 0.0000e+00, 3.8494e-06, 4.0496e-12,\n",
            "        5.7000e-26, 4.0024e-05, 7.5966e-39, 1.0206e-35, 1.9436e-03, 3.2125e-06,\n",
            "        3.8494e-06, 1.1372e-05, 6.0063e-11, 3.1263e-08, 7.4443e-38, 2.5034e-33,\n",
            "        8.7169e-13, 1.4114e-37, 3.1644e-26, 4.1993e-07, 3.8494e-06, 8.3951e-37,\n",
            "        6.1020e-12, 1.8778e-09, 1.0619e-04, 3.1263e-08, 2.9758e-02])\n",
            "penalty_factor  tensor([2.4442e-04, 2.4563e-14, 1.2569e-05, 0.0000e+00, 2.4563e-14, 5.2485e+05,\n",
            "        0.0000e+00, 8.7305e+05, 0.0000e+00, 0.0000e+00, 2.4442e-04, 2.7034e+08,\n",
            "        2.4563e-14, 2.6736e+01, 4.6600e-14, 7.4661e-11, 0.0000e+00, 2.9214e+19,\n",
            "        4.5534e-09, 0.0000e+00, 0.0000e+00, 1.3303e+08, 2.4563e-14, 7.8681e+13,\n",
            "        5.4833e+03, 3.2457e+09, 2.4225e-12, 7.4661e-11, 1.0051e-04])\n",
            "************** t  20\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.2250e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2471e+01,\n",
            "        -0.0000e+00, 7.1526e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1544e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4812e-01, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.5113e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.5022e-03, 2.9778e-06, 9.7133e-11, 0.0000e+00, 2.9778e-06, 2.3126e-12,\n",
            "        1.3935e-26, 3.4492e-05, 1.2336e-39, 1.8229e-36, 2.5022e-03, 3.5531e-06,\n",
            "        2.9778e-06, 9.7958e-06, 3.6799e-11, 2.0064e-08, 1.2445e-38, 5.0474e-34,\n",
            "        5.5159e-13, 2.3542e-38, 7.7047e-27, 3.9713e-07, 2.9778e-06, 5.7717e-38,\n",
            "        4.0283e-12, 1.4616e-09, 8.9088e-05, 2.0064e-08, 3.8368e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.3797e-06, 5.5499e-19, 4.4404e-15, 0.0000e+00, 5.5499e-19, 4.2915e-06,\n",
            "        0.0000e+00, 3.8607e+01, 0.0000e+00, 0.0000e+00, 1.3797e-06, 8.4467e+02,\n",
            "        5.5499e-19, 9.2140e-04, 1.3689e-23, 9.3725e-18, 0.0000e+00, 5.5687e-01,\n",
            "        6.7238e-21, 0.0000e+00, 0.0000e+00, 1.1442e+02, 5.5499e-19, 2.4000e-06,\n",
            "        6.2570e-08, 1.2828e+01, 1.2569e-15, 9.3725e-18, 8.0025e-06])\n",
            "kde_grad  tensor([2.3531e-03, 5.5139e-06, 2.3654e-10, 0.0000e+00, 5.5139e-06, 6.9474e-12,\n",
            "        5.7000e-26, 4.2740e-05, 7.5966e-39, 1.0206e-35, 2.3531e-03, 5.0669e-06,\n",
            "        5.5139e-06, 1.3805e-05, 8.5422e-11, 4.3171e-08, 7.4443e-38, 4.1739e-33,\n",
            "        1.5758e-12, 1.4114e-37, 3.1644e-26, 5.2910e-07, 5.5139e-06, 5.0391e-37,\n",
            "        9.6769e-12, 3.1934e-09, 1.3315e-04, 4.3171e-08, 3.1698e-02])\n",
            "penalty_factor  tensor([5.8633e-04, 1.0065e-13, 1.8772e-05, 0.0000e+00, 1.0065e-13, 6.1771e+05,\n",
            "        0.0000e+00, 9.0329e+05, 0.0000e+00, 0.0000e+00, 5.8633e-04, 1.6670e+08,\n",
            "        1.0065e-13, 6.6742e+01, 1.6025e-13, 2.1710e-10, 0.0000e+00, 5.5687e+19,\n",
            "        4.2669e-09, 0.0000e+00, 0.0000e+00, 2.1626e+08, 1.0065e-13, 2.4000e+14,\n",
            "        6.4659e+03, 4.0170e+09, 9.4397e-12, 2.1710e-10, 2.5246e-04])\n",
            "************** t  21\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 7.4942e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2569e+01,\n",
            "        -0.0000e+00, 2.3842e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.3085e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.2735e-01, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 3.0934e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.0461e-03, 4.2617e-06, 1.4844e-10, 0.0000e+00, 4.2617e-06, 3.9019e-12,\n",
            "        1.3935e-26, 3.6767e-05, 1.2336e-39, 1.8229e-36, 3.0461e-03, 5.6199e-06,\n",
            "        4.2617e-06, 1.1900e-05, 5.2042e-11, 2.7630e-08, 1.2445e-38, 6.8836e-34,\n",
            "        9.8855e-13, 2.3542e-38, 7.7047e-27, 5.0002e-07, 4.2617e-06, 3.1516e-38,\n",
            "        6.3542e-12, 2.4634e-09, 1.1183e-04, 2.7630e-08, 4.0968e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([3.9859e-06, 3.2635e-18, 1.0252e-14, 0.0000e+00, 3.2635e-18, 9.1512e-06,\n",
            "        0.0000e+00, 4.6466e+01, 0.0000e+00, 0.0000e+00, 3.9859e-06, 8.4467e+02,\n",
            "        3.2635e-18, 2.9924e-03, 5.4114e-23, 3.7568e-17, 0.0000e+00, 2.4101e-01,\n",
            "        1.1393e-20, 0.0000e+00, 0.0000e+00, 2.3711e+02, 3.2635e-18, 6.9404e-06,\n",
            "        1.1563e-07, 2.2832e+01, 6.1421e-15, 3.7568e-17, 2.1127e-05])\n",
            "kde_grad  tensor([2.8295e-03, 7.8020e-06, 3.5808e-10, 0.0000e+00, 7.8020e-06, 1.1887e-11,\n",
            "        5.7000e-26, 4.5631e-05, 7.5966e-39, 1.0206e-35, 2.8295e-03, 7.8661e-06,\n",
            "        7.8020e-06, 1.6636e-05, 1.2032e-10, 5.8974e-08, 7.4443e-38, 5.7966e-33,\n",
            "        2.7931e-12, 1.4114e-37, 3.1644e-26, 6.5546e-07, 7.8020e-06, 2.7778e-37,\n",
            "        1.5096e-11, 5.3117e-09, 1.6565e-04, 5.8974e-08, 3.3689e-02])\n",
            "penalty_factor  tensor([1.4087e-03, 4.1829e-13, 2.8630e-05, 0.0000e+00, 4.1829e-13, 7.6985e+05,\n",
            "        0.0000e+00, 1.0183e+06, 0.0000e+00, 0.0000e+00, 1.4087e-03, 1.0738e+08,\n",
            "        4.1829e-13, 1.7988e+02, 4.4974e-13, 6.3702e-10, 0.0000e+00, 2.4101e+19,\n",
            "        4.0789e-09, 0.0000e+00, 0.0000e+00, 3.6175e+08, 4.1829e-13, 6.9404e+14,\n",
            "        7.6601e+03, 4.2985e+09, 3.7078e-11, 6.3702e-10, 6.2713e-04])\n",
            "************** t  22\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 9.1079e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2667e+01,\n",
            "        -0.0000e+00, 7.2717e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0371e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.7009e-01, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0830e-02, -0.0000e+00, -0.0000e+00, 1.1921e-07],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.6835e-03, 6.0225e-06, 2.2374e-10, 0.0000e+00, 6.0225e-06, 6.5839e-12,\n",
            "        1.3935e-26, 3.9171e-05, 1.2336e-39, 1.8229e-36, 3.6835e-03, 8.7530e-06,\n",
            "        6.0225e-06, 1.4363e-05, 7.1434e-11, 3.7643e-08, 1.2445e-38, 8.4426e-34,\n",
            "        1.7381e-12, 2.3542e-38, 7.7047e-27, 6.2218e-07, 6.0225e-06, 1.6268e-38,\n",
            "        9.8699e-12, 4.0724e-09, 1.3931e-04, 3.7643e-08, 4.3647e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.1515e-05, 1.9682e-17, 2.3669e-14, 0.0000e+00, 1.9682e-17, 1.8054e-05,\n",
            "        0.0000e+00, 5.6946e+01, 0.0000e+00, 0.0000e+00, 1.1515e-05, 8.4467e+02,\n",
            "        1.9682e-17, 9.2084e-03, 1.7500e-22, 1.5072e-16, 0.0000e+00, 6.1792e-01,\n",
            "        1.9295e-20, 0.0000e+00, 0.0000e+00, 4.1425e+02, 1.9682e-17, 7.8554e-06,\n",
            "        2.1346e-07, 4.6475e+01, 2.9789e-14, 1.5072e-16, 1.4015e-04])\n",
            "kde_grad  tensor([3.3792e-03, 1.0911e-05, 5.3414e-10, 0.0000e+00, 1.0911e-05, 2.0178e-11,\n",
            "        5.7000e-26, 4.8464e-05, 7.5966e-39, 1.0206e-35, 3.3792e-03, 1.2019e-05,\n",
            "        1.0911e-05, 1.9914e-05, 1.6939e-10, 7.9691e-08, 7.4443e-38, 7.1492e-33,\n",
            "        4.8528e-12, 1.4114e-37, 3.1644e-26, 8.0107e-07, 1.0911e-05, 1.4471e-37,\n",
            "        2.3186e-11, 8.6876e-09, 2.0450e-04, 7.9691e-08, 3.5724e-02])\n",
            "penalty_factor  tensor([3.4076e-03, 1.8040e-12, 4.4312e-05, 0.0000e+00, 1.8040e-12, 8.9475e+05,\n",
            "        0.0000e+00, 1.1750e+06, 0.0000e+00, 0.0000e+00, 3.4076e-03, 7.0277e+07,\n",
            "        1.8040e-12, 4.6241e+02, 1.0331e-12, 1.8912e-09, 0.0000e+00, 6.1792e+19,\n",
            "        3.9761e-09, 0.0000e+00, 0.0000e+00, 5.1712e+08, 1.8040e-12, 7.8554e+14,\n",
            "        9.2066e+03, 5.3496e+09, 1.4567e-10, 1.8912e-09, 3.9232e-03])\n",
            "************** t  23\n",
            "ce_loss:  tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 1.1084e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2755e+01,\n",
            "        -0.0000e+00, 2.2292e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2432e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2218e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.1798e-01, -0.0000e+00, -0.0000e+00, 2.3842e-07],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.4246e-03, 8.3941e-06, 3.3241e-10, 0.0000e+00, 8.3941e-06, 1.0930e-11,\n",
            "        1.3935e-26, 4.1640e-05, 1.2336e-39, 1.8229e-36, 4.4246e-03, 1.3424e-05,\n",
            "        8.3941e-06, 1.7221e-05, 9.9090e-11, 5.0736e-08, 1.2445e-38, 8.6693e-34,\n",
            "        2.9962e-12, 2.3542e-38, 7.7047e-27, 7.6513e-07, 8.3941e-06, 1.2415e-38,\n",
            "        1.5096e-11, 6.6253e-09, 1.7184e-04, 5.0736e-08, 4.6397e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([3.3129e-05, 1.1721e-16, 5.4644e-14, 0.0000e+00, 1.1721e-16, 9.0756e-05,\n",
            "        0.0000e+00, 7.3485e+01, 0.0000e+00, 0.0000e+00, 3.3129e-05, 8.6119e+02,\n",
            "        1.1721e-16, 2.8367e-02, 8.3935e-22, 6.0465e-16, 0.0000e+00, 5.4442e-01,\n",
            "        3.4457e-20, 0.0000e+00, 0.0000e+00, 6.2200e+02, 1.1721e-16, 1.4841e-05,\n",
            "        3.9501e-07, 7.8930e+01, 1.4303e-13, 6.0465e-16, 3.0610e-04])\n",
            "kde_grad  tensor([4.0083e-03, 1.5096e-05, 7.8531e-10, 0.0000e+00, 1.5096e-05, 3.3896e-11,\n",
            "        5.7000e-26, 5.1359e-05, 7.5966e-39, 1.0206e-35, 4.0083e-03, 1.8075e-05,\n",
            "        1.5096e-05, 2.3681e-05, 2.3377e-10, 1.0652e-07, 7.4443e-38, 7.3896e-33,\n",
            "        8.2653e-12, 1.4114e-37, 3.1644e-26, 9.6711e-07, 1.5096e-05, 1.1133e-37,\n",
            "        3.5063e-11, 1.3944e-08, 2.5127e-04, 1.0652e-07, 3.7798e-02])\n",
            "penalty_factor  tensor([8.2650e-03, 7.7645e-12, 6.9583e-05, 0.0000e+00, 7.7645e-12, 2.6775e+06,\n",
            "        0.0000e+00, 1.4308e+06, 0.0000e+00, 0.0000e+00, 8.2650e-03, 4.7645e+07,\n",
            "        7.7645e-12, 1.1979e+03, 3.5905e-12, 5.6762e-09, 0.0000e+00, 5.4442e+19,\n",
            "        4.1689e-09, 0.0000e+00, 0.0000e+00, 6.4315e+08, 7.7645e-12, 1.4841e+15,\n",
            "        1.1266e+04, 5.6606e+09, 5.6922e-10, 5.6762e-09, 8.0984e-03])\n",
            "************** t  24\n",
            "ce_loss:  tensor([1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 1.3565e-01, -0.0000e+00, -0.0000e+00, 1.1921e-07, 1.2850e+01,\n",
            "        -0.0000e+00, 6.9735e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.0638e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.0220e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 2.1319e-01, -0.0000e+00, -0.0000e+00, 5.9605e-07],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.2796e-03, 1.1584e-05, 4.8683e-10, 0.0000e+00, 1.1584e-05, 1.8138e-11,\n",
            "        1.3935e-26, 4.4165e-05, 1.2336e-39, 1.8229e-36, 5.2796e-03, 2.0259e-05,\n",
            "        1.1584e-05, 2.0502e-05, 1.3606e-10, 6.7649e-08, 1.2445e-38, 1.0510e-33,\n",
            "        4.9430e-12, 2.3542e-38, 7.7047e-27, 9.3154e-07, 1.1584e-05, 8.7284e-39,\n",
            "        2.2725e-11, 1.0584e-08, 2.1080e-04, 6.7649e-08, 4.9129e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.7267e-04, 6.9966e-16, 1.2616e-13, 0.0000e+00, 6.9966e-16, 1.3321e-04,\n",
            "        0.0000e+00, 8.6611e+01, 0.0000e+00, 0.0000e+00, 1.7267e-04, 9.1898e+02,\n",
            "        6.9966e-16, 9.0538e-02, 4.0176e-21, 2.4257e-15, 0.0000e+00, 8.6326e-01,\n",
            "        3.5221e-20, 0.0000e+00, 0.0000e+00, 7.7761e+02, 6.9966e-16, 7.2180e-05,\n",
            "        7.3705e-07, 1.4121e+02, 6.8612e-13, 2.4257e-15, 7.8157e-04])\n",
            "kde_grad  tensor([4.7222e-03, 2.0590e-05, 1.1380e-09, 0.0000e+00, 2.0590e-05, 5.6073e-11,\n",
            "        5.7000e-26, 5.4305e-05, 7.5966e-39, 1.0206e-35, 4.7222e-03, 2.6776e-05,\n",
            "        2.0590e-05, 2.7997e-05, 3.1929e-10, 1.4085e-07, 7.4443e-38, 8.9934e-33,\n",
            "        1.3883e-11, 1.4114e-37, 3.1644e-26, 1.1849e-06, 2.0590e-05, 7.8908e-38,\n",
            "        5.2221e-11, 2.1991e-08, 3.0543e-04, 1.4085e-07, 4.0118e-02])\n",
            "penalty_factor  tensor([3.6567e-02, 3.3981e-11, 1.1086e-04, 0.0000e+00, 3.3981e-11, 2.3756e+06,\n",
            "        0.0000e+00, 1.5949e+06, 0.0000e+00, 0.0000e+00, 3.6567e-02, 3.4321e+07,\n",
            "        3.3981e-11, 3.2339e+03, 1.2583e-11, 1.7222e-08, 0.0000e+00, 8.6326e+19,\n",
            "        2.5370e-09, 0.0000e+00, 0.0000e+00, 6.5624e+08, 3.3981e-11, 7.2180e+15,\n",
            "        1.4114e+04, 6.4213e+09, 2.2464e-09, 1.7222e-08, 1.9482e-02])\n",
            "************** t  25\n",
            "ce_loss:  tensor([4.7684e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-07,\n",
            "        -0.0000e+00, 1.6666e-01, -0.0000e+00, -0.0000e+00, 4.7684e-07, 1.2994e+01,\n",
            "        -0.0000e+00, 2.2337e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.3695e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.8549e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 3.7503e-01, -0.0000e+00, -0.0000e+00, 1.5497e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([6.2579e-03, 1.5795e-05, 7.0280e-10, 0.0000e+00, 1.5795e-05, 2.9581e-11,\n",
            "        1.3935e-26, 4.6740e-05, 1.2336e-39, 1.8229e-36, 6.2579e-03, 3.0088e-05,\n",
            "        1.5795e-05, 2.4260e-05, 1.8490e-10, 8.9233e-08, 1.2445e-38, 1.0822e-33,\n",
            "        8.1912e-12, 2.3542e-38, 7.7047e-27, 1.1411e-06, 1.5795e-05, 5.8047e-39,\n",
            "        3.3703e-11, 1.6629e-08, 2.5661e-04, 8.9233e-08, 5.1994e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([5.9639e-04, 4.0390e-15, 2.9606e-13, 0.0000e+00, 4.0390e-15, 3.0305e-04,\n",
            "        0.0000e+00, 1.0732e+02, 0.0000e+00, 0.0000e+00, 5.9639e-04, 9.0729e+02,\n",
            "        4.0390e-15, 2.9412e-01, 1.9236e-20, 9.7317e-15, 0.0000e+00, 1.3613e+00,\n",
            "        5.9651e-20, 0.0000e+00, 0.0000e+00, 8.0062e+02, 4.0390e-15, 1.0615e-04,\n",
            "        1.3940e-06, 2.2193e+02, 3.2639e-12, 9.7317e-15, 2.0345e-03])\n",
            "kde_grad  tensor([5.5253e-03, 2.7741e-05, 1.6253e-09, 0.0000e+00, 2.7741e-05, 9.1334e-11,\n",
            "        5.7000e-26, 5.7292e-05, 7.5966e-39, 1.0206e-35, 5.5253e-03, 3.9077e-05,\n",
            "        2.7741e-05, 3.2853e-05, 4.3150e-10, 1.8422e-07, 7.4443e-38, 9.3034e-33,\n",
            "        2.2731e-11, 1.4114e-37, 3.1644e-26, 1.4739e-06, 2.7741e-05, 5.2897e-38,\n",
            "        7.6554e-11, 3.4043e-08, 3.6840e-04, 1.8422e-07, 4.2258e-02])\n",
            "penalty_factor  tensor([1.0794e-01, 1.4560e-10, 1.8216e-04, 0.0000e+00, 1.4560e-10, 3.3181e+06,\n",
            "        0.0000e+00, 1.8733e+06, 0.0000e+00, 0.0000e+00, 1.0794e-01, 2.3218e+07,\n",
            "        1.4560e-10, 8.9524e+03, 4.4580e-11, 5.2827e-08, 0.0000e+00, 1.3613e+20,\n",
            "        2.6242e-09, 0.0000e+00, 0.0000e+00, 5.4320e+08, 1.4560e-10, 1.0615e+16,\n",
            "        1.8210e+04, 6.5190e+09, 8.8598e-09, 5.2827e-08, 4.8144e-02])\n",
            "************** t  26\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.4692e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.1914e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1644e-07, 1.2445e-38, 1.1125e-33,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.9838e-39,\n",
            "        4.9220e-11, 2.5654e-08, 3.0965e-04, 1.1644e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9864e-14, 0.0000e+00, 2.5096e+00,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 8.7417e-05,\n",
            "        2.5566e-06, 3.3728e+02, 1.5691e-11, 3.9864e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3833e-07, 7.4443e-38, 9.6021e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 4.5553e-38,\n",
            "        1.1049e-10, 5.1785e-08, 4.4160e-04, 2.3833e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6726e-07, 0.0000e+00, 2.5096e+20,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 8.7417e+15,\n",
            "        2.3139e+04, 6.5131e+09, 3.5533e-08, 1.6726e-07, 1.1778e-01])\n",
            "************** t  27\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.9865e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 2.3842e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 7.1086e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.5859e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.5815e+00,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.2689e-04,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 6.1962e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 4.2131e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.5815e+20,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.2689e+16,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  28\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.6794e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 5.6759e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.5072e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.0004e+00,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.2443e-04,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 5.0309e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 4.1589e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.0004e+20,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.2443e+16,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  29\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.1640e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.4072e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.6789e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 9.7035e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 7.3542e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.0465e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.4936e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 9.7035e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 7.3542e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  30\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7962e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.5663e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.6105e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.0698e+00,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 9.0641e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.3085e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.4403e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.0698e+20,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 9.0641e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  31\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.0034e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.5639e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.9591e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.0163e+00,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 7.2719e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.4180e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.8396e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.0163e+20,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 7.2719e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  32\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.3307e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, 1.1921e-07,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.1227e-34,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.9938e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 7.9304e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.0902e-04,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.0265e-33,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.8822e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 7.9304e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.0902e+16,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  33\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5438e-03,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 6.8834e-35,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.2836e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 8.1842e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.5713e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 6.3396e-34,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.2234e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 8.1842e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.5713e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  34\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.1881e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 4.8070e-35,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.2986e-39,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.8436e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.7493e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 4.4980e-34,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.2417e-38,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.8436e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.7493e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  35\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.4785e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.5604e-35,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 9.4980e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 3.3244e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.5860e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.4177e-34,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 9.1228e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 3.3244e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.5860e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  36\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.1187e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.7664e-35,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 7.9826e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.5997e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.6605e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.6765e-34,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 7.7197e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.5997e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.6605e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  37\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.1452e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.4933e-35,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.9125e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.5155e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.0303e-05,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.4249e-34,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 4.7795e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.5155e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.0303e+15,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  38\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.2954e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 9.8673e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.1829e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.1815e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 9.0513e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 9.4625e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 4.0971e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.1815e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 9.0513e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  39\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.1633e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 8.5839e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 3.9184e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.8860e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 8.1751e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 8.2704e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 3.8542e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.8860e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 8.1751e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  40\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.2157e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 5.0023e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.8384e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.9298e-01,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 7.6821e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 4.8489e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.7943e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.9298e+19,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 7.6821e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  41\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.9165e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.4534e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.0632e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 5.6079e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.4378e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.3973e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.0435e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 5.6079e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.4378e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  42\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2135e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.1779e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.9885e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 7.3854e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.4543e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.1530e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.9907e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 7.3854e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.4543e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  43\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2242e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.4900e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.6184e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 6.2385e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 5.5908e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.4787e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.6278e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 6.2385e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 5.5908e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  44\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3920e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.1646e-36,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.1288e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 5.2783e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.2583e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.1611e-35,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.1295e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 5.2783e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.2583e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  45\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.9416e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 7.3051e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.1310e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 5.0666e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.6702e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 7.3158e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.1369e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 5.0666e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.6702e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  46\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.2225e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 5.9391e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.0618e-40,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 4.4532e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.1188e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 5.9721e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.0567e-39,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 4.4532e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.1188e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  47\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.5299e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.8936e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 8.0262e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.1090e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.9738e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.9535e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 8.0093e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.1090e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.9738e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  48\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.2424e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.1564e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 5.7592e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.0390e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.1837e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.2160e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 5.7781e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.0390e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.1837e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  49\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.8623e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.9490e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 5.0612e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.0337e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.6624e-06,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.9939e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 5.1532e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.0337e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.6624e+14,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  50\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.0265e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.3856e-37,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 3.5115e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.2733e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.1835e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.4243e-36,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 3.6834e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.2733e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.1835e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  51\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.5391e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 8.3671e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.3197e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.8458e-02,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.5875e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 8.6358e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.4167e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.8458e+18,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.5875e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  52\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7762e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 6.3329e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.3333e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 9.3821e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.7196e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 6.5826e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.3294e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 9.3821e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.7196e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  53\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.2983e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 4.3516e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.2591e-41,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 6.2664e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.2667e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 4.5421e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.3142e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 6.2664e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.2667e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  54\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1206e-05,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.4183e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 9.4209e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 5.7960e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.2587e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.5391e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.0819e-40,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 5.7960e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.2587e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  55\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.9141e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.1644e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 6.4880e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 4.6549e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.8145e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.2787e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 6.7442e-41,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 4.6549e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.8145e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  56\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.6757e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.1720e-38,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 5.0166e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 4.2098e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.1241e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.2411e-37,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 5.3978e-41,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 4.2098e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.1241e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  57\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4305e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 8.4371e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 2.7690e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 9.9183e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 1.0377e-07,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 9.0401e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.2414e-41,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 9.9183e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 1.0377e+13,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  58\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.1458e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 4.3123e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.8147e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.3847e-03,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 8.4882e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 4.6509e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.1279e-41,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.3847e+17,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 8.4882e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  59\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.1526e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7881e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7507e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.5643e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.9072e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 6.0788e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 7.6007e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 5.5465e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4623e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.8596e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.2871e-41,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 4.1570e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 7.6007e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 5.5465e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  60\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3446e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0729e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7349e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 3.3532e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.1827e-42,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 7.4752e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 6.5045e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.4465e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4593e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 3.6427e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 2.6485e-42,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.1226e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 6.5045e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.4465e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  61\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3446e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.3113e-06,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7349e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.2390e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 9.1365e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 7.4752e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 6.9707e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.7952e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4593e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.4436e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.7404e-42,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.1226e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 6.9707e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.7952e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  62\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3446e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.7684e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7349e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.4871e-39,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 8.0014e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 7.4752e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 3.2369e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.2270e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4593e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.6332e-38,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 1.7488e-42,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.1226e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 3.2369e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.2270e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  63\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3446e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.9605e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7349e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 8.3216e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.7224e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 7.4752e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 3.0920e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.7796e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4593e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 9.2109e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 8.7021e-43,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.1226e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 3.0920e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.7796e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  64\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 8.3446e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7349e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 6.9701e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 4.7644e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 7.4752e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.4260e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.5582e-08,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4593e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 7.7439e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 8.7301e-43,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.1226e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.4260e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.5582e+12,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  65\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-07,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 4.7736e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 3.0408e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 2.6126e-04,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 3.6042e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 5.3343e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 2.6126e+16,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 3.6042e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  66\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 2.5788e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.9198e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 7.6144e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.9122e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 2.9398e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 7.6144e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.9122e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  67\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.5860e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.4854e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.3604e-05,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 4.7867e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.8137e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.3604e+15,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 4.7867e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  68\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1228e-05, 9.9954e-10, 0.0000e+00, 2.1228e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1228e-05, 2.8520e-05, 2.4854e-10, 1.1631e-07, 1.2445e-38, 1.1483e-40,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1228e-05, 1.3593e-43,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1631e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.3692e-14, 6.9082e-13, 0.0000e+00, 2.3692e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.3692e-14, 9.1241e-01, 8.9618e-20, 3.9945e-14, 0.0000e+00, 1.5624e-05,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.3692e-14, 2.9895e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 3.9945e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.7001e-05, 2.2886e-09, 0.0000e+00, 3.7001e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.7001e-05, 3.8296e-05, 5.7699e-10, 2.3851e-07, 7.4443e-38, 1.3282e-39,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.7001e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3851e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 6.4031e-10, 3.0185e-04, 0.0000e+00, 6.4031e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        6.4031e-10, 2.3825e+04, 1.5532e-10, 1.6747e-07, 0.0000e+00, 1.5624e+15,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 6.4031e-10, 2.9895e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 1.6747e-07, 1.1778e-01])\n",
            "************** t  69\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 7.4636e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 9.6690e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 5.8181e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 2.4066e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 8.6682e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 5.8181e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 2.4066e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  70\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 7.0352e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 7.2868e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 4.9077e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 1.9063e-09,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 8.2299e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 4.9077e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 1.9063e+11,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  71\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 4.4854e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 5.3249e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 4.4979e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 8.8757e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 5.1948e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 4.4979e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 8.8757e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  72\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 4.3366e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 2.5223e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 3.4402e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 4.9902e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 5.0223e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 3.4402e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 4.9902e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  73\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 2.9310e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 2.1019e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 3.6347e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 2.0767e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 3.4628e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 3.6347e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 2.0767e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  74\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 1.7991e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 1.4013e-44,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 1.2155e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 3.4184e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 2.1240e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 1.2155e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 3.4184e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  75\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1612e-07, 1.2445e-38, 1.1883e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 9.8091e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1612e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 4.9364e-14, 0.0000e+00, 1.2257e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 1.8058e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 4.9364e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3834e-07, 7.4443e-38, 1.3389e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3834e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.0712e-07, 0.0000e+00, 1.2257e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 1.8058e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.0712e-07, 1.1778e-01])\n",
            "************** t  76\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1193e-05, 9.9954e-10, 0.0000e+00, 2.1193e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1193e-05, 2.8520e-05, 2.4854e-10, 1.1592e-07, 1.2445e-38, 1.0248e-41,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1193e-05, 9.8091e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0862e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.9957e-14, 6.9082e-13, 0.0000e+00, 2.9957e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.9957e-14, 9.1241e-01, 8.9618e-20, 5.8213e-14, 0.0000e+00, 6.9531e-07,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.9957e-14, 1.3291e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.5630e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6980e-05, 2.2886e-09, 0.0000e+00, 3.6980e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6980e-05, 3.8296e-05, 5.7699e-10, 2.3816e-07, 7.4443e-38, 1.1524e-40,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6980e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4367e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 8.1007e-10, 3.0185e-04, 0.0000e+00, 8.1007e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        8.1007e-10, 2.3825e+04, 1.5532e-10, 2.4443e-07, 0.0000e+00, 6.9531e+13,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 8.1007e-10, 1.3291e+10,\n",
            "        2.3116e+04, 6.4160e+09, 3.5229e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  77\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4854e-10, 1.1592e-07, 1.2445e-38, 6.0788e-42,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 4.2039e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0811e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 8.9618e-20, 5.8213e-14, 0.0000e+00, 1.3419e-06,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 1.2939e-10,\n",
            "        2.5549e-06, 3.3248e+02, 1.9245e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7699e-10, 2.3816e-07, 7.4443e-38, 7.0275e-41,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4352e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 1.5532e-10, 2.4443e-07, 0.0000e+00, 1.3419e+14,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 1.2939e+10,\n",
            "        2.3116e+04, 6.4160e+09, 4.3391e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  78\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4812e-10, 1.1592e-07, 1.2445e-38, 3.5341e-42,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 2.8026e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0811e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.0539e-19, 5.8213e-14, 0.0000e+00, 5.1062e-07,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 9.9462e-11,\n",
            "        2.5549e-06, 3.3248e+02, 1.9245e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7651e-10, 2.3816e-07, 7.4443e-38, 4.0412e-41,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4352e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 1.8281e-10, 2.4443e-07, 0.0000e+00, 5.1062e+13,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 9.9462e+09,\n",
            "        2.3116e+04, 6.4160e+09, 4.3391e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  79\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4812e-10, 1.1592e-07, 1.2445e-38, 2.9974e-42,\n",
            "        1.3308e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 2.8026e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0811e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.0539e-19, 5.8213e-14, 0.0000e+00, 3.9513e-07,\n",
            "        1.0103e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 3.5979e-11,\n",
            "        2.5549e-06, 3.3248e+02, 1.9245e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7651e-10, 2.3816e-07, 7.4443e-38, 3.4629e-41,\n",
            "        3.6484e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4352e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 1.8281e-10, 2.4443e-07, 0.0000e+00, 3.9513e+13,\n",
            "        2.7690e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 3.5979e+09,\n",
            "        2.3116e+04, 6.4160e+09, 4.3391e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  80\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 9.5367e-07,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.7270e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4771e-10, 1.1592e-07, 1.2445e-38, 1.7026e-42,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 1.4013e-45,\n",
            "        4.9193e-11, 2.5629e-08, 3.0811e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 8.4679e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.2169e-19, 5.8213e-14, 0.0000e+00, 3.0875e-07,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 3.9535e-11,\n",
            "        2.5549e-06, 3.3248e+02, 1.9245e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4577e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7602e-10, 2.3816e-07, 7.4443e-38, 1.8158e-41,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4352e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 5.8089e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 2.1126e-10, 2.4443e-07, 0.0000e+00, 3.0875e+13,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 3.9535e+09,\n",
            "        2.3116e+04, 6.4160e+09, 4.3391e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  81\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.7008e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4771e-10, 1.1592e-07, 1.2445e-38, 1.4181e-42,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 0.0000e+00,\n",
            "        4.9193e-11, 2.5629e-08, 3.0811e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 9.5206e-04,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.2169e-19, 5.8213e-14, 0.0000e+00, 2.6122e-07,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 2.3322e-11,\n",
            "        2.5549e-06, 3.3248e+02, 1.9245e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4515e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7602e-10, 2.3816e-07, 7.4443e-38, 1.5112e-41,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4352e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 6.5593e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 2.1126e-10, 2.4443e-07, 0.0000e+00, 2.6122e+13,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 2.3322e+09,\n",
            "        2.3116e+04, 6.4160e+09, 4.3391e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  82\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9954e-10, 0.0000e+00, 2.1157e-05, 4.6670e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4771e-10, 1.1592e-07, 1.2445e-38, 8.8422e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 0.0000e+00,\n",
            "        4.9193e-11, 2.5629e-08, 3.0759e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 6.9082e-13, 0.0000e+00, 3.5665e-14, 1.0261e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.2169e-19, 5.8213e-14, 0.0000e+00, 2.1329e-07,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 3.6155e-11,\n",
            "        2.5549e-06, 3.3248e+02, 2.2617e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2886e-09, 0.0000e+00, 3.6959e-05, 1.4526e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7602e-10, 2.3816e-07, 7.4443e-38, 8.1387e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4337e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.0185e-04, 0.0000e+00, 9.6498e-10, 7.0637e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 2.1126e-10, 2.4443e-07, 0.0000e+00, 2.1329e+13,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 3.6155e+09,\n",
            "        2.3116e+04, 6.4160e+09, 5.1012e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  83\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.3113e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9788e-10, 0.0000e+00, 2.1157e-05, 4.6540e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4771e-10, 1.1592e-07, 1.2445e-38, 6.1657e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 0.0000e+00,\n",
            "        4.9193e-11, 2.5629e-08, 3.0759e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 7.7334e-13, 0.0000e+00, 3.5665e-14, 1.1348e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.2169e-19, 5.8213e-14, 0.0000e+00, 1.3439e-07,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 1.4856e-11,\n",
            "        2.5549e-06, 3.3248e+02, 2.2617e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2867e-09, 0.0000e+00, 3.6959e-05, 1.4495e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7602e-10, 2.3816e-07, 7.4443e-38, 7.1088e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4337e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.3819e-04, 0.0000e+00, 9.6498e-10, 7.8286e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 2.1126e-10, 2.4443e-07, 0.0000e+00, 1.3439e+13,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 1.4856e+09,\n",
            "        2.3116e+04, 6.4160e+09, 5.1012e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  84\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.3113e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1157e-05, 9.9788e-10, 0.0000e+00, 2.1157e-05, 4.6463e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1157e-05, 2.8520e-05, 2.4771e-10, 1.1592e-07, 1.2445e-38, 4.0357e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1157e-05, 0.0000e+00,\n",
            "        4.9193e-11, 2.5629e-08, 3.0759e-04, 1.1592e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 3.5665e-14, 7.7334e-13, 0.0000e+00, 3.5665e-14, 1.1652e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        3.5665e-14, 9.1241e-01, 1.2169e-19, 5.8213e-14, 0.0000e+00, 1.5055e-07,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 3.5665e-14, 1.3647e-11,\n",
            "        2.5549e-06, 3.3248e+02, 2.2617e-11, 5.8213e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6959e-05, 2.2867e-09, 0.0000e+00, 3.6959e-05, 1.4480e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6959e-05, 3.8296e-05, 5.7602e-10, 2.3816e-07, 7.4443e-38, 4.0189e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6959e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4337e-04, 2.3816e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 9.6498e-10, 3.3819e-04, 0.0000e+00, 9.6498e-10, 8.0473e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        9.6498e-10, 2.3825e+04, 2.1126e-10, 2.4443e-07, 0.0000e+00, 1.5055e+13,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 9.6498e-10, 1.3647e+09,\n",
            "        2.3116e+04, 6.4160e+09, 5.1012e-08, 2.4443e-07, 1.1778e-01])\n",
            "************** t  85\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.4305e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1099e-05, 9.9788e-10, 0.0000e+00, 2.1099e-05, 4.6385e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1099e-05, 2.8520e-05, 2.4771e-10, 1.1560e-07, 1.2445e-38, 3.2790e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1099e-05, 0.0000e+00,\n",
            "        4.9193e-11, 2.5629e-08, 3.0759e-04, 1.1560e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 4.5095e-14, 7.7334e-13, 0.0000e+00, 4.5095e-14, 1.2494e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        4.5095e-14, 9.1241e-01, 1.2169e-19, 7.1940e-14, 0.0000e+00, 9.4592e-08,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 4.5095e-14, 1.0531e-11,\n",
            "        2.5549e-06, 3.3248e+02, 2.2617e-11, 7.1940e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6897e-05, 2.2867e-09, 0.0000e+00, 3.6897e-05, 1.4465e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6897e-05, 3.8296e-05, 5.7602e-10, 2.3772e-07, 7.4443e-38, 4.0217e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6897e-05, 0.0000e+00,\n",
            "        1.1052e-10, 5.1820e-08, 4.4337e-04, 2.3772e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.2222e-09, 3.3819e-04, 0.0000e+00, 1.2222e-09, 8.6373e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.2222e-09, 2.3825e+04, 2.1126e-10, 3.0262e-07, 0.0000e+00, 9.4592e+12,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.2222e-09, 1.0531e+09,\n",
            "        2.3116e+04, 6.4160e+09, 5.1012e-08, 3.0262e-07, 1.1778e-01])\n",
            "************** t  86\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5497e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1099e-05, 9.9788e-10, 0.0000e+00, 2.1099e-05, 4.6231e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1099e-05, 2.8520e-05, 2.4771e-10, 1.1560e-07, 1.2445e-38, 2.2141e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1099e-05, 0.0000e+00,\n",
            "        4.9128e-11, 2.5629e-08, 3.0759e-04, 1.1560e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 4.5095e-14, 7.7334e-13, 0.0000e+00, 4.5095e-14, 1.2218e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        4.5095e-14, 9.1241e-01, 1.2169e-19, 7.1940e-14, 0.0000e+00, 7.8410e-08,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 4.5095e-14, 9.2384e-12,\n",
            "        2.7097e-06, 3.3248e+02, 2.2617e-11, 7.1940e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6897e-05, 2.2867e-09, 0.0000e+00, 3.6897e-05, 1.4434e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6897e-05, 3.8296e-05, 5.7602e-10, 2.3772e-07, 7.4443e-38, 2.0123e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6897e-05, 0.0000e+00,\n",
            "        1.1047e-10, 5.1820e-08, 4.4337e-04, 2.3772e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.2222e-09, 3.3819e-04, 0.0000e+00, 1.2222e-09, 8.4643e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.2222e-09, 2.3825e+04, 2.1126e-10, 3.0262e-07, 0.0000e+00, 7.8410e+12,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.2222e-09, 9.2384e+08,\n",
            "        2.4529e+04, 6.4160e+09, 5.1012e-08, 3.0262e-07, 1.1778e-01])\n",
            "************** t  87\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5497e-06,\n",
            "        -0.0000e+00, 2.0537e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1099e-05, 9.9788e-10, 0.0000e+00, 2.1099e-05, 4.6205e-11,\n",
            "        1.3935e-26, 4.9356e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1099e-05, 2.8520e-05, 2.4771e-10, 1.1560e-07, 1.2445e-38, 1.8777e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1099e-05, 0.0000e+00,\n",
            "        4.9128e-11, 2.5629e-08, 3.0759e-04, 1.1560e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 4.5095e-14, 7.7334e-13, 0.0000e+00, 4.5095e-14, 1.2220e-03,\n",
            "        0.0000e+00, 1.3274e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        4.5095e-14, 9.1241e-01, 1.2169e-19, 7.1940e-14, 0.0000e+00, 4.9371e-08,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 4.5095e-14, 6.6160e-13,\n",
            "        2.7097e-06, 3.3248e+02, 2.2617e-11, 7.1940e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6897e-05, 2.2867e-09, 0.0000e+00, 3.6897e-05, 1.4435e-10,\n",
            "        5.7000e-26, 6.0308e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6897e-05, 3.8296e-05, 5.7602e-10, 2.3772e-07, 7.4443e-38, 2.0207e-42,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6897e-05, 0.0000e+00,\n",
            "        1.1047e-10, 5.1820e-08, 4.4337e-04, 2.3772e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.2222e-09, 3.3819e-04, 0.0000e+00, 1.2222e-09, 8.4653e+06,\n",
            "        0.0000e+00, 2.2010e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.2222e-09, 2.3825e+04, 2.1126e-10, 3.0262e-07, 0.0000e+00, 4.9371e+12,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.2222e-09, 6.6160e+07,\n",
            "        2.4529e+04, 6.4160e+09, 5.1012e-08, 3.0262e-07, 1.1778e-01])\n",
            "************** t  88\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5497e-06,\n",
            "        -0.0000e+00, 2.2213e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1099e-05, 9.9656e-10, 0.0000e+00, 2.1099e-05, 4.6128e-11,\n",
            "        1.3935e-26, 4.9274e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1099e-05, 2.8520e-05, 2.4730e-10, 1.1541e-07, 1.2445e-38, 1.0370e-43,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1099e-05, 0.0000e+00,\n",
            "        4.9128e-11, 2.5629e-08, 3.0759e-04, 1.1541e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 4.5095e-14, 8.1875e-13, 0.0000e+00, 4.5095e-14, 1.2441e-03,\n",
            "        0.0000e+00, 1.4242e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        4.5095e-14, 9.1241e-01, 1.3339e-19, 7.8562e-14, 0.0000e+00, 5.9498e-08,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 4.5095e-14, 1.0612e-12,\n",
            "        2.7097e-06, 3.3248e+02, 2.2617e-11, 7.8562e-14, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6897e-05, 2.2855e-09, 0.0000e+00, 3.6897e-05, 1.4420e-10,\n",
            "        5.7000e-26, 6.0302e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6897e-05, 3.8296e-05, 5.7554e-10, 2.3755e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6897e-05, 0.0000e+00,\n",
            "        1.1047e-10, 5.1820e-08, 4.4337e-04, 2.3755e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.2222e-09, 3.5823e-04, 0.0000e+00, 1.2222e-09, 8.6280e+06,\n",
            "        0.0000e+00, 2.3617e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.2222e-09, 2.3825e+04, 2.3177e-10, 3.3072e-07, 0.0000e+00, 5.9498e+12,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.2222e-09, 1.0612e+08,\n",
            "        2.4529e+04, 6.4160e+09, 5.1012e-08, 3.3072e-07, 1.1778e-01])\n",
            "************** t  89\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.5497e-06,\n",
            "        -0.0000e+00, 2.2213e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1040e-05, 9.9656e-10, 0.0000e+00, 2.1040e-05, 4.6128e-11,\n",
            "        1.3935e-26, 4.9274e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1040e-05, 2.8520e-05, 2.4689e-10, 1.1471e-07, 1.2445e-38, 6.8664e-44,\n",
            "        1.3290e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1040e-05, 0.0000e+00,\n",
            "        4.9046e-11, 2.5629e-08, 3.0674e-04, 1.1471e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 5.5132e-14, 8.1875e-13, 0.0000e+00, 5.5132e-14, 1.2441e-03,\n",
            "        0.0000e+00, 1.4242e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        5.5132e-14, 9.1241e-01, 1.4532e-19, 1.0796e-13, 0.0000e+00, 2.1288e-08,\n",
            "        1.0847e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 5.5132e-14, 1.2506e-12,\n",
            "        2.9089e-06, 3.3248e+02, 2.7848e-11, 1.0796e-13, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6835e-05, 2.2855e-09, 0.0000e+00, 3.6835e-05, 1.4420e-10,\n",
            "        5.7000e-26, 6.0302e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6835e-05, 3.8296e-05, 5.7505e-10, 2.3676e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.6460e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6835e-05, 0.0000e+00,\n",
            "        1.1038e-10, 5.1820e-08, 4.4273e-04, 2.3676e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.4967e-09, 3.5823e-04, 0.0000e+00, 1.4967e-09, 8.6280e+06,\n",
            "        0.0000e+00, 2.3617e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.4967e-09, 2.3825e+04, 2.5271e-10, 4.5600e-07, 0.0000e+00, 2.1288e+12,\n",
            "        2.9752e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.4967e-09, 1.2506e+08,\n",
            "        2.6354e+04, 6.4160e+09, 6.2901e-08, 4.5600e-07, 1.1778e-01])\n",
            "************** t  90\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.6689e-06,\n",
            "        -0.0000e+00, 2.3793e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.1040e-05, 9.9324e-10, 0.0000e+00, 2.1040e-05, 4.5991e-11,\n",
            "        1.3935e-26, 4.9192e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.1040e-05, 2.8520e-05, 2.4547e-10, 1.1350e-07, 1.2445e-38, 4.3440e-44,\n",
            "        1.3268e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.1040e-05, 0.0000e+00,\n",
            "        4.8964e-11, 2.5629e-08, 3.0674e-04, 1.1350e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 5.5132e-14, 9.5788e-13, 0.0000e+00, 5.5132e-14, 1.3269e-03,\n",
            "        0.0000e+00, 1.5139e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        5.5132e-14, 9.1241e-01, 1.9381e-19, 1.7821e-13, 0.0000e+00, 3.7844e-08,\n",
            "        1.1426e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 5.5132e-14, 6.7272e-13,\n",
            "        3.0879e-06, 3.3248e+02, 2.7848e-11, 1.7821e-13, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6835e-05, 2.2817e-09, 0.0000e+00, 3.6835e-05, 1.4394e-10,\n",
            "        5.7000e-26, 6.0296e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6835e-05, 3.8296e-05, 5.7316e-10, 2.3536e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.6425e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6835e-05, 0.0000e+00,\n",
            "        1.1029e-10, 5.1820e-08, 4.4273e-04, 2.3536e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.4967e-09, 4.1980e-04, 0.0000e+00, 1.4967e-09, 9.2185e+06,\n",
            "        0.0000e+00, 2.5108e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.4967e-09, 2.3825e+04, 3.3814e-10, 7.5717e-07, 0.0000e+00, 3.7844e+12,\n",
            "        3.1369e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.4967e-09, 6.7272e+07,\n",
            "        2.7998e+04, 6.4160e+09, 6.2901e-08, 7.5717e-07, 1.1778e-01])\n",
            "************** t  91\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.6689e-06,\n",
            "        -0.0000e+00, 2.3793e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 3.9339e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.0889e-05, 9.9324e-10, 0.0000e+00, 2.0889e-05, 4.5915e-11,\n",
            "        1.3935e-26, 4.9192e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.0889e-05, 2.8520e-05, 2.4438e-10, 1.1312e-07, 1.2445e-38, 3.0829e-44,\n",
            "        1.3202e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.0889e-05, 0.0000e+00,\n",
            "        4.8964e-11, 2.5629e-08, 3.0674e-04, 1.1312e-07, 5.4903e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 7.9119e-14, 9.5788e-13, 0.0000e+00, 7.9119e-14, 1.3458e-03,\n",
            "        0.0000e+00, 1.5139e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        7.9119e-14, 9.1241e-01, 2.3971e-19, 2.0206e-13, 0.0000e+00, 2.1529e-08,\n",
            "        1.3183e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 7.9119e-14, 4.7593e-13,\n",
            "        3.0879e-06, 3.3248e+02, 2.7848e-11, 2.0206e-13, 5.2307e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6690e-05, 2.2817e-09, 0.0000e+00, 3.6690e-05, 1.4379e-10,\n",
            "        5.7000e-26, 6.0296e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6690e-05, 3.8296e-05, 5.7156e-10, 2.3501e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.6319e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6690e-05, 0.0000e+00,\n",
            "        1.1029e-10, 5.1820e-08, 4.4273e-04, 2.3501e-07, 4.4412e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 2.1564e-09, 4.1980e-04, 0.0000e+00, 2.1564e-09, 9.3597e+06,\n",
            "        0.0000e+00, 2.5108e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        2.1564e-09, 2.3825e+04, 4.1940e-10, 8.5980e-07, 0.0000e+00, 2.1529e+12,\n",
            "        3.6298e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 2.1564e-09, 4.7593e+07,\n",
            "        2.7998e+04, 6.4160e+09, 6.2901e-08, 8.5980e-07, 1.1778e-01])\n",
            "************** t  92\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.0266e-06,\n",
            "        -0.0000e+00, 2.3793e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.0797e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 4.7684e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.0784e-05, 9.9046e-10, 0.0000e+00, 2.0784e-05, 4.5584e-11,\n",
            "        1.3935e-26, 4.9192e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.0784e-05, 2.8520e-05, 2.4398e-10, 1.1275e-07, 1.2445e-38, 2.6625e-44,\n",
            "        1.3061e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.0784e-05, 0.0000e+00,\n",
            "        4.8964e-11, 2.5629e-08, 3.0436e-04, 1.1275e-07, 5.4812e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 9.9385e-14, 1.0489e-12, 0.0000e+00, 9.9385e-14, 1.5875e-03,\n",
            "        0.0000e+00, 1.5139e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        9.9385e-14, 9.1241e-01, 2.5426e-19, 2.2778e-13, 0.0000e+00, 1.1966e-08,\n",
            "        1.7405e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 9.9385e-14, 4.0324e-13,\n",
            "        3.0879e-06, 3.3248e+02, 4.1812e-11, 2.2778e-13, 6.2966e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6626e-05, 2.2791e-09, 0.0000e+00, 3.6626e-05, 1.4302e-10,\n",
            "        5.7000e-26, 6.0296e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6626e-05, 3.8296e-05, 5.7107e-10, 2.3466e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.6079e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6626e-05, 0.0000e+00,\n",
            "        1.1029e-10, 5.1820e-08, 4.4163e-04, 2.3466e-07, 4.4443e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 2.7135e-09, 4.6024e-04, 0.0000e+00, 2.7135e-09, 1.1100e+07,\n",
            "        0.0000e+00, 2.5108e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        2.7135e-09, 2.3825e+04, 4.4523e-10, 9.7069e-07, 0.0000e+00, 1.1966e+12,\n",
            "        4.8240e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 2.7135e-09, 4.0324e+07,\n",
            "        2.7998e+04, 6.4160e+09, 9.4675e-08, 9.7069e-07, 1.4168e-01])\n",
            "************** t  93\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.1458e-06,\n",
            "        -0.0000e+00, 2.3793e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.7277e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 4.7684e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.0543e-05, 9.8343e-10, 0.0000e+00, 2.0543e-05, 4.5357e-11,\n",
            "        1.3935e-26, 4.9192e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.0543e-05, 2.8473e-05, 2.4316e-10, 1.1119e-07, 1.2445e-38, 1.2612e-44,\n",
            "        1.2838e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.0543e-05, 0.0000e+00,\n",
            "        4.8964e-11, 2.5629e-08, 3.0217e-04, 1.1119e-07, 5.4812e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 1.5609e-13, 1.3278e-12, 0.0000e+00, 1.5609e-13, 1.6920e-03,\n",
            "        0.0000e+00, 1.5139e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        1.5609e-13, 9.8517e-01, 2.8340e-19, 3.8897e-13, 0.0000e+00, 1.7106e-08,\n",
            "        2.3371e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 1.5609e-13, 3.4996e-13,\n",
            "        3.0879e-06, 3.3248e+02, 5.9412e-11, 3.8897e-13, 6.2966e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6398e-05, 2.2702e-09, 0.0000e+00, 3.6398e-05, 1.4257e-10,\n",
            "        5.7000e-26, 6.0296e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6398e-05, 3.8287e-05, 5.7010e-10, 2.3249e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.5833e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6398e-05, 0.0000e+00,\n",
            "        1.1029e-10, 5.1820e-08, 4.4019e-04, 2.3249e-07, 4.4443e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 4.2884e-09, 5.8488e-04, 0.0000e+00, 4.2884e-09, 1.1868e+07,\n",
            "        0.0000e+00, 2.5108e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        4.2884e-09, 2.5731e+04, 4.9711e-10, 1.6730e-06, 0.0000e+00, 1.7106e+12,\n",
            "        6.5222e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 4.2884e-09, 3.4996e+07,\n",
            "        2.7998e+04, 6.4160e+09, 1.3497e-07, 1.6730e-06, 1.4168e-01])\n",
            "************** t  94\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.8610e-06,\n",
            "        -0.0000e+00, 2.4862e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 7.9243e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.0152e-01, -0.0000e+00, -0.0000e+00, 4.7684e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 2.0204e-05, 9.7853e-10, 0.0000e+00, 2.0204e-05, 4.4459e-11,\n",
            "        1.3935e-26, 4.9110e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        2.0204e-05, 2.8447e-05, 2.4048e-10, 1.0948e-07, 1.2445e-38, 8.4078e-45,\n",
            "        1.2689e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 2.0204e-05, 0.0000e+00,\n",
            "        4.8964e-11, 2.5629e-08, 3.0067e-04, 1.0948e-07, 5.4812e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 2.7386e-13, 1.5421e-12, 0.0000e+00, 2.7386e-13, 2.2602e-03,\n",
            "        0.0000e+00, 1.5738e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        2.7386e-13, 1.0187e+00, 4.2651e-19, 5.9814e-13, 0.0000e+00, 4.1129e-09,\n",
            "        2.9197e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 2.7386e-13, 3.3061e-13,\n",
            "        3.0879e-06, 3.3248e+02, 7.1959e-11, 5.9814e-13, 6.2966e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.6184e-05, 2.2645e-09, 0.0000e+00, 3.6184e-05, 1.4042e-10,\n",
            "        5.7000e-26, 6.0290e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.6184e-05, 3.8301e-05, 5.6565e-10, 2.3078e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.5587e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.6184e-05, 0.0000e+00,\n",
            "        1.1029e-10, 5.1820e-08, 4.3972e-04, 2.3078e-07, 4.4443e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 7.5687e-09, 6.8099e-04, 0.0000e+00, 7.5687e-09, 1.6096e+07,\n",
            "        0.0000e+00, 2.6104e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        7.5687e-09, 2.6596e+04, 7.5402e-10, 2.5918e-06, 0.0000e+00, 4.1129e+11,\n",
            "        8.2044e-09, 0.0000e+00, 0.0000e+00, 4.5195e+08, 7.5687e-09, 3.3061e+07,\n",
            "        2.7998e+04, 6.4160e+09, 1.6365e-07, 2.5918e-06, 1.4168e-01])\n",
            "************** t  95\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-06,\n",
            "        -0.0000e+00, 2.7832e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 8.3721e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.1954e-01, -0.0000e+00, -0.0000e+00, 5.4836e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 1.9947e-05, 9.5915e-10, 0.0000e+00, 1.9947e-05, 4.3595e-11,\n",
            "        1.3935e-26, 4.8892e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        1.9947e-05, 2.8392e-05, 2.3598e-10, 1.0642e-07, 1.2445e-38, 5.6052e-45,\n",
            "        1.2442e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 1.9947e-05, 0.0000e+00,\n",
            "        4.8710e-11, 2.5553e-08, 2.9567e-04, 1.0642e-07, 5.4674e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 4.1850e-13, 2.6178e-12, 0.0000e+00, 4.1850e-13, 2.8565e-03,\n",
            "        0.0000e+00, 1.7715e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        4.1850e-13, 1.0673e+00, 6.2750e-19, 1.3455e-12, 0.0000e+00, 7.1434e-09,\n",
            "        4.1078e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 4.1850e-13, 2.7901e-13,\n",
            "        3.5176e-06, 3.4682e+02, 1.3490e-10, 1.3455e-12, 7.1250e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.5915e-05, 2.2381e-09, 0.0000e+00, 3.5915e-05, 1.3853e-10,\n",
            "        5.7000e-26, 6.0210e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.5915e-05, 3.8324e-05, 5.5868e-10, 2.2658e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.5133e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.5915e-05, 0.0000e+00,\n",
            "        1.0990e-10, 5.1763e-08, 4.3636e-04, 2.2658e-07, 4.4537e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 1.1652e-08, 1.1697e-03, 0.0000e+00, 1.1652e-08, 2.0621e+07,\n",
            "        0.0000e+00, 2.9422e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        1.1652e-08, 2.7849e+04, 1.1232e-09, 5.9384e-06, 0.0000e+00, 7.1434e+11,\n",
            "        1.1692e-08, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.1652e-08, 2.7901e+07,\n",
            "        3.2007e+04, 6.7003e+09, 3.0915e-07, 5.9384e-06, 1.5998e-01])\n",
            "************** t  96\n",
            "ce_loss:  tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.8876e-06,\n",
            "        -0.0000e+00, 3.0219e-01, -0.0000e+00, -0.0000e+00, 1.4305e-06, 1.3179e+01,\n",
            "        -0.0000e+00, 8.9188e-04, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.4070e-01, -0.0000e+00, -0.0000e+00, 5.3644e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3683e-03, 1.9121e-05, 9.3529e-10, 0.0000e+00, 1.9121e-05, 4.2283e-11,\n",
            "        1.3935e-26, 4.8702e-05, 1.2336e-39, 1.8229e-36, 7.3683e-03, 3.5426e-05,\n",
            "        1.9121e-05, 2.8345e-05, 2.2511e-10, 1.0084e-07, 1.2445e-38, 4.2039e-45,\n",
            "        1.1994e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 1.9121e-05, 0.0000e+00,\n",
            "        4.8414e-11, 2.5468e-08, 2.9030e-04, 1.0084e-07, 5.4522e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([1.9002e-03, 1.2881e-12, 4.5810e-12, 0.0000e+00, 1.2881e-12, 4.0113e-03,\n",
            "        0.0000e+00, 1.8173e+02, 0.0000e+00, 0.0000e+00, 1.9002e-03, 9.0729e+02,\n",
            "        1.2881e-12, 1.1465e+00, 1.5331e-18, 4.7265e-12, 0.0000e+00, 8.1981e-09,\n",
            "        7.2053e-19, 0.0000e+00, 0.0000e+00, 7.5818e+02, 1.2881e-12, 1.0414e-13,\n",
            "        4.0162e-06, 3.5115e+02, 2.2382e-10, 4.7265e-12, 7.1324e-03])\n",
            "kde_grad  tensor([6.4210e-03, 3.5122e-05, 2.2074e-09, 0.0000e+00, 3.5122e-05, 1.3525e-10,\n",
            "        5.7000e-26, 6.0139e-05, 7.5966e-39, 1.0206e-35, 6.4210e-03, 4.5603e-05,\n",
            "        3.5122e-05, 3.8315e-05, 5.4374e-10, 2.1875e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.4282e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.5122e-05, 0.0000e+00,\n",
            "        1.0951e-10, 5.1688e-08, 4.3455e-04, 2.1875e-07, 4.4936e-02])\n",
            "penalty_factor  tensor([2.9594e-01, 3.6675e-08, 2.0753e-03, 0.0000e+00, 3.6675e-08, 2.9659e+07,\n",
            "        0.0000e+00, 3.0218e+06, 0.0000e+00, 0.0000e+00, 2.9594e-01, 1.9896e+07,\n",
            "        3.6675e-08, 2.9923e+04, 2.8196e-09, 2.1607e-05, 0.0000e+00, 8.1981e+11,\n",
            "        2.1018e-08, 0.0000e+00, 0.0000e+00, 4.5195e+08, 3.6675e-08, 1.0414e+07,\n",
            "        3.6674e+04, 6.7936e+09, 5.1507e-07, 2.1607e-05, 1.5872e-01])\n",
            "************** t  97\n",
            "ce_loss:  tensor([1.5497e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.9870e-06,\n",
            "        -0.0000e+00, 3.4971e-01, -0.0000e+00, -0.0000e+00, 1.5497e-06, 1.3211e+01,\n",
            "        -0.0000e+00, 1.1195e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6016e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 6.6328e-01, -0.0000e+00, -0.0000e+00, 7.7486e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3438e-03, 1.7888e-05, 8.8329e-10, 0.0000e+00, 1.7888e-05, 4.0080e-11,\n",
            "        1.3935e-26, 4.8230e-05, 1.2336e-39, 1.8229e-36, 7.3438e-03, 3.5367e-05,\n",
            "        1.7888e-05, 2.8125e-05, 2.1263e-10, 9.4373e-08, 1.2445e-38, 2.8026e-45,\n",
            "        1.1338e-11, 2.3542e-38, 7.7047e-27, 1.3064e-06, 1.7888e-05, 0.0000e+00,\n",
            "        4.8172e-11, 2.5342e-08, 2.7614e-04, 9.4373e-08, 5.4134e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.1339e-03, 5.4040e-12, 1.2580e-11, 0.0000e+00, 5.4040e-12, 6.3600e-03,\n",
            "        0.0000e+00, 2.0563e+02, 0.0000e+00, 0.0000e+00, 2.1339e-03, 9.2584e+02,\n",
            "        5.4040e-12, 1.4200e+00, 5.0864e-18, 1.6838e-11, 0.0000e+00, 3.4510e-09,\n",
            "        1.4544e-18, 0.0000e+00, 0.0000e+00, 7.5818e+02, 5.4040e-12, 1.4629e-13,\n",
            "        4.2553e-06, 3.6174e+02, 8.0768e-10, 1.6838e-11, 1.0088e-02])\n",
            "kde_grad  tensor([6.4277e-03, 3.3780e-05, 2.1304e-09, 0.0000e+00, 3.3780e-05, 1.2966e-10,\n",
            "        5.7000e-26, 6.0017e-05, 7.5966e-39, 1.0206e-35, 6.4277e-03, 4.5595e-05,\n",
            "        3.3780e-05, 3.8234e-05, 5.2254e-10, 2.0942e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.2927e-11, 1.4114e-37, 3.1644e-26, 1.6776e-06, 3.3780e-05, 0.0000e+00,\n",
            "        1.0924e-10, 5.1577e-08, 4.2500e-04, 2.0942e-07, 4.5011e-02])\n",
            "penalty_factor  tensor([3.3198e-01, 1.5997e-07, 5.9050e-03, 0.0000e+00, 1.5997e-07, 4.9053e+07,\n",
            "        0.0000e+00, 3.4262e+06, 0.0000e+00, 0.0000e+00, 3.3198e-01, 2.0306e+07,\n",
            "        1.5997e-07, 3.7140e+04, 9.7340e-09, 8.0406e-05, 0.0000e+00, 3.4510e+11,\n",
            "        4.4172e-08, 0.0000e+00, 0.0000e+00, 4.5195e+08, 1.5997e-07, 1.4629e+07,\n",
            "        3.8953e+04, 7.0135e+09, 1.9004e-06, 8.0406e-05, 2.2412e-01])\n",
            "************** t  98\n",
            "ce_loss:  tensor([1.5497e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7285e-05,\n",
            "        -0.0000e+00, 4.4721e-01, -0.0000e+00, -0.0000e+00, 1.5497e-06, 1.3241e+01,\n",
            "        -0.0000e+00, 1.5107e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6242e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 7.2348e-01, -0.0000e+00, -0.0000e+00, 8.5830e-06],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3438e-03, 1.6265e-05, 8.1999e-10, 0.0000e+00, 1.6265e-05, 3.5905e-11,\n",
            "        1.3935e-26, 4.7223e-05, 1.2336e-39, 1.8229e-36, 7.3438e-03, 3.5288e-05,\n",
            "        1.6265e-05, 2.7753e-05, 1.8955e-10, 8.4881e-08, 1.2445e-38, 1.4013e-45,\n",
            "        1.0168e-11, 2.3542e-38, 7.7047e-27, 1.3043e-06, 1.6265e-05, 0.0000e+00,\n",
            "        4.7080e-11, 2.4979e-08, 2.4968e-04, 8.4881e-08, 5.3954e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.1339e-03, 2.8229e-11, 3.5035e-11, 0.0000e+00, 2.8229e-11, 1.4107e-02,\n",
            "        0.0000e+00, 1.8642e+02, 0.0000e+00, 0.0000e+00, 2.1339e-03, 9.0729e+02,\n",
            "        2.8229e-11, 1.9575e+00, 1.9307e-17, 7.9623e-11, 0.0000e+00, 1.8764e-09,\n",
            "        4.3260e-18, 0.0000e+00, 0.0000e+00, 7.5865e+02, 2.8229e-11, 6.6025e-14,\n",
            "        5.8134e-06, 3.7138e+02, 5.2958e-09, 7.9623e-11, 1.1202e-02])\n",
            "kde_grad  tensor([6.4277e-03, 3.1836e-05, 2.0261e-09, 0.0000e+00, 3.1836e-05, 1.1863e-10,\n",
            "        5.7000e-26, 5.9487e-05, 7.5966e-39, 1.0206e-35, 6.4277e-03, 4.5629e-05,\n",
            "        3.1836e-05, 3.8153e-05, 4.8325e-10, 1.9437e-07, 7.4443e-38, 0.0000e+00,\n",
            "        3.0326e-11, 1.4114e-37, 3.1644e-26, 1.6773e-06, 3.1836e-05, 0.0000e+00,\n",
            "        1.0766e-10, 5.1220e-08, 4.0575e-04, 1.9437e-07, 4.5068e-02])\n",
            "penalty_factor  tensor([3.3198e-01, 8.8672e-07, 1.7292e-02, 0.0000e+00, 8.8672e-07, 1.1892e+08,\n",
            "        0.0000e+00, 3.1338e+06, 0.0000e+00, 0.0000e+00, 3.3198e-01, 1.9884e+07,\n",
            "        8.8672e-07, 5.1308e+04, 3.9953e-08, 4.0964e-04, 0.0000e+00, 1.8764e+11,\n",
            "        1.4265e-07, 0.0000e+00, 0.0000e+00, 4.5231e+08, 8.8672e-07, 6.6025e+06,\n",
            "        5.3996e+04, 7.2507e+09, 1.3052e-05, 4.0964e-04, 2.4857e-01])\n",
            "************** t  99\n",
            "ce_loss:  tensor([2.1458e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 4.7087e-05,\n",
            "        -0.0000e+00, 4.4754e-01, -0.0000e+00, -0.0000e+00, 2.1458e-06, 1.3320e+01,\n",
            "        -0.0000e+00, 2.7434e-03, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.6575e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 8.4011e-01, -0.0000e+00, -0.0000e+00, 1.7404e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.2578e-03, 1.2266e-05, 6.6911e-10, 0.0000e+00, 1.2266e-05, 2.9898e-11,\n",
            "        1.3935e-26, 4.6031e-05, 1.2336e-39, 1.8229e-36, 7.2578e-03, 3.5015e-05,\n",
            "        1.2266e-05, 2.6827e-05, 1.4931e-10, 6.4846e-08, 1.2445e-38, 1.4013e-45,\n",
            "        7.7829e-12, 2.3542e-38, 7.7047e-27, 1.2999e-06, 1.2266e-05, 0.0000e+00,\n",
            "        4.4662e-11, 2.4025e-08, 2.0739e-04, 6.4846e-08, 5.2651e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "ce_grad  tensor([2.9067e-03, 5.8433e-10, 2.7148e-10, 0.0000e+00, 5.8433e-10, 3.7705e-02,\n",
            "        0.0000e+00, 2.4103e+02, 0.0000e+00, 0.0000e+00, 2.9067e-03, 9.2584e+02,\n",
            "        5.8433e-10, 3.4353e+00, 1.8361e-16, 1.1033e-09, 0.0000e+00, 1.4530e-09,\n",
            "        3.1562e-17, 0.0000e+00, 0.0000e+00, 7.6962e+02, 5.8433e-10, 9.8756e-14,\n",
            "        9.7430e-06, 4.1465e+02, 5.3841e-08, 1.1033e-09, 2.2469e-02])\n",
            "kde_grad  tensor([6.4358e-03, 2.6570e-05, 1.7454e-09, 0.0000e+00, 2.6570e-05, 1.0181e-10,\n",
            "        5.7000e-26, 5.9750e-05, 7.5966e-39, 1.0206e-35, 6.4358e-03, 4.5543e-05,\n",
            "        2.6570e-05, 3.7699e-05, 4.0898e-10, 1.5991e-07, 7.4443e-38, 0.0000e+00,\n",
            "        2.4614e-11, 1.4114e-37, 3.1644e-26, 1.6767e-06, 2.6570e-05, 0.0000e+00,\n",
            "        1.0401e-10, 5.0178e-08, 3.6442e-04, 1.5991e-07, 4.5190e-02])\n",
            "penalty_factor  tensor([4.5164e-01, 2.1992e-05, 1.5554e-01, 0.0000e+00, 2.1992e-05, 3.7035e+08,\n",
            "        0.0000e+00, 4.0340e+06, 0.0000e+00, 0.0000e+00, 4.5164e-01, 2.0329e+07,\n",
            "        2.1992e-05, 9.1123e+04, 4.4895e-07, 6.8995e-03, 0.0000e+00, 1.4530e+11,\n",
            "        1.2823e-06, 0.0000e+00, 0.0000e+00, 4.5902e+08, 2.1992e-05, 9.8756e+06,\n",
            "        9.3670e+04, 8.2634e+09, 1.4774e-04, 6.8995e-03, 4.9722e-01])\n",
            "PGD linf: Attack effectiveness 55.172%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KDE(top_500_high_confidence_benign_samples[:1], top_500_high_confidence_benign_samples, 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f69a7a-f057-4d64-dfed-ed340f57e640",
        "id": "VOMCDbzGz4WV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0462], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KDE(mals.to(device), top_500_high_confidence_benign_samples, 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou-DsXVKz__Q",
        "outputId": "79272e49-ab94-4c52-8590-778242ebc244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.3344e-05, 2.2177e-10, 1.1448e-15, 0.0000e+00, 2.2177e-10, 3.4137e-18,\n",
              "        1.3935e-26, 6.6971e-06, 1.2336e-39, 1.8229e-36, 1.3344e-05, 5.0246e-11,\n",
              "        2.2177e-10, 5.7728e-08, 3.5037e-15, 4.1197e-12, 1.2445e-38, 3.1590e-40,\n",
              "        8.5677e-20, 2.3542e-38, 7.7047e-27, 2.7702e-10, 2.2177e-10, 4.0407e-39,\n",
              "        1.8901e-17, 1.1674e-15, 2.3014e-07, 4.1197e-12, 7.4781e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KDE(top_500_high_confidence_benign_samples, top_500_high_confidence_benign_samples, 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad26ebd-178b-4472-abeb-5e391dc05ddf",
        "id": "lTIxz5QFz4WW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0995, 1.0995, 1.0995, 1.0995, 1.0995, 1.0244, 1.0995, 1.0244, 1.0244,\n",
              "        1.0244, 0.1000, 1.0995, 0.1000, 0.2199, 0.3237, 0.1000, 1.0995, 1.9344,\n",
              "        0.9219, 0.9219, 0.1569, 0.8929, 0.1552, 1.0244, 1.9344, 0.4189, 1.0244,\n",
              "        0.1063, 1.0244, 0.1000, 0.1000, 0.1300, 0.1000, 1.9344, 0.1038, 1.9344,\n",
              "        1.9344, 0.1000, 1.9344, 0.9219, 0.2001, 0.1000, 0.3195, 0.1081, 1.9344,\n",
              "        0.1250, 0.1143, 0.2000, 0.1026, 0.9219, 1.9344, 1.0995, 0.2000, 0.1000,\n",
              "        1.9344, 0.4189, 0.1251, 1.9344, 0.9219, 0.1559, 0.2273, 0.3001, 0.9219,\n",
              "        0.2273, 0.1065, 0.3001, 0.1307, 0.9219, 0.8929, 0.5761, 1.9344, 1.9344,\n",
              "        1.9344, 0.1021, 0.2199, 0.1000, 0.2000, 1.9344, 0.8929, 1.9344, 0.1307,\n",
              "        0.9219, 1.0244, 0.1584, 0.1157, 0.3195, 0.1388, 0.5761, 0.1076, 0.2000,\n",
              "        0.3001, 1.9344, 0.2199, 0.3237, 0.1569, 1.9344, 0.2001, 0.5823, 0.8929,\n",
              "        0.3164])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zf0SckOmaOJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KDE(top_500_high_confidence_benign_samples, top_500_high_confidence_benign_samples, 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhmvPK-7gnx5",
        "outputId": "881a224d-ac2d-4495-e071-aa4c4be2b620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0995, 1.0995, 1.0995, 1.0995, 1.0995, 1.0244, 1.0995, 1.0244, 1.0244,\n",
              "        1.0244, 0.1000, 1.0995, 0.1000, 0.2199, 0.3237, 0.1000, 1.0995, 1.9344,\n",
              "        0.9219, 0.9219, 0.1569, 0.8929, 0.1552, 1.0244, 1.9344, 0.4189, 1.0244,\n",
              "        0.1063, 1.0244, 0.1000, 0.1000, 0.1300, 0.1000, 1.9344, 0.1038, 1.9344,\n",
              "        1.9344, 0.1000, 1.9344, 0.9219, 0.2001, 0.1000, 0.3195, 0.1081, 1.9344,\n",
              "        0.1250, 0.1143, 0.2000, 0.1026, 0.9219, 1.9344, 1.0995, 0.2000, 0.1000,\n",
              "        1.9344, 0.4189, 0.1251, 1.9344, 0.9219, 0.1559, 0.2273, 0.3001, 0.9219,\n",
              "        0.2273, 0.1065, 0.3001, 0.1307, 0.9219, 0.8929, 0.5761, 1.9344, 1.9344,\n",
              "        1.9344, 0.1021, 0.2199, 0.1000, 0.2000, 1.9344, 0.8929, 1.9344, 0.1307,\n",
              "        0.9219, 1.0244, 0.1584, 0.1157, 0.3195, 0.1388, 0.5761, 0.1076, 0.2000,\n",
              "        0.3001, 1.9344, 0.2199, 0.3237, 0.1569, 1.9344, 0.2001, 0.5823, 0.8929,\n",
              "        0.3164])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KDE(mals, top_500_high_confidence_benign_samples, 0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-Cj5yipzIyT",
        "outputId": "a82ba158-1747-4e03-982c-45b8490fa331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.3344e-05, 2.2177e-10, 1.1448e-15, 0.0000e+00, 2.2177e-10, 3.4137e-18,\n",
              "        1.3935e-26, 6.6971e-06, 1.2336e-39, 1.8229e-36, 1.3344e-05, 5.0246e-11,\n",
              "        2.2177e-10, 5.7728e-08, 3.5037e-15, 4.1197e-12, 1.2445e-38, 3.1590e-40,\n",
              "        8.5677e-20, 2.3542e-38, 7.7047e-27, 2.7702e-10, 2.2177e-10, 4.0407e-39,\n",
              "        1.8901e-17, 1.1674e-15, 2.3014e-07, 4.1197e-12, 7.4781e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9dCx4yRxESe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "def get_loss_kde(adv_x,y,model,benigns, bandwidth, penalty_factor):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute CE loss\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y)\n",
        "    #print('ce ',ce)\n",
        "\n",
        "    # Compute KDE loss\n",
        "    kde = KDE(adv_x, benigns, bandwidth)\n",
        "    #print('kde ',kde)\n",
        "\n",
        "    # Combine the losses with the penalty factor\n",
        "    loss = ce - penalty_factor * kde\n",
        "    #print('loss ',loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def gkde(x, y, model,benigns, bandwidth, insertion_array, removal_array, k=25, step_length=0.02, norm='linf',\n",
        "        initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "\n",
        "    :param x: Feature vector\n",
        "    :param y: Ground truth labels\n",
        "    :param model: Neural network model\n",
        "    :param RBFModel: Gaussian model for KDE\n",
        "    :param insertion_array: Array for insertion operations\n",
        "    :param removal_array: Array for removal operations\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf', 'l2', 'l1')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    #target's class\n",
        "    traget_labels = torch.zeros_like(y.view(-1).long())\n",
        "\n",
        "    # Compute natural loss and penalty_factor\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), traget_labels)\n",
        "    kde = KDE(x, benigns, bandwidth)\n",
        "    #penalty_factor = 0.\n",
        "    penalty_factor = 1e6\n",
        "    #print(penalty_factor)\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    for t in range(k):\n",
        "        print('*************** t ',t)\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        outputs = model(x_var)\n",
        "        ce_loss = criterion(outputs, traget_labels)\n",
        "        print('ce_loss: ', ce_loss)\n",
        "        kde_loss = KDE(x_var, benigns, bandwidth)\n",
        "        print('kde_loss: ', kde_loss)\n",
        "        ce_grad = torch.autograd.grad(ce_loss.mean(), x_var, retain_graph=True)[0].data\n",
        "        kde_grad = torch.autograd.grad(kde_loss.mean(), x_var)[0].data\n",
        "        print('ce_grad ',torch.abs(ce_grad).sum(dim=-1).detach())\n",
        "        print('kde_grad ',torch.abs(kde_grad).sum(dim=-1).detach())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde(x_var,traget_labels,model,benigns, bandwidth, penalty_factor)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "        elif norm == 'l1':\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1', 'l2', or 'linf' norm.\")\n",
        "\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    if random:\n",
        "        round_threshold = torch.rand_like(x_next)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    outputs = model(x_next)\n",
        "    loss_adv = criterion(outputs, traget_labels).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size(0) * 100:.3f}%.\")\n",
        "\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "VBdPXckRESw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv = gkde(mals.to(torch.float32).to(device), mals_y.to(device), model_AT_rFGSM, top_500_high_confidence_benign_samples,0.6, insertion_array, removal_array, k=100, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c202679-c741-496f-c09a-6b7993b55a69",
        "id": "ep1TpS0gESw7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************** t  0\n",
            "ce_loss:  tensor([4.1028e+01, 8.6630e+01, 5.8158e+01, 1.8685e+02, 8.6630e+01, 3.6857e+01,\n",
            "        1.5623e+02, 9.2854e+00, 1.8450e+02, 1.0696e+02, 4.1028e+01, 1.2040e-05,\n",
            "        8.6630e+01, 3.8186e+01, 9.1592e+01, 7.5509e+01, 1.2146e+02, 2.8487e+01,\n",
            "        6.3303e+01, 9.9407e+01, 1.4897e+02, 1.9638e+01, 8.6630e+01, 4.9636e+01,\n",
            "        3.4985e+01, 2.0074e+01, 7.7117e+01, 7.5509e+01, 3.8682e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.3344e-05, 2.2177e-10, 1.1448e-15, 0.0000e+00, 2.2177e-10, 3.4137e-18,\n",
            "        1.3935e-26, 6.6971e-06, 1.2336e-39, 1.8229e-36, 1.3344e-05, 5.0246e-11,\n",
            "        2.2177e-10, 5.7728e-08, 3.5037e-15, 4.1197e-12, 1.2445e-38, 3.1590e-40,\n",
            "        8.5677e-20, 2.3542e-38, 7.7047e-27, 2.7702e-10, 2.2177e-10, 4.0407e-39,\n",
            "        1.8901e-17, 1.1674e-15, 2.3014e-07, 4.1197e-12, 7.4781e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  1\n",
            "ce_loss:  tensor([3.8930e+01, 8.1252e+01, 5.4561e+01, 1.8050e+02, 8.1252e+01, 3.0279e+01,\n",
            "        1.4982e+02, 6.5895e+00, 1.7741e+02, 1.0231e+02, 3.8930e+01, 8.4638e-06,\n",
            "        8.1252e+01, 3.4357e+01, 8.6054e+01, 7.0332e+01, 1.1706e+02, 2.4853e+01,\n",
            "        6.0047e+01, 9.3228e+01, 1.4309e+02, 1.7637e+01, 8.1252e+01, 4.6975e+01,\n",
            "        3.3508e+01, 1.7785e+01, 6.9806e+01, 7.0332e+01, 3.6410e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.7481e-05, 3.2444e-10, 1.5661e-15, 0.0000e+00, 3.2444e-10, 5.1213e-18,\n",
            "        2.2153e-26, 6.9365e-06, 3.6995e-39, 7.1828e-36, 1.7481e-05, 9.2380e-11,\n",
            "        3.2444e-10, 7.0069e-08, 4.8779e-15, 5.5370e-12, 5.8609e-38, 1.0459e-39,\n",
            "        1.2037e-19, 4.9772e-38, 1.4259e-26, 3.8639e-10, 3.2444e-10, 1.0220e-38,\n",
            "        2.9978e-17, 1.9726e-15, 2.5856e-07, 5.5370e-12, 7.7116e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  2\n",
            "ce_loss:  tensor([3.7697e+01, 7.5129e+01, 5.0083e+01, 1.7474e+02, 7.5129e+01, 2.5249e+01,\n",
            "        1.4335e+02, 5.0435e+00, 1.7031e+02, 9.7716e+01, 3.7697e+01, 8.2254e-06,\n",
            "        7.5129e+01, 3.0199e+01, 8.0268e+01, 6.5271e+01, 1.1249e+02, 2.1447e+01,\n",
            "        5.6916e+01, 8.7844e+01, 1.3668e+02, 1.5970e+01, 7.5129e+01, 4.4444e+01,\n",
            "        3.1907e+01, 1.5580e+01, 6.2219e+01, 6.5271e+01, 3.6912e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.3883e-05, 3.8361e-10, 2.0089e-15, 0.0000e+00, 3.8361e-10, 5.7168e-18,\n",
            "        2.6224e-26, 7.0469e-06, 8.1165e-39, 2.0340e-35, 2.3883e-05, 1.7992e-10,\n",
            "        3.8361e-10, 6.9996e-08, 5.4021e-15, 5.9843e-12, 2.2279e-37, 2.1919e-39,\n",
            "        1.3901e-19, 8.4430e-38, 2.0278e-26, 4.7897e-10, 3.8361e-10, 2.1283e-38,\n",
            "        4.3941e-17, 2.5209e-15, 2.1052e-07, 5.9843e-12, 9.0025e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  3\n",
            "ce_loss:  tensor([3.6372e+01, 6.8012e+01, 4.6156e+01, 1.6872e+02, 6.8012e+01, 2.1276e+01,\n",
            "        1.3708e+02, 4.0988e+00, 1.6330e+02, 9.2996e+01, 3.6372e+01, 8.1062e-06,\n",
            "        6.8012e+01, 2.5732e+01, 7.4219e+01, 5.9230e+01, 1.0791e+02, 1.8765e+01,\n",
            "        5.3867e+01, 8.3245e+01, 1.3027e+02, 1.3951e+01, 6.8012e+01, 4.1988e+01,\n",
            "        3.0173e+01, 1.3419e+01, 5.5117e+01, 5.9230e+01, 3.4501e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.2474e-05, 3.5571e-10, 2.0376e-15, 0.0000e+00, 3.5571e-10, 7.6090e-18,\n",
            "        2.2872e-26, 6.9976e-06, 1.3032e-38, 4.8171e-35, 3.2474e-05, 3.3850e-10,\n",
            "        3.5571e-10, 5.6601e-08, 4.7640e-15, 5.8324e-12, 6.6298e-37, 5.0324e-39,\n",
            "        1.4140e-19, 1.1735e-37, 2.1743e-26, 5.5507e-10, 3.5571e-10, 5.0171e-38,\n",
            "        5.8801e-17, 2.9546e-15, 1.2714e-07, 5.8324e-12, 9.5536e-03],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  4\n",
            "ce_loss:  tensor([3.5693e+01, 6.0436e+01, 4.2251e+01, 1.6281e+02, 6.0436e+01, 1.7927e+01,\n",
            "        1.3122e+02, 2.9584e+00, 1.5689e+02, 8.8479e+01, 3.5693e+01, 1.3351e-05,\n",
            "        6.0436e+01, 2.1148e+01, 6.9034e+01, 5.2443e+01, 1.0354e+02, 1.6370e+01,\n",
            "        5.0637e+01, 7.8845e+01, 1.2464e+02, 1.2020e+01, 6.0436e+01, 3.9746e+01,\n",
            "        2.8391e+01, 1.1675e+01, 4.8357e+01, 5.2443e+01, 3.4963e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.6947e-05, 2.5335e-10, 1.8091e-15, 0.0000e+00, 2.5335e-10, 1.0246e-17,\n",
            "        1.7469e-26, 8.0763e-06, 1.6811e-38, 1.0668e-34, 4.6947e-05, 6.5464e-10,\n",
            "        2.5335e-10, 3.5302e-08, 3.2485e-15, 4.1125e-12, 1.5588e-36, 8.9929e-39,\n",
            "        1.1511e-19, 1.2433e-37, 2.2122e-26, 5.4535e-10, 2.5335e-10, 1.0030e-37,\n",
            "        7.3290e-17, 2.8866e-15, 5.8597e-08, 4.1125e-12, 1.1083e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  5\n",
            "ce_loss:  tensor([3.4153e+01, 5.4125e+01, 3.8365e+01, 1.5739e+02, 5.4125e+01, 1.6181e+01,\n",
            "        1.2538e+02, 3.1100e+00, 1.5047e+02, 8.4070e+01, 3.4153e+01, 7.6294e-06,\n",
            "        5.4125e+01, 1.8826e+01, 6.3195e+01, 4.6690e+01, 9.9030e+01, 1.4972e+01,\n",
            "        4.7197e+01, 7.4499e+01, 1.1906e+02, 1.0320e+01, 5.4125e+01, 3.8128e+01,\n",
            "        2.6527e+01, 9.6715e+00, 4.3945e+01, 4.6690e+01, 3.2560e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([6.0873e-05, 1.3940e-10, 1.3233e-15, 0.0000e+00, 1.3940e-10, 7.8138e-18,\n",
            "        9.8363e-27, 9.0182e-06, 1.7344e-38, 2.0068e-34, 6.0873e-05, 1.1552e-09,\n",
            "        1.3940e-10, 1.9010e-08, 2.1643e-15, 2.5068e-12, 3.3104e-36, 1.7560e-38,\n",
            "        8.3941e-20, 1.3680e-37, 1.7296e-26, 4.8314e-10, 1.3940e-10, 1.5532e-37,\n",
            "        8.0618e-17, 3.1185e-15, 2.1080e-08, 2.5068e-12, 1.1773e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  6\n",
            "ce_loss:  tensor([3.4010e+01, 4.8596e+01, 3.4752e+01, 1.5182e+02, 4.8596e+01, 1.3492e+01,\n",
            "        1.1958e+02, 2.6949e+00, 1.4409e+02, 7.9969e+01, 3.4010e+01, 1.1921e-05,\n",
            "        4.8596e+01, 1.8937e+01, 5.7903e+01, 4.0158e+01, 9.4331e+01, 1.2748e+01,\n",
            "        4.3888e+01, 7.0171e+01, 1.1359e+02, 8.8632e+00, 4.8596e+01, 3.6127e+01,\n",
            "        2.4837e+01, 9.4074e+00, 3.9406e+01, 4.0158e+01, 3.2815e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([8.6440e-05, 1.0158e-10, 7.7744e-16, 0.0000e+00, 1.0158e-10, 9.3283e-18,\n",
            "        4.1493e-27, 1.0434e-05, 1.3102e-38, 4.6153e-34, 8.6440e-05, 2.1388e-09,\n",
            "        1.0158e-10, 1.7292e-08, 1.0008e-15, 1.0983e-12, 5.7929e-36, 5.1872e-38,\n",
            "        4.8253e-20, 1.1814e-37, 1.0012e-26, 3.9555e-10, 1.0158e-10, 2.8090e-37,\n",
            "        8.0780e-17, 4.2774e-15, 1.1214e-08, 1.0983e-12, 1.3749e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  7\n",
            "ce_loss:  tensor([3.2153e+01, 4.4087e+01, 3.3239e+01, 1.4629e+02, 4.4087e+01, 1.1476e+01,\n",
            "        1.1361e+02, 2.5695e+00, 1.3775e+02, 7.6312e+01, 3.2153e+01, 6.6757e-06,\n",
            "        4.4087e+01, 1.5597e+01, 5.3609e+01, 3.5886e+01, 8.9677e+01, 1.1759e+01,\n",
            "        4.0825e+01, 6.6155e+01, 1.0816e+02, 7.5149e+00, 4.4087e+01, 3.4263e+01,\n",
            "        2.3218e+01, 7.5782e+00, 3.5800e+01, 3.5886e+01, 3.0683e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.1005e-04, 5.1548e-11, 9.3912e-16, 0.0000e+00, 5.1548e-11, 9.0561e-18,\n",
            "        1.4918e-27, 1.1225e-05, 7.4976e-39, 5.6524e-34, 1.1005e-04, 3.6655e-09,\n",
            "        5.1548e-11, 1.9620e-08, 4.9293e-16, 5.9008e-13, 7.7354e-36, 7.2068e-38,\n",
            "        2.2751e-20, 9.2832e-38, 4.3627e-27, 3.7771e-10, 5.1548e-11, 4.9585e-37,\n",
            "        7.2501e-17, 4.0019e-15, 6.0169e-09, 5.9008e-13, 1.4340e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  8\n",
            "ce_loss:  tensor([3.2166e+01, 3.9352e+01, 3.0879e+01, 1.4076e+02, 3.9352e+01, 1.0441e+01,\n",
            "        1.0791e+02, 2.6108e+00, 1.3175e+02, 7.2063e+01, 3.2166e+01, 1.0133e-05,\n",
            "        3.9352e+01, 1.6618e+01, 5.0377e+01, 3.2789e+01, 8.5046e+01, 9.6797e+00,\n",
            "        3.8293e+01, 6.2264e+01, 1.0274e+02, 7.1608e+00, 3.9352e+01, 3.2438e+01,\n",
            "        2.0866e+01, 7.1068e+00, 3.1948e+01, 3.2789e+01, 3.0697e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.5413e-04, 2.4821e-11, 4.7479e-16, 0.0000e+00, 2.4821e-11, 9.8035e-18,\n",
            "        5.4667e-28, 1.3350e-05, 3.1532e-39, 8.7733e-34, 1.5413e-04, 6.6402e-09,\n",
            "        2.4821e-11, 1.4917e-08, 3.1224e-16, 3.4873e-13, 8.1030e-36, 1.9693e-37,\n",
            "        1.0023e-20, 5.7106e-38, 1.4431e-27, 8.3776e-10, 2.4821e-11, 7.4939e-37,\n",
            "        5.7613e-17, 4.1681e-15, 6.1339e-09, 3.4873e-13, 1.6802e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  9\n",
            "ce_loss:  tensor([3.0319e+01, 3.5838e+01, 2.8099e+01, 1.3528e+02, 3.5838e+01, 1.0520e+01,\n",
            "        1.0230e+02, 2.4138e+00, 1.2573e+02, 6.8229e+01, 3.0319e+01, 5.4836e-06,\n",
            "        3.5838e+01, 1.3429e+01, 4.6459e+01, 2.9519e+01, 8.0610e+01, 8.0906e+00,\n",
            "        3.6513e+01, 5.9381e+01, 9.8085e+01, 6.0780e+00, 3.5838e+01, 3.0702e+01,\n",
            "        1.8984e+01, 6.2101e+00, 2.9006e+01, 2.9519e+01, 2.8741e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.9385e-04, 9.2558e-12, 4.6361e-16, 0.0000e+00, 9.2558e-12, 1.9438e-17,\n",
            "        1.3779e-28, 1.4175e-05, 1.5986e-39, 1.3124e-33, 1.9385e-04, 1.1332e-08,\n",
            "        9.2558e-12, 1.4697e-08, 1.9281e-16, 2.9821e-13, 6.5077e-36, 3.3926e-37,\n",
            "        8.0055e-21, 2.8475e-38, 3.6321e-28, 5.7929e-10, 9.2558e-12, 1.0145e-36,\n",
            "        4.4128e-17, 3.3585e-15, 3.5561e-09, 2.9821e-13, 1.7360e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  10\n",
            "ce_loss:  tensor([3.0361e+01, 3.2539e+01, 2.7095e+01, 1.2994e+02, 3.2539e+01, 9.1584e+00,\n",
            "        9.6984e+01, 2.5860e+00, 1.2004e+02, 6.5066e+01, 3.0361e+01, 8.8214e-06,\n",
            "        3.2539e+01, 1.3960e+01, 4.2792e+01, 2.6997e+01, 7.6399e+01, 6.9529e+00,\n",
            "        3.4944e+01, 5.6746e+01, 9.3713e+01, 5.3933e+00, 3.2539e+01, 2.9421e+01,\n",
            "        1.7172e+01, 5.9933e+00, 2.5695e+01, 2.6997e+01, 2.8654e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.6685e-04, 6.5668e-12, 3.0685e-16, 0.0000e+00, 6.5668e-12, 1.8554e-17,\n",
            "        3.1733e-29, 1.6543e-05, 6.2563e-40, 1.3894e-33, 2.6685e-04, 2.1211e-08,\n",
            "        6.5668e-12, 9.2419e-09, 1.3699e-16, 1.6664e-13, 4.6991e-36, 5.2027e-37,\n",
            "        2.2694e-21, 2.6258e-38, 1.5758e-28, 1.2546e-09, 6.5668e-12, 1.2218e-36,\n",
            "        2.9813e-17, 4.2569e-15, 2.2972e-09, 1.6664e-13, 2.0071e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  11\n",
            "ce_loss:  tensor([2.8225e+01, 2.9476e+01, 2.4512e+01, 1.2471e+02, 2.9476e+01, 8.7359e+00,\n",
            "        9.2509e+01, 2.0550e+00, 1.1476e+02, 6.1659e+01, 2.8225e+01, 5.7220e-06,\n",
            "        2.9476e+01, 1.1969e+01, 3.9288e+01, 2.3714e+01, 7.2707e+01, 5.6736e+00,\n",
            "        3.2897e+01, 5.3758e+01, 8.9624e+01, 4.9634e+00, 2.9476e+01, 2.7452e+01,\n",
            "        1.5670e+01, 5.1828e+00, 2.5034e+01, 2.3714e+01, 2.6819e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.3075e-04, 6.5836e-12, 1.9371e-16, 0.0000e+00, 6.5836e-12, 4.0721e-17,\n",
            "        1.3238e-29, 1.7419e-05, 4.4818e-40, 1.4603e-33, 3.3075e-04, 3.8476e-08,\n",
            "        6.5836e-12, 9.2313e-09, 6.0136e-17, 1.3758e-13, 3.0403e-36, 8.8526e-37,\n",
            "        3.0239e-21, 3.7140e-38, 1.1006e-28, 9.7228e-10, 6.5836e-12, 1.2623e-36,\n",
            "        1.8949e-17, 3.2929e-15, 9.9868e-10, 1.3758e-13, 2.0726e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  12\n",
            "ce_loss:  tensor([2.8177e+01, 2.6952e+01, 2.2112e+01, 1.1949e+02, 2.6952e+01, 9.5389e+00,\n",
            "        8.7798e+01, 2.3999e+00, 1.0980e+02, 5.8715e+01, 2.8177e+01, 7.1525e-06,\n",
            "        2.6952e+01, 1.0179e+01, 3.6835e+01, 2.1433e+01, 6.8981e+01, 4.8317e+00,\n",
            "        3.1625e+01, 5.1693e+01, 8.6030e+01, 5.0014e+00, 2.6952e+01, 2.5698e+01,\n",
            "        1.4825e+01, 3.7228e+00, 2.3241e+01, 2.1433e+01, 2.6577e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.4859e-04, 2.3624e-12, 1.4023e-16, 0.0000e+00, 2.3624e-12, 2.7602e-17,\n",
            "        2.2904e-30, 2.0128e-05, 1.4820e-40, 1.1573e-33, 4.4859e-04, 7.4124e-08,\n",
            "        2.3624e-12, 1.0043e-08, 2.9125e-17, 7.7540e-14, 2.4236e-36, 2.1854e-36,\n",
            "        2.0070e-21, 2.1498e-38, 3.3468e-29, 1.4016e-09, 2.3624e-12, 1.1834e-36,\n",
            "        2.5482e-17, 3.9978e-15, 2.5103e-09, 7.7540e-14, 2.3989e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  13\n",
            "ce_loss:  tensor([2.6093e+01, 2.4256e+01, 2.1059e+01, 1.1439e+02, 2.4256e+01, 7.5106e+00,\n",
            "        8.3343e+01, 1.8842e+00, 1.0444e+02, 5.5686e+01, 2.6093e+01, 5.0068e-06,\n",
            "        2.4256e+01, 9.5717e+00, 3.5082e+01, 2.2721e+01, 6.5309e+01, 4.1872e+00,\n",
            "        2.9868e+01, 4.9893e+01, 8.1769e+01, 3.9296e+00, 2.4256e+01, 2.4055e+01,\n",
            "        1.3727e+01, 3.5677e+00, 2.0930e+01, 2.2721e+01, 2.4829e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.4871e-04, 2.4888e-12, 1.5264e-16, 0.0000e+00, 2.4888e-12, 5.9678e-17,\n",
            "        1.0077e-30, 2.0913e-05, 8.2241e-41, 8.9597e-34, 5.4871e-04, 1.3197e-07,\n",
            "        2.4888e-12, 1.2445e-08, 2.4968e-17, 4.2783e-14, 1.6511e-36, 2.6825e-36,\n",
            "        9.2691e-22, 5.4920e-38, 1.5752e-29, 1.0312e-09, 2.4888e-12, 9.6559e-37,\n",
            "        1.4052e-17, 3.0484e-15, 1.7410e-09, 4.2783e-14, 2.4416e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  14\n",
            "ce_loss:  tensor([2.6116e+01, 2.2150e+01, 1.9908e+01, 1.0945e+02, 2.2150e+01, 8.0429e+00,\n",
            "        7.9491e+01, 2.1196e+00, 9.9586e+01, 5.3317e+01, 2.6116e+01, 4.8876e-06,\n",
            "        2.2150e+01, 1.0518e+01, 3.2422e+01, 1.9530e+01, 6.1778e+01, 3.0575e+00,\n",
            "        2.8138e+01, 4.7200e+01, 7.7675e+01, 3.6365e+00, 2.2150e+01, 2.2730e+01,\n",
            "        1.2610e+01, 2.1501e+00, 2.0194e+01, 1.9530e+01, 2.4554e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([7.3267e-04, 1.7433e-12, 5.5143e-17, 0.0000e+00, 1.7433e-12, 4.9468e-17,\n",
            "        3.6823e-31, 2.3968e-05, 3.4237e-41, 2.3987e-33, 7.3267e-04, 2.4630e-07,\n",
            "        1.7433e-12, 1.0458e-08, 1.2749e-17, 3.9192e-14, 8.6367e-37, 8.1521e-36,\n",
            "        8.5610e-22, 4.8823e-38, 6.1725e-30, 1.6726e-09, 1.7433e-12, 7.6385e-37,\n",
            "        1.7523e-17, 3.5995e-15, 1.9369e-09, 3.9192e-14, 2.8324e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  15\n",
            "ce_loss:  tensor([2.3942e+01, 2.1279e+01, 1.9508e+01, 1.0500e+02, 2.1279e+01, 6.2354e+00,\n",
            "        7.5647e+01, 1.7008e+00, 9.5483e+01, 5.1006e+01, 2.3942e+01, 3.3379e-06,\n",
            "        2.1279e+01, 9.0695e+00, 3.0269e+01, 1.8889e+01, 5.8730e+01, 3.1739e+00,\n",
            "        2.7514e+01, 4.5701e+01, 7.4198e+01, 2.9821e+00, 2.1279e+01, 2.2061e+01,\n",
            "        1.4937e+01, 2.3869e+00, 1.8083e+01, 1.8889e+01, 2.2789e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([8.8628e-04, 8.5598e-13, 1.3578e-16, 0.0000e+00, 8.5598e-13, 9.3441e-17,\n",
            "        7.0917e-32, 2.5586e-05, 1.2514e-41, 1.4156e-33, 8.8628e-04, 4.2383e-07,\n",
            "        8.5598e-13, 1.0019e-08, 1.0728e-17, 3.9246e-14, 5.6956e-37, 8.7475e-36,\n",
            "        2.5507e-22, 9.0462e-38, 1.7802e-30, 1.0965e-09, 8.5598e-13, 8.3105e-37,\n",
            "        2.2088e-17, 2.5046e-15, 1.9703e-09, 3.9246e-14, 2.8605e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  16\n",
            "ce_loss:  tensor([2.4206e+01, 1.9655e+01, 1.8462e+01, 1.0049e+02, 1.9655e+01, 7.3036e+00,\n",
            "        7.1824e+01, 2.4572e+00, 9.0619e+01, 4.8324e+01, 2.4206e+01, 3.5763e-06,\n",
            "        1.9655e+01, 6.3381e+00, 2.8099e+01, 1.7080e+01, 5.6461e+01, 1.9973e+00,\n",
            "        2.6865e+01, 4.3609e+01, 7.0732e+01, 1.6490e+00, 1.9655e+01, 2.0710e+01,\n",
            "        1.2041e+01, 1.4656e+00, 1.7988e+01, 1.7080e+01, 2.2704e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.1681e-03, 3.0480e-12, 8.9182e-17, 0.0000e+00, 3.0480e-12, 8.1178e-17,\n",
            "        1.7466e-32, 2.8984e-05, 6.6800e-42, 3.5487e-33, 1.1681e-03, 7.5470e-07,\n",
            "        3.0480e-12, 1.9064e-08, 1.5783e-17, 3.6753e-14, 2.0194e-36, 2.3217e-35,\n",
            "        5.6135e-22, 3.7848e-38, 4.7865e-31, 2.0036e-09, 3.0480e-12, 6.2851e-37,\n",
            "        2.4554e-17, 3.0089e-15, 3.1004e-09, 3.6753e-14, 3.3073e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  17\n",
            "ce_loss:  tensor([2.1977e+01, 1.8071e+01, 1.9891e+01, 9.7608e+01, 1.8071e+01, 5.0471e+00,\n",
            "        6.8821e+01, 1.3486e+00, 8.6058e+01, 4.6682e+01, 2.1977e+01, 2.5034e-06,\n",
            "        1.8071e+01, 5.7812e+00, 2.6443e+01, 1.7092e+01, 5.3780e+01, 1.2412e+00,\n",
            "        2.4849e+01, 4.1451e+01, 6.7235e+01, 1.3668e+00, 1.8071e+01, 1.9215e+01,\n",
            "        1.3407e+01, 8.3890e-01, 1.6676e+01, 1.7092e+01, 2.0778e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.3926e-03, 2.3270e-12, 9.1585e-17, 0.0000e+00, 2.3270e-12, 1.4139e-16,\n",
            "        2.5736e-32, 2.9693e-05, 2.7690e-42, 1.8196e-33, 1.3926e-03, 1.2466e-06,\n",
            "        2.3270e-12, 1.9287e-08, 6.2643e-18, 5.3939e-14, 8.9073e-37, 3.7610e-35,\n",
            "        5.4865e-22, 6.6769e-38, 2.1266e-31, 3.1841e-09, 2.3270e-12, 5.6177e-37,\n",
            "        2.7105e-17, 2.7090e-15, 2.4707e-09, 5.3939e-14, 3.3383e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  18\n",
            "ce_loss:  tensor([2.2013e+01, 1.6773e+01, 1.6994e+01, 9.3360e+01, 1.6773e+01, 5.4467e+00,\n",
            "        6.6223e+01, 1.9694e+00, 8.2025e+01, 4.3745e+01, 2.2013e+01, 2.5034e-06,\n",
            "        1.6773e+01, 7.0613e+00, 2.5578e+01, 1.5793e+01, 5.0919e+01, 1.0557e+00,\n",
            "        2.5027e+01, 3.9658e+01, 6.4660e+01, 1.2907e+00, 1.6773e+01, 1.8592e+01,\n",
            "        9.9354e+00, 5.7871e-01, 1.5751e+01, 1.5793e+01, 2.0639e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.8137e-03, 3.1064e-12, 6.7468e-17, 0.0000e+00, 3.1064e-12, 2.5256e-16,\n",
            "        2.3421e-33, 3.3514e-05, 1.7698e-42, 3.2529e-33, 1.8137e-03, 2.1270e-06,\n",
            "        3.1064e-12, 1.5352e-08, 5.0899e-18, 3.0001e-14, 2.1347e-36, 4.4845e-35,\n",
            "        8.4475e-22, 3.0566e-38, 3.7930e-31, 2.5292e-09, 3.1064e-12, 3.5964e-37,\n",
            "        2.7210e-17, 1.9341e-15, 2.6295e-09, 3.0001e-14, 3.8426e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  19\n",
            "ce_loss:  tensor([1.9672e+01, 1.5918e+01, 1.6716e+01, 9.0029e+01, 1.5918e+01, 5.1999e+00,\n",
            "        6.3207e+01, 1.3196e+00, 7.8904e+01, 4.2088e+01, 1.9672e+01, 2.0266e-06,\n",
            "        1.5918e+01, 6.3989e+00, 2.3160e+01, 1.3436e+01, 4.9448e+01, 5.8847e-01,\n",
            "        2.2852e+01, 3.8102e+01, 6.1725e+01, 6.0001e-01, 1.5918e+01, 1.7096e+01,\n",
            "        9.7839e+00, 2.7849e-01, 1.4825e+01, 1.3436e+01, 1.8822e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.1455e-03, 1.4460e-12, 9.5575e-17, 0.0000e+00, 1.4460e-12, 1.9964e-16,\n",
            "        4.5558e-33, 3.5045e-05, 3.5313e-43, 5.9003e-33, 2.1455e-03, 3.3581e-06,\n",
            "        1.4460e-12, 1.1013e-08, 5.4618e-18, 7.5676e-14, 6.8760e-37, 9.3766e-35,\n",
            "        2.9113e-22, 3.1284e-38, 1.2031e-31, 4.0008e-09, 1.4460e-12, 2.9233e-37,\n",
            "        4.2729e-17, 1.8455e-15, 1.8013e-09, 7.5676e-14, 3.8399e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  20\n",
            "ce_loss:  tensor([1.9643e+01, 1.4240e+01, 1.4835e+01, 8.6439e+01, 1.4240e+01, 4.0998e+00,\n",
            "        6.0304e+01, 2.2266e+00, 7.5410e+01, 3.9924e+01, 1.9643e+01, 1.7881e-06,\n",
            "        1.4240e+01, 5.4978e+00, 2.1828e+01, 1.3610e+01, 4.7861e+01, 4.0302e-01,\n",
            "        2.2212e+01, 3.6581e+01, 5.9404e+01, 6.8242e-01, 1.4240e+01, 1.5850e+01,\n",
            "        8.6865e+00, 2.5945e-01, 1.3430e+01, 1.3610e+01, 1.8572e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.7405e-03, 4.1643e-12, 8.5173e-17, 0.0000e+00, 4.1643e-12, 4.6201e-16,\n",
            "        1.7676e-33, 3.9155e-05, 1.4027e-42, 3.1230e-33, 2.7405e-03, 5.5701e-06,\n",
            "        4.1643e-12, 8.1600e-09, 3.2419e-18, 5.0746e-14, 6.6765e-36, 1.2883e-34,\n",
            "        2.1649e-22, 1.8538e-38, 1.3000e-31, 3.2686e-09, 4.1643e-12, 2.0776e-37,\n",
            "        2.2764e-17, 1.2899e-15, 2.0354e-09, 5.0746e-14, 4.4077e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  21\n",
            "ce_loss:  tensor([1.7375e+01, 1.4344e+01, 1.4716e+01, 8.2429e+01, 1.4344e+01, 5.1285e+00,\n",
            "        5.6939e+01, 9.9396e-01, 7.2113e+01, 3.8089e+01, 1.7375e+01, 1.4305e-06,\n",
            "        1.4344e+01, 3.2504e+00, 2.0137e+01, 1.4417e+01, 4.5695e+01, 2.2342e-01,\n",
            "        2.1870e+01, 3.3889e+01, 5.7013e+01, 3.5319e-01, 1.4344e+01, 1.5034e+01,\n",
            "        8.0217e+00, 1.1194e-01, 1.2279e+01, 1.4417e+01, 1.6923e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.2348e-03, 2.5751e-12, 8.9800e-17, 0.0000e+00, 2.5751e-12, 3.1245e-16,\n",
            "        2.2930e-33, 3.9933e-05, 6.6141e-43, 7.6455e-33, 3.2348e-03, 8.5912e-06,\n",
            "        2.5751e-12, 1.1890e-08, 1.8657e-18, 4.5685e-14, 1.0423e-35, 3.8489e-34,\n",
            "        4.5002e-22, 4.1106e-38, 1.9121e-32, 4.4103e-09, 2.5751e-12, 1.1408e-37,\n",
            "        2.3778e-17, 1.2894e-15, 2.2463e-09, 4.5685e-14, 4.3998e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  22\n",
            "ce_loss:  tensor([1.7484e+01, 1.1966e+01, 1.4026e+01, 7.9643e+01, 1.1966e+01, 2.9334e+00,\n",
            "        5.5441e+01, 1.6438e+00, 6.8557e+01, 3.6389e+01, 1.7484e+01, 1.1921e-06,\n",
            "        1.1966e+01, 2.5427e+00, 1.9675e+01, 1.3335e+01, 4.6571e+01, 1.6399e-01,\n",
            "        2.1412e+01, 3.3621e+01, 5.3585e+01, 2.9682e-01, 1.1966e+01, 1.3578e+01,\n",
            "        7.6932e+00, 8.9878e-02, 1.0006e+01, 1.3335e+01, 1.6574e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.0346e-03, 4.2906e-12, 7.5003e-17, 0.0000e+00, 4.2906e-12, 4.6089e-16,\n",
            "        4.0105e-34, 4.4590e-05, 2.0417e-42, 3.1086e-33, 4.0346e-03, 1.3715e-05,\n",
            "        4.2906e-12, 1.2899e-08, 7.7742e-18, 2.8883e-14, 1.4200e-35, 8.3001e-34,\n",
            "        5.1630e-22, 3.4159e-38, 3.0460e-32, 3.3235e-09, 4.2906e-12, 8.2443e-38,\n",
            "        5.0444e-17, 1.2127e-15, 4.0775e-09, 2.8883e-14, 5.0113e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  23\n",
            "ce_loss:  tensor([1.5023e+01, 1.2180e+01, 1.5112e+01, 7.5552e+01, 1.2180e+01, 4.1376e+00,\n",
            "        5.2433e+01, 6.7151e-01, 6.6211e+01, 3.3920e+01, 1.5023e+01, 8.3446e-07,\n",
            "        1.2180e+01, 4.8253e+00, 1.8579e+01, 1.0727e+01, 4.2628e+01, 1.5910e-01,\n",
            "        2.2487e+01, 3.4581e+01, 5.1841e+01, 1.2301e-01, 1.2180e+01, 1.2759e+01,\n",
            "        7.6965e+00, 5.5545e-02, 1.1458e+01, 1.0727e+01, 1.5041e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.6961e-03, 3.6705e-12, 8.2787e-17, 0.0000e+00, 3.6705e-12, 8.7444e-16,\n",
            "        4.1425e-34, 4.5461e-05, 3.1669e-43, 6.9690e-33, 4.6961e-03, 2.0449e-05,\n",
            "        3.6705e-12, 7.7224e-09, 2.9832e-18, 3.7101e-14, 2.1275e-35, 7.2055e-34,\n",
            "        3.5244e-22, 3.9902e-38, 9.5511e-33, 5.3268e-09, 3.6705e-12, 5.2009e-38,\n",
            "        2.4901e-17, 1.0758e-15, 2.0740e-09, 3.7101e-14, 4.9443e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  24\n",
            "ce_loss:  tensor([1.5234e+01, 1.0495e+01, 1.3032e+01, 7.2154e+01, 1.0495e+01, 4.9132e+00,\n",
            "        5.0659e+01, 1.2283e+00, 6.2259e+01, 3.2701e+01, 1.5234e+01, 7.1526e-07,\n",
            "        1.0495e+01, 3.5272e+00, 2.0377e+01, 1.0060e+01, 3.9933e+01, 9.8363e-02,\n",
            "        1.9971e+01, 3.0088e+01, 4.9081e+01, 7.1411e-02, 1.0495e+01, 1.1994e+01,\n",
            "        6.5052e+00, 3.2479e-02, 9.2378e+00, 1.0060e+01, 1.4734e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.7992e-03, 5.5766e-12, 5.8388e-17, 0.0000e+00, 5.5766e-12, 5.6796e-16,\n",
            "        3.6854e-33, 5.0317e-05, 7.6511e-43, 5.3774e-33, 5.7992e-03, 3.1751e-05,\n",
            "        5.5766e-12, 5.0452e-09, 4.1585e-18, 4.4548e-14, 1.3965e-34, 1.4559e-33,\n",
            "        3.7221e-22, 6.3653e-38, 6.8484e-33, 7.4569e-09, 5.5766e-12, 3.1058e-38,\n",
            "        5.1644e-17, 1.2595e-15, 1.7578e-09, 4.4548e-14, 5.6502e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  25\n",
            "ce_loss:  tensor([1.2680e+01, 1.1741e+01, 1.1590e+01, 6.8885e+01, 1.1741e+01, 2.6597e+00,\n",
            "        4.9908e+01, 5.5398e-01, 6.0348e+01, 3.0033e+01, 1.2680e+01, 5.9605e-07,\n",
            "        1.1741e+01, 1.2849e+00, 1.6576e+01, 9.1883e+00, 4.2379e+01, 7.6340e-02,\n",
            "        2.1253e+01, 2.9492e+01, 4.8575e+01, 3.3864e-02, 1.1741e+01, 1.0874e+01,\n",
            "        6.4180e+00, 1.7458e-02, 7.8190e+00, 9.1883e+00, 1.2948e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([6.6641e-03, 2.7522e-12, 6.9669e-17, 0.0000e+00, 2.7522e-12, 1.1795e-15,\n",
            "        2.6086e-34, 5.1658e-05, 1.4125e-42, 9.7499e-33, 6.6641e-03, 4.5891e-05,\n",
            "        2.7522e-12, 6.1870e-09, 4.0130e-18, 4.9660e-14, 1.3582e-34, 2.3120e-33,\n",
            "        2.2422e-22, 2.9567e-38, 1.0680e-32, 9.0548e-09, 2.7522e-12, 1.7192e-38,\n",
            "        4.8571e-17, 1.0179e-15, 2.8158e-09, 4.9660e-14, 5.5871e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  26\n",
            "ce_loss:  tensor([1.2734e+01, 8.5219e+00, 1.0903e+01, 6.4947e+01, 8.5219e+00, 3.6707e+00,\n",
            "        4.7012e+01, 1.1825e+00, 6.0341e+01, 2.9155e+01, 1.2734e+01, 4.7684e-07,\n",
            "        8.5219e+00, 6.3199e-01, 1.5813e+01, 8.8044e+00, 3.8916e+01, 5.7303e-02,\n",
            "        1.9466e+01, 2.8537e+01, 4.7324e+01, 1.6871e-02, 8.5219e+00, 1.0342e+01,\n",
            "        7.1414e+00, 9.4348e-03, 7.3639e+00, 8.8044e+00, 1.2644e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([8.0890e-03, 4.8379e-12, 6.4162e-17, 0.0000e+00, 4.8379e-12, 7.1827e-16,\n",
            "        2.1448e-33, 5.6586e-05, 1.2261e-42, 1.0148e-32, 8.0890e-03, 6.9034e-05,\n",
            "        4.8379e-12, 5.3085e-09, 7.1603e-18, 4.5760e-14, 2.1109e-34, 2.4000e-33,\n",
            "        3.4968e-22, 1.0380e-37, 8.1513e-33, 1.7499e-08, 4.8379e-12, 8.9247e-39,\n",
            "        5.2879e-17, 8.6078e-16, 2.4480e-09, 4.5760e-14, 6.3072e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  27\n",
            "ce_loss:  tensor([1.0361e+01, 7.8391e+00, 9.8513e+00, 6.2402e+01, 7.8391e+00, 1.6850e+00,\n",
            "        4.6601e+01, 3.5696e-01, 5.6101e+01, 3.0166e+01, 1.0361e+01, 3.5763e-07,\n",
            "        7.8391e+00, 2.6323e-01, 1.5350e+01, 7.3644e+00, 3.6291e+01, 4.3522e-02,\n",
            "        1.7418e+01, 2.7762e+01, 4.4931e+01, 5.5149e-03, 7.8391e+00, 9.8083e+00,\n",
            "        5.2764e+00, 5.0051e-03, 5.4537e+00, 7.3644e+00, 1.0936e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([9.2133e-03, 1.0141e-11, 6.8842e-17, 0.0000e+00, 1.0141e-11, 1.2028e-15,\n",
            "        2.6078e-33, 5.7473e-05, 1.1743e-42, 1.5095e-32, 9.2133e-03, 9.7909e-05,\n",
            "        1.0141e-11, 5.9781e-09, 3.1024e-18, 4.8063e-14, 3.8043e-33, 3.8146e-33,\n",
            "        4.2782e-22, 1.3980e-37, 1.0502e-32, 3.0315e-08, 1.0141e-11, 1.0149e-38,\n",
            "        3.8288e-17, 8.8940e-16, 2.9232e-09, 4.8063e-14, 6.2402e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  28\n",
            "ce_loss:  tensor([9.9003e+00, 7.6373e+00, 1.0169e+01, 5.9955e+01, 7.6373e+00, 4.1579e+00,\n",
            "        4.6417e+01, 8.1046e-01, 5.2726e+01, 2.7399e+01, 9.9003e+00, 3.5763e-07,\n",
            "        7.6373e+00, 2.3391e-01, 1.3012e+01, 8.7862e+00, 3.6519e+01, 3.4934e-02,\n",
            "        1.7323e+01, 2.8053e+01, 4.4478e+01, 3.2658e-03, 7.6373e+00, 8.9009e+00,\n",
            "        6.1685e+00, 1.8613e-03, 6.2471e+00, 8.7862e+00, 1.0504e+01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.1004e-02, 4.0630e-12, 4.7888e-17, 0.0000e+00, 4.0630e-12, 6.7569e-16,\n",
            "        1.7675e-33, 6.2701e-05, 9.5821e-42, 1.2884e-32, 1.1004e-02, 1.4151e-04,\n",
            "        4.0630e-12, 5.1484e-09, 6.9825e-18, 2.2826e-14, 9.1102e-34, 4.1620e-33,\n",
            "        1.3123e-21, 1.1983e-37, 9.2683e-33, 6.5556e-08, 4.0630e-12, 5.3180e-39,\n",
            "        4.0916e-17, 7.6160e-16, 2.6785e-09, 2.2826e-14, 6.9745e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  29\n",
            "ce_loss:  tensor([7.7026e+00, 5.8384e+00, 8.0544e+00, 5.6255e+01, 5.8384e+00, 1.0837e+00,\n",
            "        4.3904e+01, 2.0897e-01, 5.1982e+01, 2.4985e+01, 7.7026e+00, 2.3842e-07,\n",
            "        5.8384e+00, 7.2960e-02, 1.2820e+01, 5.7877e+00, 3.3960e+01, 2.4265e-02,\n",
            "        1.7137e+01, 2.5890e+01, 4.2889e+01, 2.4790e-03, 5.8384e+00, 9.1140e+00,\n",
            "        4.4160e+00, 7.8480e-04, 5.3155e+00, 5.7877e+00, 8.9914e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.2416e-02, 7.4410e-12, 7.6589e-17, 0.0000e+00, 7.4410e-12, 9.3655e-16,\n",
            "        1.2658e-33, 6.3658e-05, 3.0100e-42, 6.2212e-32, 1.2416e-02, 1.9358e-04,\n",
            "        7.4410e-12, 5.0962e-09, 1.7613e-17, 2.9979e-14, 1.3904e-32, 8.5307e-33,\n",
            "        4.0240e-22, 1.5704e-37, 5.9049e-33, 1.2878e-07, 7.4410e-12, 5.3848e-39,\n",
            "        3.4239e-17, 7.6520e-16, 1.3385e-09, 2.9979e-14, 6.8888e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  30\n",
            "ce_loss:  tensor([7.2468e+00, 5.5440e+00, 8.6821e+00, 5.4924e+01, 5.5440e+00, 2.3846e+00,\n",
            "        4.0669e+01, 5.2440e-01, 4.8612e+01, 2.4741e+01, 7.2468e+00, 2.3842e-07,\n",
            "        5.5440e+00, 3.3122e-02, 1.2455e+01, 5.8726e+00, 3.3815e+01, 2.5131e-02,\n",
            "        1.6011e+01, 2.6187e+01, 3.9562e+01, 1.6334e-03, 5.5440e+00, 7.6882e+00,\n",
            "        4.8530e+00, 4.6314e-04, 3.4738e+00, 5.8726e+00, 8.4373e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.4552e-02, 1.0046e-11, 1.1167e-16, 0.0000e+00, 1.0046e-11, 1.4180e-15,\n",
            "        1.1070e-32, 6.8811e-05, 1.8839e-41, 4.2485e-32, 1.4552e-02, 2.7062e-04,\n",
            "        1.0046e-11, 4.5800e-09, 6.8134e-18, 4.6126e-14, 1.5732e-31, 2.1699e-32,\n",
            "        1.0999e-21, 1.9188e-37, 7.1962e-32, 2.3304e-07, 1.0046e-11, 2.3596e-39,\n",
            "        3.2337e-17, 7.8304e-16, 1.8903e-09, 4.6126e-14, 7.6270e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  31\n",
            "ce_loss:  tensor([4.9094e+00, 6.6864e+00, 7.9782e+00, 5.2993e+01, 6.6864e+00, 2.6414e+00,\n",
            "        4.1439e+01, 1.2373e-01, 4.6590e+01, 2.2250e+01, 4.9094e+00, 1.1921e-07,\n",
            "        6.6864e+00, 1.3552e-02, 1.3447e+01, 5.9865e+00, 3.2641e+01, 1.9355e-02,\n",
            "        1.7466e+01, 2.3921e+01, 4.0607e+01, 1.0901e-03, 6.6864e+00, 7.6519e+00,\n",
            "        3.3442e+00, 1.9143e-04, 4.4208e+00, 5.9865e+00, 7.0630e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.6220e-02, 4.2372e-12, 4.5532e-17, 0.0000e+00, 4.2372e-12, 1.0005e-15,\n",
            "        1.0515e-32, 6.9653e-05, 1.4010e-40, 2.0097e-31, 1.6220e-02, 3.6186e-04,\n",
            "        4.2372e-12, 5.0644e-09, 7.2752e-18, 2.4182e-14, 4.0488e-32, 1.4545e-32,\n",
            "        9.2172e-22, 2.2902e-37, 5.1996e-32, 3.8408e-07, 4.2372e-12, 2.0959e-39,\n",
            "        2.4583e-17, 7.3112e-16, 2.8159e-09, 2.4182e-14, 7.5292e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  32\n",
            "ce_loss:  tensor([4.4832e+00, 4.4800e+00, 6.6056e+00, 5.0166e+01, 4.4800e+00, 7.3508e-01,\n",
            "        3.9132e+01, 3.1141e-01, 4.5876e+01, 2.1857e+01, 4.4832e+00, 1.1921e-07,\n",
            "        4.4800e+00, 4.6255e-03, 1.0730e+01, 3.9075e+00, 3.1078e+01, 1.1133e-02,\n",
            "        1.7139e+01, 2.4838e+01, 3.7923e+01, 7.2382e-04, 4.4800e+00, 6.5686e+00,\n",
            "        2.4609e+00, 1.0943e-04, 4.7691e+00, 3.9075e+00, 6.3101e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([1.8748e-02, 4.8206e-12, 6.4995e-17, 0.0000e+00, 4.8206e-12, 1.6605e-15,\n",
            "        7.1519e-33, 7.4853e-05, 3.2917e-41, 1.1942e-30, 1.8748e-02, 4.8819e-04,\n",
            "        4.8206e-12, 7.5617e-09, 5.8044e-18, 2.9607e-14, 4.0609e-31, 2.6724e-32,\n",
            "        5.5438e-22, 2.9167e-37, 4.1731e-32, 6.1833e-07, 4.8206e-12, 8.7658e-40,\n",
            "        3.6159e-17, 4.9292e-16, 1.7038e-09, 2.9607e-14, 8.3222e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  33\n",
            "ce_loss:  tensor([2.0850e+00, 4.7386e+00, 6.1405e+00, 4.8316e+01, 4.7386e+00, 1.9009e+00,\n",
            "        3.6119e+01, 6.4286e-02, 4.2708e+01, 2.1072e+01, 2.0850e+00, 1.1921e-07,\n",
            "        4.7386e+00, 1.8949e-03, 9.0711e+00, 5.2002e+00, 3.1163e+01, 8.5015e-03,\n",
            "        1.5955e+01, 2.3311e+01, 3.5171e+01, 4.0702e-04, 4.7386e+00, 6.3566e+00,\n",
            "        2.9377e+00, 5.7219e-05, 2.5661e+00, 5.2002e+00, 5.0255e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.0619e-02, 3.0556e-12, 4.3166e-17, 0.0000e+00, 3.0556e-12, 1.0637e-15,\n",
            "        2.6882e-32, 7.5497e-05, 3.8868e-40, 4.0756e-31, 2.0619e-02, 6.3034e-04,\n",
            "        3.0556e-12, 1.9735e-08, 1.0040e-17, 3.6078e-14, 8.1547e-31, 4.5243e-32,\n",
            "        6.6490e-22, 2.5044e-37, 5.1018e-31, 9.2526e-07, 3.0556e-12, 6.8583e-40,\n",
            "        2.9771e-17, 5.9839e-16, 1.4847e-09, 3.6078e-14, 8.1926e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  34\n",
            "ce_loss:  tensor([1.6846e+00, 3.0193e+00, 8.0543e+00, 4.4719e+01, 3.0193e+00, 3.1581e-01,\n",
            "        3.4001e+01, 1.6410e-01, 4.0802e+01, 1.8953e+01, 1.6846e+00, 1.1921e-07,\n",
            "        3.0193e+00, 1.0344e-03, 7.9534e+00, 6.0331e+00, 3.0757e+01, 6.2645e-03,\n",
            "        1.3999e+01, 2.0465e+01, 3.8563e+01, 2.7152e-04, 3.0193e+00, 5.4370e+00,\n",
            "        1.7562e+00, 1.1741e-04, 4.1742e+00, 6.0331e+00, 4.0988e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.3453e-02, 3.3231e-12, 3.8999e-17, 0.0000e+00, 3.3231e-12, 1.5752e-15,\n",
            "        1.5829e-31, 8.0776e-05, 4.7884e-39, 2.2733e-30, 2.3453e-02, 8.2756e-04,\n",
            "        3.3231e-12, 4.9999e-08, 1.3522e-17, 1.4590e-14, 8.0322e-31, 7.3257e-32,\n",
            "        1.3226e-21, 1.0423e-36, 4.1885e-31, 1.3690e-06, 3.3231e-12, 2.3858e-40,\n",
            "        1.9340e-17, 3.8617e-16, 9.8402e-10, 1.4590e-14, 8.9703e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  35\n",
            "ce_loss:  tensor([2.8706e-01, 2.4744e+00, 5.9123e+00, 4.3348e+01, 2.4744e+00, 8.0279e-02,\n",
            "        3.4305e+01, 3.3500e-02, 4.0827e+01, 1.9541e+01, 2.8706e-01, -0.0000e+00,\n",
            "        2.4744e+00, 7.6253e-04, 7.3373e+00, 2.8939e+00, 2.8659e+01, 4.0092e-03,\n",
            "        1.3534e+01, 2.0156e+01, 3.4904e+01, 1.5663e-04, 2.4744e+00, 5.1107e+00,\n",
            "        1.9217e+00, 2.8371e-05, 1.8174e+00, 2.8939e+00, 3.0214e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.5565e-02, 4.0536e-12, 2.3257e-17, 0.0000e+00, 4.0536e-12, 1.7727e-15,\n",
            "        5.8276e-32, 8.1241e-05, 2.2130e-39, 3.0412e-30, 2.5565e-02, 1.0375e-03,\n",
            "        4.0536e-12, 1.1594e-07, 2.3540e-17, 1.9642e-14, 1.8104e-30, 1.0597e-31,\n",
            "        2.9670e-21, 3.7891e-36, 2.3811e-31, 1.9082e-06, 4.0536e-12, 1.5207e-40,\n",
            "        1.5738e-17, 5.5051e-16, 9.0954e-10, 1.9642e-14, 8.9642e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  36\n",
            "ce_loss:  tensor([4.2514e-01, 4.5562e+00, 3.8287e+00, 4.1690e+01, 4.5562e+00, 4.8421e-02,\n",
            "        3.1226e+01, 8.0305e-02, 3.9974e+01, 1.8591e+01, 4.2514e-01, -0.0000e+00,\n",
            "        4.5562e+00, 3.5244e-04, 6.8710e+00, 6.7786e+00, 2.6148e+01, 2.8925e-03,\n",
            "        1.4468e+01, 2.0000e+01, 3.1663e+01, 1.1038e-04, 4.5562e+00, 4.5093e+00,\n",
            "        1.2376e+00, 2.6583e-05, 1.7388e+00, 6.7786e+00, 1.9307e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([2.8566e-02, 1.9916e-12, 2.5807e-17, 0.0000e+00, 1.9916e-12, 1.7584e-15,\n",
            "        2.6014e-31, 8.6489e-05, 2.5113e-39, 3.0960e-30, 2.8566e-02, 1.3270e-03,\n",
            "        1.9916e-12, 2.4270e-07, 1.2513e-17, 8.2475e-15, 9.3738e-30, 1.0526e-31,\n",
            "        9.1198e-22, 9.3413e-37, 2.4182e-30, 2.6466e-06, 1.9916e-12, 8.6967e-41,\n",
            "        9.7569e-18, 3.7147e-16, 1.2769e-09, 8.2475e-15, 9.5781e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  37\n",
            "ce_loss:  tensor([2.1956e-02, 1.7368e+00, 3.0148e+00, 4.1442e+01, 1.7368e+00, 1.6917e-02,\n",
            "        3.1129e+01, 1.5805e-02, 3.7407e+01, 1.5842e+01, 2.1956e-02, -0.0000e+00,\n",
            "        1.7368e+00, 3.0513e-04, 5.7654e+00, 3.9140e+00, 2.4994e+01, 2.2121e-03,\n",
            "        1.2597e+01, 1.7842e+01, 3.2357e+01, 6.5325e-05, 1.7368e+00, 3.6060e+00,\n",
            "        6.6441e-01, 1.8477e-05, 3.8078e+00, 3.9140e+00, 1.2252e+00],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.0573e-02, 1.8086e-12, 3.2593e-17, 0.0000e+00, 1.8086e-12, 1.7195e-15,\n",
            "        1.2643e-30, 8.6938e-05, 3.4587e-39, 1.5868e-29, 3.0573e-02, 1.6217e-03,\n",
            "        1.8086e-12, 4.6495e-07, 1.1749e-17, 5.4131e-15, 5.5551e-29, 1.7105e-31,\n",
            "        1.8673e-21, 3.0956e-36, 3.5552e-30, 3.5277e-06, 1.8086e-12, 4.0360e-41,\n",
            "        8.7356e-18, 5.2948e-16, 7.5143e-10, 5.4131e-15, 9.5715e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  38\n",
            "ce_loss:  tensor([4.3266e-02, 2.0502e+00, 2.9672e+00, 3.7828e+01, 2.0502e+00, 8.8074e-03,\n",
            "        3.0251e+01, 4.0702e-02, 3.4553e+01, 1.6038e+01, 4.3266e-02, -0.0000e+00,\n",
            "        2.0502e+00, 1.4280e-04, 6.3330e+00, 3.6932e+00, 2.6418e+01, 1.4466e-03,\n",
            "        1.2655e+01, 1.9750e+01, 3.1337e+01, 4.2557e-05, 2.0502e+00, 3.4506e+00,\n",
            "        8.2110e-01, 7.6294e-06, 7.6769e-01, 3.6932e+00, 5.2498e-01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.3954e-02, 2.5143e-12, 2.1350e-17, 0.0000e+00, 2.5143e-12, 1.4233e-15,\n",
            "        2.1743e-31, 9.1633e-05, 2.9669e-38, 5.9875e-29, 3.3954e-02, 1.9896e-03,\n",
            "        2.5143e-12, 8.5965e-07, 3.9921e-18, 4.2269e-15, 3.6300e-29, 1.3939e-31,\n",
            "        1.9492e-21, 3.0014e-36, 2.7126e-30, 4.6764e-06, 2.5143e-12, 5.3106e-41,\n",
            "        7.4123e-18, 4.8418e-16, 9.1047e-10, 4.2269e-15, 1.0199e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  39\n",
            "ce_loss:  tensor([2.4873e-03, 3.7330e+00, 1.6091e+00, 3.7479e+01, 3.7330e+00, 6.9091e-03,\n",
            "        2.7455e+01, 8.3192e-03, 3.2619e+01, 1.5735e+01, 2.4873e-03, -0.0000e+00,\n",
            "        3.7330e+00, 1.1384e-04, 4.1837e+00, 1.8534e+00, 2.3934e+01, 9.6692e-04,\n",
            "        1.3370e+01, 1.6913e+01, 2.9711e+01, 2.5391e-05, 3.7330e+00, 2.9910e+00,\n",
            "        4.3695e-01, 1.9073e-05, 1.5113e+00, 1.8534e+00, 2.9972e-01],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.5778e-02, 1.2825e-12, 2.2566e-17, 0.0000e+00, 1.2825e-12, 1.4712e-15,\n",
            "        1.2216e-30, 9.1803e-05, 1.3790e-37, 2.0975e-29, 3.5778e-02, 2.3387e-03,\n",
            "        1.2825e-12, 1.4717e-06, 5.1473e-18, 5.4417e-15, 7.1946e-29, 1.4501e-31,\n",
            "        1.7000e-21, 2.6163e-36, 2.4897e-30, 5.9785e-06, 1.2825e-12, 1.5811e-41,\n",
            "        4.5711e-18, 4.0201e-16, 1.0007e-09, 5.4417e-15, 9.9954e-02],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  40\n",
            "ce_loss:  tensor([4.5661e-03, 8.7514e-01, 1.0927e+00, 3.4443e+01, 8.7514e-01, 1.0924e-02,\n",
            "        2.7221e+01, 2.1179e-02, 3.3340e+01, 1.3181e+01, 4.5661e-03, -0.0000e+00,\n",
            "        8.7514e-01, 6.4729e-05, 3.0094e+00, 1.1887e+00, 2.1455e+01, 5.9837e-04,\n",
            "        1.3951e+01, 1.5512e+01, 2.7573e+01, 1.6808e-05, 8.7514e-01, 2.3397e+00,\n",
            "        2.5889e-01, 3.3379e-06, 2.4538e+00, 1.1887e+00, 9.4124e-02],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([3.9034e-02, 1.0613e-12, 1.6150e-17, 0.0000e+00, 1.0613e-12, 9.8479e-16,\n",
            "        3.1632e-30, 9.6064e-05, 1.3218e-37, 6.1097e-29, 3.9034e-02, 2.8031e-03,\n",
            "        1.0613e-12, 2.4502e-06, 6.9174e-18, 6.7700e-15, 3.4055e-28, 1.5288e-31,\n",
            "        8.6679e-22, 8.6900e-36, 1.6199e-29, 7.6001e-06, 1.0613e-12, 2.1586e-41,\n",
            "        3.5350e-18, 5.3906e-16, 5.2902e-10, 6.7700e-15, 1.0710e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  41\n",
            "ce_loss:  tensor([4.2072e-04, 1.0382e+00, 5.8139e-01, 3.1320e+01, 1.0382e+00, 2.9816e-03,\n",
            "        2.6855e+01, 4.5296e-03, 3.0832e+01, 1.3659e+01, 4.2072e-04, -0.0000e+00,\n",
            "        1.0382e+00, 8.2132e-05, 2.9106e+00, 3.3597e+00, 2.3453e+01, 4.8483e-04,\n",
            "        1.3345e+01, 1.6666e+01, 2.8506e+01, 9.8943e-06, 1.0382e+00, 1.9283e+00,\n",
            "        3.2388e-01, 1.3590e-05, 1.6281e-01, 3.3597e+00, 4.5345e-02],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.0972e-02, 1.1842e-12, 1.9053e-17, 1.4013e-45, 1.1842e-12, 1.2045e-15,\n",
            "        8.3008e-31, 9.6028e-05, 2.1516e-37, 6.8499e-29, 4.0972e-02, 3.1869e-03,\n",
            "        1.1842e-12, 3.7852e-06, 6.8752e-18, 2.5143e-15, 2.2092e-28, 1.7783e-31,\n",
            "        6.0428e-22, 5.4445e-36, 9.7868e-30, 9.2728e-06, 1.1842e-12, 1.0845e-41,\n",
            "        1.7345e-18, 3.9470e-16, 6.1286e-10, 2.5143e-15, 1.0479e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  42\n",
            "ce_loss:  tensor([9.3631e-04, 2.2363e+00, 3.4986e-01, 2.9451e+01, 2.2363e+00, 2.1406e-03,\n",
            "        2.3975e+01, 1.0945e-02, 2.8064e+01, 1.3383e+01, 9.3631e-04, -0.0000e+00,\n",
            "        2.2363e+00, 3.6477e-05, 3.1837e+00, 6.6820e-01, 2.0616e+01, 2.5615e-04,\n",
            "        1.0789e+01, 1.4550e+01, 2.6851e+01, 7.0333e-06, 2.2363e+00, 2.1788e+00,\n",
            "        1.7533e-01, 1.5497e-06, 2.8973e-01, 6.6820e-01, 1.7123e-02],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.3889e-02, 5.8873e-13, 1.0363e-17, 1.6816e-44, 5.8873e-13, 9.7665e-16,\n",
            "        3.7618e-30, 9.9985e-05, 1.1453e-36, 5.6611e-29, 4.3889e-02, 3.6924e-03,\n",
            "        5.8873e-13, 5.8632e-06, 2.3669e-18, 2.9319e-15, 1.4521e-28, 1.5882e-31,\n",
            "        9.7847e-22, 5.8528e-36, 1.0411e-29, 1.1419e-05, 5.8873e-13, 1.6203e-41,\n",
            "        1.4696e-18, 5.1345e-16, 7.4587e-10, 2.9319e-15, 1.1165e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  43\n",
            "ce_loss:  tensor([9.4409e-05, 3.0194e-01, 1.1254e-01, 3.0987e+01, 3.0194e-01, 1.3000e-03,\n",
            "        2.3669e+01, 2.4963e-03, 2.7387e+01, 1.1530e+01, 9.4409e-05, -0.0000e+00,\n",
            "        3.0194e-01, 1.2921e-04, 1.7353e+00, 3.9993e-01, 1.8315e+01, 1.4590e-04,\n",
            "        1.0246e+01, 1.5960e+01, 2.4078e+01, 4.0531e-06, 3.0194e-01, 1.5485e+00,\n",
            "        1.7543e-01, 1.3113e-06, 9.4324e-01, 3.9993e-01, 9.4218e-03],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.5787e-02, 7.3709e-13, 1.0722e-17, 2.2421e-44, 7.3709e-13, 7.5565e-16,\n",
            "        1.4779e-29, 9.9615e-05, 3.3349e-36, 1.5076e-28, 4.5787e-02, 4.1058e-03,\n",
            "        7.3709e-13, 8.1182e-06, 2.7665e-18, 2.8583e-15, 6.0011e-28, 2.2247e-31,\n",
            "        1.8151e-21, 4.9457e-36, 5.9817e-29, 1.3512e-05, 7.3709e-13, 7.8192e-42,\n",
            "        9.0827e-19, 3.2678e-16, 4.0356e-10, 2.8583e-15, 1.0918e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  44\n",
            "ce_loss:  tensor([1.9775e-04, 1.1757e+01, 5.0511e-02, 2.6657e+01, 1.1757e+01, 1.3333e-03,\n",
            "        2.3040e+01, 5.5021e-03, 2.7944e+01, 9.6109e+00, 1.9775e-04, -0.0000e+00,\n",
            "        1.1757e+01, 1.9312e-05, 1.2608e+00, 1.3940e+00, 1.7790e+01, 1.1324e-04,\n",
            "        1.0920e+01, 1.4304e+01, 2.6028e+01, 3.4571e-06, 1.1757e+01, 1.1026e+00,\n",
            "        1.5291e-01, 1.0729e-06, 5.0260e-02, 1.3940e+00, 3.9162e-03],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.8183e-02, 6.5128e-13, 8.8440e-18, 4.4842e-44, 6.5128e-13, 4.7590e-16,\n",
            "        5.5016e-30, 1.0349e-04, 2.2411e-36, 4.3764e-28, 4.8183e-02, 4.5908e-03,\n",
            "        6.5128e-13, 1.2083e-05, 3.8247e-18, 1.0507e-15, 1.9968e-27, 2.0831e-31,\n",
            "        8.1431e-22, 2.2512e-36, 3.6632e-29, 1.5983e-05, 6.5128e-13, 3.6434e-42,\n",
            "        4.9500e-19, 3.1180e-16, 4.0437e-10, 1.0507e-15, 1.1480e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  45\n",
            "ce_loss:  tensor([2.2530e-05, 1.3911e+00, 2.6095e-02, 2.3745e+01, 1.3911e+00, 7.3072e-04,\n",
            "        2.0374e+01, 1.3893e-03, 2.5204e+01, 8.9671e+00, 2.2530e-05, -0.0000e+00,\n",
            "        1.3911e+00, 1.9550e-05, 1.7359e+00, 1.5431e-01, 1.8074e+01, 8.9761e-05,\n",
            "        9.3604e+00, 1.2066e+01, 2.3796e+01, 2.1458e-06, 1.3911e+00, 8.5435e-01,\n",
            "        1.1739e-01, 4.7684e-07, 9.8488e-03, 1.5431e-01, 2.3139e-03],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([4.9518e-02, 3.2267e-13, 6.3700e-18, 2.9147e-43, 3.2267e-13, 5.8684e-16,\n",
            "        1.9878e-29, 1.0236e-04, 3.7643e-36, 8.2332e-28, 4.9518e-02, 5.0315e-03,\n",
            "        3.2267e-13, 1.5852e-05, 1.6977e-18, 1.4029e-15, 7.5630e-28, 2.8392e-31,\n",
            "        1.4907e-21, 5.2866e-36, 3.0824e-29, 1.8405e-05, 3.2267e-13, 3.0352e-42,\n",
            "        3.1951e-19, 2.3566e-16, 5.1688e-10, 1.4029e-15, 1.1276e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  46\n",
            "ce_loss:  tensor([4.8517e-05, 1.3394e-01, 1.4914e-02, 2.5551e+01, 1.3394e-01, 2.0862e-03,\n",
            "        2.0856e+01, 2.9510e-03, 2.2396e+01, 1.0505e+01, 4.8517e-05, -0.0000e+00,\n",
            "        1.3394e-01, 6.6757e-06, 9.5280e-01, 1.1030e+00, 1.5439e+01, 5.1378e-05,\n",
            "        9.3933e+00, 1.1663e+01, 2.0773e+01, 1.4305e-06, 1.3394e-01, 8.9196e-01,\n",
            "        8.1850e-02, 2.3842e-07, 3.7373e-03, 1.1030e+00, 8.9296e-04],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.1362e-02, 3.6044e-13, 7.3234e-18, 2.6485e-43, 3.6044e-13, 3.2312e-16,\n",
            "        2.5778e-29, 1.0581e-04, 2.1903e-35, 1.0198e-27, 5.1362e-02, 5.3605e-03,\n",
            "        3.6044e-13, 2.1922e-05, 1.5293e-18, 1.3612e-15, 2.2862e-27, 2.7455e-31,\n",
            "        1.8307e-21, 1.3558e-35, 1.2478e-28, 2.1220e-05, 3.6044e-13, 1.2163e-42,\n",
            "        1.3610e-19, 1.7939e-16, 5.4456e-10, 1.3612e-15, 1.1738e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  47\n",
            "ce_loss:  tensor([5.8412e-06, 2.6433e+00, 5.8116e-03, 2.1754e+01, 2.6433e+00, 5.7299e-04,\n",
            "        2.0248e+01, 7.6396e-04, 2.1201e+01, 8.0831e+00, 5.8412e-06, -0.0000e+00,\n",
            "        2.6433e+00, 4.8876e-06, 6.1613e-01, 1.4081e+00, 1.8404e+01, 3.9934e-05,\n",
            "        1.0182e+01, 1.1951e+01, 2.0562e+01, 8.3446e-07, 2.6433e+00, 6.0096e-01,\n",
            "        5.1611e-02, 1.3113e-06, 1.8414e-03, 1.4081e+00, 6.4269e-04],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.2143e-02, 4.3594e-13, 4.0898e-18, 4.8065e-43, 4.3594e-13, 3.0884e-16,\n",
            "        1.3302e-29, 1.0432e-04, 9.5519e-35, 1.0215e-27, 5.2143e-02, 5.7237e-03,\n",
            "        4.3594e-13, 2.7039e-05, 1.9551e-18, 6.9022e-16, 1.4852e-27, 3.3198e-31,\n",
            "        7.9618e-22, 4.1905e-36, 3.7715e-28, 2.3846e-05, 4.3594e-13, 1.1435e-42,\n",
            "        8.1654e-20, 1.2175e-16, 1.0516e-09, 6.9022e-16, 1.1466e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  48\n",
            "ce_loss:  tensor([1.5020e-05, 4.3684e-01, 2.5664e-03, 1.8784e+01, 4.3684e-01, 3.7151e-04,\n",
            "        1.9519e+01, 1.5390e-03, 2.2067e+01, 6.4566e+00, 1.5020e-05, -0.0000e+00,\n",
            "        4.3684e-01, 2.1458e-06, 7.9996e-01, 6.3883e-02, 1.6194e+01, 2.4199e-05,\n",
            "        8.2748e+00, 9.6899e+00, 1.9982e+01, 5.9605e-07, 4.3684e-01, 5.8374e-01,\n",
            "        4.2797e-02, 1.1921e-07, 1.3628e-03, 6.3883e-02, 2.1265e-04],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3487e-02, 2.3281e-13, 3.8342e-18, 2.4369e-42, 2.3281e-13, 2.1577e-16,\n",
            "        1.6081e-29, 1.0723e-04, 6.3319e-35, 1.8879e-27, 5.3487e-02, 5.9111e-03,\n",
            "        2.3281e-13, 3.5415e-05, 8.9315e-19, 7.9152e-16, 2.2904e-27, 1.6953e-31,\n",
            "        9.1823e-22, 9.9205e-36, 6.3606e-29, 2.6739e-05, 2.3281e-13, 8.6320e-43,\n",
            "        6.9982e-20, 1.0950e-16, 2.5080e-09, 7.9152e-16, 1.1882e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  49\n",
            "ce_loss:  tensor([1.6689e-06, 7.4897e-02, 9.0963e-04, 1.7914e+01, 7.4897e-02, 3.5542e-04,\n",
            "        1.6537e+01, 4.2561e-04, 1.9473e+01, 9.2550e+00, 1.6689e-06, -0.0000e+00,\n",
            "        7.4897e-02, 1.6689e-06, 2.8926e-01, 5.1430e+00, 1.3285e+01, 1.6212e-05,\n",
            "        9.4689e+00, 1.2538e+01, 1.7569e+01, 3.5763e-07, 7.4897e-02, 4.8088e-01,\n",
            "        3.0286e-02, 1.1921e-07, 8.5675e-04, 5.1430e+00, 1.4721e-04],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3463e-02, 2.1586e-13, 2.8278e-18, 9.5765e-42, 2.1586e-13, 1.8969e-16,\n",
            "        4.7540e-29, 1.0531e-04, 8.5829e-35, 1.5018e-27, 5.3463e-02, 6.0810e-03,\n",
            "        2.1586e-13, 4.1307e-05, 1.1159e-18, 5.4751e-16, 6.7208e-27, 1.2960e-31,\n",
            "        6.5497e-22, 8.2071e-36, 1.7612e-28, 2.9026e-05, 2.1586e-13, 3.1529e-43,\n",
            "        3.0221e-20, 6.6095e-17, 8.3632e-09, 5.4751e-16, 1.1587e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  50\n",
            "ce_loss:  tensor([5.0068e-06, 1.2121e+00, 5.8133e-04, 1.6697e+01, 1.2121e+00, 2.3696e-04,\n",
            "        1.4717e+01, 7.8611e-04, 1.6875e+01, 6.7465e+00, 5.0068e-06, -0.0000e+00,\n",
            "        1.2121e+00, 8.3446e-07, 1.1806e-01, 1.9963e-01, 1.1944e+01, 1.1921e-05,\n",
            "        1.0141e+01, 9.7113e+00, 1.9536e+01, 3.5763e-07, 1.2121e+00, 3.6733e-01,\n",
            "        2.3205e-02, 1.1921e-07, 4.9817e-04, 1.9963e-01, 5.2212e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.4145e-02, 2.0393e-13, 2.5872e-18, 4.5332e-42, 2.0393e-13, 1.1261e-16,\n",
            "        1.4037e-28, 1.0771e-04, 2.9746e-34, 1.1665e-27, 5.4145e-02, 6.0945e-03,\n",
            "        2.0393e-13, 5.2034e-05, 1.4841e-18, 2.9437e-16, 1.2421e-26, 9.5905e-32,\n",
            "        3.5800e-22, 4.5090e-36, 1.1613e-28, 3.1939e-05, 2.0393e-13, 2.1019e-43,\n",
            "        1.7634e-20, 4.9201e-17, 2.6771e-08, 2.9437e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  51\n",
            "ce_loss:  tensor([9.5367e-07, 1.6974e-01, 2.8320e-04, 1.5458e+01, 1.6974e-01, 3.5649e-04,\n",
            "        1.8997e+01, 3.2551e-04, 1.7899e+01, 4.7905e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.6974e-01, 1.1802e-05, 6.8670e-02, 2.7911e-02, 1.2990e+01, 1.0133e-05,\n",
            "        1.0139e+01, 8.5150e+00, 1.8476e+01, 2.3842e-07, 1.6974e-01, 3.5476e-01,\n",
            "        1.8689e-02, 1.7881e-06, 4.0511e-04, 2.7911e-02, 8.4754e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3285e-02, 1.1285e-13, 1.7649e-18, 1.8983e-41, 1.1285e-13, 6.7884e-17,\n",
            "        1.0370e-28, 1.0554e-04, 9.3213e-34, 2.0249e-27, 5.3285e-02, 6.1387e-03,\n",
            "        1.1285e-13, 5.6063e-05, 1.8619e-18, 3.6988e-16, 2.5818e-26, 8.9733e-32,\n",
            "        2.3604e-22, 9.1454e-36, 7.6778e-29, 3.3938e-05, 1.1285e-13, 1.2472e-43,\n",
            "        9.4276e-21, 2.6802e-17, 7.2183e-08, 3.6988e-16, 1.1529e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  52\n",
            "ce_loss:  tensor([5.3644e-06, 6.0739e-02, 3.5816e-04, 1.6893e+01, 6.0739e-02, 5.7967e-04,\n",
            "        1.6219e+01, 7.8742e-04, 1.7682e+01, 4.7661e+00, 5.3644e-06, -0.0000e+00,\n",
            "        6.0739e-02, 4.7684e-07, 3.9877e-02, 3.2007e-02, 1.2870e+01, 1.3709e-05,\n",
            "        7.8504e+00, 8.9780e+00, 1.7868e+01, 2.3842e-07, 6.0739e-02, 4.3767e-01,\n",
            "        1.5577e-02, -0.0000e+00, 3.7413e-04, 3.2007e-02, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3995e-02, 1.1723e-13, 1.6425e-18, 1.8777e-41, 1.1723e-13, 1.0153e-16,\n",
            "        8.1260e-29, 1.0765e-04, 4.0634e-34, 3.1286e-27, 5.3995e-02, 6.0878e-03,\n",
            "        1.1723e-13, 7.1452e-05, 1.1181e-18, 4.3440e-16, 1.3459e-26, 5.2467e-32,\n",
            "        2.7645e-22, 4.2106e-36, 6.9618e-29, 3.6828e-05, 1.1723e-13, 1.1351e-43,\n",
            "        7.6497e-21, 3.5975e-17, 1.8666e-07, 4.3440e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  53\n",
            "ce_loss:  tensor([9.5367e-07, 7.4793e-02, 1.7069e-04, 1.7541e+01, 7.4793e-02, 5.4654e-04,\n",
            "        1.3705e+01, 3.2551e-04, 1.6576e+01, 8.4663e+00, 9.5367e-07, -0.0000e+00,\n",
            "        7.4793e-02, 1.1563e-05, 3.5602e-02, 4.3850e-03, 1.2742e+01, 9.1791e-06,\n",
            "        8.0773e+00, 8.0802e+00, 1.6412e+01, 2.3842e-07, 7.4793e-02, 2.8948e-01,\n",
            "        1.4052e-02, 4.7684e-07, 2.3769e-02, 4.3850e-03, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 6.2017e-13, 1.0557e-18, 1.5902e-41, 6.2017e-13, 8.9811e-17,\n",
            "        1.9146e-28, 1.0554e-04, 8.9980e-34, 2.5041e-27, 5.3552e-02, 6.1387e-03,\n",
            "        6.2017e-13, 7.4491e-05, 1.2078e-18, 3.5535e-16, 2.9536e-26, 5.6050e-32,\n",
            "        4.0493e-22, 4.6772e-36, 2.3093e-28, 3.8648e-05, 6.2017e-13, 4.9045e-44,\n",
            "        4.5019e-21, 1.7207e-17, 3.8497e-07, 3.5535e-16, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  54\n",
            "ce_loss:  tensor([5.3644e-06, 3.7485e-01, 2.5698e-04, 1.5385e+01, 3.7485e-01, 2.6425e-04,\n",
            "        1.5358e+01, 7.8742e-04, 1.7517e+01, 5.2954e+00, 5.3644e-06, -0.0000e+00,\n",
            "        3.7485e-01, 3.5763e-07, 3.4866e-02, 1.6247e-02, 1.2717e+01, 8.5830e-06,\n",
            "        9.0831e+00, 9.2945e+00, 1.9612e+01, 2.3842e-07, 3.7485e-01, 2.1984e-01,\n",
            "        1.3313e-02, -0.0000e+00, 4.4622e-04, 1.6247e-02, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 6.4968e-13, 1.1208e-18, 2.2501e-41, 6.4968e-13, 9.3450e-17,\n",
            "        1.7469e-28, 1.0765e-04, 3.6917e-34, 1.6228e-27, 5.3935e-02, 6.0878e-03,\n",
            "        6.4968e-13, 9.2231e-05, 5.4857e-19, 3.0814e-16, 1.2742e-26, 2.8126e-32,\n",
            "        1.8989e-22, 2.2468e-36, 1.6059e-28, 4.1346e-05, 6.4968e-13, 5.3249e-44,\n",
            "        3.0217e-21, 2.2489e-17, 8.5949e-07, 3.0814e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  55\n",
            "ce_loss:  tensor([9.5367e-07, 5.0354e-01, 1.1396e-04, 1.7242e+01, 5.0354e-01, 1.6128e-04,\n",
            "        1.4670e+01, 3.2551e-04, 1.6131e+01, 3.6132e+00, 9.5367e-07, -0.0000e+00,\n",
            "        5.0354e-01, 8.1062e-06, 2.2581e-02, 2.8277e-03, 1.2332e+01, 6.7949e-06,\n",
            "        7.1968e+00, 9.8078e+00, 1.7413e+01, 2.3842e-07, 5.0354e-01, 3.9468e-01,\n",
            "        1.0452e-02, 3.5763e-07, 3.3594e-01, 2.8277e-03, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 4.2831e-13, 7.9649e-19, 1.7708e-41, 4.2831e-13, 5.2012e-17,\n",
            "        1.1657e-28, 1.0554e-04, 1.3174e-33, 2.5441e-27, 5.3552e-02, 6.1387e-03,\n",
            "        4.2831e-13, 9.3534e-05, 7.0352e-19, 2.1895e-16, 2.5302e-26, 2.0406e-32,\n",
            "        3.2353e-22, 1.6875e-36, 1.1023e-28, 4.2840e-05, 4.2831e-13, 4.7644e-44,\n",
            "        1.6217e-21, 1.1410e-17, 1.5988e-06, 2.1895e-16, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  56\n",
            "ce_loss:  tensor([5.3644e-06, 5.1554e-02, 1.0514e-04, 1.4929e+01, 5.1554e-02, 1.7284e-04,\n",
            "        1.4052e+01, 7.8742e-04, 1.7238e+01, 4.6120e+00, 5.3644e-06, -0.0000e+00,\n",
            "        5.1554e-02, 2.3842e-07, 3.7778e-02, 8.7044e-04, 1.2434e+01, 6.0797e-06,\n",
            "        8.2190e+00, 8.8176e+00, 1.5999e+01, 2.3842e-07, 5.1554e-02, 2.0694e-01,\n",
            "        8.1808e-03, -0.0000e+00, 3.4898e-04, 8.7044e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 3.9285e-13, 9.0518e-19, 2.4497e-41, 3.9285e-13, 3.5686e-17,\n",
            "        2.5228e-28, 1.0765e-04, 5.6285e-34, 3.3791e-27, 5.3935e-02, 6.0878e-03,\n",
            "        3.9285e-13, 1.1301e-04, 3.7559e-19, 1.9204e-16, 1.2250e-26, 1.1544e-32,\n",
            "        2.3263e-22, 8.7429e-37, 3.1239e-28, 4.5485e-05, 3.9285e-13, 2.2421e-44,\n",
            "        9.5246e-22, 1.3236e-17, 2.9926e-06, 1.9204e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  57\n",
            "ce_loss:  tensor([9.5367e-07, 3.6923e+01, 7.4503e-05, 1.7005e+01, 3.6923e+01, 1.3696e-04,\n",
            "        1.5750e+01, 3.2551e-04, 1.6107e+01, 5.1721e+00, 9.5367e-07, -0.0000e+00,\n",
            "        3.6923e+01, 9.6559e-06, 4.5119e-02, 5.1247e-04, 1.2117e+01, 6.1989e-06,\n",
            "        7.3989e+00, 9.5768e+00, 1.6726e+01, 2.3842e-07, 3.6923e+01, 2.0349e-01,\n",
            "        5.5235e-03, 2.3842e-07, 8.8343e-01, 5.1247e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.9529e-13, 5.7092e-19, 1.8852e-41, 1.9529e-13, 4.0018e-17,\n",
            "        2.1586e-28, 1.0554e-04, 1.8723e-33, 1.7329e-27, 5.3552e-02, 6.1387e-03,\n",
            "        1.9529e-13, 1.1110e-04, 5.4657e-19, 2.0081e-16, 2.4413e-26, 6.4976e-33,\n",
            "        3.0729e-22, 5.5000e-37, 7.9739e-29, 4.6817e-05, 1.9529e-13, 2.3822e-44,\n",
            "        5.0654e-22, 7.9504e-18, 5.1630e-06, 2.0081e-16, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  58\n",
            "ce_loss:  tensor([5.3644e-06, 1.1083e-01, 5.1259e-05, 1.5167e+01, 1.1083e-01, 1.0597e-04,\n",
            "        1.6984e+01, 7.8742e-04, 1.6589e+01, 3.6952e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.1083e-01, 1.1921e-07, 2.6315e-02, 4.3109e-04, 1.1853e+01, 5.9604e-06,\n",
            "        9.6222e+00, 7.9456e+00, 1.5822e+01, 2.3842e-07, 1.1083e-01, 2.0364e-01,\n",
            "        5.8833e-03, -0.0000e+00, 2.7998e-04, 4.3109e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 2.2396e-13, 5.4601e-19, 1.7285e-41, 2.2396e-13, 2.4948e-17,\n",
            "        1.2732e-28, 1.0765e-04, 8.4480e-34, 1.8145e-27, 5.3935e-02, 6.0878e-03,\n",
            "        2.2396e-13, 1.3245e-04, 3.3826e-19, 1.3813e-16, 1.1074e-26, 5.1422e-33,\n",
            "        1.7583e-22, 4.2367e-37, 2.1966e-28, 4.9412e-05, 2.2396e-13, 1.1210e-44,\n",
            "        3.7523e-22, 1.0353e-17, 9.3537e-06, 1.3813e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  59\n",
            "ce_loss:  tensor([9.5367e-07, 1.0855e+00, 4.5537e-05, 1.7159e+01, 1.0855e+00, 2.1634e-04,\n",
            "        1.5198e+01, 3.2551e-04, 1.6308e+01, 4.8022e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.0855e+00, 1.3947e-05, 1.1622e-02, 5.5334e-04, 1.2324e+01, 4.8876e-06,\n",
            "        9.3524e+00, 9.6656e+00, 1.7499e+01, 2.3842e-07, 1.0855e+00, 1.7517e-01,\n",
            "        4.7569e-03, -0.0000e+00, 5.0494e-01, 5.5334e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 2.6290e-13, 4.6397e-19, 1.2373e-41, 2.6290e-13, 1.2906e-17,\n",
            "        1.4228e-28, 1.0554e-04, 2.4691e-33, 1.1351e-27, 5.3552e-02, 6.1387e-03,\n",
            "        2.6290e-13, 1.2675e-04, 3.1894e-19, 1.1977e-16, 2.0662e-26, 4.1403e-33,\n",
            "        1.1645e-22, 3.5937e-37, 1.3037e-28, 5.0522e-05, 2.6290e-13, 9.8091e-45,\n",
            "        2.0958e-22, 5.1511e-18, 1.5047e-05, 1.1977e-16, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  60\n",
            "ce_loss:  tensor([5.3644e-06, 1.2950e-01, 3.7193e-05, 1.4372e+01, 1.2950e-01, 1.1062e-04,\n",
            "        1.3153e+01, 7.8742e-04, 1.7487e+01, 3.3862e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.2950e-01, 1.1921e-07, 2.6075e-02, 6.2232e-04, 1.2706e+01, 6.1989e-06,\n",
            "        9.4422e+00, 8.2915e+00, 1.8733e+01, 2.3842e-07, 1.2950e-01, 1.8780e-01,\n",
            "        4.1310e-03, -0.0000e+00, 2.5508e-04, 6.2232e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.4824e-13, 3.5436e-19, 1.7540e-41, 1.4824e-13, 2.2111e-17,\n",
            "        3.2374e-28, 1.0765e-04, 2.2743e-33, 1.6221e-27, 5.3935e-02, 6.0878e-03,\n",
            "        1.4824e-13, 1.4908e-04, 1.7194e-19, 1.2740e-16, 1.0479e-26, 3.9625e-33,\n",
            "        8.1562e-23, 1.7661e-37, 9.4651e-29, 5.2947e-05, 1.4824e-13, 5.6052e-45,\n",
            "        1.3880e-22, 4.6756e-18, 2.6659e-05, 1.2740e-16, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  61\n",
            "ce_loss:  tensor([9.5367e-07, 6.3472e-02, 7.9033e-05, 1.3947e+01, 6.3472e-02, 1.0240e-04,\n",
            "        1.4646e+01, 3.2551e-04, 1.9458e+01, 5.1992e+00, 9.5367e-07, -0.0000e+00,\n",
            "        6.3472e-02, 3.6955e-06, 1.6113e-02, 2.6485e-04, 1.6238e+01, 5.1260e-06,\n",
            "        7.4134e+00, 7.0363e+00, 1.6995e+01, 2.3842e-07, 6.3472e-02, 1.6373e-01,\n",
            "        3.3660e-03, -0.0000e+00, 5.1888e-01, 2.6485e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.6733e-13, 2.9726e-19, 5.3960e-41, 1.6733e-13, 1.2416e-17,\n",
            "        1.7857e-28, 1.0554e-04, 1.8668e-33, 1.3398e-27, 5.3552e-02, 6.1387e-03,\n",
            "        1.6733e-13, 1.4189e-04, 2.1359e-19, 9.0245e-17, 5.9665e-27, 2.8347e-33,\n",
            "        9.2325e-23, 1.8372e-37, 5.5171e-29, 5.3778e-05, 1.6733e-13, 4.2039e-45,\n",
            "        6.8497e-23, 2.1527e-18, 3.9470e-05, 9.0245e-17, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  62\n",
            "ce_loss:  tensor([5.3644e-06, 5.6743e-01, 3.7550e-05, 1.4109e+01, 5.6743e-01, 7.8317e-05,\n",
            "        1.3063e+01, 7.8742e-04, 1.6610e+01, 5.9270e+00, 5.3644e-06, -0.0000e+00,\n",
            "        5.6743e-01, 1.1921e-07, 1.1870e-02, 8.1470e-04, 1.2649e+01, 5.1260e-06,\n",
            "        8.4673e+00, 7.7370e+00, 1.8596e+01, 2.3842e-07, 5.6743e-01, 1.8053e-01,\n",
            "        3.5638e-03, -0.0000e+00, 1.2790e-04, 8.1470e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.1645e-13, 2.9084e-19, 2.3470e-41, 1.1645e-13, 6.8520e-18,\n",
            "        3.9765e-28, 1.0765e-04, 1.2689e-33, 9.0269e-28, 5.3935e-02, 6.0878e-03,\n",
            "        1.1645e-13, 1.6254e-04, 1.2904e-19, 6.4343e-17, 3.5798e-27, 3.4067e-33,\n",
            "        6.9697e-23, 1.0286e-37, 4.1796e-29, 5.6022e-05, 1.1645e-13, 2.8026e-45,\n",
            "        5.7670e-23, 3.2749e-18, 6.6184e-05, 6.4343e-17, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  63\n",
            "ce_loss:  tensor([9.5367e-07, 4.7383e-02, 1.0931e-04, 1.3700e+01, 4.7383e-02, 7.5337e-05,\n",
            "        1.4982e+01, 3.2551e-04, 1.5427e+01, 4.2700e+00, 9.5367e-07, -0.0000e+00,\n",
            "        4.7383e-02, 4.6492e-06, 5.2628e-03, 3.8938e-04, 1.1351e+01, 3.9339e-06,\n",
            "        9.0458e+00, 7.3302e+00, 1.6264e+01, 2.3842e-07, 4.7383e-02, 2.2317e-01,\n",
            "        1.9979e-03, 1.1921e-07, 3.7520e-01, 3.8938e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.3595e-13, 2.0703e-19, 6.6961e-41, 1.3595e-13, 4.4104e-18,\n",
            "        2.7736e-28, 1.0554e-04, 3.7178e-33, 7.2203e-28, 5.3552e-02, 6.1387e-03,\n",
            "        1.3595e-13, 1.5200e-04, 1.5243e-19, 3.8981e-17, 8.1980e-27, 2.0932e-33,\n",
            "        7.5093e-23, 1.9563e-37, 3.0414e-29, 5.6525e-05, 1.3595e-13, 2.8026e-45,\n",
            "        2.4133e-23, 1.6135e-18, 8.9890e-05, 3.8981e-17, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  64\n",
            "ce_loss:  tensor([5.3644e-06, 1.1639e+01, 3.5047e-05, 1.3791e+01, 1.1639e+01, 5.4940e-04,\n",
            "        1.6414e+01, 7.8742e-04, 1.5883e+01, 6.1409e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.1639e+01, 1.1921e-07, 2.1248e-02, 1.1646e-04, 1.1424e+01, 3.9339e-06,\n",
            "        8.3320e+00, 8.0336e+00, 1.4657e+01, 1.1921e-07, 1.1639e+01, 1.5883e-01,\n",
            "        1.8632e-03, -0.0000e+00, 8.7853e-05, 1.1646e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.2414e-13, 2.0127e-19, 2.3889e-41, 1.2414e-13, 5.2080e-18,\n",
            "        2.3934e-28, 1.0765e-04, 1.6833e-33, 5.3597e-28, 5.3935e-02, 6.0878e-03,\n",
            "        1.2414e-13, 1.7298e-04, 6.5434e-20, 2.5793e-17, 7.2841e-27, 1.4488e-33,\n",
            "        6.0869e-23, 8.4246e-38, 6.8791e-29, 5.8500e-05, 1.2414e-13, 1.4013e-45,\n",
            "        1.7345e-23, 2.7156e-18, 1.3779e-04, 2.5793e-17, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  65\n",
            "ce_loss:  tensor([9.5367e-07, 1.0063e-01, 4.2795e-05, 1.3568e+01, 1.0063e-01, 9.3098e-05,\n",
            "        1.5122e+01, 3.2551e-04, 1.5553e+01, 4.7941e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.0063e-01, 3.8147e-06, 3.9831e-03, 9.1310e-05, 1.1027e+01, 3.4571e-06,\n",
            "        7.3548e+00, 9.0522e+00, 1.6665e+01, 1.1921e-07, 1.0063e-01, 1.9146e-01,\n",
            "        1.9670e-03, -0.0000e+00, 5.9769e-01, 9.1310e-05, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 9.3160e-14, 1.4768e-19, 6.3419e-41, 9.3160e-14, 6.0945e-18,\n",
            "        1.0898e-28, 1.0554e-04, 4.7465e-33, 3.7222e-28, 5.3552e-02, 6.1387e-03,\n",
            "        9.3160e-14, 1.6079e-04, 6.8123e-20, 1.8502e-17, 1.2934e-26, 1.0955e-33,\n",
            "        6.7995e-23, 7.9334e-38, 5.4598e-29, 5.8895e-05, 9.3160e-14, 1.4013e-45,\n",
            "        9.5605e-24, 1.2741e-18, 1.7554e-04, 1.8502e-17, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  66\n",
            "ce_loss:  tensor([5.3644e-06, 3.2503e+00, 3.1590e-05, 1.3965e+01, 3.2503e+00, 6.2106e-05,\n",
            "        1.2803e+01, 7.8742e-04, 1.6521e+01, 2.8180e+00, 5.3644e-06, -0.0000e+00,\n",
            "        3.2503e+00, 1.1921e-07, 5.6364e-03, 9.1429e-05, 1.1279e+01, 3.9339e-06,\n",
            "        7.4278e+00, 7.8834e+00, 1.7929e+01, 1.1921e-07, 3.2503e+00, 1.3682e-01,\n",
            "        1.2629e-03, -0.0000e+00, 4.0054e-05, 9.1429e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 8.8935e-14, 1.5535e-19, 2.4133e-41, 8.8935e-14, 3.7249e-18,\n",
            "        2.4890e-28, 1.0765e-04, 2.4570e-33, 5.3730e-28, 5.3935e-02, 6.0878e-03,\n",
            "        8.8935e-14, 1.8291e-04, 5.2340e-20, 1.4290e-17, 6.1089e-27, 7.4292e-34,\n",
            "        1.0927e-22, 4.2705e-38, 3.9193e-29, 6.0320e-05, 8.8935e-14, 0.0000e+00,\n",
            "        5.8360e-24, 1.4942e-18, 2.5278e-04, 1.4290e-17, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  67\n",
            "ce_loss:  tensor([9.5367e-07, 3.7764e-02, 1.0514e-04, 1.3134e+01, 3.7764e-02, 5.9960e-05,\n",
            "        1.3999e+01, 3.2551e-04, 1.9636e+01, 3.7130e+00, 9.5367e-07, -0.0000e+00,\n",
            "        3.7764e-02, 4.4107e-06, 3.3208e-03, 3.5405e-05, 1.0902e+01, 3.0994e-06,\n",
            "        9.8563e+00, 8.7286e+00, 1.6794e+01, 1.1921e-07, 3.7764e-02, 1.6470e-01,\n",
            "        1.3679e-03, -0.0000e+00, 1.1097e-01, 3.5405e-05, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 6.0269e-14, 1.2412e-19, 6.5295e-41, 6.0269e-14, 3.1233e-18,\n",
            "        2.6200e-28, 1.0554e-04, 1.9573e-33, 8.1116e-28, 5.3552e-02, 6.1387e-03,\n",
            "        6.0269e-14, 1.6890e-04, 4.2684e-20, 9.0092e-18, 1.2171e-26, 5.2646e-34,\n",
            "        7.9922e-23, 3.5724e-38, 2.2709e-29, 6.0862e-05, 6.0269e-14, 0.0000e+00,\n",
            "        2.6017e-24, 7.8408e-19, 3.0288e-04, 9.0092e-18, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  68\n",
            "ce_loss:  tensor([5.3644e-06, 3.6230e+01, 2.9563e-05, 1.3415e+01, 3.6230e+01, 7.6053e-05,\n",
            "        1.6009e+01, 7.8742e-04, 1.6014e+01, 3.9892e+00, 5.3644e-06, -0.0000e+00,\n",
            "        3.6230e+01, 1.1921e-07, 3.0251e-03, 8.2244e-04, 1.2243e+01, 3.9339e-06,\n",
            "        7.6887e+00, 6.9208e+00, 1.8094e+01, 1.1921e-07, 3.6230e+01, 1.2910e-01,\n",
            "        9.3226e-04, -0.0000e+00, 2.3961e-05, 8.2244e-04, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 3.6305e-14, 1.2289e-19, 2.3343e-41, 3.6305e-14, 1.7654e-18,\n",
            "        1.4473e-28, 1.0765e-04, 1.4574e-33, 4.4827e-28, 5.3935e-02, 6.0878e-03,\n",
            "        3.6305e-14, 1.9121e-04, 3.6478e-20, 8.2194e-18, 8.7597e-27, 4.5078e-34,\n",
            "        7.1327e-23, 2.5959e-38, 1.7337e-29, 6.1345e-05, 3.6305e-14, 0.0000e+00,\n",
            "        1.7001e-24, 9.5482e-19, 4.1874e-04, 8.2194e-18, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  69\n",
            "ce_loss:  tensor([9.5367e-07, 8.6856e-02, 4.4941e-05, 1.2697e+01, 8.6856e-02, 8.0940e-05,\n",
            "        1.4707e+01, 3.2551e-04, 1.5174e+01, 3.4931e+00, 9.5367e-07, -0.0000e+00,\n",
            "        8.6856e-02, 4.4107e-06, 3.5738e-03, 1.1515e-04, 1.4432e+01, 4.7684e-06,\n",
            "        7.3545e+00, 8.4274e+00, 1.6651e+01, 1.1921e-07, 8.6856e-02, 1.4700e-01,\n",
            "        1.0367e-03, -0.0000e+00, 8.2971e-04, 1.1515e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 3.5904e-14, 8.8349e-20, 5.5535e-41, 3.5904e-14, 1.5017e-18,\n",
            "        1.4047e-28, 1.0554e-04, 4.1698e-33, 6.2519e-28, 5.3552e-02, 6.1387e-03,\n",
            "        3.5904e-14, 1.7589e-04, 2.8219e-20, 5.7412e-18, 4.9354e-27, 3.5268e-34,\n",
            "        8.5874e-23, 1.8341e-38, 7.6802e-30, 6.1657e-05, 3.5904e-14, 0.0000e+00,\n",
            "        9.4391e-25, 8.0712e-19, 4.8722e-04, 5.7412e-18, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  70\n",
            "ce_loss:  tensor([5.3644e-06, 8.1387e-01, 2.1934e-05, 1.4217e+01, 8.1387e-01, 5.6861e-05,\n",
            "        1.2417e+01, 7.8742e-04, 1.5349e+01, 4.0409e+00, 5.3644e-06, -0.0000e+00,\n",
            "        8.1387e-01, 1.1921e-07, 7.0033e-03, 5.1974e-05, 1.1922e+01, 3.9339e-06,\n",
            "        9.1735e+00, 6.7555e+00, 1.4431e+01, 1.1921e-07, 8.1387e-01, 1.1514e-01,\n",
            "        6.8486e-04, -0.0000e+00, 1.9073e-05, 5.1974e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 3.6564e-14, 9.1581e-20, 3.7692e-41, 3.6564e-14, 1.3896e-18,\n",
            "        2.1747e-28, 1.0765e-04, 2.1489e-33, 4.2627e-28, 5.3935e-02, 6.0878e-03,\n",
            "        3.6564e-14, 1.9837e-04, 2.5121e-20, 5.1387e-18, 4.2414e-27, 4.2559e-34,\n",
            "        5.4879e-23, 1.3451e-38, 2.0721e-29, 6.1979e-05, 3.6564e-14, 0.0000e+00,\n",
            "        7.9849e-25, 5.6663e-19, 6.3434e-04, 5.1387e-18, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  71\n",
            "ce_loss:  tensor([9.5367e-07, 2.8086e-02, 2.5510e-05, 1.5082e+01, 2.8086e-02, 6.1987e-05,\n",
            "        1.4114e+01, 3.2551e-04, 1.5056e+01, 6.4101e+00, 9.5367e-07, -0.0000e+00,\n",
            "        2.8086e-02, 4.2915e-06, 1.7644e-03, 2.6092e-04, 1.0723e+01, 3.2186e-06,\n",
            "        8.4984e+00, 6.3412e+00, 1.7901e+01, 1.1921e-07, 2.8086e-02, 9.3886e-02,\n",
            "        7.2953e-04, -0.0000e+00, 7.3311e-05, 2.6092e-04, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 2.1929e-14, 6.3495e-20, 2.3113e-41, 2.1929e-14, 5.6731e-19,\n",
            "        3.8852e-28, 1.0554e-04, 4.9896e-33, 3.4230e-28, 5.3552e-02, 6.1387e-03,\n",
            "        2.1929e-14, 1.8158e-04, 1.9348e-20, 3.6743e-18, 8.5774e-27, 2.6866e-34,\n",
            "        4.0190e-23, 2.5539e-38, 1.0736e-29, 6.2156e-05, 2.1929e-14, 0.0000e+00,\n",
            "        4.0644e-25, 6.1636e-19, 7.2992e-04, 3.6743e-18, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  72\n",
            "ce_loss:  tensor([5.3644e-06, 3.5591e+00, 2.0504e-05, 1.3138e+01, 3.5591e+00, 4.2557e-05,\n",
            "        1.3872e+01, 7.8742e-04, 1.5607e+01, 3.3667e+00, 5.3644e-06, -0.0000e+00,\n",
            "        3.5591e+00, 1.1921e-07, 1.7455e-03, 3.8981e-05, 1.1318e+01, 3.4571e-06,\n",
            "        8.0341e+00, 7.3334e+00, 1.5193e+01, 1.1921e-07, 3.5591e+00, 1.5796e-01,\n",
            "        4.9007e-04, -0.0000e+00, 1.2994e-05, 3.8981e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 2.0188e-14, 5.7094e-20, 2.2810e-41, 2.0188e-14, 6.7104e-19,\n",
            "        1.7619e-28, 1.0765e-04, 2.0508e-33, 2.3990e-28, 5.3935e-02, 6.0878e-03,\n",
            "        2.0188e-14, 2.0389e-04, 1.4035e-20, 4.0952e-18, 4.6024e-27, 2.3073e-34,\n",
            "        3.9280e-23, 9.0870e-39, 8.3919e-30, 6.2348e-05, 2.0188e-14, 0.0000e+00,\n",
            "        3.1474e-25, 4.6868e-19, 9.0839e-04, 4.0952e-18, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  73\n",
            "ce_loss:  tensor([9.5367e-07, 1.9104e-02, 4.2676e-05, 1.5477e+01, 1.9104e-02, 3.7312e-05,\n",
            "        1.4050e+01, 3.2551e-04, 1.5053e+01, 2.8120e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.9104e-02, 2.6226e-06, 3.2597e-03, 1.5603e-03, 1.0484e+01, 2.6226e-06,\n",
            "        6.3145e+00, 6.3619e+00, 1.4149e+01, 1.1921e-07, 1.9104e-02, 9.3440e-02,\n",
            "        6.0087e-04, -0.0000e+00, 5.6265e-05, 1.5603e-03, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.6559e-14, 4.9633e-20, 1.6405e-41, 1.6559e-14, 4.4964e-19,\n",
            "        3.1009e-28, 1.0554e-04, 4.9741e-33, 3.6890e-28, 5.3552e-02, 6.1387e-03,\n",
            "        1.6559e-14, 1.8686e-04, 6.9002e-21, 2.5483e-18, 8.1218e-27, 1.2607e-34,\n",
            "        5.5272e-23, 1.6513e-38, 1.6160e-29, 6.2387e-05, 1.6559e-14, 0.0000e+00,\n",
            "        2.4713e-25, 3.7738e-19, 1.0115e-03, 2.5483e-18, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  74\n",
            "ce_loss:  tensor([5.3644e-06, 6.4994e+00, 1.5497e-05, 1.3533e+01, 6.4994e+00, 2.5749e-05,\n",
            "        1.3586e+01, 7.8742e-04, 1.5710e+01, 4.1855e+00, 5.3644e-06, -0.0000e+00,\n",
            "        6.4994e+00, 1.1921e-07, 1.7571e-03, 3.2901e-05, 1.1232e+01, 3.2186e-06,\n",
            "        6.8870e+00, 7.6255e+00, 1.6670e+01, 1.1921e-07, 6.4994e+00, 8.5987e-02,\n",
            "        3.5089e-04, -0.0000e+00, 1.0610e-05, 3.2901e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.6004e-14, 4.6595e-20, 9.0748e-42, 1.6004e-14, 3.3999e-19,\n",
            "        1.6954e-28, 1.0765e-04, 2.0099e-33, 2.6366e-28, 5.3935e-02, 6.0878e-03,\n",
            "        1.6004e-14, 2.0752e-04, 9.9963e-21, 1.5034e-18, 3.7972e-27, 1.2352e-34,\n",
            "        8.4402e-23, 6.7074e-39, 1.0334e-29, 6.2445e-05, 1.6004e-14, 0.0000e+00,\n",
            "        1.1479e-25, 2.8782e-19, 1.2187e-03, 1.5034e-18, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  75\n",
            "ce_loss:  tensor([9.5367e-07, 1.2405e-02, 1.5020e-05, 1.5331e+01, 1.2405e-02, 2.9802e-05,\n",
            "        1.4096e+01, 3.2551e-04, 1.4970e+01, 5.2091e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.2405e-02, 1.6689e-06, 2.5556e-03, 1.8358e-05, 1.0480e+01, 2.6226e-06,\n",
            "        7.9924e+00, 7.5466e+00, 1.6674e+01, 1.1921e-07, 1.2405e-02, 9.3538e-02,\n",
            "        2.9905e-04, -0.0000e+00, 4.9351e-05, 1.8358e-05, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.0555e-14, 3.3121e-20, 5.7902e-42, 1.0555e-14, 2.5539e-19,\n",
            "        3.0595e-28, 1.0554e-04, 4.3489e-33, 1.9480e-28, 5.3552e-02, 6.1387e-03,\n",
            "        1.0555e-14, 1.9061e-04, 6.4439e-21, 8.7238e-19, 7.3419e-27, 7.9760e-35,\n",
            "        4.6861e-23, 4.7235e-39, 6.3421e-30, 6.2387e-05, 1.0555e-14, 0.0000e+00,\n",
            "        7.1214e-26, 2.4707e-19, 1.3182e-03, 8.7238e-19, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  76\n",
            "ce_loss:  tensor([5.3644e-06, 4.4467e-03, 1.0967e-05, 1.2593e+01, 4.4467e-03, 2.0504e-05,\n",
            "        1.3643e+01, 7.8742e-04, 1.6051e+01, 3.8934e+00, 5.3644e-06, -0.0000e+00,\n",
            "        4.4467e-03, 1.1921e-07, 8.3817e-04, 1.6689e-05, 1.1666e+01, 3.0994e-06,\n",
            "        9.1573e+00, 7.2156e+00, 1.5017e+01, 1.1921e-07, 4.4467e-03, 7.9438e-02,\n",
            "        2.5305e-04, -0.0000e+00, 8.4638e-06, 1.6689e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.1322e-14, 2.9207e-20, 6.1531e-42, 1.1322e-14, 2.4891e-19,\n",
            "        1.3683e-28, 1.0765e-04, 2.9729e-33, 1.6353e-28, 5.3935e-02, 6.0878e-03,\n",
            "        1.1322e-14, 2.1030e-04, 9.1629e-21, 6.8541e-19, 6.2527e-27, 1.0808e-34,\n",
            "        4.4324e-23, 2.6866e-39, 4.4006e-30, 6.2445e-05, 1.1322e-14, 0.0000e+00,\n",
            "        3.6795e-26, 2.7196e-19, 1.5517e-03, 6.8541e-19, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  77\n",
            "ce_loss:  tensor([9.5367e-07, 5.7821e-02, 7.4265e-05, 1.5250e+01, 5.7821e-02, 5.5312e-05,\n",
            "        1.4006e+01, 3.2551e-04, 1.9314e+01, 5.3801e+00, 9.5367e-07, -0.0000e+00,\n",
            "        5.7821e-02, 1.7881e-06, 2.5852e-03, 3.1590e-05, 1.4237e+01, 3.5763e-06,\n",
            "        7.3039e+00, 7.9665e+00, 1.3892e+01, 1.1921e-07, 5.7821e-02, 8.6903e-02,\n",
            "        2.8153e-04, -0.0000e+00, 4.4702e-05, 3.1590e-05, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 9.5674e-15, 2.3322e-20, 3.6364e-42, 9.5674e-15, 1.6608e-19,\n",
            "        2.3057e-28, 1.0554e-04, 2.2476e-33, 1.0470e-28, 5.3552e-02, 6.1387e-03,\n",
            "        9.5674e-15, 1.9253e-04, 4.1455e-21, 4.1363e-19, 3.2839e-27, 7.6122e-35,\n",
            "        2.5610e-23, 2.2213e-39, 1.0384e-29, 6.2387e-05, 9.5674e-15, 0.0000e+00,\n",
            "        2.1330e-26, 1.4936e-19, 1.6399e-03, 4.1363e-19, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  78\n",
            "ce_loss:  tensor([5.3644e-06, 1.8370e-03, 1.3590e-05, 1.2417e+01, 1.8370e-03, 1.6570e-05,\n",
            "        1.4133e+01, 7.8742e-04, 1.5571e+01, 3.4129e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.8370e-03, 1.1921e-07, 6.5353e-04, 1.1444e-05, 1.1601e+01, 3.0994e-06,\n",
            "        6.5266e+00, 6.3244e+00, 1.5458e+01, 1.1921e-07, 1.8370e-03, 7.0812e-02,\n",
            "        2.2897e-04, -0.0000e+00, 6.7949e-06, 1.1444e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.3799e-14, 2.4485e-20, 2.5237e-42, 1.3799e-14, 2.0112e-19,\n",
            "        1.5770e-28, 1.0765e-04, 1.2480e-33, 7.2620e-29, 5.3935e-02, 6.0878e-03,\n",
            "        1.3799e-14, 2.1196e-04, 6.2702e-21, 4.7918e-19, 3.0746e-27, 6.3308e-35,\n",
            "        4.1211e-23, 1.5804e-39, 3.1183e-30, 6.2445e-05, 1.3799e-14, 0.0000e+00,\n",
            "        1.2419e-26, 1.4967e-19, 1.8909e-03, 4.7918e-19, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  79\n",
            "ce_loss:  tensor([9.5367e-07, 6.3823e+00, 1.8835e-05, 1.2286e+01, 6.3823e+00, 1.9908e-05,\n",
            "        1.6520e+01, 3.2551e-04, 1.4702e+01, 2.2485e+00, 9.5367e-07, -0.0000e+00,\n",
            "        6.3823e+00, 1.6689e-06, 1.1030e-03, 8.7022e-06, 1.0157e+01, 2.3842e-06,\n",
            "        8.2546e+00, 7.8352e+00, 1.3815e+01, 1.1921e-07, 6.3823e+00, 8.9538e-02,\n",
            "        2.5019e-04, 2.3842e-07, 2.6941e-05, 8.7022e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.2575e-14, 1.7663e-20, 6.0816e-42, 1.2575e-14, 1.3827e-19,\n",
            "        9.1885e-29, 1.0554e-04, 2.9085e-33, 1.0414e-28, 5.3552e-02, 6.1387e-03,\n",
            "        1.2575e-14, 1.9374e-04, 4.9307e-21, 2.8989e-19, 5.5687e-27, 3.0196e-35,\n",
            "        2.5128e-23, 1.2083e-39, 8.1051e-30, 6.2387e-05, 1.2575e-14, 0.0000e+00,\n",
            "        1.0618e-26, 1.0720e-19, 1.9887e-03, 2.8989e-19, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  80\n",
            "ce_loss:  tensor([5.3644e-06, 1.0889e-03, 1.1444e-05, 1.2573e+01, 1.0889e-03, 1.3828e-05,\n",
            "        1.3735e+01, 7.8742e-04, 1.5645e+01, 4.0973e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.0889e-03, 1.1921e-07, 1.4935e-03, 1.5020e-05, 1.1465e+01, 2.3842e-06,\n",
            "        8.5531e+00, 6.8366e+00, 1.5048e+01, 1.1921e-07, 1.0889e-03, 6.5681e-02,\n",
            "        1.6974e-04, -0.0000e+00, 5.8412e-06, 1.5020e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 9.3976e-15, 1.4538e-20, 2.1636e-42, 9.3976e-15, 9.9349e-20,\n",
            "        8.2302e-29, 1.0765e-04, 3.2612e-33, 1.0820e-28, 5.3935e-02, 6.0878e-03,\n",
            "        9.3976e-15, 2.1271e-04, 2.4404e-21, 2.4934e-19, 3.3971e-27, 2.7867e-35,\n",
            "        1.7419e-23, 8.4926e-40, 3.3705e-30, 6.2445e-05, 9.3976e-15, 0.0000e+00,\n",
            "        6.4073e-27, 1.7341e-19, 2.2300e-03, 2.4934e-19, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  81\n",
            "ce_loss:  tensor([9.5367e-07, 2.0928e+00, 7.8678e-06, 1.2190e+01, 2.0928e+00, 3.5643e-05,\n",
            "        1.2633e+01, 3.2551e-04, 1.8322e+01, 3.7643e+00, 9.5367e-07, -0.0000e+00,\n",
            "        2.0928e+00, 1.7881e-06, 6.0957e-04, 1.6332e-05, 1.4262e+01, 2.3842e-06,\n",
            "        8.6121e+00, 5.4691e+00, 1.7880e+01, 1.1921e-07, 2.0928e+00, 1.0901e-01,\n",
            "        2.3148e-04, -0.0000e+00, 2.5272e-05, 1.6332e-05, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 6.7648e-15, 9.5352e-21, 5.0755e-42, 6.7648e-15, 6.9179e-20,\n",
            "        1.0730e-28, 1.0554e-04, 1.8712e-33, 6.4405e-29, 5.3552e-02, 6.1387e-03,\n",
            "        6.7648e-15, 1.9400e-04, 2.8769e-21, 1.5174e-19, 2.0770e-27, 2.2883e-35,\n",
            "        1.2636e-23, 1.0844e-39, 2.2564e-30, 6.2387e-05, 6.7648e-15, 0.0000e+00,\n",
            "        5.1428e-27, 9.3036e-20, 2.2942e-03, 1.5174e-19, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  82\n",
            "ce_loss:  tensor([5.3644e-06, 5.3475e-04, 1.0490e-05, 1.2669e+01, 5.3475e-04, 1.1086e-05,\n",
            "        1.3138e+01, 7.8742e-04, 1.6046e+01, 3.3031e+00, 5.3644e-06, -0.0000e+00,\n",
            "        5.3475e-04, 1.1921e-07, 2.1689e-03, 1.0490e-05, 1.1322e+01, 2.6226e-06,\n",
            "        6.6583e+00, 6.8347e+00, 1.5369e+01, 1.1921e-07, 5.3475e-04, 6.8252e-02,\n",
            "        1.6688e-04, -0.0000e+00, 4.7684e-06, 1.0490e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 5.2419e-15, 7.8523e-21, 1.8988e-42, 5.2419e-15, 5.3771e-20,\n",
            "        1.1561e-28, 1.0765e-04, 1.2484e-33, 6.8435e-29, 5.3935e-02, 6.0878e-03,\n",
            "        5.2419e-15, 2.1271e-04, 1.5104e-21, 1.0172e-19, 1.1574e-27, 2.5284e-35,\n",
            "        1.4557e-23, 7.6628e-40, 8.5807e-31, 6.2445e-05, 5.2419e-15, 0.0000e+00,\n",
            "        3.3898e-27, 7.8816e-20, 2.5435e-03, 1.0172e-19, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  83\n",
            "ce_loss:  tensor([9.5367e-07, 1.7410e-01, 7.0333e-06, 1.2258e+01, 1.7410e-01, 1.8477e-05,\n",
            "        1.5682e+01, 3.2551e-04, 1.8268e+01, 3.4275e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.7410e-01, 1.7881e-06, 5.5822e-04, 9.2983e-06, 1.0080e+01, 2.1458e-06,\n",
            "        7.4965e+00, 7.0863e+00, 1.3407e+01, 1.1921e-07, 1.7410e-01, 8.8089e-02,\n",
            "        1.1741e-04, -0.0000e+00, 2.3246e-05, 9.2983e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 3.2042e-15, 4.5895e-21, 5.2381e-42, 3.2042e-15, 4.1230e-20,\n",
            "        5.9828e-29, 1.0554e-04, 9.3013e-34, 3.4315e-29, 5.3552e-02, 6.1387e-03,\n",
            "        3.2042e-15, 1.9400e-04, 1.9317e-21, 8.6537e-20, 2.6490e-27, 1.1229e-35,\n",
            "        1.2200e-23, 4.9125e-40, 2.3978e-30, 6.2387e-05, 3.2042e-15, 0.0000e+00,\n",
            "        1.7700e-27, 4.3095e-20, 2.5782e-03, 8.6537e-20, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  84\n",
            "ce_loss:  tensor([5.3644e-06, 2.7474e-04, 8.5830e-06, 1.2889e+01, 2.7474e-04, 9.7751e-06,\n",
            "        1.3561e+01, 7.8742e-04, 1.5930e+01, 3.1959e+00, 5.3644e-06, -0.0000e+00,\n",
            "        2.7474e-04, 1.1921e-07, 1.9001e-03, 5.1260e-06, 1.1415e+01, 2.3842e-06,\n",
            "        5.7497e+00, 6.2826e+00, 1.4882e+01, 1.1921e-07, 2.7474e-04, 6.8215e-02,\n",
            "        1.1241e-04, -0.0000e+00, 4.1723e-06, 5.1260e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 2.7443e-15, 3.3328e-21, 1.6409e-42, 2.7443e-15, 4.9922e-20,\n",
            "        5.1023e-29, 1.0765e-04, 4.5866e-34, 3.7821e-29, 5.3935e-02, 6.0878e-03,\n",
            "        2.7443e-15, 2.1271e-04, 8.5017e-22, 5.3653e-20, 1.9981e-27, 1.1150e-35,\n",
            "        1.6099e-23, 4.3358e-40, 9.3978e-31, 6.2445e-05, 2.7443e-15, 0.0000e+00,\n",
            "        1.2866e-27, 6.8367e-20, 2.8197e-03, 5.3653e-20, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  85\n",
            "ce_loss:  tensor([9.5367e-07, 5.9026e-02, 6.3181e-06, 1.2174e+01, 5.9026e-02, 1.8358e-05,\n",
            "        1.2069e+01, 3.2551e-04, 1.4130e+01, 3.9965e+00, 9.5367e-07, -0.0000e+00,\n",
            "        5.9026e-02, 1.7881e-06, 5.5488e-04, 8.3446e-06, 1.2834e+01, 2.7418e-06,\n",
            "        6.6673e+00, 7.3477e+00, 1.7390e+01, 1.1921e-07, 5.9026e-02, 5.5724e-02,\n",
            "        1.2981e-04, -0.0000e+00, 2.0981e-05, 8.3446e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.6031e-15, 2.7949e-21, 3.7471e-42, 1.6031e-15, 3.1099e-20,\n",
            "        8.6966e-29, 1.0554e-04, 1.0954e-33, 2.0255e-29, 5.3552e-02, 6.1387e-03,\n",
            "        1.6031e-15, 1.9400e-04, 1.2864e-21, 4.4110e-20, 1.4339e-27, 1.0806e-35,\n",
            "        2.9803e-23, 3.4888e-40, 6.4507e-31, 6.2387e-05, 1.6031e-15, 0.0000e+00,\n",
            "        9.2995e-28, 3.8965e-20, 2.8252e-03, 4.4110e-20, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  86\n",
            "ce_loss:  tensor([5.3644e-06, 2.0001e-04, 3.3259e-05, 1.2565e+01, 2.0001e-04, 7.0333e-06,\n",
            "        1.3636e+01, 7.8742e-04, 1.5130e+01, 6.0645e+00, 5.3644e-06, -0.0000e+00,\n",
            "        2.0001e-04, 1.1921e-07, 9.0880e-04, 1.9431e-05, 1.1345e+01, 2.0266e-06,\n",
            "        7.3090e+00, 6.7435e+00, 1.5472e+01, 1.1921e-07, 2.0001e-04, 9.5303e-02,\n",
            "        9.2979e-05, -0.0000e+00, 3.6955e-06, 1.9431e-05, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.7336e-15, 2.2365e-21, 1.4700e-42, 1.7336e-15, 3.6023e-20,\n",
            "        4.5626e-29, 1.0765e-04, 1.2929e-33, 1.3747e-29, 5.3935e-02, 6.0878e-03,\n",
            "        1.7336e-15, 2.1271e-04, 4.9500e-22, 3.6948e-20, 1.4559e-27, 1.0672e-35,\n",
            "        1.4390e-23, 2.2674e-40, 3.4762e-31, 6.2445e-05, 1.7336e-15, 0.0000e+00,\n",
            "        6.6080e-28, 3.9351e-20, 3.0428e-03, 3.6948e-20, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  87\n",
            "ce_loss:  tensor([9.5367e-07, 4.8951e-03, 5.6028e-06, 1.2292e+01, 4.8951e-03, 1.0610e-05,\n",
            "        1.2138e+01, 3.2551e-04, 1.7255e+01, 2.8779e+00, 9.5367e-07, -0.0000e+00,\n",
            "        4.8951e-03, 1.7881e-06, 4.3538e-04, 4.8876e-06, 1.0068e+01, 1.7881e-06,\n",
            "        8.7673e+00, 7.2396e+00, 1.3455e+01, 1.1921e-07, 4.8951e-03, 5.8598e-02,\n",
            "        1.0967e-04, -0.0000e+00, 1.8596e-05, 4.8876e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.2106e-15, 1.6969e-21, 3.8354e-42, 1.2106e-15, 2.2420e-20,\n",
            "        8.1510e-29, 1.0554e-04, 1.0010e-33, 8.3117e-30, 5.3552e-02, 6.1387e-03,\n",
            "        1.2106e-15, 1.9400e-04, 7.7199e-22, 3.6844e-20, 1.6837e-27, 5.7367e-36,\n",
            "        1.2784e-23, 1.8427e-40, 8.7468e-31, 6.2387e-05, 1.2106e-15, 0.0000e+00,\n",
            "        4.6043e-28, 3.9198e-20, 2.9969e-03, 3.6844e-20, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  88\n",
            "ce_loss:  tensor([5.3644e-06, 1.1110e-04, 5.4836e-06, 1.3267e+01, 1.1110e-04, 6.9141e-06,\n",
            "        1.3134e+01, 7.8742e-04, 1.5752e+01, 2.6991e+00, 5.3644e-06, -0.0000e+00,\n",
            "        1.1110e-04, 1.1921e-07, 1.3854e-03, 6.9141e-06, 1.1518e+01, 1.9073e-06,\n",
            "        7.2033e+00, 5.9564e+00, 1.4550e+01, 1.1921e-07, 1.1110e-04, 5.4306e-02,\n",
            "        7.4741e-05, -0.0000e+00, 3.4571e-06, 6.9141e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 1.2142e-15, 1.1751e-21, 2.2281e-42, 1.2142e-15, 1.8693e-20,\n",
            "        6.5559e-29, 1.0765e-04, 6.9819e-34, 1.0335e-29, 5.3935e-02, 6.0878e-03,\n",
            "        1.2142e-15, 2.1271e-04, 4.6808e-22, 2.4852e-20, 1.5836e-27, 5.6984e-36,\n",
            "        6.7291e-24, 1.2988e-40, 1.6039e-30, 6.2445e-05, 1.2142e-15, 0.0000e+00,\n",
            "        3.0624e-28, 2.9638e-20, 3.2035e-03, 2.4852e-20, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  89\n",
            "ce_loss:  tensor([9.5367e-07, 9.0236e-04, 4.2915e-06, 1.5135e+01, 9.0236e-04, 8.4638e-06,\n",
            "        1.5778e+01, 3.2551e-04, 1.3832e+01, 3.8973e+00, 9.5367e-07, -0.0000e+00,\n",
            "        9.0236e-04, 1.7881e-06, 4.6969e-04, 3.4571e-06, 1.2214e+01, 2.0266e-06,\n",
            "        5.7628e+00, 7.7688e+00, 1.4934e+01, 1.1921e-07, 9.0236e-04, 6.2189e-02,\n",
            "        8.8807e-05, -0.0000e+00, 2.1100e-05, 3.4571e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 8.3636e-16, 9.5114e-22, 1.8637e-42, 8.3636e-16, 1.2311e-20,\n",
            "        3.5998e-29, 1.0554e-04, 1.6713e-33, 6.6919e-30, 5.3552e-02, 6.1387e-03,\n",
            "        8.3636e-16, 1.9400e-04, 8.0055e-22, 2.0803e-20, 8.9639e-28, 4.9004e-36,\n",
            "        1.2075e-23, 1.0950e-40, 4.5855e-31, 6.2387e-05, 8.3636e-16, 0.0000e+00,\n",
            "        2.1060e-28, 3.8439e-20, 3.1101e-03, 2.0803e-20, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  90\n",
            "ce_loss:  tensor([5.3644e-06, 6.1987e-05, 7.9870e-06, 1.1919e+01, 6.1987e-05, 9.7751e-06,\n",
            "        1.3473e+01, 7.8742e-04, 1.4915e+01, 5.1603e+00, 5.3644e-06, -0.0000e+00,\n",
            "        6.1987e-05, 1.1921e-07, 3.1196e-03, 2.5034e-06, 1.1496e+01, 1.6689e-06,\n",
            "        6.4270e+00, 6.6778e+00, 1.4013e+01, 1.1921e-07, 6.1987e-05, 5.1317e-02,\n",
            "        5.8172e-05, -0.0000e+00, 3.3379e-06, 2.5034e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 9.2335e-16, 6.5385e-22, 1.3060e-42, 9.2335e-16, 8.4650e-21,\n",
            "        2.0856e-29, 1.0765e-04, 1.4710e-33, 5.1300e-30, 5.3935e-02, 6.0878e-03,\n",
            "        9.2335e-16, 2.1271e-04, 3.8228e-22, 9.7326e-21, 7.2084e-28, 3.9337e-36,\n",
            "        8.0252e-24, 4.1904e-41, 1.4843e-30, 6.2445e-05, 9.2335e-16, 0.0000e+00,\n",
            "        9.7133e-29, 1.9840e-20, 3.3306e-03, 9.7326e-21, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  91\n",
            "ce_loss:  tensor([9.5367e-07, 5.3404e-05, 4.0531e-06, 1.5541e+01, 5.3404e-05, 6.6757e-06,\n",
            "        1.1931e+01, 3.2551e-04, 1.8265e+01, 3.6106e+00, 9.5367e-07, -0.0000e+00,\n",
            "        5.3404e-05, 1.7881e-06, 2.9917e-04, 3.3379e-06, 9.3006e+00, 1.6689e-06,\n",
            "        6.1918e+00, 5.1693e+00, 1.4372e+01, 1.1921e-07, 5.3404e-05, 5.6408e-02,\n",
            "        7.2238e-05, -0.0000e+00, 2.7537e-05, 3.3379e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 5.8558e-16, 5.3724e-22, 8.5479e-43, 5.8558e-16, 6.1356e-21,\n",
            "        3.6542e-29, 1.0554e-04, 9.8855e-34, 4.2036e-30, 5.3552e-02, 6.1387e-03,\n",
            "        5.8558e-16, 1.9400e-04, 6.0263e-22, 5.9822e-21, 1.2919e-27, 2.5435e-36,\n",
            "        9.7027e-24, 8.4165e-41, 4.7641e-31, 6.2387e-05, 5.8558e-16, 0.0000e+00,\n",
            "        7.6778e-29, 3.4839e-20, 3.1434e-03, 5.9822e-21, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  92\n",
            "ce_loss:  tensor([5.3644e-06, 2.3722e-05, 1.1086e-05, 1.2587e+01, 2.3722e-05, 3.5763e-06,\n",
            "        1.3511e+01, 7.8742e-04, 1.5686e+01, 2.0657e+00, 5.3644e-06, -0.0000e+00,\n",
            "        2.3722e-05, 1.1921e-07, 2.4113e-04, 7.6294e-06, 1.0238e+01, 1.6689e-06,\n",
            "        7.8049e+00, 6.5966e+00, 1.4063e+01, 1.1921e-07, 2.3722e-05, 6.4857e-02,\n",
            "        4.8636e-05, -0.0000e+00, 3.4571e-06, 7.6294e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 4.4531e-16, 4.9940e-22, 6.1097e-43, 4.4531e-16, 2.8149e-21,\n",
            "        1.8359e-29, 1.0765e-04, 4.0448e-34, 3.2290e-30, 5.3935e-02, 6.0878e-03,\n",
            "        4.4531e-16, 2.1271e-04, 2.8401e-22, 5.2188e-21, 1.5907e-27, 1.9329e-36,\n",
            "        6.0127e-24, 5.8396e-41, 1.0643e-30, 6.2445e-05, 4.4531e-16, 0.0000e+00,\n",
            "        4.3768e-29, 1.7862e-20, 3.3744e-03, 5.2188e-21, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  93\n",
            "ce_loss:  tensor([9.5367e-07, 8.6423e-05, 3.6955e-06, 1.2227e+01, 8.6423e-05, 4.1723e-06,\n",
            "        1.1952e+01, 3.2551e-04, 1.3753e+01, 3.8522e+00, 9.5367e-07, -0.0000e+00,\n",
            "        8.6423e-05, 1.7881e-06, 2.4852e-04, 3.5763e-06, 1.0972e+01, 2.1458e-06,\n",
            "        7.5636e+00, 6.9719e+00, 1.5602e+01, 1.1921e-07, 8.6423e-05, 5.6963e-02,\n",
            "        5.0663e-05, -0.0000e+00, 1.7881e-05, 3.5763e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 2.3306e-16, 4.5065e-22, 1.3144e-42, 2.3306e-16, 1.9813e-21,\n",
            "        3.1831e-29, 1.0554e-04, 9.5937e-34, 2.6144e-30, 5.3552e-02, 6.1387e-03,\n",
            "        2.3306e-16, 1.9400e-04, 3.1884e-22, 6.4684e-21, 1.3153e-27, 1.5958e-36,\n",
            "        5.4240e-24, 5.3656e-41, 4.9564e-31, 6.2387e-05, 2.3306e-16, 0.0000e+00,\n",
            "        3.3095e-29, 2.4742e-20, 3.1859e-03, 6.4684e-21, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  94\n",
            "ce_loss:  tensor([5.3644e-06, 2.1219e-05, 1.0014e-05, 1.2595e+01, 2.1219e-05, 2.7418e-06,\n",
            "        1.4078e+01, 7.8742e-04, 1.5912e+01, 5.0770e+00, 5.3644e-06, -0.0000e+00,\n",
            "        2.1219e-05, 1.1921e-07, 6.1040e-04, 1.9073e-06, 1.2254e+01, 1.5497e-06,\n",
            "        7.1712e+00, 6.6193e+00, 1.6466e+01, 1.1921e-07, 2.1219e-05, 6.1111e-02,\n",
            "        3.0040e-05, -0.0000e+00, 3.4571e-06, 1.9073e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 2.1663e-16, 3.8955e-22, 1.5428e-42, 2.1663e-16, 8.9086e-22,\n",
            "        1.4729e-29, 1.0765e-04, 8.5335e-34, 1.4217e-30, 5.3935e-02, 6.0878e-03,\n",
            "        2.1663e-16, 2.1271e-04, 1.7356e-22, 2.7650e-21, 6.6919e-28, 2.6010e-36,\n",
            "        3.7923e-24, 3.3710e-41, 3.6224e-31, 6.2445e-05, 2.1663e-16, 0.0000e+00,\n",
            "        1.4546e-29, 1.3865e-20, 3.3955e-03, 2.7650e-21, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  95\n",
            "ce_loss:  tensor([9.5367e-07, 1.0133e-05, 3.4571e-06, 1.4684e+01, 1.0133e-05, 2.3842e-06,\n",
            "        1.5613e+01, 3.2551e-04, 1.6303e+01, 2.9628e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.0133e-05, 1.7881e-06, 1.5532e-04, 2.1458e-06, 1.1371e+01, 1.4305e-06,\n",
            "        5.6466e+00, 7.1336e+00, 1.4845e+01, 1.1921e-07, 1.0133e-05, 5.3017e-02,\n",
            "        3.7550e-05, -0.0000e+00, 1.9669e-05, 2.1458e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 1.2091e-16, 3.6796e-22, 8.0995e-43, 1.2091e-16, 1.0220e-21,\n",
            "        1.1746e-29, 1.0554e-04, 3.8656e-34, 1.2493e-30, 5.3552e-02, 6.1387e-03,\n",
            "        1.2091e-16, 1.9400e-04, 2.6047e-22, 2.4493e-21, 5.3127e-28, 2.2048e-36,\n",
            "        4.1893e-24, 3.1811e-41, 2.4494e-31, 6.2387e-05, 1.2091e-16, 0.0000e+00,\n",
            "        1.5871e-29, 9.7931e-21, 3.1953e-03, 2.4493e-21, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  96\n",
            "ce_loss:  tensor([5.3644e-06, 2.9206e-05, 1.0371e-05, 1.1848e+01, 2.9206e-05, 6.4373e-06,\n",
            "        1.3778e+01, 7.8742e-04, 1.5805e+01, 1.8100e+00, 5.3644e-06, -0.0000e+00,\n",
            "        2.9206e-05, 1.1921e-07, 2.6735e-04, 7.7486e-06, 1.1211e+01, 1.7881e-06,\n",
            "        6.5510e+00, 5.8741e+00, 1.3427e+01, 1.1921e-07, 2.9206e-05, 5.9885e-02,\n",
            "        2.3007e-05, -0.0000e+00, 3.4571e-06, 7.7486e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 8.0215e-17, 2.8088e-22, 8.2957e-43, 8.0215e-17, 5.4988e-22,\n",
            "        7.2997e-30, 1.0765e-04, 3.2205e-34, 1.9050e-30, 5.3935e-02, 6.0878e-03,\n",
            "        8.0215e-17, 2.1271e-04, 1.8522e-22, 2.1443e-21, 3.1422e-28, 2.0255e-36,\n",
            "        4.5851e-24, 2.3309e-41, 5.6629e-31, 6.2445e-05, 8.0215e-17, 0.0000e+00,\n",
            "        8.4145e-30, 1.0255e-20, 3.4001e-03, 2.1443e-21, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  97\n",
            "ce_loss:  tensor([9.5367e-07, 1.1206e-05, 3.3379e-06, 1.5128e+01, 1.1206e-05, 2.1458e-06,\n",
            "        1.1594e+01, 3.2551e-04, 1.3687e+01, 3.7241e+00, 9.5367e-07, -0.0000e+00,\n",
            "        1.1206e-05, 1.7881e-06, 1.0299e-04, 4.7684e-06, 1.0500e+01, 1.6689e-06,\n",
            "        7.6681e+00, 7.5492e+00, 1.4612e+01, 1.1921e-07, 1.1206e-05, 5.0008e-02,\n",
            "        4.2438e-05, -0.0000e+00, 1.8239e-05, 4.7684e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 6.4616e-17, 2.8570e-22, 5.4090e-43, 6.4616e-17, 7.3684e-22,\n",
            "        1.1447e-29, 1.0554e-04, 5.2284e-34, 1.6461e-30, 5.3552e-02, 6.1387e-03,\n",
            "        6.4616e-17, 1.9400e-04, 2.2339e-22, 2.8907e-21, 2.6747e-28, 2.1793e-36,\n",
            "        3.3064e-24, 1.8871e-41, 2.6576e-31, 6.2387e-05, 6.4616e-17, 0.0000e+00,\n",
            "        7.6742e-30, 6.8688e-21, 3.1989e-03, 2.8907e-21, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  98\n",
            "ce_loss:  tensor([5.3644e-06, 5.3644e-06, 6.3181e-06, 1.2534e+01, 5.3644e-06, 1.1921e-06,\n",
            "        1.2746e+01, 7.8742e-04, 1.4803e+01, 4.5225e+00, 5.3644e-06, -0.0000e+00,\n",
            "        5.3644e-06, 1.1921e-07, 2.0621e-04, 6.5565e-06, 8.9167e+00, 1.4305e-06,\n",
            "        6.7271e+00, 6.3392e+00, 1.6927e+01, 1.1921e-07, 5.3644e-06, 4.1950e-02,\n",
            "        2.1219e-05, -0.0000e+00, 3.4571e-06, 6.5565e-06, 5.1855e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3935e-02, 3.3747e-17, 2.0422e-22, 3.5173e-43, 3.3747e-17, 3.8772e-22,\n",
            "        1.0189e-29, 1.0765e-04, 1.1395e-33, 1.0298e-30, 5.3935e-02, 6.0878e-03,\n",
            "        3.3747e-17, 2.1271e-04, 1.3335e-22, 2.5123e-21, 4.8971e-28, 1.6127e-36,\n",
            "        2.4535e-24, 9.8904e-42, 2.1490e-31, 6.2445e-05, 3.3747e-17, 0.0000e+00,\n",
            "        5.3578e-30, 1.0805e-20, 3.4001e-03, 2.5123e-21, 1.1902e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "*************** t  99\n",
            "ce_loss:  tensor([9.5367e-07, 2.5391e-05, 2.8610e-06, 1.4799e+01, 2.5391e-05, 1.3113e-06,\n",
            "        1.5295e+01, 3.2551e-04, 1.4968e+01, 2.7837e+00, 9.5367e-07, -0.0000e+00,\n",
            "        2.5391e-05, 1.7881e-06, 8.1297e-05, 3.2186e-06, 9.3950e+00, 1.7881e-06,\n",
            "        5.4443e+00, 5.0777e+00, 1.5164e+01, 1.1921e-07, 2.5391e-05, 6.4237e-02,\n",
            "        3.7073e-05, -0.0000e+00, 1.8239e-05, 3.2186e-06, 8.2251e-05],\n",
            "       grad_fn=<NllLossBackward0>)\n",
            "kde_loss:  tensor([5.3552e-02, 2.6728e-17, 1.6308e-22, 2.6625e-43, 2.6728e-17, 1.3729e-22,\n",
            "        7.6119e-30, 1.0554e-04, 5.4178e-34, 7.4133e-31, 5.3552e-02, 6.1387e-03,\n",
            "        2.6728e-17, 1.9400e-04, 7.9936e-23, 2.2176e-21, 8.2115e-28, 8.3784e-37,\n",
            "        2.4493e-24, 2.0777e-41, 1.0287e-31, 6.2387e-05, 2.6728e-17, 0.0000e+00,\n",
            "        3.5654e-30, 5.5248e-21, 3.1989e-03, 2.2176e-21, 1.1542e-01],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "PGD linf: Attack effectiveness 72.414%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different kinds of GKDE"
      ],
      "metadata": {
        "id": "ghFEgIubQVGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-select benign samples\n",
        "benign_samples = []\n",
        "for x_batch, y_batch in test_loader:\n",
        "  benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "# Forward pass to get logits for benign samples\n",
        "with torch.no_grad():  # No need for gradients\n",
        "    outputs = model_AT_rFGSM(ben_x.to(torch.float32))\n",
        "\n",
        "# Calculate softmax probabilities\n",
        "probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "# Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "# Select the top 1000 high confidence benign samples\n",
        "top_1000_high_confidence_benign_samples = ben_x[sorted_indices[:1000]]\n",
        "\n",
        "# Set to track indices of samples to be removed\n",
        "removed_set = set()\n",
        "\n",
        "# Step 1: Identify similar samples\n",
        "for i in range(len(top_1000_high_confidence_benign_samples)):\n",
        "    if i in removed_set:\n",
        "        continue\n",
        "    sample1 = top_1000_high_confidence_benign_samples[i]\n",
        "    for j in range(i + 1, len(top_1000_high_confidence_benign_samples)):\n",
        "        if j in removed_set:\n",
        "            continue\n",
        "        sample2 = top_1000_high_confidence_benign_samples[j]\n",
        "        if torch.abs(sample1 - sample2).sum().item() < 5:\n",
        "            removed_set.add(j)\n",
        "\n",
        "# Step 2: Select samples that are not in removed_set\n",
        "selected_samples = [sample for idx, sample in enumerate(top_1000_high_confidence_benign_samples) if idx not in removed_set]\n",
        "\n",
        "# Step 3: Stack tensors along a new dimension\n",
        "selected_benigns = torch.stack(selected_samples)\n",
        "\n",
        "del benign_samples, outputs, probabilities, top_1000_high_confidence_benign_samples, selected_samples   # Free up memory"
      ],
      "metadata": {
        "id": "YLO4kDBORXNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done"
      ],
      "metadata": {
        "id": "sbCNYQPAQvLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multidimensional KDE implementation\n",
        "def KDE(x, data, bandwidth,kernel):\n",
        "    \"\"\"\n",
        "    Compute the kernel density estimate (KDE) for given data points.\n",
        "\n",
        "    Parameters:\n",
        "        x (torch.Tensor): Points at which to evaluate the KDE (shape: [num_samples, num_dimensions]).\n",
        "        data (torch.Tensor): Data points used to estimate the density (shape: [num_data_points, num_dimensions]).\n",
        "        bandwidth (float): Bandwidth parameter for the KDE.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Density estimate at each point in x (shape: [num_samples,]).\n",
        "    \"\"\"\n",
        "    n = data.shape[0]  # Number of data points\n",
        "    d = x.shape[1]  # Dimensionality of the data\n",
        "\n",
        "    # Convert bandwidth to tensor\n",
        "    bandwidth_tensor = torch.tensor(bandwidth)\n",
        "\n",
        "    # Calculate standardized distances for all data points\n",
        "    u = torch.abs(x[:, None, :] - data)\n",
        "\n",
        "    # Compute kernel contributions for all data points\n",
        "    if kernel == 'gaussian':\n",
        "        kernel_contributions = 10. * torch.exp(-0.5 * torch.sum(u**2, dim=-1) / bandwidth**2)\n",
        "    else:\n",
        "        kernel_contributions = (1./d) * torch.exp(-1. * torch.sum(u, dim=-1) / bandwidth_tensor)\n",
        "\n",
        "    # Sum contributions across all data points\n",
        "    estimate = torch.mean(kernel_contributions, dim=1)\n",
        "\n",
        "    # Normalize the density estimate by the number of points and the bandwidth raised to the dimensionality\n",
        "    #estimate /= (bandwidth_tensor ** d)\n",
        "\n",
        "    return estimate\n"
      ],
      "metadata": {
        "id": "izCn5s8Y7tdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde_old(adv_x,y,model,benigns, bandwidth, penalty_factor,kernel):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y.view(-1).long())\n",
        "    #print('ce: ', ce)\n",
        "    kde = KDE(adv_x, benigns, bandwidth,kernel)\n",
        "    #print('kde : ', kde)\n",
        "    loss_no_reduction = ce + penalty_factor * kde\n",
        "    return loss_no_reduction\n",
        "\n",
        "\n",
        "def gkde_old(x, y, model,bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde_old(x_var,y,model,bens, bandwidth, penalty_factor, kernel)\n",
        "        #loss,_ = get_loss_kde(x_var,y,model,bens, bandwidth, penalty_factor)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "        pos_insertion = (x_var <= 0.5) * 1 * insertion_array\n",
        "        #pos_insertion = (x_var <= 0.5) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.5) * 1 * removal_array\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "        #gradients =  grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x_next - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "\n",
        "            val, _ = torch.topk(torch.abs(gradients), 1)\n",
        "            perturbation = (torch.abs(gradients) >= val.expand_as(gradients)).float() * torch.sign(gradients).float()\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(x_next)\n",
        "    loss_adv = criterion(outputs, y.view(-1).long()).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "y8be3sudY43I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde_old(adv_x,y,model,benigns, bandwidth, penalty_factor,kernel):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y.view(-1).long())\n",
        "    #print('ce: ', ce)\n",
        "    kde = KDE(adv_x, benigns, bandwidth,kernel)\n",
        "    #print('kde : ', kde)\n",
        "    loss_no_reduction = ce + penalty_factor * kde\n",
        "    return loss_no_reduction\n",
        "\n",
        "\n",
        "def gkde_old2(x, y, model,bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), y.view(-1).long())\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde_old(x_var,y,model,bens, bandwidth, penalty_factor, kernel)\n",
        "        #loss,_ = get_loss_kde(x_var,y,model,bens, bandwidth, penalty_factor)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = grad_vars[0].data\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / l2norm).float()\n",
        "            perturbation = torch.minimum(torch.tensor(1., dtype=x.dtype, device=x.device), gradients / l2norm).float()\n",
        "            perturbation = torch.maximum(torch.tensor(-1., dtype=x.dtype, device=x.device), perturbation).float()\n",
        "            perturbation[torch.isnan(perturbation)] = 0.\n",
        "            perturbation[torch.isinf(perturbation)] = 1.\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x_next - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "\n",
        "            val, _ = torch.topk(torch.abs(gradients), 1)\n",
        "            perturbation = (torch.abs(gradients) >= val.expand_as(gradients)).float() * torch.sign(gradients).float()\n",
        "            # stop perturbing the examples that are successful to evade the victim\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    outputs = model(x_next)\n",
        "    loss_adv = criterion(outputs, y.view(-1).long()).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv < loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "co0DuPkBMf2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adv_predict(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    if (attack ==  gkde) or (attack ==  mimicry) or (attack ==  gkde_old) or (attack ==  gkde_old2):\n",
        "      # Pre-select benign samples\n",
        "      benign_samples = []\n",
        "      for x_batch, y_batch in test_loader:\n",
        "        benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "      ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "      # Forward pass to get logits for benign samples\n",
        "      with torch.no_grad():  # No need for gradients\n",
        "          outputs = model_AT_rFGSM(ben_x.to(torch.float32))\n",
        "\n",
        "      # Calculate softmax probabilities\n",
        "      probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "      # Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "      sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "      # Select the top 500 high confidence benign samples\n",
        "      top_1000_high_confidence_benign_samples = ben_x[sorted_indices[:1000]]\n",
        "\n",
        "      # Set to track indices of samples to be removed\n",
        "      removed_set = set()\n",
        "\n",
        "      # Step 1: Identify similar samples\n",
        "      for i in range(len(top_1000_high_confidence_benign_samples)):\n",
        "          if i in removed_set:\n",
        "              continue\n",
        "          sample1 = top_1000_high_confidence_benign_samples[i]\n",
        "          for j in range(i + 1, len(top_1000_high_confidence_benign_samples)):\n",
        "              if j in removed_set:\n",
        "                  continue\n",
        "              sample2 = top_1000_high_confidence_benign_samples[j]\n",
        "              if torch.abs(sample1 - sample2).sum().item() < 5:\n",
        "                  removed_set.add(j)\n",
        "\n",
        "      # Step 2: Select samples that are not in removed_set\n",
        "      selected_samples = [sample for idx, sample in enumerate(top_1000_high_confidence_benign_samples) if idx not in removed_set]\n",
        "\n",
        "      # Step 3: Stack tensors along a new dimension\n",
        "      selected_benigns = torch.stack(selected_samples)\n",
        "\n",
        "      del benign_samples, outputs, probabilities, ben_x, top_1000_high_confidence_benign_samples, selected_samples   # Free up memory\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            if attack == mimicry:\n",
        "                pertb_mal_x = mimicry(top_500_high_confidence_benign_samples, mal_x_batch, model, **kwargs)\n",
        "            elif (attack == gkde):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old2):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old2(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            else :\n",
        "                with torch.enable_grad():\n",
        "                    pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "p5bJtpokI_uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adv_predict0(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    if (attack ==  gkde) or (attack ==  mimicry) or (attack ==  gkde_old) or (attack ==  gkde_old2):\n",
        "      # Pre-select benign samples\n",
        "      benign_samples = []\n",
        "      for x_batch, y_batch in test_loader:\n",
        "        benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "      ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "      # Forward pass to get logits for benign samples\n",
        "      with torch.no_grad():  # No need for gradients\n",
        "          outputs = model_AT_rFGSM(ben_x.to(torch.float32))\n",
        "\n",
        "      # Calculate softmax probabilities\n",
        "      probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "      # Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "      sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "      # Select the top 500 high confidence benign samples\n",
        "      selected_benigns = ben_x[sorted_indices[:500]]\n",
        "\n",
        "\n",
        "      del benign_samples, outputs, probabilities, ben_x   # Free up memory\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            if attack == mimicry:\n",
        "                pertb_mal_x = mimicry(selected_benigns, mal_x_batch, model, **kwargs)\n",
        "            elif (attack == gkde):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old2):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old2(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            else :\n",
        "                with torch.enable_grad():\n",
        "                    pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Gv05ZocqXdFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adv_predict2(test_loader, model, attack, device, **kwargs):\n",
        "\n",
        "    if (attack ==  gkde) or (attack ==  mimicry) or (attack ==  gkde_old) or (attack ==  gkde_old2):\n",
        "      # Pre-select benign samples\n",
        "      benign_samples = []\n",
        "      for x_batch, y_batch in test_loader:\n",
        "        benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "      ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "      selected_samples = ben_x[100]\n",
        "      del benign_samples\n",
        "\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    cor_test = 0\n",
        "    cor_ad_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(torch.float32).to(device), y_test.to(device)\n",
        "            #outputs = model(x_test)\n",
        "            #predicted = outputs.argmax(1).unsqueeze(1)\n",
        "            #acc_test = (predicted == y_test).sum().item() / len(y_test)\n",
        "            #avg_acc_test.append(acc_test)\n",
        "\n",
        "            mal_x_batch, mal_y_batch = x_test[y_test.squeeze() == 1], y_test[y_test.squeeze() == 1]\n",
        "            n_samples += len(mal_y_batch)\n",
        "\n",
        "            outputs = model(mal_x_batch)\n",
        "            predicted = outputs.argmax(1)\n",
        "            cor_test += (predicted == 1).sum().item()\n",
        "\n",
        "            # Generate adversarial examples for test set\n",
        "            if attack == mimicry:\n",
        "                pertb_mal_x = mimicry(top_500_high_confidence_benign_samples, mal_x_batch, model, **kwargs)\n",
        "            elif (attack == gkde):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            elif (attack == gkde_old2):\n",
        "                with torch.enable_grad():\n",
        "                  pertb_mal_x = gkde_old2(mal_x_batch, mal_y_batch, model, selected_benigns, **kwargs)\n",
        "            else :\n",
        "                with torch.enable_grad():\n",
        "                    pertb_mal_x = attack(mal_x_batch, mal_y_batch, model, **kwargs)\n",
        "\n",
        "            outputs = model(pertb_mal_x)\n",
        "            y_pred = outputs.argmax(1)\n",
        "            cor_ad_test += (y_pred == 1).sum().item()\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    print(f\"Accuracy of just malwares (without attack): {(cor_test / n_samples) * 100:.4}% | Under attack: {(cor_ad_test / n_samples) * 100:.4}%.\")\n"
      ],
      "metadata": {
        "id": "mX2CUeHZ0j3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old(maximize loss & default removal insertion arrays), adv_predict(top benigns), laplacian\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e5]\n",
        "bandwidths = [1.,2.,3.,5.,10.,15.,20.,30.,40.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbc498a-4692-4dee-eb5b-b960fd31245a",
        "id": "N_lecXmXcIEa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 76.37%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 83.45%.\n",
            "********* penalty_factor: 1 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 70.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 67.52%.\n",
            "********* penalty_factor: 1 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 70.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 62.83%.\n",
            "********* penalty_factor: 1 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 64.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 58.05%.\n",
            "********* penalty_factor: 1 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.46%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.77%.\n",
            "********* penalty_factor: 1 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 54.6%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.42%.\n",
            "********* penalty_factor: 1 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.24%.\n",
            "********* penalty_factor: 1 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.15%.\n",
            "********* penalty_factor: 1 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.06%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 75.4%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 79.38%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 68.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 65.04%.\n",
            "********* penalty_factor: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.43%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 66.11%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 60.8%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 64.51%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.11%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.42%.\n",
            "********* penalty_factor: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.08%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.24%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.62%.\n",
            "********* penalty_factor: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.53%.\n",
            "********* penalty_factor: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 75.22%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 77.35%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 68.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 63.19%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 65.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 59.73%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 61.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.27%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.11%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "********* penalty_factor: 100 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.8%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 100 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "********* penalty_factor: 100 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 73.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 75.66%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 64.69%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 61.86%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 64.6%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 57.26%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 57.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.65%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "********* penalty_factor: 1000 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.53%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 1000 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "********* penalty_factor: 1000 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 73.45%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 72.3%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 63.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 59.47%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 63.98%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 55.4%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 58.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.42%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.15%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 71.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 69.91%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 63.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 57.7%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 59.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.98%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.8%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.45%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.8%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old(maximize loss & default removal insertion arrays), adv_predict2(random benigns), laplacian\n",
        "\n",
        "penalty_factors = [10,100,1000,1e4,1e5]\n",
        "bandwidths = [5.,10.,15.,20.,30.,40.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e71da573-eba6-4745-f572-d34968d0ac89",
        "id": "NOaZSky91tDS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 61.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.48%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.81%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.33%.\n",
            "********* penalty_factor: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.1%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.8%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "********* penalty_factor: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.62%.\n",
            "********* penalty_factor: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.74%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 59.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.68%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.06%.\n",
            "********* penalty_factor: 100 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.53%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 100 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.46%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.15%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "********* penalty_factor: 1000 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 1000 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.55%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 30.0 ******************\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3e10c3c015c9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'laplac'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0madv_predict2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_AT_rFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattack_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'laplac'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-d49f3b3ba210>\u001b[0m in \u001b[0;36madv_predict2\u001b[0;34m(test_loader, model, attack, device, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                   \u001b[0mpertb_mal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmal_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmal_y_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_benigns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-87032243fc66>\u001b[0m in \u001b[0;36mgkde_old\u001b[0;34m(x, y, model, bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k, step_length, norm, initial_rounding_threshold, round_threshold, random, is_report_loss_diff, is_sample)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mperturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;31m# stop perturbing the examples that are successful to evade the victim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old(maximize loss & default removal insertion arrays), adv_predict0(top 500 confidence benigns), laplacian\n",
        "\n",
        "penalty_factors = [10,100,1000]\n",
        "bandwidths = [10.,20.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict0(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict0(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict0(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8MXwjFUYd2j",
        "outputId": "6e298225-9a15-40b9-b58e-c90082c8aa4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.43%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 56.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.19%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 54.69%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.04%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 55.22%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.57%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.74%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.59%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.77%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.5%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tEuv7Nay1Xky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old2(maximize loss & updated removal insertion arrays), adv_predict(top benigns), laplacian\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e5]\n",
        "bandwidths = [1.,2.,3.,5.,10.,15.,20.,30.,40.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1rExFdCclMZ",
        "outputId": "8be64a63-679e-416c-edde-42812c239161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.2%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "********* penalty_factor: 1 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.61%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 1 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.43%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "********* penalty_factor: 1 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.28%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.54%.\n",
            "********* penalty_factor: 1 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.63%.\n",
            "********* penalty_factor: 1 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.72%.\n",
            "********* penalty_factor: 1 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.16%.\n",
            "********* penalty_factor: 1 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.07%.\n",
            "********* penalty_factor: 1 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.45%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.03%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.26%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.61%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.48%.\n",
            "********* penalty_factor: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.98%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.04%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.67%.\n",
            "********* penalty_factor: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.17%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.31%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.96%.\n",
            "********* penalty_factor: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.42%.\n",
            "********* penalty_factor: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.51%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.38%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.61%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.92%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.47%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.81%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.87%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.69%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.03%.\n",
            "********* penalty_factor: 100 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.29%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.37%.\n",
            "********* penalty_factor: 100 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.6%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.29%.\n",
            "********* penalty_factor: 100 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.2%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.52%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.19%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.16%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "********* penalty_factor: 1000 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.0%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.24%.\n",
            "********* penalty_factor: 1000 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.42%.\n",
            "********* penalty_factor: 1000 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.95%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.78%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.38%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.62%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.74%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.07%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.07%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.89%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.3%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.64%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.58%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.48%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.19%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.45%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.07%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.49%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.34%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.84%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.34%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.02%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.66%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.25%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.66%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old2(maximize loss & updated removal insertion arrays), adv_predict2(random benigns), laplacian\n",
        "\n",
        "penalty_factors = [1,10,100]\n",
        "bandwidths = [5.,10.,20.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "93dw0vya6Khr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "54698735-a6e3-49ff-8d16-021f269f5f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1 bandwidth: 5.0 ******************\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ee9ea09d1222>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'laplac'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0madv_predict2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_AT_rFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattack_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'laplac'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d49f3b3ba210>\u001b[0m in \u001b[0;36madv_predict2\u001b[0;34m(test_loader, model, attack, device, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                   \u001b[0mpertb_mal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmal_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmal_y_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_benigns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-406d89ad1df1>\u001b[0m in \u001b[0;36mgkde_old2\u001b[0;34m(x, y, model, bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k, step_length, norm, initial_rounding_threshold, round_threshold, random, is_report_loss_diff, is_sample)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# stop perturbing the examples that are successful to evade the victim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mperturbation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Obl7KqKq7ZYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70iWYt9Z7ZPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old(maximize loss & default removal insertion arrays), adv_predict(top benigns), gaussian\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e5]\n",
        "bandwidths = [0.1,0.2,0.4,0.6,1.,1.5,2.,5.,10.,20.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "-6go4nY4ck6Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "116f5138-3e86-4176-d361-42ac032e494a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1 bandwidth: 0.1 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 1 bandwidth: 0.2 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 1 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 84.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 83.01%.\n",
            "********* penalty_factor: 1 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 79.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 66.11%.\n",
            "********* penalty_factor: 1 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 54.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.5%.\n",
            "********* penalty_factor: 1 bandwidth: 1.5 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.38%.\n",
            "********* penalty_factor: 1 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 1 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "********* penalty_factor: 1 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "********* penalty_factor: 10 bandwidth: 0.1 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 10 bandwidth: 0.2 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 10 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 85.13%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 81.15%.\n",
            "********* penalty_factor: 10 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.47%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 75.22%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 63.89%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.72%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 51.5%.\n",
            "********* penalty_factor: 10 bandwidth: 1.5 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.47%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.25%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 0.1 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 100 bandwidth: 0.2 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 100 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 85.13%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 80.62%.\n",
            "********* penalty_factor: 100 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.47%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 75.93%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 62.83%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 53.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.88%.\n",
            "********* penalty_factor: 100 bandwidth: 1.5 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.64%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.1 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.2 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 84.16%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 77.35%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 70.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 60.8%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.88%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.5 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.1 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.2 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.92%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 87.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 92.65%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 82.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 76.19%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 68.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 59.47%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 52.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.62%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.5 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-61c3583ca183>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0madv_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_AT_rFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattack_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d2a3f468a2b5>\u001b[0m in \u001b[0;36madv_predict\u001b[0;34m(test_loader, model, attack, device, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                   \u001b[0mpertb_mal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgkde_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmal_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmal_y_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_benigns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-87032243fc66>\u001b[0m in \u001b[0;36mgkde_old\u001b[0;34m(x, y, model, bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k, step_length, norm, initial_rounding_threshold, round_threshold, random, is_report_loss_diff, is_sample)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss_kde_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m#loss,_ = get_loss_kde(x_var,y,model,bens, bandwidth, penalty_factor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Compute gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-406d89ad1df1>\u001b[0m in \u001b[0;36mget_loss_kde_old\u001b[0;34m(adv_x, y, model, benigns, bandwidth, penalty_factor, kernel)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print('ce: ', ce)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mkde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKDE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenigns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print('kde : ', kde)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss_no_reduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpenalty_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d0c6fc92c747>\u001b[0m in \u001b[0;36mKDE\u001b[0;34m(x, data, bandwidth, kernel)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Calculate standardized distances for all data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Compute kernel contributions for all data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old(maximize loss & default removal insertion arrays), adv_predict2(random benigns), gaussian\n",
        "\n",
        "penalty_factors = [10,100,1000]\n",
        "bandwidths = [10.,20.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "85LtK4b11ycV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74f9c55-e39b-4983-cbcd-14a0f9ed07e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.34%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old2(maximize loss & updated removal insertion arrays), adv_predict(top benigns), gaussian\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e5]\n",
        "bandwidths = [0.6,0.8,1.,2.,3.,4.,5.,10.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "50ny1UUT78eY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "715849d3-4d2d-48a6-bf90-00669aeeb985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.59%.\n",
            "********* penalty_factor: 1 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.89%.\n",
            "********* penalty_factor: 1 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.94%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "********* penalty_factor: 1 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.4%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.11%.\n",
            "********* penalty_factor: 1 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.98%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.43%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "********* penalty_factor: 1 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "********* penalty_factor: 1 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.03%.\n",
            "********* penalty_factor: 1 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.67%.\n",
            "********* penalty_factor: 10 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* penalty_factor: 10 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.03%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.4%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.96%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.81%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.3%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.2%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.94%.\n",
            "********* penalty_factor: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.17%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.47%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.38%.\n",
            "********* penalty_factor: 10 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.38%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.25%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.03%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.26%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.07%.\n",
            "********* penalty_factor: 100 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.5%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.1%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.38%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 100 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.81%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.45%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.93%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ab094ca2a76e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mattack_params\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m'bandwidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'penalty_factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insertion_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minsertion_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removal_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mremoval_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'norm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'linf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_report_loss_diff'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0madv_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_AT_rFGSM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattack_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-d2a3f468a2b5>\u001b[0m in \u001b[0;36madv_predict\u001b[0;34m(test_loader, model, attack, device, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                   \u001b[0mpertb_mal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgkde_old2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmal_x_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmal_y_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_benigns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-406d89ad1df1>\u001b[0m in \u001b[0;36mgkde_old2\u001b[0;34m(x, y, model, bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k, step_length, norm, initial_rounding_threshold, round_threshold, random, is_report_loss_diff, is_sample)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#loss,_ = get_loss_kde(x_var,y,model,bens, bandwidth, penalty_factor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Compute gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mgrad_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gkde_old2(maximize loss & updated removal insertion arrays), adv_predict2(random benigns), gaussian\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e5]\n",
        "bandwidths = [0.6,0.8,1.,2.,3.,4.,5.,10.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict2(test_loader, model_AT_rFGSM, gkde_old2, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "J677NN_j78eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAkAN5JU778i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde(adv_x,y,model,benigns, bandwidth, penalty_factor,kernel):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute CE loss\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y)\n",
        "    #print('ce ',ce)\n",
        "\n",
        "    # Compute KDE loss\n",
        "    kde = KDE(adv_x, benigns, bandwidth,kernel)\n",
        "    #print('kde ',kde)\n",
        "\n",
        "    # Combine the losses with the penalty factor\n",
        "    loss = ce - penalty_factor * kde\n",
        "    #print('loss ',loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def gkde(x, y, model,bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k=25, step_length=0.02, norm='linf',\n",
        "        initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "\n",
        "    :param x: Feature vector\n",
        "    :param y: Ground truth labels\n",
        "    :param model: Neural network model\n",
        "    :param RBFModel: Gaussian model for KDE\n",
        "    :param insertion_array: Array for insertion operations\n",
        "    :param removal_array: Array for removal operations\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf', 'l2', 'l1')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    #target's class\n",
        "    traget_labels = torch.zeros_like(y.view(-1).long())\n",
        "\n",
        "    # Compute natural loss and penalty_factor\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), traget_labels)\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    for t in range(k):\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde(x_var,traget_labels,model,bens, bandwidth, penalty_factor, kernel)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "        elif norm == 'l1':\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1', 'l2', or 'linf' norm.\")\n",
        "\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    if random:\n",
        "        round_threshold = torch.rand_like(x_next)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_adv = criterion(model(x_next), traget_labels).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size(0) * 100:.3f}%.\")\n",
        "\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "xZkNUBSk7zzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fixed penalty factor\n",
        "\n",
        "penalty_factors = [1,10,100,1000,1e4,1e6,1e8,1e10]\n",
        "bandwidths = [1.,2.,3.,5.,10.,15.,20.,30.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n"
      ],
      "metadata": {
        "id": "t9vYJDKg7zzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a5d89d-389d-4dc7-cd66-5ec1d90d966b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 1.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "********* penalty_factor: 100 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.44%.\n",
            "********* penalty_factor: 100 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.73%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.57%.\n",
            "********* penalty_factor: 1000 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.27%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.68%.\n",
            "********* penalty_factor: 1000 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.31%.\n",
            "********* penalty_factor: 1000 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.59%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.39%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.83%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.49%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.65%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.65%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.56%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.74%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.11%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.75%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.73%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.23%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.15%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.26%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.19%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.61%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.78%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.11%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.55%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.72%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.74%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.16%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.61%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.19%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.02%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.76%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.45%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.23%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.4%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.29%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.13%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.12%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.72%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.14%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.99%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.07%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.76%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.2%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fixed penalty factor\n",
        "\n",
        "penalty_factors = [100,1000,1e4,1e6,1e8]\n",
        "bandwidths = [0.6,0.8,1.,2.,3.,4.,5.,10.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'gaussian','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)"
      ],
      "metadata": {
        "id": "LS1hW_l_JQW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2ccae5-abe1-48cf-fe41-c60013663fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 100 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.47%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.93%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "********* penalty_factor: 100 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.38%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.67%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.08%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.9%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.8%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.47%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 26.64%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.36%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.62%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.28%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.18%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.65%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.98%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.81%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.22%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.87%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.55%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.37%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.79%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.33%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.69%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.37%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.55%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.37%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde(adv_x,y,model,benigns, bandwidth, penalty_factor,kernel,alpha):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute CE loss\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y)\n",
        "    #print('ce ',ce)\n",
        "\n",
        "    # Compute KDE loss\n",
        "    kde = KDE(adv_x, benigns, bandwidth,kernel)\n",
        "    #print('kde ',kde)\n",
        "\n",
        "    # Combine the losses with the penalty factor\n",
        "    loss = alpha * ce - penalty_factor * kde\n",
        "    #print('loss ',loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def gkde(x, y, model,bens, bandwidth, penalty_factor, kernel,t_threshold, insertion_array, removal_array, k=25, step_length=0.02, norm='linf',\n",
        "        initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "\n",
        "    :param x: Feature vector\n",
        "    :param y: Ground truth labels\n",
        "    :param model: Neural network model\n",
        "    :param RBFModel: Gaussian model for KDE\n",
        "    :param insertion_array: Array for insertion operations\n",
        "    :param removal_array: Array for removal operations\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf', 'l2', 'l1')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    #target's class\n",
        "    traget_labels = torch.zeros_like(y.view(-1).long())\n",
        "\n",
        "    # Compute natural loss and penalty_factor\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), traget_labels)\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    for t in range(k):\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "\n",
        "        if t<t_threshold:\n",
        "          alpha = 0.\n",
        "        else:\n",
        "          alpha = 1.\n",
        "          penalty_factor = 0.\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde(x_var,traget_labels,model,bens, bandwidth, penalty_factor, kernel,alpha)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "        elif norm == 'l1':\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1', 'l2', or 'linf' norm.\")\n",
        "\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    if random:\n",
        "        round_threshold = torch.rand_like(x_next)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_adv = criterion(model(x_next), traget_labels).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size(0) * 100:.3f}%.\")\n",
        "\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "QRoDHAXrjpgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first t_threshold steps, kde , then ce(penalty factor is meaningless here)\n",
        "\n",
        "t_thresholds = [3,5,10,15,20,25,30]\n",
        "bandwidths = [1.,2.,3.,5.,10.,15.,20.,30.,40.]\n",
        "\n",
        "for t_threshold in t_thresholds:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','t_threshold:',t_threshold,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'laplac','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'laplac','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'laplac','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwm37B4apzFE",
        "outputId": "3fc548cd-4207-4702-f3b2-5aa8f80ad13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* t_threshold: 3 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "********* t_threshold: 3 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 3 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.8%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 3 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 3 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.07%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.35%.\n",
            "********* t_threshold: 3 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.62%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.07%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.5%.\n",
            "********* t_threshold: 3 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 3 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.16%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.61%.\n",
            "********* t_threshold: 3 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.61%.\n",
            "********* t_threshold: 5 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.58%.\n",
            "********* t_threshold: 5 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.96%.\n",
            "********* t_threshold: 5 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.96%.\n",
            "********* t_threshold: 5 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.23%.\n",
            "********* t_threshold: 5 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.66%.\n",
            "********* t_threshold: 5 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.0%.\n",
            "********* t_threshold: 5 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.18%.\n",
            "********* t_threshold: 5 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.18%.\n",
            "********* t_threshold: 5 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.18%.\n",
            "********* t_threshold: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.64%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.24%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.5%.\n",
            "********* t_threshold: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.02%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 20.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.41%.\n",
            "********* t_threshold: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.66%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.76%.\n",
            "********* t_threshold: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.67%.\n",
            "********* t_threshold: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.16%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.23%.\n",
            "********* t_threshold: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.67%.\n",
            "********* t_threshold: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "********* t_threshold: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* t_threshold: 15 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 15 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.14%.\n",
            "********* t_threshold: 15 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.78%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.41%.\n",
            "********* t_threshold: 15 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.47%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.5%.\n",
            "********* t_threshold: 15 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.85%.\n",
            "********* t_threshold: 15 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.03%.\n",
            "********* t_threshold: 15 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.18%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.5%.\n",
            "********* t_threshold: 15 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.23%.\n",
            "********* t_threshold: 15 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.23%.\n",
            "********* t_threshold: 20 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.41%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.29%.\n",
            "********* t_threshold: 20 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 20 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.99%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.14%.\n",
            "********* t_threshold: 20 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.11%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.24%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.76%.\n",
            "********* t_threshold: 20 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.5%.\n",
            "********* t_threshold: 20 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.75%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.32%.\n",
            "********* t_threshold: 20 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.75%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.85%.\n",
            "********* t_threshold: 20 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.66%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 20 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.03%.\n",
            "********* t_threshold: 25 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.79%.\n",
            "********* t_threshold: 25 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.32%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.61%.\n",
            "********* t_threshold: 25 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.7%.\n",
            "********* t_threshold: 25 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.37%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 25 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 25 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.14%.\n",
            "********* t_threshold: 25 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.62%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 25 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.57%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n",
            "********* t_threshold: 25 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.41%.\n",
            "********* t_threshold: 30 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.14%.\n",
            "********* t_threshold: 30 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.17%.\n",
            "********* t_threshold: 30 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 48.14%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.35%.\n",
            "********* t_threshold: 30 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.17%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.52%.\n",
            "********* t_threshold: 30 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 44.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.94%.\n",
            "********* t_threshold: 30 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.56%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.79%.\n",
            "********* t_threshold: 30 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.41%.\n",
            "********* t_threshold: 30 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n",
            "********* t_threshold: 30 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 38.85%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.57%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ysfPisrTn8n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first t_threshold steps, kde , then ce(penalty factor is meaningless here)\n",
        "\n",
        "t_thresholds = [3,5,10,15,20,25,30]\n",
        "bandwidths = [0.4,0.6,0.8,1.,2.,3.,4.,5.,10.]\n",
        "\n",
        "for t_threshold in t_thresholds:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','t_threshold:',t_threshold,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'gaussian','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'gaussian','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': 1000,'kernel':'gaussian','t_threshold':t_threshold,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a04f247-bc40-4e12-c0ee-dbbb1e7967dc",
        "id": "D3fLArOwsQRr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* t_threshold: 3 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n",
            "********* t_threshold: 3 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.6%.\n",
            "********* t_threshold: 3 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.5%.\n",
            "********* t_threshold: 3 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.29%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.33%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.97%.\n",
            "********* t_threshold: 3 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.18%.\n",
            "********* t_threshold: 3 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.62%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.77%.\n",
            "********* t_threshold: 3 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.34%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "********* t_threshold: 3 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.89%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* t_threshold: 3 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.89%.\n",
            "********* t_threshold: 5 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.58%.\n",
            "********* t_threshold: 5 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.24%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.76%.\n",
            "********* t_threshold: 5 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.43%.\n",
            "********* t_threshold: 5 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 20.97%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "********* t_threshold: 5 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.0%.\n",
            "********* t_threshold: 5 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.89%.\n",
            "********* t_threshold: 5 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.16%.\n",
            "********* t_threshold: 5 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.57%.\n",
            "********* t_threshold: 5 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.51%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* t_threshold: 10 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.2%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.0%.\n",
            "********* t_threshold: 10 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.53%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.23%.\n",
            "********* t_threshold: 10 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.81%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.15%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.29%.\n",
            "********* t_threshold: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.76%.\n",
            "********* t_threshold: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.12%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.77%.\n",
            "********* t_threshold: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.54%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.86%.\n",
            "********* t_threshold: 10 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.24%.\n",
            "********* t_threshold: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.3%.\n",
            "********* t_threshold: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.74%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.84%.\n",
            "********* t_threshold: 15 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.05%.\n",
            "********* t_threshold: 15 bandwidth: 0.6 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.96%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.32%.\n",
            "********* t_threshold: 15 bandwidth: 0.8 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.08%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.42%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.32%.\n",
            "********* t_threshold: 15 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.14%.\n",
            "********* t_threshold: 15 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.27%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.77%.\n",
            "********* t_threshold: 15 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.35%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n",
            "********* t_threshold: 15 bandwidth: 4.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.48%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* t_threshold: 15 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.39%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.48%.\n",
            "********* t_threshold: 15 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 42.83%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.3%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* t_threshold: 20 bandwidth: 0.4 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.44%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KYqW-o_jo56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_kde(adv_x,y,model,benigns, bandwidth, penalty_factor,kernel):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute CE loss\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y)\n",
        "    #print('ce ',ce)\n",
        "\n",
        "    # Compute KDE loss\n",
        "    kde = KDE(adv_x, benigns, bandwidth,kernel)\n",
        "    #print('kde ',kde)\n",
        "\n",
        "    # Combine the losses with the penalty factor\n",
        "    loss = ce - penalty_factor * kde\n",
        "    #print('loss ',loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def gkde(x, y, model,bens, bandwidth, penalty_factor, kernel, insertion_array, removal_array, k=25, step_length=0.02, norm='linf',\n",
        "        initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "\n",
        "    :param x: Feature vector\n",
        "    :param y: Ground truth labels\n",
        "    :param model: Neural network model\n",
        "    :param RBFModel: Gaussian model for KDE\n",
        "    :param insertion_array: Array for insertion operations\n",
        "    :param removal_array: Array for removal operations\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf', 'l2', 'l1')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    #target's class\n",
        "    traget_labels = torch.zeros_like(y.view(-1).long())\n",
        "\n",
        "    # Compute natural loss and penalty_factor\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), traget_labels)\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    for t in range(k):\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        decayed_penalty_factor = penalty_factor * (1 - t / k)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_kde(x_var,traget_labels,model,bens, bandwidth, decayed_penalty_factor, kernel)\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "        elif norm == 'l1':\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1', 'l2', or 'linf' norm.\")\n",
        "\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    if random:\n",
        "        round_threshold = torch.rand_like(x_next)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_adv = criterion(model(x_next), traget_labels).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size(0) * 100:.3f}%.\")\n",
        "\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "JACMzQSN0xDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "penalty_factors = [1.,10,100,1000,1e4,1e5,1e6,1e7,1e8]\n",
        "bandwidths = [1.,2.,3.,5.,10.,15.,20.,30.,40.]\n",
        "\n",
        "for penalty_factor in penalty_factors:\n",
        "  for bandwidth in bandwidths:\n",
        "    print('*********','penalty_factor:',penalty_factor,'bandwidth:',bandwidth,'******************')\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n",
        "\n",
        "    attack_params =  {'bandwidth': bandwidth,'penalty_factor': penalty_factor,'kernel':'laplac','insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : False}\n",
        "    adv_predict(test_loader, model_AT_rFGSM, gkde, device, **attack_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669663f6-ccc8-4d93-b0b9-07519f4bba98",
        "id": "zFKmvfGc0xDn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* penalty_factor: 1.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 100 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 100 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 100 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 100 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.56%.\n",
            "********* penalty_factor: 1000 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 1000 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 1000 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 1000 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.47%.\n",
            "********* penalty_factor: 1000 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.12%.\n",
            "********* penalty_factor: 1000 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.77%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.5%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.19%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.67%.\n",
            "********* penalty_factor: 10000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.59%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.95%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.27%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.68%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.5%.\n",
            "********* penalty_factor: 100000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.65%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.86%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.59%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.04%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.06%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.5%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.57%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.74%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.01%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.01%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.91%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.19%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.63%.\n",
            "********* penalty_factor: 1000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.0%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 23.1%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.53%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.86%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.69%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.04%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.31%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.4%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.34%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.01%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 34.51%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.93%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.0%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.58%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.9%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.29%.\n",
            "********* penalty_factor: 10000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.46%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.26%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.8%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 1.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.77%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.04%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 2.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 21.86%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.86%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 3.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 22.21%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.83%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 5.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 31.95%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 27.17%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 33.72%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 10.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.36%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.7%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 15.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 37.88%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 45.31%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 20.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 36.37%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 47.26%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 50.09%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 30.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 39.73%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 46.46%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.91%.\n",
            "********* penalty_factor: 100000000.0 bandwidth: 40.0 ******************\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 40.71%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 43.63%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 49.82%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vf0vRQQJ7zQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**study the centers **"
      ],
      "metadata": {
        "id": "XoeGnFXncAWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-select benign samples\n",
        "benign_samples = []\n",
        "for x_batch, y_batch in test_loader:\n",
        "  benign_samples.append(x_batch[y_batch.squeeze() == 0])\n",
        "\n",
        "ben_x = torch.cat(benign_samples, dim=0).to(device)\n",
        "\n",
        "# Forward pass to get logits for benign samples\n",
        "with torch.no_grad():  # No need for gradients\n",
        "    outputs = model_AT_rFGSM(ben_x.to(torch.float32))\n",
        "\n",
        "# Calculate softmax probabilities\n",
        "probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "# Sort indices based on probabilities of class 1 (assuming class 1 is the \"positive\" class)\n",
        "sorted_indices = torch.argsort(probabilities[:, 1], descending=False)\n",
        "\n",
        "# Select the top 1000 high confidence benign samples\n",
        "top_1000_high_confidence_benign_samples = ben_x[sorted_indices[:5000]]\n",
        "\n",
        "# Set to track indices of samples to be removed\n",
        "removed_set = set()\n",
        "\n",
        "# Step 1: Identify similar samples\n",
        "for i in range(len(top_1000_high_confidence_benign_samples)):\n",
        "    if i in removed_set:\n",
        "        continue\n",
        "    sample1 = top_1000_high_confidence_benign_samples[i]\n",
        "    for j in range(i + 1, len(top_1000_high_confidence_benign_samples)):\n",
        "        if j in removed_set:\n",
        "            continue\n",
        "        sample2 = top_1000_high_confidence_benign_samples[j]\n",
        "        if torch.abs(sample1 - sample2).sum().item() < 10.1:\n",
        "            removed_set.add(j)\n",
        "\n",
        "# Step 2: Select samples that are not in removed_set\n",
        "selected_samples = [sample for idx, sample in enumerate(top_1000_high_confidence_benign_samples) if idx not in removed_set]\n",
        "\n",
        "# Step 3: Stack tensors along a new dimension\n",
        "selected_benigns = torch.stack(selected_samples)\n",
        "\n",
        "print(selected_benigns.shape)\n",
        "del benign_samples, outputs, probabilities, top_1000_high_confidence_benign_samples, selected_samples   # Free up memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkhPb4YakW_7",
        "outputId": "ff9aa6e1-ffdd-47df-ad9f-3634e6575205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([376, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_447_high_confidence = ben_x[sorted_indices[:479]]\n",
        "bens = ben_x[:479]"
      ],
      "metadata": {
        "id": "PHHOwisFl7Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_50_high_confidence = ben_x[sorted_indices[:50]]"
      ],
      "metadata": {
        "id": "Sj7QGiLIztDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_AT_rFGSM(ben_x[sorted_indices[:5]].to(torch.float32))"
      ],
      "metadata": {
        "id": "FAuqOEgCb0u-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16cdd3ac-2bd9-4a1e-db95-be882f300cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 20.1652, -18.1917],\n",
              "        [ 16.0224, -12.1752],\n",
              "        [ 16.3716, -12.1128],\n",
              "        [ 19.6965, -16.6035],\n",
              "        [ 23.4934, -12.6843]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-select benign samples\n",
        "mal_samples = []\n",
        "for x_batch, y_batch in test_loader:\n",
        "  mal_samples.append(x_batch[y_batch.squeeze() == 1])\n",
        "\n",
        "mal_x = torch.cat(mal_samples, dim=0).to(device)\n",
        "\n",
        "print(mal_x.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlr6kEUmoaUp",
        "outputId": "5be94fde-4b5e-4071-f726-cdadaebbe7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1130, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "centers = [bens,top_50_high_confidence,top_447_high_confidence,selected_benigns]\n",
        "titles = ['random benigns','top_50_high_confidence', 'Top 447 High Confidence', 'Selected Benigns']\n",
        "\n",
        "# Prepare for subplots\n",
        "fig, axes = plt.subplots(len(centers), 1, figsize=(10, 15))\n",
        "\n",
        "for i, center in enumerate(centers):\n",
        "    mins = []\n",
        "    for mal in mal_x:\n",
        "        min_diff = 1000\n",
        "        for sample in center:\n",
        "            dif = (torch.abs(sample - mal).sum()).item()\n",
        "            if dif < min_diff:\n",
        "                min_diff = dif\n",
        "        mins.append(min_diff)\n",
        "\n",
        "    mins = np.array(mins)\n",
        "\n",
        "    print(f'{titles[i]} - Min: {np.min(mins)}')\n",
        "    print(f'{titles[i]} - Max: {np.max(mins)}')\n",
        "    print(f'{titles[i]} - Mean: {np.mean(mins)}')\n",
        "\n",
        "    # Plot histogram in the corresponding subplot\n",
        "    axes[i].hist(mins, bins=120, edgecolor='black')\n",
        "    axes[i].set_title(f'Distribution of Values - {titles[i]}')\n",
        "    axes[i].set_xlabel('Value')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show all plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HJRNzLqqtc9o",
        "outputId": "c3f24a3f-3418-44eb-8ed3-0c7f92b9288e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random benigns - Min: 0\n",
            "random benigns - Max: 129\n",
            "random benigns - Mean: 26.45840707964602\n",
            "top_50_high_confidence - Min: 0\n",
            "top_50_high_confidence - Max: 140\n",
            "top_50_high_confidence - Mean: 32.190265486725664\n",
            "Top 447 High Confidence - Min: 0\n",
            "Top 447 High Confidence - Max: 135\n",
            "Top 447 High Confidence - Mean: 28.926548672566373\n",
            "Selected Benigns - Min: 2\n",
            "Selected Benigns - Max: 126\n",
            "Selected Benigns - Mean: 27.593805309734513\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PGD_min VS RBF"
      ],
      "metadata": {
        "id": "No0B0LWHxde5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_AT_rFGSM(ben_x[sorted_indices[-5:]].to(torch.float32))"
      ],
      "metadata": {
        "id": "hVCkiKb7xiqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbca427-c418-4661-c7f9-e43e9321669f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-12.9775,  17.0473],\n",
              "        [-39.4226,  49.3918],\n",
              "        [ -5.6200,   6.6027],\n",
              "        [-48.5828,  53.0153],\n",
              "        [-86.9643,  90.6685]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_min(x, y, model, insertion_array, removal_array, k=25, step_length=0.02, norm='linf', initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack (loss based on goal's class, which we have to minimize the loss).\n",
        "    :param y: Ground truth labels\n",
        "    :param x: Feature vector\n",
        "    :param model: Neural network model\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf' or 'l2')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compute natural loss\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), torch.zeros_like(y.view(-1).long()))\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = x.clone()\n",
        "    x_next = get_x0(x_next, initial_rounding_threshold, is_sample)\n",
        "\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "\n",
        "    # Multi-step PGD\n",
        "    for t in range(k):\n",
        "        #print('*********** ',t)\n",
        "        # Forward pass\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        y_model = model(x_var)\n",
        "        loss = criterion(y_model, torch.zeros_like(y.view(-1).long()))\n",
        "        #print('loss_mal : ',criterion(y_model, torch.zeros_like(y.view(-1).long())))\n",
        "        #print('loss : ',criterion(y_model, y.view(-1).long()))\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        # Norm\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "            #print(torch.abs(perturbation).sum())\n",
        "            #print('torch.abs(perturbation).sum(dim=-1) : ',torch.abs(perturbation).sum(dim=-1))\n",
        "\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.linalg.norm(gradients, dim=-1, keepdim=True)\n",
        "            #print('l2norm ; ',l2norm)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "\n",
        "        elif norm == 'l1':\n",
        "            # consider just features of a sample which are not updated yet(because our update is 0to1 or 1to0 not stepwise)\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1' or 'l2' or 'linf' norm.\")\n",
        "\n",
        "        # Update x_next\n",
        "        #print(torch.abs(x_next - torch.clamp(x_next + perturbation * step_length, min=0., max=1.)).sum())\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    # Rounding step\n",
        "    if random:\n",
        "       round_threshold = torch.rand(x_next.size())\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    # Feasible projection\n",
        "    #x_next = or_float_tensors(x_next, x)\n",
        "\n",
        "    # Compute adversarial loss\n",
        "    loss_adv = criterion(model(x_next), torch.zeros_like(y.view(-1).long())).data\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        done = get_done(x_next, y, model)\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size()[0] * 100:.3f}%.\")\n",
        "\n",
        "    # Replace with natural if adversarial loss is higher\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next"
      ],
      "metadata": {
        "id": "frsaxe7U8t7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_done(x, y, model):\n",
        "    # Get the model's predictions\n",
        "    outputs = model(x)\n",
        "\n",
        "    # Use argmax to get the predicted class indices\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Ensure y is in the same shape as predicted for comparison\n",
        "    y = y.view_as(predicted)\n",
        "\n",
        "    # Determine if the predictions are incorrect\n",
        "    done = (predicted != y).bool()\n",
        "\n",
        "    return done\n",
        "\n",
        "def get_loss_rbf(adv_x: torch.Tensor, y: torch.Tensor, model: nn.Module, RBFModel: nn.Module, penalty_factor: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute CE loss\n",
        "    outputs = model(adv_x)\n",
        "    ce = criterion(outputs, y)\n",
        "    #print('ce ',ce)\n",
        "\n",
        "    # Compute KDE loss\n",
        "    outputs_rbf = RBFModel(adv_x)\n",
        "    kde = criterion(outputs_rbf, y)\n",
        "    #print('kde ',kde)\n",
        "\n",
        "    # Combine the losses with the penalty factor\n",
        "    loss = ce + penalty_factor * kde\n",
        "    #print('loss ',loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "def rbf(x, y, model, RBFModel, insertion_array, removal_array, k=25, step_length=0.02, norm='linf',\n",
        "        initial_rounding_threshold=0.5, round_threshold=0.5, random=False, is_report_loss_diff=True, is_sample=False):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent (PGD) adversarial attack.\n",
        "\n",
        "    :param x: Feature vector\n",
        "    :param y: Ground truth labels\n",
        "    :param model: Neural network model\n",
        "    :param RBFModel: Gaussian model for KDE\n",
        "    :param insertion_array: Array for insertion operations\n",
        "    :param removal_array: Array for removal operations\n",
        "    :param k: Number of steps\n",
        "    :param step_length: Step size for each iteration\n",
        "    :param norm: Norm used for perturbation ('linf', 'l2', 'l1')\n",
        "    :param initial_rounding_threshold: Threshold parameter for rounding the initial x_next\n",
        "    :param round_threshold: Threshold parameter for rounding\n",
        "    :param random: Flag to generate random thresholds\n",
        "    :param is_report_loss_diff: Flag to report loss difference\n",
        "    :param is_sample: Flag to sample randomly from the feasible area\n",
        "    :return: The adversarial version of x (tensor)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    #target's class\n",
        "    traget_labels = torch.zeros_like(y.view(-1).long())\n",
        "\n",
        "    # Compute natural loss and penalty_factor\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_natural = criterion(model(x), traget_labels)\n",
        "    kde = criterion(RBFModel(x), traget_labels)\n",
        "    #penalty_factor = 0.\n",
        "    penalty_factor = (loss_natural / kde).detach()\n",
        "\n",
        "    # Initialize starting point\n",
        "    x_next = get_x0(x.clone(), initial_rounding_threshold, is_sample)\n",
        "\n",
        "    # Expand insertion_array and removal_array to match the batch size\n",
        "    expanded_insertion_array = insertion_array.expand(x.shape[0], -1)\n",
        "    expanded_removal_array = removal_array.expand(x.shape[0], -1)\n",
        "\n",
        "    # Update insertion and removal arrays based on input x\n",
        "    insertion_array_updated = torch.bitwise_or(expanded_insertion_array, x.to(torch.uint8))\n",
        "    removal_array_updated = torch.bitwise_or(expanded_removal_array, 1 - x.to(torch.uint8))\n",
        "\n",
        "    for t in range(k):\n",
        "        x_var = x_next.clone().detach().requires_grad_(True)\n",
        "        decayed_penalty_factor = penalty_factor * (1 - t / k)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = get_loss_rbf(x_var, traget_labels, model, RBFModel, decayed_penalty_factor)\n",
        "\n",
        "        # Compute gradient\n",
        "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
        "        gradients = -(grad_vars[0].data)\n",
        "        #print('torch.abs(gradients).sum() : ',torch.abs(gradients).sum(dim=-1))\n",
        "\n",
        "        pos_insertion = (x_var <= 0.999) * 1 * insertion_array_updated\n",
        "        #pos_insertion = (x_var <= 0.999) * 1\n",
        "        grad4insertion = (gradients >= 0) * pos_insertion * gradients\n",
        "\n",
        "        pos_removal = (x_var > 0.001) * 1 * removal_array_updated\n",
        "        grad4removal = (gradients < 0) * pos_removal * gradients\n",
        "\n",
        "        gradients = grad4removal + grad4insertion\n",
        "\n",
        "        if norm == 'linf':\n",
        "            perturbation = torch.sign(gradients).float()\n",
        "        elif norm == 'l2':\n",
        "            l2norm = torch.norm(gradients, dim=-1, keepdim=True)\n",
        "            perturbation = (gradients / (l2norm + 1e-20)).float()\n",
        "        elif norm == 'l1':\n",
        "            un_mod = torch.abs(x - x_var) <= 1e-6\n",
        "            gradients = gradients * un_mod\n",
        "            max_grad = torch.topk(torch.abs(gradients).view(gradients.size(0), -1), 1, dim=-1)[0]\n",
        "            #print('max_grad ',max_grad)\n",
        "            perturbation = (torch.abs(gradients) >= max_grad).float() * torch.sign(gradients).float()\n",
        "            done = get_done(x_next, y, model)\n",
        "            if torch.all(done):\n",
        "                break\n",
        "            perturbation[done] = 0.\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Expect 'l1', 'l2', or 'linf' norm.\")\n",
        "\n",
        "        x_next = torch.clamp(x_next + perturbation * step_length, min=0., max=1.)\n",
        "\n",
        "    if random:\n",
        "        round_threshold = torch.rand_like(x_next)\n",
        "    x_next = round_x(x_next, round_threshold=round_threshold)\n",
        "\n",
        "    outputs = model(x_next)\n",
        "    loss_adv = criterion(outputs, traget_labels).data\n",
        "    done = get_done(x_next, y, model)\n",
        "\n",
        "    if is_report_loss_diff:\n",
        "        print(f\"PGD {norm}: Attack effectiveness {done.sum().item() / x.size(0) * 100:.3f}%.\")\n",
        "\n",
        "    replace_flag = (loss_adv > loss_natural).squeeze()\n",
        "    x_next[replace_flag] = x[replace_flag]\n",
        "\n",
        "    return x_next\n"
      ],
      "metadata": {
        "id": "A33nGhZDzOF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = 0\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUH6bf-X4im3",
        "outputId": "181db644-6de1-4a17-b701-8a43a2cd0fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 46.154%.\n",
            "PGD l1: Attack effectiveness 77.143%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 70.588%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 85.000%.\n",
            "PGD l1: Attack effectiveness 61.765%.\n",
            "PGD l1: Attack effectiveness 55.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 64.516%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 76.667%.\n",
            "PGD l1: Attack effectiveness 45.455%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 63.636%.\n",
            "PGD l1: Attack effectiveness 61.538%.\n",
            "PGD l1: Attack effectiveness 82.143%.\n",
            "PGD l1: Attack effectiveness 70.270%.\n",
            "PGD l1: Attack effectiveness 56.250%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 72.222%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 78.947%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl4c1lyu0h3H",
        "outputId": "fac3d9f9-32f5-4ce9-bf26-8f0e72791e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 46.154%.\n",
            "PGD l1: Attack effectiveness 77.143%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 70.588%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 85.000%.\n",
            "PGD l1: Attack effectiveness 61.765%.\n",
            "PGD l1: Attack effectiveness 55.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 64.516%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 76.667%.\n",
            "PGD l1: Attack effectiveness 45.455%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 63.636%.\n",
            "PGD l1: Attack effectiveness 61.538%.\n",
            "PGD l1: Attack effectiveness 82.143%.\n",
            "PGD l1: Attack effectiveness 70.270%.\n",
            "PGD l1: Attack effectiveness 56.250%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 72.222%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 78.947%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.88%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzZWr19C-SYp",
        "outputId": "ce8faf41-ec53-4bd3-b02e-188a19f01316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 53.846%.\n",
            "PGD l1: Attack effectiveness 82.857%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 70.588%.\n",
            "PGD l1: Attack effectiveness 63.636%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 75.862%.\n",
            "PGD l1: Attack effectiveness 90.000%.\n",
            "PGD l1: Attack effectiveness 64.706%.\n",
            "PGD l1: Attack effectiveness 75.000%.\n",
            "PGD l1: Attack effectiveness 77.778%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 70.968%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 83.333%.\n",
            "PGD l1: Attack effectiveness 50.000%.\n",
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 75.000%.\n",
            "PGD l1: Attack effectiveness 65.385%.\n",
            "PGD l1: Attack effectiveness 85.714%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 78.125%.\n",
            "PGD l1: Attack effectiveness 83.333%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 81.579%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.84%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSMxrhlx13T2",
        "outputId": "247a1367-9cb9-4e3c-ed76-86f6385db1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l2: Attack effectiveness 72.414%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 53.846%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 68.000%.\n",
            "PGD l2: Attack effectiveness 76.471%.\n",
            "PGD l2: Attack effectiveness 72.727%.\n",
            "PGD l2: Attack effectiveness 71.429%.\n",
            "PGD l2: Attack effectiveness 68.966%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 64.706%.\n",
            "PGD l2: Attack effectiveness 65.000%.\n",
            "PGD l2: Attack effectiveness 81.481%.\n",
            "PGD l2: Attack effectiveness 68.000%.\n",
            "PGD l2: Attack effectiveness 83.871%.\n",
            "PGD l2: Attack effectiveness 70.000%.\n",
            "PGD l2: Attack effectiveness 92.000%.\n",
            "PGD l2: Attack effectiveness 77.419%.\n",
            "PGD l2: Attack effectiveness 77.778%.\n",
            "PGD l2: Attack effectiveness 80.000%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 50.000%.\n",
            "PGD l2: Attack effectiveness 68.966%.\n",
            "PGD l2: Attack effectiveness 72.727%.\n",
            "PGD l2: Attack effectiveness 65.385%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 75.676%.\n",
            "PGD l2: Attack effectiveness 62.500%.\n",
            "PGD l2: Attack effectiveness 75.000%.\n",
            "PGD l2: Attack effectiveness 80.556%.\n",
            "PGD l2: Attack effectiveness 72.973%.\n",
            "PGD l2: Attack effectiveness 81.579%.\n",
            "PGD l2: Attack effectiveness 74.194%.\n",
            "PGD l2: Attack effectiveness 78.571%.\n",
            "PGD l2: Attack effectiveness 82.143%.\n",
            "PGD l2: Attack effectiveness 82.353%.\n",
            "PGD l2: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 25.4%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.5, 'norm': 'l2', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czJU9XFM-2mv",
        "outputId": "2cc001dc-f491-414e-d263-6b5cae5d1cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l2: Attack effectiveness 72.414%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 53.846%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 68.000%.\n",
            "PGD l2: Attack effectiveness 79.412%.\n",
            "PGD l2: Attack effectiveness 66.667%.\n",
            "PGD l2: Attack effectiveness 76.190%.\n",
            "PGD l2: Attack effectiveness 68.966%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 64.706%.\n",
            "PGD l2: Attack effectiveness 70.000%.\n",
            "PGD l2: Attack effectiveness 85.185%.\n",
            "PGD l2: Attack effectiveness 72.000%.\n",
            "PGD l2: Attack effectiveness 83.871%.\n",
            "PGD l2: Attack effectiveness 73.333%.\n",
            "PGD l2: Attack effectiveness 92.000%.\n",
            "PGD l2: Attack effectiveness 77.419%.\n",
            "PGD l2: Attack effectiveness 77.778%.\n",
            "PGD l2: Attack effectiveness 83.333%.\n",
            "PGD l2: Attack effectiveness 90.000%.\n",
            "PGD l2: Attack effectiveness 50.000%.\n",
            "PGD l2: Attack effectiveness 72.414%.\n",
            "PGD l2: Attack effectiveness 75.000%.\n",
            "PGD l2: Attack effectiveness 65.385%.\n",
            "PGD l2: Attack effectiveness 85.714%.\n",
            "PGD l2: Attack effectiveness 72.973%.\n",
            "PGD l2: Attack effectiveness 62.500%.\n",
            "PGD l2: Attack effectiveness 71.875%.\n",
            "PGD l2: Attack effectiveness 83.333%.\n",
            "PGD l2: Attack effectiveness 72.973%.\n",
            "PGD l2: Attack effectiveness 81.579%.\n",
            "PGD l2: Attack effectiveness 77.419%.\n",
            "PGD l2: Attack effectiveness 78.571%.\n",
            "PGD l2: Attack effectiveness 82.143%.\n",
            "PGD l2: Attack effectiveness 82.353%.\n",
            "PGD l2: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 24.69%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCx0_k-sDjcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8N1KkEy19rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, pgd_min, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hLhT6VX1_V1",
        "outputId": "2c79f354-895a-4e6b-88fd-0a6250b80b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 64.000%.\n",
            "PGD linf: Attack effectiveness 38.462%.\n",
            "PGD linf: Attack effectiveness 74.286%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 70.588%.\n",
            "PGD linf: Attack effectiveness 60.606%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 62.069%.\n",
            "PGD linf: Attack effectiveness 85.000%.\n",
            "PGD linf: Attack effectiveness 55.882%.\n",
            "PGD linf: Attack effectiveness 45.000%.\n",
            "PGD linf: Attack effectiveness 74.074%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 61.290%.\n",
            "PGD linf: Attack effectiveness 63.333%.\n",
            "PGD linf: Attack effectiveness 76.000%.\n",
            "PGD linf: Attack effectiveness 67.742%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 40.909%.\n",
            "PGD linf: Attack effectiveness 58.621%.\n",
            "PGD linf: Attack effectiveness 61.364%.\n",
            "PGD linf: Attack effectiveness 61.538%.\n",
            "PGD linf: Attack effectiveness 71.429%.\n",
            "PGD linf: Attack effectiveness 67.568%.\n",
            "PGD linf: Attack effectiveness 43.750%.\n",
            "PGD linf: Attack effectiveness 59.375%.\n",
            "PGD linf: Attack effectiveness 69.444%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 65.789%.\n",
            "PGD linf: Attack effectiveness 64.516%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 60.714%.\n",
            "PGD linf: Attack effectiveness 79.412%.\n",
            "PGD linf: Attack effectiveness 40.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 35.58%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 0.02, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4916de19-0231-48bf-a16b-77420f16b486",
        "id": "d5BJy7BG1_WE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 68.966%.\n",
            "PGD linf: Attack effectiveness 52.000%.\n",
            "PGD linf: Attack effectiveness 38.462%.\n",
            "PGD linf: Attack effectiveness 65.714%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 48.000%.\n",
            "PGD linf: Attack effectiveness 61.765%.\n",
            "PGD linf: Attack effectiveness 57.576%.\n",
            "PGD linf: Attack effectiveness 54.762%.\n",
            "PGD linf: Attack effectiveness 62.069%.\n",
            "PGD linf: Attack effectiveness 70.000%.\n",
            "PGD linf: Attack effectiveness 55.882%.\n",
            "PGD linf: Attack effectiveness 35.000%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 48.000%.\n",
            "PGD linf: Attack effectiveness 58.065%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 54.839%.\n",
            "PGD linf: Attack effectiveness 62.963%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 40.909%.\n",
            "PGD linf: Attack effectiveness 51.724%.\n",
            "PGD linf: Attack effectiveness 54.545%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 71.429%.\n",
            "PGD linf: Attack effectiveness 59.459%.\n",
            "PGD linf: Attack effectiveness 37.500%.\n",
            "PGD linf: Attack effectiveness 53.125%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 54.054%.\n",
            "PGD linf: Attack effectiveness 63.158%.\n",
            "PGD linf: Attack effectiveness 61.290%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 57.143%.\n",
            "PGD linf: Attack effectiveness 79.412%.\n",
            "PGD linf: Attack effectiveness 33.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 41.59%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 200, 'step_length': 0.01, 'norm': 'linf', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBDplVFa2Xzi",
        "outputId": "396cfc92-d78a-4f8f-c754-f6fe45a60494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD linf: Attack effectiveness 72.414%.\n",
            "PGD linf: Attack effectiveness 72.000%.\n",
            "PGD linf: Attack effectiveness 46.154%.\n",
            "PGD linf: Attack effectiveness 80.000%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 60.000%.\n",
            "PGD linf: Attack effectiveness 70.588%.\n",
            "PGD linf: Attack effectiveness 60.606%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 62.069%.\n",
            "PGD linf: Attack effectiveness 85.000%.\n",
            "PGD linf: Attack effectiveness 58.824%.\n",
            "PGD linf: Attack effectiveness 40.000%.\n",
            "PGD linf: Attack effectiveness 81.481%.\n",
            "PGD linf: Attack effectiveness 68.000%.\n",
            "PGD linf: Attack effectiveness 67.742%.\n",
            "PGD linf: Attack effectiveness 66.667%.\n",
            "PGD linf: Attack effectiveness 84.000%.\n",
            "PGD linf: Attack effectiveness 64.516%.\n",
            "PGD linf: Attack effectiveness 70.370%.\n",
            "PGD linf: Attack effectiveness 73.333%.\n",
            "PGD linf: Attack effectiveness 76.667%.\n",
            "PGD linf: Attack effectiveness 45.455%.\n",
            "PGD linf: Attack effectiveness 62.069%.\n",
            "PGD linf: Attack effectiveness 68.182%.\n",
            "PGD linf: Attack effectiveness 57.692%.\n",
            "PGD linf: Attack effectiveness 71.429%.\n",
            "PGD linf: Attack effectiveness 67.568%.\n",
            "PGD linf: Attack effectiveness 50.000%.\n",
            "PGD linf: Attack effectiveness 62.500%.\n",
            "PGD linf: Attack effectiveness 72.222%.\n",
            "PGD linf: Attack effectiveness 72.973%.\n",
            "PGD linf: Attack effectiveness 68.421%.\n",
            "PGD linf: Attack effectiveness 67.742%.\n",
            "PGD linf: Attack effectiveness 75.000%.\n",
            "PGD linf: Attack effectiveness 67.857%.\n",
            "PGD linf: Attack effectiveness 79.412%.\n",
            "PGD linf: Attack effectiveness 40.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 32.92%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()/2.\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkn64rNd--PM",
        "outputId": "243e5f6f-7f90-42b3-dc60-84516f1e8a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 46.154%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 90.000%.\n",
            "PGD l1: Attack effectiveness 61.765%.\n",
            "PGD l1: Attack effectiveness 55.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 64.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 70.968%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 73.333%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 50.000%.\n",
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 68.182%.\n",
            "PGD l1: Attack effectiveness 65.385%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 56.250%.\n",
            "PGD l1: Attack effectiveness 68.750%.\n",
            "PGD l1: Attack effectiveness 75.000%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 73.684%.\n",
            "PGD l1: Attack effectiveness 70.968%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.03%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()  fixed\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybfmw4np_24q",
        "outputId": "451f0adc-3c4f-456f-ef60-f660a24ed5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 42.308%.\n",
            "PGD l1: Attack effectiveness 82.857%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 76.471%.\n",
            "PGD l1: Attack effectiveness 54.545%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 90.000%.\n",
            "PGD l1: Attack effectiveness 58.824%.\n",
            "PGD l1: Attack effectiveness 65.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 64.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 67.742%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 45.455%.\n",
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 68.182%.\n",
            "PGD l1: Attack effectiveness 65.385%.\n",
            "PGD l1: Attack effectiveness 85.714%.\n",
            "PGD l1: Attack effectiveness 67.568%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 59.375%.\n",
            "PGD l1: Attack effectiveness 77.778%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 73.684%.\n",
            "PGD l1: Attack effectiveness 67.742%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 64.286%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 53.333%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 30.09%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()  decrease by /2\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN_8pD9wALwe",
        "outputId": "8bb71b0f-2eb4-4033-8df1-ce871ba9b326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 50.000%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 76.471%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 69.048%.\n",
            "PGD l1: Attack effectiveness 75.862%.\n",
            "PGD l1: Attack effectiveness 90.000%.\n",
            "PGD l1: Attack effectiveness 64.706%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 80.645%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 67.742%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 73.333%.\n",
            "PGD l1: Attack effectiveness 73.333%.\n",
            "PGD l1: Attack effectiveness 50.000%.\n",
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 65.909%.\n",
            "PGD l1: Attack effectiveness 57.692%.\n",
            "PGD l1: Attack effectiveness 82.143%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 62.500%.\n",
            "PGD l1: Attack effectiveness 71.875%.\n",
            "PGD l1: Attack effectiveness 75.000%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 78.947%.\n",
            "PGD l1: Attack effectiveness 70.968%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 28.32%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()  penalty_factor /(2**(t-1))\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 50, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1037d2-c97f-4414-accc-c0f0f20fb11f",
        "id": "EciUXJgpBhG5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 53.846%.\n",
            "PGD l1: Attack effectiveness 74.286%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 76.471%.\n",
            "PGD l1: Attack effectiveness 60.606%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 75.862%.\n",
            "PGD l1: Attack effectiveness 85.000%.\n",
            "PGD l1: Attack effectiveness 64.706%.\n",
            "PGD l1: Attack effectiveness 55.000%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 63.333%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 67.742%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 73.333%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 50.000%.\n",
            "PGD l1: Attack effectiveness 65.517%.\n",
            "PGD l1: Attack effectiveness 65.909%.\n",
            "PGD l1: Attack effectiveness 61.538%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 59.375%.\n",
            "PGD l1: Attack effectiveness 71.875%.\n",
            "PGD l1: Attack effectiveness 75.000%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 76.316%.\n",
            "PGD l1: Attack effectiveness 70.968%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 71.429%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 29.29%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when penalty_factor = (loss_natural / kde).detach()\n",
        "attack_params =  {'RBFModel':model_gaussian_1000,'insertion_array':insertion_array, 'removal_array':removal_array,'k': 100, 'step_length': 1., 'norm': 'l1', 'is_report_loss_diff' : True}\n",
        "adv_predict(test_loader, model_AT_rFGSM, rbf, device, **attack_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhvM7ueGCpqR",
        "outputId": "d9edecc2-810d-4eed-eac6-65859a74c32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD l1: Attack effectiveness 68.966%.\n",
            "PGD l1: Attack effectiveness 72.000%.\n",
            "PGD l1: Attack effectiveness 46.154%.\n",
            "PGD l1: Attack effectiveness 80.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 76.000%.\n",
            "PGD l1: Attack effectiveness 67.647%.\n",
            "PGD l1: Attack effectiveness 63.636%.\n",
            "PGD l1: Attack effectiveness 66.667%.\n",
            "PGD l1: Attack effectiveness 75.862%.\n",
            "PGD l1: Attack effectiveness 95.000%.\n",
            "PGD l1: Attack effectiveness 64.706%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "PGD l1: Attack effectiveness 85.185%.\n",
            "PGD l1: Attack effectiveness 68.000%.\n",
            "PGD l1: Attack effectiveness 77.419%.\n",
            "PGD l1: Attack effectiveness 70.000%.\n",
            "PGD l1: Attack effectiveness 88.000%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 74.074%.\n",
            "PGD l1: Attack effectiveness 83.333%.\n",
            "PGD l1: Attack effectiveness 90.000%.\n",
            "PGD l1: Attack effectiveness 54.545%.\n",
            "PGD l1: Attack effectiveness 72.414%.\n",
            "PGD l1: Attack effectiveness 70.455%.\n",
            "PGD l1: Attack effectiveness 65.385%.\n",
            "PGD l1: Attack effectiveness 85.714%.\n",
            "PGD l1: Attack effectiveness 72.973%.\n",
            "PGD l1: Attack effectiveness 59.375%.\n",
            "PGD l1: Attack effectiveness 78.125%.\n",
            "PGD l1: Attack effectiveness 77.778%.\n",
            "PGD l1: Attack effectiveness 75.676%.\n",
            "PGD l1: Attack effectiveness 84.211%.\n",
            "PGD l1: Attack effectiveness 74.194%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 78.571%.\n",
            "PGD l1: Attack effectiveness 79.412%.\n",
            "PGD l1: Attack effectiveness 60.000%.\n",
            "Accuracy of just malwares (without attack): 94.69% | Under attack: 26.37%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vRkMRmST4QDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}